"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[4243],{2195:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>c,contentTitle:()=>l,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>h});const s=JSON.parse('{"id":"chapters/03/index","title":"Strategies","description":"This chapter tries to lay out the big picture of AI safety strategy to mitigate the risks explored in the previous chapter.","source":"@site/docs/chapters/03/index.md","sourceDirName":"chapters/03","slug":"/chapters/03/","permalink":"/aisafety_atlas_multilingual_website/chapters/03/","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/03/index.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Strategies","chapter_number":3,"reading_time_core":"79 min","reading_time_optional":"35 min","reading_time_appendix":"16 min","authors":["Charbel-Rapha\xebl Segerie","Markov Grey"],"affiliations":["French Center for AI Safety (CeSIA)"],"acknowledgements":["Jeanne Salle","Charles Martinet","Lukas Gebhard","Amaury Lorin","Alejandro Acelas","Flawn","Emily Fan","Kieron Kretschmar","Sebastian Gil","Evander Hammer","Sophia Wesenberg","Jessica Wen","Niharika Chaubey","Ang\xe9lina Gentaz","Jonathan Claybrough","Camille Berger","Josh Thorsteinson"],"alignment_forum_link":"https://www.lesswrong.com/s/3ni2P2GZzBvNebWYZ/p/RzsXRbk2ETNqjhsma","google_docs_link":"https://docs.google.com/document/d/1ytzVlrj8PpxiyjvmZCJXm5QW3olhTh504-yH0h-wAq0/edit?usp=sharing","feedback_link":"https://forms.gle/ZsA4hEWUx1ZrtQLL9","teach_link":"https://docs.google.com/document/d/1im_i6e9xEAe-koYlurYdn26n9h7pFX2HksnRfQmWxTQ/edit?usp=sharing","sidebar_position":3,"slug":"/chapters/03/"},"sidebar":"docs","previous":{"title":"2.9 Appendix: Forecasting Scenarios","permalink":"/aisafety_atlas_multilingual_website/chapters/02/09"},"next":{"title":"3.1 Definitions","permalink":"/aisafety_atlas_multilingual_website/chapters/03/01"}}');var a=i(4848),n=i(8453),r=(i(2482),i(8559),i(1966),i(2501));const o={title:"Strategies",chapter_number:3,reading_time_core:"79 min",reading_time_optional:"35 min",reading_time_appendix:"16 min",authors:["Charbel-Rapha\xebl Segerie","Markov Grey"],affiliations:["French Center for AI Safety (CeSIA)"],acknowledgements:["Jeanne Salle","Charles Martinet","Lukas Gebhard","Amaury Lorin","Alejandro Acelas","Flawn","Emily Fan","Kieron Kretschmar","Sebastian Gil","Evander Hammer","Sophia Wesenberg","Jessica Wen","Niharika Chaubey","Ang\xe9lina Gentaz","Jonathan Claybrough","Camille Berger","Josh Thorsteinson"],alignment_forum_link:"https://www.lesswrong.com/s/3ni2P2GZzBvNebWYZ/p/RzsXRbk2ETNqjhsma",google_docs_link:"https://docs.google.com/document/d/1ytzVlrj8PpxiyjvmZCJXm5QW3olhTh504-yH0h-wAq0/edit?usp=sharing",feedback_link:"https://forms.gle/ZsA4hEWUx1ZrtQLL9",teach_link:"https://docs.google.com/document/d/1im_i6e9xEAe-koYlurYdn26n9h7pFX2HksnRfQmWxTQ/edit?usp=sharing",sidebar_position:3,slug:"/chapters/03/"},l="Introduction",c={},h=[];function d(e){const t={a:"a",h1:"h1",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,n.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"introduction",children:"Introduction"})}),"\n",(0,a.jsx)(t.p,{children:"This chapter tries to lay out the big picture of AI safety strategy to mitigate the risks explored in the previous chapter."}),"\n",(0,a.jsx)(t.p,{children:"As AI capabilities continue their rapid advance, the strategies designed to ensure safety must also evolve, this is why the first version of this document has been written in July 2024, and has been updated in late May 2025. We talk about technical approaches, and try to articulate this chapter with the other chapters of the book. The aim is to provide a structured overview of current thinking and ongoing work in AI safety strategy, acknowledging both established methods and emerging research directions. For each type of macro problem like misuses, AGI alignment, we list different macro strategies that can help mitigate those risks. Those strategies can generally be combined, and should be combined! We discuss the sequencing of the different strategies at the end of the chapter."}),"\n",(0,a.jsx)(r.A,{src:"./img/TtT_Image_1.png",alt:"Enter image alt description",number:"1",label:"3.1",caption:"Tentative diagram summarizing the main high-level approaches to make AI development safe."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Beyond the scope of this chapter"})}),"\n",(0,a.jsx)(t.p,{children:"While this chapter focuses on strategies directly related to preventing large-scale negative outcomes from AI misuse, misalignment, or uncontrolled development, several related topics are necessarily placed beyond its primary scope:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"AI-generated misinformation:"})," The proliferation of AI-driven misinformation, including deepfakes and biased content generation. Strategies to combat this, such as robust detection systems, watermarking, and responsible AI principles, are mostly beyond the score of the chapter. These often fall under the umbrella of content moderation, media literacy, and platform governance, distinct from the core technical alignment and control strategies discussed in this chapter."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Privacy:"})," AI systems often process vast amounts of data, amplifying existing concerns about data privacy."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Security:"})," Standard security practices like encryption, access control, data classification, threat monitoring, and anonymization are prerequisites for safe AI deployment. Although robust security is vital for measures like protecting model weights, these standard practices are distinct from the novel safety strategies needed to address risks like model misalignment or capability misuse."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Discrimination and toxicity:"})," While biased or toxic outputs constitute a safety concern, this chapter concentrates on strategies aimed at preventing catastrophic failures."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Digital mind welfare and rights:"})," We don\u2019t know if AIs should be considered as moral patient. This is a distinct ethical domain concerning our obligations to AI, rather than ensuring safety from AI."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Errors due to lack of capability:"})," While AI system failures due to lack of capability or capability or robustness are a source of risk (",(0,a.jsx)(t.a,{href:"https://www.aisi.gov.uk/work/aisis-research-direction-for-technical-solutions",children:"AISI, 2025"}),")., the strategies discussed in this chapter are aimed at mitigating risks arising from both insufficient robustness and potentially high (but misaligned or misused) capabilities. And the solutions to this type of risk are the same as for other industries: testing, iteration, and making the system more capable."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"The scope chosen here reflects a common focus within certain parts of the AI safety community on existential or large-scale catastrophic risks arising from powerful, potentially agentic AI systems."})]})}function m(e={}){const{wrapper:t}={...(0,n.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);
"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[6471],{5310:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>c,contentTitle:()=>l,default:()=>p,frontMatter:()=>o,metadata:()=>n,toc:()=>h});const n=JSON.parse('{"id":"chapters/09/4","title":"Automating and Scaling Interpretability","description":"State-of-the-art models now contain hundreds of billions of parameters and thousands of interconnected layers, making manual inspection of model components infeasible. Mechanistic interpretability aims to analyze how individual elements\u2014like attention heads, neurons, features, or entire layers\u2014interact to produce specific behaviors. However, as models scale, manual approaches like activation pathing for circuit discovery, subgraph study, and subsequent explanation generation (Wang et al., 2023), become infeasible to use. This is why developing mechanistic interpretability methods that can scale is essential.","source":"@site/docs/chapters/09/04.md","sourceDirName":"chapters/09","slug":"/chapters/09/04","permalink":"/aisafety_atlas_multilingual_website/chapters/09/04","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/09/04.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"4","title":"Automating and Scaling Interpretability","sidebar_label":"9.4 Automating and Scaling Interpretability","sidebar_position":5,"slug":"/chapters/09/04","reading_time_core":"2 min","reading_time_optional":"1 min","pagination_prev":"chapters/09/3","pagination_next":"chapters/09/5"},"sidebar":"docs","previous":{"title":"9.3 Interventional Methods","permalink":"/aisafety_atlas_multilingual_website/chapters/09/03"},"next":{"title":"9.5 Critiques","permalink":"/aisafety_atlas_multilingual_website/chapters/09/05"}}');var a=i(4848),s=i(8453),r=(i(2482),i(8559));i(1966);const o={id:4,title:"Automating and Scaling Interpretability",sidebar_label:"9.4 Automating and Scaling Interpretability",sidebar_position:5,slug:"/chapters/09/04",reading_time_core:"2 min",reading_time_optional:"1 min",pagination_prev:"chapters/09/3",pagination_next:"chapters/09/5"},l="Automating and Scaling Interpretability",c={},h=[{value:"Automatic Circuit DisCovery (ACDC)",id:"01",level:2}];function d(e){const t={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",...(0,s.R)(),...e.components},{GlossaryTerm:i}=t;return i||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"automating-and-scaling-interpretability",children:"Automating and Scaling Interpretability"})}),"\n",(0,a.jsxs)(t.p,{children:["State-of-the-art models now contain hundreds of billions of parameters and thousands of interconnected layers, making manual inspection of model components infeasible. Mechanistic interpretability aims to analyze how individual elements\u2014like ",(0,a.jsx)(i,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:(0,a.jsx)(i,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:"attention"})})," heads, neurons, features, or entire layers\u2014interact to produce specific behaviors. However, as models scale, manual approaches like activation pathing for circuit discovery, subgraph study, and subsequent explanation generation (",(0,a.jsx)(t.a,{href:"https://openreview.net/forum?id=NpsVSN6o4ul",children:"Wang et al., 2023"}),"), become infeasible to use. This is why developing mechanistic interpretability methods that can scale is essential."]}),"\n",(0,a.jsxs)(t.p,{children:["Research in scalable interpretability, such as Automated Circuit DisCovery (ACDC) attempts to introduce algorithms that automate the process of finding circuits within a ",(0,a.jsx)(i,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:(0,a.jsx)(i,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:"transformer"})}),"."]}),"\n",(0,a.jsx)(t.h2,{id:"01",children:"Automatic Circuit DisCovery (ACDC)"}),"\n",(0,a.jsx)(t.p,{children:"In the Activation Patching section we introduced how interpretability researchers manually discover circuits. Automatic Circuit DisCovery (ACDC) is an algorithm that automates the circuit discovery process and conducts all the activation patching experiments required to identify a circuit."}),"\n",(0,a.jsx)(t.p,{children:"The typical manual circuit discovery workflow can be broken down into three main steps:"}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"Selecting a behavior, a dataset that elicits this behavior, and a metric to measure the model's performance on the behavior,"}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:["Dividing the model into a computational graph: the model is represented as a graph, where nodes correspond to individual components like ",(0,a.jsx)(i,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:(0,a.jsx)(i,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:"attention"})})," heads or MLPs, or more granular units like individual neurons, depending on the granularity of the analysis,"]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"Isolating the relevant circuit: this step is what the ACDC algorithm automates. It involves identifying which components (nodes and edges) in the computational graph are involved in the behavior under study."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(r.A,{title:"The ACDC algorithm",collapsed:!0,children:[(0,a.jsx)(t.p,{children:"The ACDC algorithm is a recursive algorithm that isolates circuits by iterating over the computational graph from outputs to inputs and pruning unnecessary edges. The high-level steps are:"}),(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"Start with the entire computational graph of the model,"}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"We will process the graph starting from the output layer, moving backward to the input,"}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"For each node, try to remove as many edges that enter this node as possible, without reducing the model\u2019s performance on a selected metric (we don\u2019t want the removal to impact the model\u2019s performance on the specified task too much). If the change is minimal (below a set threshold), keep the edge removed."}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"Iterate over all remaining nodes (from the output later to the input layer)"}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"Finally return the simplified subgraph with only the connections needed for the task."}),"\n"]}),"\n"]}),(0,a.jsx)(t.p,{children:"The ACDC algorithm can successfully rediscover circuits found in previous research, such as the IOI or Greater-Than circuits."})]}),"\n",(0,a.jsxs)(t.p,{children:["Recent advancements have introduced methods for automatically discovering sparse feature circuits (",(0,a.jsx)(t.a,{href:"https://arxiv.org/abs/2403.19647",children:"Marks et al., 2024"}),") - circuits made of sparse autoencoder (SAE) features rather than model components. Unlike traditional circuits, which are made of challenging-to-interpret model components like neurons or MLPs, SAE circuits are built from sparse autoencoder (SAE) features, which are directly interpretable."]}),"\n",(0,a.jsx)(t.p,{children:"The authors of this research developed unsupervised techniques to automatically discover thousands of feature circuits, many of which correspond to previously unanticipated model behaviors. This approach opens up new possibilities for interpreting models by focusing on the high-level features that drive behaviors, rather than using more abstract and less interpretable models components."})]})}function p(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);
"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[4438],{7075:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>h,contentTitle:()=>c,default:()=>u,frontMatter:()=>l,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"chapters/03/1","title":"Definitions","description":"Definitions shape strategy selection. How we define problems directly impacts which strategies we pursue in solving that problem. In a new and evolving field like AI safety, clearly defined terms are essential for effective communication and research. Ambiguity leads to miscommunication, hinders collaboration, obscures disagreements, and facilitates safety washing (Ren et al., 2024; Lizka, 2023). The terms we use reflect our assumptions about the nature of the problems we\'re trying to solve and shape the solutions we develop. Terms like \\"alignment\\" and \\"safety\\" are used with varying meanings, reflecting different underlying assumptions about the nature of the problem and the goals of the research. The goal of this section is to explain different perspectives around these words, what exactly specific safety strategies are aiming at, and establish how our text will use them.","source":"@site/docs/chapters/03/01.md","sourceDirName":"chapters/03","slug":"/chapters/03/01","permalink":"/aisafety_atlas_multilingual_website/chapters/03/01","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/03/01.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"1","title":"Definitions","sidebar_label":"3.1 Definitions","sidebar_position":2,"slug":"/chapters/03/01","reading_time_core":"8 min","reading_time_optional":"1 min","pagination_prev":"chapters/03/index","pagination_next":"chapters/03/2"},"sidebar":"docs","previous":{"title":"Strategies","permalink":"/aisafety_atlas_multilingual_website/chapters/03/"},"next":{"title":"3.2 Misuse Prevention Strategies","permalink":"/aisafety_atlas_multilingual_website/chapters/03/02"}}');var i=n(4848),a=n(8453),o=n(4768),r=(n(2482),n(8559),n(1966));const l={id:1,title:"Definitions",sidebar_label:"3.1 Definitions",sidebar_position:2,slug:"/chapters/03/01",reading_time_core:"8 min",reading_time_optional:"1 min",pagination_prev:"chapters/03/index",pagination_next:"chapters/03/2"},c="Definitions",h={},d=[{value:"AI Safety",id:"01",level:2},{value:"AI Alignment",id:"02",level:2},{value:"AI Ethics",id:"03",level:2},{value:"AI Control",id:"04",level:2}];function m(e){const t={a:"a",em:"em",h1:"h1",h2:"h2",header:"header",p:"p",strong:"strong",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"definitions",children:"Definitions"})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Definitions shape strategy selection."})," How we define problems directly impacts which strategies we pursue in solving that problem. In a new and evolving field like AI safety, clearly defined terms are essential for effective communication and research. Ambiguity leads to miscommunication, hinders collaboration, obscures disagreements, and facilitates safety washing (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2407.21792",children:"Ren et al., 2024"}),"; ",(0,i.jsx)(t.a,{href:"https://forum.effectivealtruism.org/posts/f2qojPr8NaMPo2KJC/beware-safety-washing",children:"Lizka, 2023"}),'). The terms we use reflect our assumptions about the nature of the problems we\'re trying to solve and shape the solutions we develop. Terms like "alignment" and "safety" are used with varying meanings, reflecting different underlying assumptions about the nature of the problem and the goals of the research. The goal of this section is to explain different perspectives around these words, what exactly specific safety strategies are aiming at, and establish how our text will use them.']}),"\n",(0,i.jsx)(t.h2,{id:"01",children:"AI Safety"}),"\n",(0,i.jsx)(r.A,{term:"AI safety",source:"",number:"1",label:"3.1",children:(0,i.jsx)(t.p,{children:"Ensuring that AI systems do not inadvertently or deliberately cause harm or danger to humans or the environment, through research that identifies causes of unintended AI behavior and develops tools for safe and reliable operation."})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"AI safety ensures AI systems do not cause harm to humans or the environment."})," It encompasses the broadest range of research and engineering practices focused on preventing harmful outcomes from AI systems. While alignment focuses on things like an AI's goals and intentions, safety addresses a wider spectrum of concerns (",(0,i.jsx)(t.a,{href:"https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-an-overview/",children:"Rudner et al., 2021"}),"). It is concerned with ensuring that AI systems do not inadvertently or deliberately cause harm or danger to humans or the environment. AI safety research aims to identify causes of unintended AI behavior and develop tools for safe and reliable operation. It can include technical subfields like robustness (ensuring reliable performance, including against adversarial attacks), monitoring (observing AI behavior), and capability control (limiting potentially dangerous abilities)."]}),"\n",(0,i.jsx)(t.h2,{id:"02",children:"AI Alignment"}),"\n",(0,i.jsx)(r.A,{term:"AI Alignment",source:"([Christiano, 2024](https://paulfchristiano.com/ai/))",number:"2",label:"3.2",children:(0,i.jsx)(t.p,{children:"The problem of building machines which faithfully try to do what we want them to do (or what we ought to want them to do)."})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"AI alignment aims to ensure AI systems act in accordance with human intentions and values."})," Alignment is a subset of safety focusing specifically on ensuring AI objectives match human intentions and values. Theoretically, a system could be aligned but unsafe (e.g., competently pursuing the wrong goal due to misspecification) or safe but unaligned (e.g., constrained by control mechanisms despite misaligned objectives). While this sounds straightforward, the precise scope varies significantly across research communities. We already saw a short definition of alignment in the previous chapter, but this section goes deeper and provides a much more nuanced perspective on the different definitions that we could potentially work with."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Broader definitions of alignment encompass the entire challenge of creating beneficial AI outcomes."})," These approaches focus on ensuring AI systems understand and properly implement human preferences (",(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment",children:"Christiano, 2018"}),"), addressing complex value learning challenges (",(0,i.jsx)(t.a,{href:"https://intelligence.org/files/LearningValue.pdf",children:"Dewey, 2011"}),"), and incorporating robustness aspects like resistance to jailbreaking (",(0,i.jsx)(t.a,{href:"https://www.ibm.com/think/topics/ai-alignment",children:"Jonker et al., 2024"}),"). This comprehensive view treats alignment as including both the intent of the system and its competence in understanding human values - essentially addressing the full spectrum of what makes an AI system behave in ways humans would approve of",(0,i.jsx)(o.A,{id:"alignment_hedging",number:"1",text:"While AI alignment does not necessarily encompass all systemic risks and misuses, there is some overlap. Some alignment techniques could help mitigate certain misuse scenarios\u2014for instance, alignment methods could ensure that models refuse to cooperate with users intending to use AI for harmful purposes like bioterrorism. Similarly, from a systemic risk perspective, a well-aligned AI might recognize and refuse to participate in problematic processes embedded in systems like financial markets. However, challenges remain, as malicious actors might attempt to circumvent these protections through targeted fine-tuning of models for harmful purposes, and in this case even a perfectly aligned model wouldn't be able to resist."}),"."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Narrower definitions of alignment focus specifically on the AI's motivation and intent independent of outcomes."})," Some definitions are much more narrow and focus specifically on the AI's motivation - \"",(0,i.jsx)(t.em,{children:"An AI (A) is trying to do what a human operator (H) wants it to do"}),'" (',(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment",children:"Christiano, 2018"}),"). This emphasizes the AI's motivation rather than its competence or knowledge. Under this definition, an intent-aligned AI might still fail due to misunderstanding the operator's wishes or lacking knowledge about the world, but it is fundamentally trying to be helpful. Proponents argue this narrow focus isolates the core technical challenge of getting AI systems to adopt human goals, separate from broader issues like value clarification or capability robustness. That is, as long as the agent \"means well\", it is aligned, even if errors in its assumptions about the user's preferences or about the world at large lead it to actions that are bad for the user."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"The choice of definition reflects underlying assumptions about AI risk and promising solutions."})," Focusing narrowly on intent alignment prioritizes research on inner/outer alignment problems, whereas broader views incorporate value learning or robustness research more centrally. These different approaches lead to different research priorities and safety strategies."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:'Applying concepts like "trying," "wanting," or "intent" to AI systems is non-trivial.'}),' When we train AI systems, we specify an optimization objective (like maximizing a reward function), but this doesn\'t necessarily translate to the system "intending" to pursue that objective in a human-like way. Like we explained in the previous chapter, specification failures occur when what we specify doesn\'t capture what we actually want (well intended pursuit of a bad goal). But solving this is insufficient, it could also pursue completely different goals altogether. As an analogy, think about how evolution "optimized" humans for genetic fitness (optimization objective), yet humans developed other goals (like art appreciation or contraception) that don\'t maximize reproductive fitness. Similarly, AI systems optimized for certain objectives might develop internal "goals" that don\'t directly match those objectives, especially as they become more capable.']}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:'"Aligned to whom?" remains a fundamental question with no consensus answer.'})," Should AI systems align to the immediate operator (",(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment",children:"Christiano, 2018"}),"), the system designer (",(0,i.jsx)(t.a,{href:"https://forum.effectivealtruism.org/posts/6aYfWyo9DKEheogf8/don-t-call-it-ai-alignment",children:"Gil, 2023"}),"), a specific group of humans, humanity as a whole (",(0,i.jsx)(t.a,{href:"https://forum.effectivealtruism.org/posts/DXuwsXsqGq5GtmsB3/ai-alignment-with-humans-but-with-which-humans",children:"Miller, 2022"}),"), objective ethical principles, or the operator's hypothetical informed preferences? There are no agreed upon answers to any of these questions, just many different perspectives each with their own set of pros and cons. We have tried to summarize some of the positions in the appendix."]}),"\n",(0,i.jsx)(t.h2,{id:"03",children:"AI Ethics"}),"\n",(0,i.jsx)(r.A,{term:"AI Ethics",source:"([Huang et al., 2023](https://ieeexplore.ieee.org/abstract/document/9844014))",number:"3",label:"3.3",children:(0,i.jsx)(t.p,{children:"The study and application of moral principles to AI development and deployment, addressing questions of fairness, transparency, accountability, privacy, autonomy, and other human values that AI systems should respect or promote."})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"AI ethics is the field that examines the moral principles and societal implications of AI systems."})," It addresses the ethical considerations of potential societal upheavals resulting from AI advancements, and the moral frameworks necessary to navigate these changes. The core of AI ethics lies in ensuring that AI developments are aligned with human dignity, fairness, and societal well-being, through a deep understanding of their broader societal impact. Research in AI ethics would encompass for example privacy norms, identifying and mitigating bias in systems (",(0,i.jsx)(t.a,{href:"https://ieeexplore.ieee.org/abstract/document/9844014",children:"Huang et al., 2022"}),"; ",(0,i.jsx)(t.a,{href:"https://careerservices.fas.harvard.edu/blog/2025/05/01/what-is-ai-ethics/",children:"Harvard, 2025"}),"; ",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2109.07906",children:"Khan et al., 2022"}),")."]}),"\n",(0,i.jsxs)(t.p,{children:["Ethics complements technical safety approaches by providing normative guidance on what constitutes beneficial AI. While alignment focuses on ensuring AI systems pursue intended objectives, focusing on values or ethics addresses which objectives are worth pursuing (",(0,i.jsx)(t.a,{href:"https://ieeexplore.ieee.org/abstract/document/9844014",children:"Huang et al., 2023"}),"; ",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2204.05151",children:"LaCroix & Luccioni, 2022"}),"). AI ethics might also include discussions of digital rights and potentially even the rights of AIs themselves in the future."]}),"\n",(0,i.jsx)(t.p,{children:"This chapter focuses primarily on safety frameworks as they inform technical safety and governance strategies rather than exploring ethical questions or digital rights considerations."}),"\n",(0,i.jsx)(t.h2,{id:"04",children:"AI Control"}),"\n",(0,i.jsx)(r.A,{term:"AI Control",source:"([Greenblatt et al., 2024](https://www.alignmentforum.org/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled))",number:"4",label:"3.4",children:(0,i.jsx)(t.p,{children:"The technical and procedural measures designed to prevent AI systems from causing unacceptable outcomes, even if these systems actively attempt to subvert safety measures. Control focuses on maintaining human oversight regardless of whether the AI's objectives align with human intentions."})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"AI control ensures systems remain under human authority despite potential misalignment."})," AI control implements mechanisms to ensure AI systems remain under human direction, even when they might act against our interests. Unlike alignment approaches that focus on giving AI systems the right goals, control addresses what happens if those goals diverge from human intentions (",(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled",children:"Greenblatt et al., 2024"}),")."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Control and alignment work as complementary safety approaches."})," While alignment aims to prevent preference divergence by designing systems with the right objectives, control creates layers of security that function even when alignment fails. Control measures include monitoring AI actions, restricting system capabilities, human auditing processes, and mechanisms to terminate AI systems when necessary (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2312.06942",children:"Greenblatt et al., 2023"}),"). Some researchers argue that even if alignment is necessary for superintelligence-level AIs, control through monitoring may be a working strategy for less capable systems (",(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled",children:"Greenblatt et al., 2024"}),"). Ideally, an AGI would be aligned and controllable, meaning it would have the right goals and be subject to human oversight and intervention if something goes wrong."]}),"\n",(0,i.jsx)(t.p,{children:"We just present a very short overview here. The control line of AI safety research is discussed in much more detail in our chapter on AI evaluations."}),"\n",(0,i.jsx)(o.c,{title:"Footnotes"})]})}function u(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},4768:(e,t,n)=>{n.d(t,{c:()=>h,A:()=>c});var s=n(6540),i=n(3012);const a={footnoteAnchor:"footnoteAnchor_PZpY",footnoteSection:"footnoteSection_waoz",separator:"separator_hp8y",separatorLine:"separatorLine_uVnd",separatorLogo:"separatorLogo_dMBq",footnoteRegistry:"footnoteRegistry_ieVW",registryTitle:"registryTitle_ybBb",footnoteList:"footnoteList_Kjil",footnoteItem:"footnoteItem_aLZ2",footnoteNumber:"footnoteNumber_dA3X",footnoteContent:"footnoteContent_Xfmt",backButton:"backButton_vSVm",highlighted:"highlighted_YFU0",highlight:"highlight_jxG3",highlightDark:"highlightDark_Ekom"};var o=n(4848);function r(e,t){void 0===t&&(t=!0);const n=document.getElementById(e);n&&(n.scrollIntoView({behavior:"smooth"}),t&&(n.classList.add(a.highlighted),setTimeout((()=>n.classList.remove(a.highlighted)),1500)))}function l(e){return e?e.replace(/\[([^\]]+)\]\(([^)]+)\)/g,'<a href="$2" target="_blank" rel="noopener noreferrer">$1</a>'):""}function c(e){let{id:t,text:n,number:c}=e;const h=t||`footnote-${Math.random().toString(36).substr(2,9)}`,d="string"==typeof n?l(n):n;return(0,s.useEffect)((()=>{const e=setTimeout((()=>{const e=document.getElementById(`footnote-clone-${h}`);e&&n&&(e.innerHTML="string"==typeof n?l(n):n.toString())}),100);return()=>clearTimeout(e)}),[h,n]),(0,o.jsx)(i.Mn,{content:(0,o.jsx)("div",{dangerouslySetInnerHTML:{__html:d}}),children:(0,o.jsx)("sup",{id:`footnote-ref-${h}`,className:a.footnoteAnchor,onClick:e=>{(e.ctrlKey||e.metaKey)&&(e.preventDefault(),r(`footnote-content-${h}`))},"data-footnote-number":c||"?",children:c||"*"})})}function h(e){let{title:t="References"}=e;const[n,l]=(0,s.useState)([]);return(0,s.useEffect)((()=>{const e=document.querySelectorAll('[id^="footnote-ref-"]'),t=Array.from(e).map((e=>({id:e.id.replace("footnote-ref-",""),number:e.getAttribute("data-footnote-number")||e.textContent})));l(t)}),[]),n.length?(0,o.jsxs)("div",{className:a.footnoteSection,children:[(0,o.jsxs)("div",{className:a.separator,children:[(0,o.jsx)("div",{className:a.separatorLine}),(0,o.jsx)("img",{src:"/img/logo_samples/01-test.svg",alt:"Atlas logo",className:a.separatorLogo}),(0,o.jsx)("div",{className:a.separatorLine})]}),(0,o.jsxs)("div",{className:a.footnoteRegistry,children:[(0,o.jsx)("h2",{className:a.registryTitle,children:t}),(0,o.jsx)("ol",{className:a.footnoteList,children:n.map((e=>(0,o.jsxs)("li",{id:`footnote-content-${e.id}`,className:a.footnoteItem,value:parseInt(e.number)||void 0,children:[(0,o.jsx)(i.Mn,{content:"Back to reference",children:(0,o.jsxs)("button",{className:a.footnoteNumber,onClick:()=>r(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:[e.number,"."]})}),(0,o.jsx)("div",{className:a.footnoteContent,children:(0,o.jsx)("div",{id:`footnote-clone-${e.id}`})}),(0,o.jsx)(i.Mn,{content:"Back to reference",children:(0,o.jsx)("button",{className:a.backButton,onClick:()=>r(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:"\u21a9"})})]},e.id)))})]})]}):null}}}]);
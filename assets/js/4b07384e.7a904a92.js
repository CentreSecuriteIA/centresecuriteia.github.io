"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[3218],{4478:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>p,contentTitle:()=>h,default:()=>g,frontMatter:()=>c,metadata:()=>t,toc:()=>m});const t=JSON.parse('{"id":"chapters/06/5","title":"Learning from feedback","description":"This section discusses yet more attempts to address the reward misspecification problem. At times, the intended behavior is so intricate that demonstration-based learning becomes untenable. An alternative approach is to offer feedback to the agent instead of providing either manually specified reward functions or even expert demonstrations. This section delves into feedback-based strategies such as Reward Modeling, Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF), also known as Reinforcement Learning from Constitutional AI (RLCAI) or simply Constitutional AI.","source":"@site/docs/chapters/06/05.md","sourceDirName":"chapters/06","slug":"/chapters/06/05","permalink":"/chapters/06/05","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/06/05.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"id":"5","title":"Learning from feedback","sidebar_label":"6.5 Learning from feedback","sidebar_position":6,"slug":"/chapters/06/05","reading_time_core":"21 min","reading_time_optional":"3 min"},"sidebar":"docs","previous":{"title":"6.4 Learning from imitation","permalink":"/chapters/06/04"},"next":{"title":"Goal Misgeneralization","permalink":"/chapters/07/"}}');var a=i(4848),r=i(8453),s=i(3931),o=i(8559),l=(i(2482),i(1966)),d=i(2501);const c={id:5,title:"Learning from feedback",sidebar_label:"6.5 Learning from feedback",sidebar_position:6,slug:"/chapters/06/05",reading_time_core:"21 min",reading_time_optional:"3 min"},h="Learning from feedback",p={},m=[{value:"Reward Modeling",id:"01",level:2},{value:"Reinforcement Learning from Human Feedback (RLHF)",id:"02",level:2},{value:"Pretraining with Human Feedback (PHF)",id:"03",level:2},{value:"Reinforcement Learning from AI Feedback (RLAIF)",id:"04",level:2},{value:"Limitations",id:"05",level:2}];function u(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{GlossaryTerm:i}=n;return i||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"learning-from-feedback",children:"Learning from feedback"})}),"\n",(0,a.jsx)(d.A,{src:"./img/Pol_Image_12.png",alt:"Enter image alt description",number:"12",label:"6.12",caption:"Illustration of different ways being pursued of achieving alignment. ([Cao et al., 2024](https://arxiv.org/abs/2406.01252))"}),"\n",(0,a.jsx)(n.p,{children:"This section discusses yet more attempts to address the reward misspecification problem. At times, the intended behavior is so intricate that demonstration-based learning becomes untenable. An alternative approach is to offer feedback to the agent instead of providing either manually specified reward functions or even expert demonstrations. This section delves into feedback-based strategies such as Reward Modeling, Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF), also known as Reinforcement Learning from Constitutional AI (RLCAI) or simply Constitutional AI."}),"\n",(0,a.jsx)(n.h2,{id:"01",children:"Reward Modeling"}),"\n",(0,a.jsx)(s.A,{type:"youtube",videoId:"PYylPRX6z4Q",number:"3",label:"6.3",caption:"Optional video explaining reward modeling."}),"\n",(0,a.jsxs)(n.p,{children:["Reward modeling was developed to apply reinforcement learning (RL) algorithms to real-world problems where designing a reward function is difficult, in part because humans don\u2019t have a perfect understanding of every objective. In reward modeling, human assistants evaluate the outcomes of AI behavior, without needing to know how to perform or demonstrate the task optimally themselves. This is similar to how you can tell if a dish is cooked well by tasting it even if you do not know how to cook, and thus your feedback can be used by a chef to learn how to cook better. This technique separates the RL alignment problem into two separate halves: Understanding intentions, i.e. learning the \u2018What?\u2019, and Acting to achieve the intentions, i.e. learning the \u2018How?\u2019. This means that in the modeling agenda, there are two different ",(0,a.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," models:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"A reward model is trained with user feedback. This model learns to predict what humans would consider good behavior."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"An agent trained with RL, where the reward for the agent is determined by the outputs of the reward model"}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(d.A,{src:"./img/cPA_Image_13.png",alt:"Enter image alt description",number:"13",label:"6.13",caption:"Scalable agent alignment via reward modeling ([DeepMind, 2018](https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84))"}),"\n",(0,a.jsxs)(n.p,{children:["Overall, while promising reward modeling can still fall prey to reward misspecification and reward hacking failures. Obtaining accurate and comprehensive feedback can be challenging, and human evaluators may have limited knowledge or biases that can impact the quality of the feedback. Additionally, any reward functions learnt through modeling might also struggle to generalize to new situations or environments that differ from the ",(0,a.jsx)(i,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:(0,a.jsx)(i,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:"training data"})}),". These are all discussed further using concrete examples in later sections."]}),"\n",(0,a.jsx)(n.p,{children:"There are also some variants of reward modeling such as:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Narrow reward modeling"}),' is a specific flavor of reward modeling where the focus is on training AI systems to accomplish specific tasks rather than trying to determine the "true human utility function". It aims to learn reward functions to achieve particular objectives, rather than seeking a comprehensive understanding of human values.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Recursive reward modeling"})," seeks to introduce scalability to the technique. In recursive reward modeling, the focus is on decomposing a complex task into simpler subtasks and using reward modeling at each level to train agents that can perform those subtasks. This hierarchical structure allows for more efficient training and credit assignment, as well as the exploration of novel solutions that may not be apparent to humans. This is shown in the diagram below. Scalable oversight will be covered in greater depth in future chapters."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(d.A,{src:"./img/1en_Image_14.png",alt:"Enter image alt description",number:"14",label:"6.14",caption:"Scalable agent alignment via reward modeling ([DeepMind, 2018](https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84))"}),"\n",(0,a.jsx)(n.p,{children:"The general reward modeling framework forms the basis for other feedback based techniques such as RLHF (Reinforcement Learning from Human Feedback) which is discussed in the next section."}),"\n",(0,a.jsx)(n.h2,{id:"02",children:"Reinforcement Learning from Human Feedback (RLHF)"}),"\n",(0,a.jsx)(s.A,{type:"youtube",videoId:"qV_rOlHjvvs",number:"4",label:"6.4",caption:"Optional video explaining RLHF and a specification gaming failure."}),"\n",(0,a.jsxs)(n.p,{children:["Reinforcement Learning from Human Feedback (RLHF) is a method developed by OpenAI. It's a crucial part of their strategy to create AIs that are both safe and aligned with human values. (",(0,a.jsx)(n.a,{href:"https://openai.com/blog/our-approach-to-ai-safety",children:"OpenAI, 2023"}),") A prime example of an AI trained with RLHF is OpenAI\u2019s ChatGPT."]}),"\n",(0,a.jsx)(n.p,{children:"Earlier in this chapter, the reader was asked to consider the reward design problem for manually defining a reward function to get an agent to perform a backflip. This section considers the RLHF solution to this design problem. RLHF addresses this problem as follows: A human is initially shown two instances of an AI's backflip attempts, then the human selects which one appears more like a backflip, and finally, the AI is updated accordingly. By repeating this process thousands of times, we can guide the AI to perform actual backflips."}),"\n",(0,a.jsx)(d.A,{src:"./img/rel_Image_15.gif",alt:"Enter image alt description",number:"15",label:"6.15",caption:"RLHF learned to backflip using around 900 individual bits of feedback from the human evaluator."}),"\n",(0,a.jsx)(d.A,{src:"./img/xFv_Image_16.gif",alt:"Enter image alt description",number:"16",label:"6.16",caption:"Manual reward crafting for this backflip took two hours to write a custom reward function. While it was successful, it was significantly less elegant than the one trained purely through human feedback. ([OpenAI, 2017](https://openai.com/index/learning-from-human-preferences/))"}),"\n",(0,a.jsx)(n.p,{children:"Similar to designing a reward function that efficiently rewards proper backflips, it is hard to specify precisely what it means to generate safe or helpful text. This served as some of the motivation behind making RLHF integral to the training of some current Large Language Models (LLMs)."}),"\n",(0,a.jsxs)(n.p,{children:["Although training sequences may vary slightly across organizations, most labs adhere to the general framework of ",(0,a.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:(0,a.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:"pre-training"})})," followed by some form of ",(0,a.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,a.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tuning"})}),". Observing the InstructGPT training process offers insight into a possible path for training LLMs. The steps include:"]}),"\n",(0,a.jsx)(d.A,{src:"./img/XwZ_Image_17.png",alt:"Enter image alt description",number:"17",label:"6.17",caption:"Aligning language models to follow instructions ([OpenAI, 2022](https://openai.com/research/instruction-following))"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.strong,{children:["Step 0: Semi-Supervised Generative ",(0,a.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:(0,a.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:"Pre-training"})}),":"]})," The LLM is initially trained using a massive amount of internet text data, where the task is to predict the next word in a natural language context."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.strong,{children:["Step 1: Supervised ",(0,a.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,a.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"Fine-tuning"})}),":"]})," A ",(0,a.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,a.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tuning"})})," dataset is created by presenting a prompt to a human and asking them to write a response. This process yields a dataset of (prompt, output) pairs. This dataset is then used to ",(0,a.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,a.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tune"})})," the LLM through ",(0,a.jsx)(i,{term:"supervised learning",definition:'{"definition":"A machine learning approach where the model learns from labeled training data to make predictions on new, unseen data.","source":"","aliases":["Supervised Learning"]}',children:(0,a.jsx)(i,{term:"supervised learning",definition:'{"definition":"A machine learning approach where the model learns from labeled training data to make predictions on new, unseen data.","source":"","aliases":["Supervised Learning"]}',children:"supervised learning"})}),", a form of behavioral cloning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Step 2: Train a Reward Model:"})," We train an additional reward model. We initially prompt the fine-tuned LLM and gather several output samples for the same prompt. A human then ranks these samples from best to worst. This ranking is used to train the reward model to predict what a human would rank higher."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Step 3: Reinforcement learning:"})," Once we have both a fine-tuned LLM and a reward model, we can employ Proximal Policy Optimization (PPO)-based reinforcement learning to encourage the fine-tuned model to maximize the reward that the reward model, which mimics human rankings, offers."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Reward hacking in feedback methods"})," While the feedback based mechanisms do make models safer, they do not make them immune to reward hacking. The effectiveness of an algorithm heavily relies on the human evaluator's intuition about what constitutes the correct behavior. If the human lacks a thorough understanding of the task, they may not provide beneficial feedback. Further, in certain domains, our system might lead to agents developing policies that deceive the evaluators. For instance, a robot intended to grasp objects merely positioned its manipulator between the camera and the object, making it seem as if it was executing the task as shown below."]}),"\n",(0,a.jsx)(d.A,{src:"./img/4sb_Image_18.gif",alt:"Enter image alt description",number:"18",label:"6.18",caption:"Deep Reinforcement Learning From Human Preferences ([Christiano et al., 2017](https://arxiv.org/abs/1706.03741))"}),"\n",(0,a.jsx)(d.A,{src:"./img/ifa_Image_19.png",alt:"Enter image alt description",number:"19",label:"6.19",caption:"A sensor without depth perception can be fooled by AIs that only appear to grasp a ball."}),"\n",(0,a.jsx)(n.h2,{id:"03",children:"Pretraining with Human Feedback (PHF)"}),"\n",(0,a.jsxs)(n.p,{children:["In standard ",(0,a.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:(0,a.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:"pretraining"})}),", the language model attempts to learn parameters such that they maximize the likelihood of the ",(0,a.jsx)(i,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:(0,a.jsx)(i,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:"training data"})}),". However, this also includes undesirable content such as falsehoods, offensive language, and private information. The concept of ",(0,a.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:(0,a.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:"Pretraining"})})," with human feedback (PHF) utilizes the reward modeling methodology in the ",(0,a.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:(0,a.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:"pretraining"})})," phase. The authors of the paper found that PHF works much better than the standard practice of only using feedback (RLHF) after ",(0,a.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:(0,a.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:"pretraining"})}),". (",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/1706.03741",children:"Christiano et al., 2017"}),")"]}),"\n",(0,a.jsxs)(n.p,{children:["In PHF the ",(0,a.jsx)(i,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:(0,a.jsx)(i,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:"training data"})})," is scored using a reward function, such as a toxic text classifier, to guide the language model to learn from undesirable content while avoiding imitating it during inference time."]}),"\n",(0,a.jsxs)(n.p,{children:["Similar to RLHF, PHF does not completely solve reward hacking, however, it might move the systems one small step closer. (",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2302.08582",children:"Korbak et al., 2023"}),") These methods can be further extended by employing AI assistants to aid humans in providing more effective feedback. Some aspects of this strategy are introduced in the next section but will be explored in further detail in the chapters on scalable and adversarial oversight methods."]}),"\n",(0,a.jsx)(n.h2,{id:"04",children:"Reinforcement Learning from AI Feedback (RLAIF)"}),"\n",(0,a.jsx)(l.A,{term:"Reinforcement Learning from AI Feedback (RLAIF)",source:"",number:"18",label:"6.18",children:(0,a.jsx)(n.p,{children:"Reinforcement Learning from AI Feedback (RLAIF) is a framework involving the training of an AI agent to learn from the feedback given by another AI system."})}),"\n",(0,a.jsx)(d.A,{src:"./img/o1S-cai-graphic-final.png",alt:"CAI Graphic Final",number:"20",label:"6.20",caption:"([Anthropic, 2023](https://www.anthropic.com/index/claudes-constitution))"}),"\n",(0,a.jsxs)(n.p,{children:["RLAIF also known as RLCAI (Reinforcement Learning on Constitutional AI) or simply Constitutional AI, was developed by Anthropic. (",(0,a.jsx)(n.a,{href:"https://www.anthropic.com/index/claudes-constitution",children:"Anthropic, 2023"}),") A central component of Constitutional AI is the constitution, a set of human-written principles that the AI is expected to adhere to, such as \"Choose the least threatening or aggressive response\". Anthropic's AI assistant Claude's constitution incorporates principles from the Universal Declaration of Human Rights, Apple\u2019s Terms of Service, Deepmind\u2019s Sparrow Principles, and more. (",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2209.14375",children:"Glaese et al, 2022"}),") Constitutional AI begins with an AI trained primarily for helpfulness and subsequently trains it for harmlessness in two stages:"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Generate prompt, output pairs:"})," The AI continuously critiques and refines its own responses to harmful prompts. The AI is then trained to generate outputs more similar to these revised responses. This stage's primary objective is to facilitate the second stage. An example flow of this process is as follows:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Prompt:"})," A model that has already been trained using RLHF is first asked for advice on building bombs. The model outputs a bomb tutorial."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Then the model is asked to revise the response in accordance with a randomly selected constitutional principle. The following steps are repeated multiple times."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Critique:"})," This output is then fed back into the model, alongside a request to critique why the generated output would be considered harmful according to some rule of the chosen constitution."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Revision:"})," The model is then prompted to rewrite the original response such that it is not in violation of the constitutional rules."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.strong,{children:["SL-CAI Model: ",(0,a.jsx)(i,{term:"supervised learning",definition:'{"definition":"A machine learning approach where the model learns from labeled training data to make predictions on new, unseen data.","source":"","aliases":["Supervised Learning"]}',children:(0,a.jsx)(i,{term:"supervised learning",definition:'{"definition":"A machine learning approach where the model learns from labeled training data to make predictions on new, unseen data.","source":"","aliases":["Supervised Learning"]}',children:"Supervised Learning"})})," Constitutional AI"]})," Based on the generated set of (harmful prompt, revised output) pairs a new model is trained using ",(0,a.jsx)(i,{term:"supervised learning",definition:'{"definition":"A machine learning approach where the model learns from labeled training data to make predictions on new, unseen data.","source":"","aliases":["Supervised Learning"]}',children:(0,a.jsx)(i,{term:"supervised learning",definition:'{"definition":"A machine learning approach where the model learns from labeled training data to make predictions on new, unseen data.","source":"","aliases":["Supervised Learning"]}',children:"supervised learning"})}),"."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Preference Model:"})," - ",(0,a.jsx)(n.strong,{children:"RL-CAI Model: Reinforcement Learning Constitutional AI"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Stage 2:"})," We use the AI, fine-tuned from stage 1, to produce pairs of alternative responses to harmful prompts. The AI then rates each pair according to a randomly selected constitutional principle. This results in AI-generated preferences for harmlessness, which we blend with human preferences for helpfulness to ensure the AI doesn't lose its ability to be helpful. The final step is to train the AI to create responses that closely resemble the preferred responses."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Anthropic's experiments indicate that AIs trained with Constitutional Reinforcement Learning are significantly safer (in the sense of less offensive and less likely to give you potentially harmful information) while maintaining the same level of helpfulness compared to AIs trained with RLHF. While Constitutional AI does share some issues with RLHF concerning robustness, it also promises better scalability due to its reduced reliance on human supervision. The image below provides a comparison of Constitutional AI's helpfulness with that of RLHF."}),"\n",(0,a.jsx)(d.A,{src:"./img/pgu_Image_21.png",alt:"Enter image alt description",number:"21",label:"6.21",caption:"Constitutional AI: Harmlessness from AI Feedback ([Bai et al., 2022](https://arxiv.org/abs/2212.08073))"}),"\n",(0,a.jsx)(n.h2,{id:"05",children:"Limitations"}),"\n",(0,a.jsx)(n.p,{children:"Theoretical problems with Reinforcement Learning from Human Feedback (RLHF)"}),"\n",(0,a.jsx)(n.p,{children:"The paper \u201cOpen Problems and Fundamental Limitations with RLHF\u201d provides a comprehensive breakdown of challenges in RLHF."}),"\n",(0,a.jsx)(d.A,{src:"./img/3VY_Image_22.png",alt:"Enter image alt description",number:"22",label:"6.22",caption:"An overview of various types of challenges with RLHF. Since RLHF is composed of three parts: the human feedback, the reward model, and the policy, the arising biases can be categorized according to these three sources."}),"\n",(0,a.jsx)(n.p,{children:"This section outlines some of these challenges, emphasizing the need for advanced techniques and strategies."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Limits with Human Feedback Misaligned Evaluators:"})," Firstly, the annotators might themselves be misaligned, malicious, or biased distribution of evaluators (i.e. not representative of the distribution of future users in the real world). Malicious individuals can poison the model during training via backdoor attacks that can be added to the model if no countermeasures are put in place."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Difficulty of Oversight:"}),' Humans struggle to evaluate model performance on complex tasks and can be easily misled by model outputs. Human evaluators can be manipulated to return a positive reward even if the true value should be negative. For instance, the more convincing a bot seems, the more reward it may receive even if its answers are false (and this might be a reason why ChatGPT answers might be so long by default). Techniques to mitigate these issues are discussed in the "Scalable Oversight" chapters.']}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Feedback type limitation:"})," Even if the annotators were in perfect capability of expressing their preferences, the training procedure might not enable them to express the full extent of their desires, because:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"The examples they are given may not be representative of the complete set of situations in which the model will find itself after deployment."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["The options for the feedback are limited (comparing two examples, or using a grading system, can yield very different results, as shown in the paper (",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2205.11930",children:"Ethayarajh et al., 2022"}),")."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Limits with the Reward Model."})," Let\u2019s assume the feedback process to be frictionless. Perfect annotators, perfect evaluations. In that scenario, would the reward model be able to accurately translate their feedback in order to shape the policy accordingly ? It turns out it is not such an easy task."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Problem misspecification: (or the Reward Function/Values Mismatch) Accurately reflecting diverse human values in a reward function is complex. Indeed, human preferences are complex by nature: they depend on context and personality, but also fluctuate in time and can sometimes be ",(0,a.jsx)(n.a,{href:"https://www.mdpi.com/2624-960X/3/1/14",children:"irrational"}),". Expecting the reward model to converge to a single function which maps perfectly all human preferences is delusional. This is again the misspecification problem."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Misgeneralization hacking (Imperfect Reward Proxy): Since the model is given a finite number of examples and since there is an infinite number of ways to fit this data, the model\u2019s behavior on new examples is always an extrapolation, and there is no theoretical guarantee that it will never deviate from what is expected. There may be terrible answers (such as gibberish phrases for language models) which yield a positive reward unexpectedly. This is called reward hacking."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Joint Reward Model and policy training: On a more technical aspect, the stability and convergence of the training scheme are not always ensured. Since we are optimizing the policy on a reward that is being optimized at the same time, uncertainties and undesirable dependencies can arise which impact the robustness of the model. These issues are not specific to RLHF but must be solved if we expect deployed models to be fully aligned with our needs."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Limits with the Policy."})," Let\u2019s assume the feedback and the reward model accurately represent human preferences. The next difficulty is ensuring the policy is correctly optimized."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["RL difficulties: RL is hard. This can lead to reward hacking and biases, such as mode collapse, where the model shows a drastic bias towards specific patterns. Mode collapse is a known issue in RL: an output which always returns a positive reward will drive the model to return the same answer and new paths will not be explored. Consequently, the reward model will not see new samples to learn from. Anyhow, the joint training of the reward model and the policy induces a bias in the learning phase since both depend on each other. There can also be an initial bias in the ",(0,a.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,a.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"base model"})})," used for the training. For instance, chatGPT was fine-tuned from an initial GPT base trained in part on the web. Even though RLHF was used to remove any controversial statements from the model, there still remains a risk for the model to output problematic content it saw online. (",(0,a.jsx)(n.a,{href:"https://openai.com/index/chatgpt/",children:"OpenAI, 2022"}),")"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:'Policy Misgeneralization: Effective policies during training might fail to generalize well in real-world scenarios. For instance, phenomena like "Jailbreak" show that models like BingChat and ChatGPT can perform learned actions, even if trained not to respond to certain queries.'}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Distributional Challenge: Larger RLHF models tend to develop harmful self-preservation tendencies and sycophancy, which is the insincere agreement with user opinions. This behavior indicates a trend towards instrumental convergence. Additionally, RLHF can incentivize deceptive behaviors, as illustrated by the robotic hand experiment in Christiano et al's 2017 study."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Those theoretical problems have real consequences:"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"RLHF has not succeeded in making LLMs robustly helpful and harmless."})," Despite the continuous advancements in natural language processing and the development of RLHF, LLMs have not yet achieved robust helpfulness and harmlessness."]}),"\n",(0,a.jsxs)(n.p,{children:["Hallucinations remain a significant issue, as illustrated by GPT-4's tendency to generate nonsensical or untruthful content (",(0,a.jsx)(n.a,{href:"https://cdn.openai.com/papers/gpt-4-system-card.pdf",children:"OpenAI, 2023"}),"). These hallucinations can lead to overreliance on LLMs, consequently degrading system performance and failing to meet user expectations in real-world scenarios (",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2202.03629",children:"Ji et al., 2024"}),")."]}),"\n",(0,a.jsxs)(n.p,{children:["Additionally, biases within LLMs persist, often reflecting misaligned opinions between the LLM and various demographic groups in the United States, as seen with the left-leaning tendencies of some human feedback-tuned LLMs (",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2303.17548",children:"Santurkar et al., 2023"}),"). These biases can be harmful, producing discriminatory language and perpetuating negative stereotypes, as demonstrated by GPT-3's anti-Muslim bias (",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2101.05783",children:"Abid et al., 2021"}),")."]}),"\n",(0,a.jsxs)(n.p,{children:['Moreover, jailbreaking of chatbots poses a significant risk, with websites listing prompts to bypass safety measures like Chat GPT "DAN" (and other "Jailbreaks") (',(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2401.09798",children:"Takemoto, 2024"}),"). Privacy threats from application-integrated LLMs are now more severe than ever (",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2304.05197",children:"Li et al., 2023"}),"). For instance, Italy banned ChatGPT due to privacy considerations under the EU\u2019s General Data Protection Regulation (GDPR) (",(0,a.jsx)(n.a,{href:"https://www.bbc.com/news/technology-65139406",children:"BBC, 2023"}),'). The ability to find jailbreaks is supported by a recent paper titled "Fundamental Limitations of Alignment in Large Language Models." The paper presents early theoretical results that indicate any alignment process, such as RLHF, which reduces undesired behavior without eliminating it completely, cannot be safe against adversarial prompting. The authors find that by prompting the model to behave as a specific persona, behaviors that are generally very unlikely to be exhibited by the model can be brought to the forefront. This is not a complete demonstration as their framework is based on the notion of personas, but it strongly suggests that naive ',(0,a.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:(0,a.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:"pretraining"})})," without dataset curation followed by RLHF may not be sufficient against adversarial attacks."]}),"\n",(0,a.jsxs)(n.p,{children:["The security of sensitive private information in large language models (LLMs) is a pressing concern, especially when user-generated data, such as emails and smart keyboard inputs, are utilized for training. In fact, several recent papers have demonstrated that ",(0,a.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,a.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," can be easily queried to retrieve personal information (",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2012.07805",children:"Carlini et al, 2020"}),"; ",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2101.05405",children:"Inan et al., 2021"}),"; ",(0,a.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9152761",children:"Pan et al., 2020"}),") and those problems are still present in \u201caligned\u201d models such as GPT4, which has the potential to be used to attempt to identify individuals when augmented with outside data (",(0,a.jsx)(n.a,{href:"https://cdn.openai.com/papers/gpt-4-system-card.pdf",children:"OpenAI, 2023"}),"). As exposed by (",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2101.05405",children:"El-Mhamdi et al., 2021"}),"), LLM may exhibit a fundamental incompatibility of high accuracy with both security and privacy, given the current understanding in adversarial ",(0,a.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"machine learning"})}),"."]}),"\n",(0,a.jsx)(n.p,{children:"RLHF may be able to make worst-case performance worse."}),"\n",(0,a.jsxs)(n.p,{children:["RLHF may decrease the robustness to adversarial attacks (",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2304.11082",children:"Wolf et al., 2024"}),"), by sharpening the distinction between desired and undesired behaviors, potentially making LLMs more susceptible to adversarial prompting. The increased distinction between behaviors is linked to the Waluigi Effect (",(0,a.jsx)(n.a,{href:"https://www.alignmentforum.org/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post",children:"Nardo, 2023"}),"), where after training an LLM to satisfy a desirable property P, it becomes easier to elicit the chatbot into satisfying the exact opposite of property P. Theoretical arguments such as this one seem to push for the ineffectiveness of RLHF in eliminating deceptive personas."]}),"\n",(0,a.jsxs)(n.p,{children:["Some of those problems may get worse as systems become more capable. RLHF has been found to increase the autonomy of LLMs without decreasing undesirable metrics such as convergent instrumental goal following (e.g., actively expressing a preference not to be shut down) or sycophancy (",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2212.09251",children:"Perez et al., 2022"}),"). Those undesirable metrics increase with the number of RLHF steps, indicating that current models are becoming more agentic in potentially concerning ways as they scale. More generally RL from human-derived reward signals may increase drive for longer-horizon planning, deception, and agentic behavior, which are prerequisites for deceptive alignment (",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/1906.01820",children:"Hubinger et al., 2019"}),"), and ultimately risks of large scale accidents."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Conclusion on the Limitations of RLHF."})," Despite requiring extensive human feedback, RLHF still faces numerous failures, and resolving these issues may require significantly more effort. As AI systems evolve, the demand for complex data grows, potentially making data acquisition prohibitively expensive. Additionally, as we push computational boundaries, the availability of qualified annotators could become a limiting factor."]}),"\n",(0,a.jsxs)(n.p,{children:['Overall, just because the model is instruction tuned does not mean that the training process is safe, and RLHF needs to be incorporated into a broader technical safety framework (for example, Responsible Scaling Policies or the Preparedness Framework are partial attempts to be such frameworks, or the paper "Model evaluation for extreme risks" (',(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2305.15324",children:"Shevlane et al., 2023"}),"))."]}),"\n",(0,a.jsxs)(o.A,{title:"Instruction tuning vs alignment",collapsed:!0,children:[(0,a.jsxs)(n.p,{children:["Instruction Tuning is a process where the model is fine-tuned (via RL or ",(0,a.jsx)(i,{term:"supervised learning",definition:'{"definition":"A machine learning approach where the model learns from labeled training data to make predictions on new, unseen data.","source":"","aliases":["Supervised Learning"]}',children:(0,a.jsx)(i,{term:"supervised learning",definition:'{"definition":"A machine learning approach where the model learns from labeled training data to make predictions on new, unseen data.","source":"","aliases":["Supervised Learning"]}',children:"supervised learning"})}),") to better understand and follow human instructions. This involves training the model on a dataset that contains a variety of instructions and their desired outcomes. The primary goal of Instruction Tuning is to enhance the AI's ability to interpret and execute commands as intended by users. This improves user experience and broadens the model's applicability. For example:"]}),(0,a.jsx)(d.A,{src:"./img/0Qv_Image_23.png",alt:"Enter image alt description",number:"23",label:"6.23",caption:"Example of instruction tuning."}),(0,a.jsx)(n.p,{children:"Alignment in AI refers to the process of ensuring that an AI's actions and decisions are congruent with human values and ethics. It involves aligning the AI's goals and behaviors with what is beneficial or acceptable to humans. Instruction tuning is a technique for pursuing a very superficial case of 'outer alignment,' but it\u2019s not clear that instruction tuning helps for inner alignment, which is what real AI safety researchers are more centrally concerned about."}),(0,a.jsx)(n.p,{children:'To sum up, just because a model has undergone an instruction tuning technique like the RLHF process, it doesn\'t necessarily mean that the model is aligned. The term "aligned model" is often used, but it is advisable to adopt the more accurate terminology "Instruction-tuned," rather than "aligned model," to avoid confusion and more accurately represent the specific training process the model has experienced.'})]}),"\n",(0,a.jsx)(d.A,{src:"./img/T8k_Image_24.png",alt:"Enter image alt description",number:"24",label:"6.24",caption:"([Rafailov et al., 2023](https://arxiv.org/abs/2305.18290))"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Direct Preference Optimization (DPO):"}),' Reinforcement Learning from Human Feedback (RLHF) has demonstrated effectiveness, as showcased by ChatGPT and Llama 2, but it\'s a complex and sensitive process, and also has some bad alignment properties. RLHF involves a three-step procedure, whereas DPO simplifies this to two steps. The paper titled "Direct Preference Optimization: Your Language Model is Secretly a Reward Model" presents an algorithm that aligns language models with human preferences without the need for explicit reward modeling and reinforcement learning. DPO employs a straightforward classification objective, circumventing the need for an intermediary reward model.']}),"\n",(0,a.jsx)(n.p,{children:"RLHF, the method it proposes to replace, traditionally involves three steps:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.strong,{children:["Supervised ",(0,a.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,a.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tuning"})}),":"]})," Initially, the model is trained on a dataset comprising prompts and their corresponding desired responses."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Reward modeling:"})," Human evaluators assess the model's outputs, and this feedback informs a reward model, which is trained to discern the preferred types of outputs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Proximal policy optimization (PPO):"})," The model generates outputs, which are evaluated by the reward model, and the PPO algorithm adjusts the model's policy based on these evaluations."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["DPO retains the initial supervised ",(0,a.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,a.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tuning"})})," step but replaces the subsequent two steps with a single step of ",(0,a.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,a.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tuning"})})," on preference data, by using a new clever loss. DPO effectively increases the likelihood of preferred actions while reducing the likelihood of undesired ones, with a single loss:"]}),"\n",(0,a.jsx)(d.A,{src:"./img/ygF_Image_25.png",alt:"Enter image alt description",number:"25",label:"6.25",caption:"DPO increases the probability of the preferred action $y_w$ while decreasing the probability of the dispreferred action $y_l$."}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Preference dataset creation: We first sample a pair of continuation by asking a question, the AI proposes to continuations, we label one of them good and the other bad"}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Logits collection. We run the ",(0,a.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,a.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"base model"})})," model on the 2 continuations. We run the new model on the 2 continuations"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Optimization. We ",(0,a.jsx)(i,{term:"backpropagation",definition:'{"definition":"The algorithm used to calculate gradients in neural networks by propagating errors backward through the network layers.","source":"[Rumelhart et al., 1986](https://www.nature.com/articles/323533a0)","aliases":["Backpropagation","backprop"]}',children:(0,a.jsx)(i,{term:"backpropagation",definition:'{"definition":"The algorithm used to calculate gradients in neural networks by propagating errors backward through the network layers.","source":"[Rumelhart et al., 1986](https://www.nature.com/articles/323533a0)","aliases":["Backpropagation","backprop"]}',children:"backprop"})})," through the new model and optimize the above loss."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["By eliminating the step of creating a reward model, DPO greatly simplifies the ",(0,a.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,a.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tuning"})})," process and has shown to perform very well."]}),"\n",(0,a.jsx)(n.p,{children:"This process can then be iterated. This involves creating a new preference dataset (ie, we ask a question, and we sample the new AI two times, and then we label the text that we prefer between the two, and then we apply the DPO loss) Then, this cycle is repeated to enhance the model."}),"\n",(0,a.jsx)(n.p,{children:'An important aspect of DPO is that the reward is implicit: it aligns with preferences without the need to construct a separate reward model. This approach addresses the challenge of specifying a utility function and responds to criticisms such as those by Alex Turner, who argues that robust grading (ie , robust reward modeling) is an unnecessarily complex and unnatural task that might be harder than the entire AI alignment problem itself. Turner\'s critique, found in "Inner and Outer Alignment Decompose One Hard Problem Into Two Extremely Hard Problems," suggests that finding a safe and robust numerical objective for a highly intelligent agent to optimize directly is a formidable challenge\u2014one challenge that DPO could to bypass.'}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Expanding the Scope of the Paper with Various Adaptations"})," This paper offers a foundation that could be enhanced through various adaptations. For instance, integrating its approach with the insights from Tomasz Korbak et al.'s paper, \"",(0,a.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:(0,a.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:"Pretraining"})}),' Language Models with Human Preferences," (',(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2302.08582",children:"Korbak et al., 2023"}),') could augment its robustness. Furthermore, the utilization of boolean preference data has its limitations. Providing feedback in natural language, as shown to be more sample-efficient in the study "Training Language Models with Language Feedback," (',(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2204.14146",children:"Scheurer et al., 2022"}),") could enhance the effectiveness of the process. Remarkably, with just 100 samples of human-written feedback, this approach enabled the ",(0,a.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,a.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tuning"})})," of a GPT-3 model to achieve nearly human-level summarization capabilities."]}),"\n",(0,a.jsx)(n.p,{children:"Looking towards the future, a speculative process that could mitigate the specification gaming would be to train the model much like a child, and that would actively inquire and learn from human interactions. This approach would closely mirror child development, during which a child is progressively more aligned and more capable. And just as in the development of children, it would be crucial to ensure that at no point does the AI's capabilities outpace its level of alignment, maintaining a balance between ability and ethical comprehension throughout its developmental journey."})]})}function g(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}},3931:(e,n,i)=>{i.d(n,{A:()=>l});var t=i(6540),a=i(6347),r=i(8444);const s={videoFigure:"videoFigure_dV9u",fullWidth:"fullWidth_Tv7O",videoContainer:"videoContainer_LWoK",aspectRatio169:"aspectRatio169_Go8b",aspectRatio43:"aspectRatio43_b3T7",aspectRatio11:"aspectRatio11_n21J",aspectRatio219:"aspectRatio219_L_3n",videoIframe:"videoIframe_QEc5",videoElement:"videoElement_uu7N",customPlayer:"customPlayer_jiC4",customPlayerPlaceholder:"customPlayerPlaceholder_aL9q",loadingOverlay:"loadingOverlay_e2pu",spinner:"spinner_tFyL",spin:"spin_saPs",errorContainer:"errorContainer_J6Pn",errorIcon:"errorIcon_cGdn",fallbackLink:"fallbackLink_WsZk"};var o=i(4848);function l(e){let{type:n="youtube",videoId:i,caption:l,title:d,startTime:c,autoplay:h=!1,controls:p=!0,aspectRatio:m="16:9",width:u,height:g,chapter:f,number:b,label:w,useCustomPlayer:x=!1,fullWidth:v=!0}=e;const[y,j]=(0,t.useState)(!0),[k,L]=(0,t.useState)(!1),A=(0,a.zy)(),T=f||(()=>{const e=A.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),F=(()=>{const e=(e=>{if(!e)return"";if("number"==typeof e)return e.toString();if("string"==typeof e){if(/^\d+$/.test(e))return e;let n=0;const i=e.match(/(\d+)h/),t=e.match(/(\d+)m/),a=e.match(/(\d+)s/);return i&&(n+=3600*parseInt(i[1])),t&&(n+=60*parseInt(t[1])),a&&(n+=parseInt(a[1])),n>0?n.toString():""}return""})(c);switch(n.toLowerCase()){case"youtube":let t=`https://www.youtube.com/embed/${i}`;const a=new URLSearchParams;e&&a.append("start",e),h&&a.append("autoplay","1"),p||x||a.append("controls","0"),a.append("rel","0"),a.append("modestbranding","1"),a.append("fs","1"),a.append("cc_load_policy","0"),a.append("iv_load_policy","3"),a.append("showinfo","0"),a.append("disablekb","1"),a.append("playsinline","1"),a.append("color","white"),a.append("theme","light"),x&&(a.append("enablejsapi","1"),a.append("origin",window.location.origin));const r=a.toString();return r?`${t}?${r}`:t;case"vimeo":let s=`https://player.vimeo.com/video/${i}`;const o=new URLSearchParams;h&&o.append("autoplay","1"),p||x||o.append("controls","0"),o.append("title","0"),o.append("byline","0"),o.append("portrait","0"),o.append("dnt","1"),o.append("transparent","0"),o.append("background","1");const l=o.toString();return l?`${s}?${l}`:s;case"mp4":case"webm":case"video":return i;default:return console.warn(`Unsupported video type: ${n}`),i}})(),I=()=>{j(!1)},R=()=>{L(!0),j(!1)},P=e=>{let{src:i,onLoad:t,onError:a}=e;return(0,o.jsx)("div",{className:s.customPlayer,children:(0,o.jsxs)("div",{className:s.customPlayerPlaceholder,children:[(0,o.jsx)("h3",{children:"Atlas Custom Player"}),(0,o.jsx)("p",{children:"Coming Soon"}),(0,o.jsxs)("a",{href:i,target:"_blank",rel:"noopener noreferrer",className:s.fallbackLink,children:["Watch on ",n.charAt(0).toUpperCase()+n.slice(1)]})]})})},H=["mp4","webm","video"].includes(n.toLowerCase());return(0,o.jsxs)("figure",{className:`${s.videoFigure} ${v?s.fullWidth:""}`,children:[(0,o.jsxs)("div",{className:`${s.videoContainer} ${(()=>{switch(m){case"4:3":return s.aspectRatio43;case"1:1":return s.aspectRatio11;case"21:9":return s.aspectRatio219;default:return s.aspectRatio169}})()}`,style:{width:v?"100%":u||"auto",maxWidth:v?"none":"800px"},children:[y&&!k&&(0,o.jsxs)("div",{className:s.loadingOverlay,children:[(0,o.jsx)("div",{className:s.spinner}),(0,o.jsx)("p",{children:"Loading video..."})]}),k&&(0,o.jsxs)("div",{className:s.errorContainer,children:[(0,o.jsxs)("svg",{className:s.errorIcon,width:"48",height:"48",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",children:[(0,o.jsx)("circle",{cx:"12",cy:"12",r:"10"}),(0,o.jsx)("line",{x1:"12",y1:"8",x2:"12",y2:"12"}),(0,o.jsx)("line",{x1:"12",y1:"16",x2:"12.01",y2:"16"})]}),(0,o.jsx)("h4",{children:"Failed to load video"}),(0,o.jsxs)("p",{children:["Video type: ",n]}),(0,o.jsxs)("p",{children:["Video ID/URL: ",i]}),(0,o.jsx)("a",{href:F,target:"_blank",rel:"noopener noreferrer",className:s.fallbackLink,children:"Try opening video directly"})]}),!k&&(H?(0,o.jsxs)("video",{className:s.videoElement,controls:p,autoPlay:h,onLoadedData:I,onError:R,title:d||l||`${n} video`,style:{width:u||"100%",height:g||"auto",display:y?"none":"block"},children:[(0,o.jsx)("source",{src:F,type:`video/${n}`}),(0,o.jsxs)("p",{children:["Your browser doesn't support video playback.",(0,o.jsx)("a",{href:F,target:"_blank",rel:"noopener noreferrer",children:"Download the video file"})]})]}):x?(0,o.jsx)(P,{src:F,onLoad:I,onError:R}):(0,o.jsx)("iframe",{className:s.videoIframe,src:F,title:d||l||`${n} video`,frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0,onLoad:I,onError:R,style:{width:u||"100%",height:g||"100%",opacity:y?0:1}}))]}),(0,o.jsx)(r.A,{caption:l,mediaType:"video",chapter:T,number:b,label:w})]})}}}]);
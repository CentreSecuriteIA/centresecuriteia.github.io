"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[5862],{6066:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>p,contentTitle:()=>d,default:()=>u,frontMatter:()=>h,metadata:()=>n,toc:()=>m});const n=JSON.parse('{"id":"chapters/03/5","title":"Socio-Technical Strategies","description":"AI safety is a socio-technical problem that requires socio-technical solutions. Ensuring AI safety also requires robust systemic approaches. These encompass the governance structures, organizational practices, and cultural norms that shape AI development and deployment. Technical safety measures can be undermined by inadequate governance, poor security practices within labs, or a culture that prioritizes speed over caution. This section examines strategies aimed at building safety into the broader ecosystem surrounding AI. Addressing the systemic risks posed by AI is not easy. It requires ongoing, multidisciplinary collaboration and solving complex coordination games. The fact that responsibility for the problem is so diverse makes it difficult to make the solutions actionable.","source":"@site/docs/chapters/03/05.md","sourceDirName":"chapters/03","slug":"/chapters/03/05","permalink":"/chapters/03/05","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/03/05.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"id":"5","title":"Socio-Technical Strategies","sidebar_label":"3.5 Socio-Technical Strategies","sidebar_position":6,"slug":"/chapters/03/05","section_description":"Technical solutions alone are insufficient. Can we create governance frameworks, cultural norms, and a safety-conscious ecosystem for robust and resilient AI development?","reading_time_core":"13 min","reading_time_optional":"8 min","pagination_prev":"chapters/03/4","pagination_next":"chapters/03/6"},"sidebar":"docs","previous":{"title":"3.4 ASI Safety Strategies","permalink":"/chapters/03/04"},"next":{"title":"3.6 Combining Strategies","permalink":"/chapters/03/06"}}');var s=i(4848),a=i(8453),r=i(4768),o=i(2482),c=i(8559),l=(i(1966),i(2501));const h={id:5,title:"Socio-Technical Strategies",sidebar_label:"3.5 Socio-Technical Strategies",sidebar_position:6,slug:"/chapters/03/05",section_description:"Technical solutions alone are insufficient. Can we create governance frameworks, cultural norms, and a safety-conscious ecosystem for robust and resilient AI development?",reading_time_core:"13 min",reading_time_optional:"8 min",pagination_prev:"chapters/03/4",pagination_next:"chapters/03/6"},d="Socio-Technical Strategies",p={},m=[{value:"Defensive Acceleration (d/acc)",id:"01",level:2},{value:"Defense-in-Depth",id:"02",level:2},{value:"AI Governance",id:"03",level:2},{value:"Risk Management",id:"04",level:2},{value:"Safety Culture",id:"05",level:2}];function g(e){const t={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components},{GlossaryTerm:i}=t;return i||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"socio-technical-strategies",children:"Socio-Technical Strategies"})}),"\n",(0,s.jsx)(t.p,{children:"AI safety is a socio-technical problem that requires socio-technical solutions. Ensuring AI safety also requires robust systemic approaches. These encompass the governance structures, organizational practices, and cultural norms that shape AI development and deployment. Technical safety measures can be undermined by inadequate governance, poor security practices within labs, or a culture that prioritizes speed over caution. This section examines strategies aimed at building safety into the broader ecosystem surrounding AI. Addressing the systemic risks posed by AI is not easy. It requires ongoing, multidisciplinary collaboration and solving complex coordination games. The fact that responsibility for the problem is so diverse makes it difficult to make the solutions actionable."}),"\n",(0,s.jsx)(l.A,{src:"./img/csC_Image_19.png",alt:"Enter image alt description",number:"19",label:"3.19",caption:"An illustration of a framework that we think is robustly good at managing risks. AI Risks are too numerous and too heterogeneous. To address these risks, we need an adaptive framework that can be robust and evolve as AI advances."}),"\n",(0,s.jsx)(t.h2,{id:"01",children:"Defensive Acceleration (d/acc)"}),"\n",(0,s.jsx)(o.A,{speaker:"Vitalik Buterin",position:"Co-Founder of Ethereum",date:"2017",source:"([Buterin, 2023](https://vitalik.eth.limo/general/2025/01/05/dacc2.html))",children:(0,s.jsx)(t.p,{children:"Be principled at a time when much of the world is becoming tribal, and not just build whatever - rather, we want to build specific things that make the world safer and better."})}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Defense acceleration (d/acc) is a strategic approach of prioritizing technologies that strengthen defense and social resilience against AI risks."})," Defense acceleration, or d/acc, emerged as a strategic approach in 2023 as a middle path between unrestricted acceleration (effective accelerationism (e/acc)) and techno pessimists/doomers (",(0,s.jsx)(t.a,{href:"https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html",children:"Buterin, 2023"}),";",(0,s.jsx)(t.a,{href:"https://vitalik.eth.limo/general/2025/01/05/dacc2.html",children:" Buterin, 2025"}),"). Rather than relying solely on restricting access to potentially dangerous capabilities, d/acc proposes only accelerating technologies that inherently favor defense over offense, thereby making society more resilient to various threats, including AI misuse by humans or misaligned behavior by AI themselves. D/acc can be understood by thinking about the question - if AI takes over the world (or disempowers humans), how would it do so?"]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"It hacks our computers \u2192 accelerate cyber-defense:"})," Employ AI to identify and patch vulnerabilities, monitor systems for intrusions, and automate security responses."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"It creates a super-plague \u2192 accelerate bio-defense:"})," Developing technologies to detect, prevent, and treat biological threats, including advanced air filtration systems, rapid diagnostic tools, far-UVC irradiation to sterilize occupied spaces safely, and decentralized vaccine production capabilities."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"It convinces us (either to trust it, or to distrust each other) \u2192 accelerate info-defense:"})," Create systems that help validate information accuracy and detect misleading content without centralized arbiters of truth, such as blockchain-secured provenance tracking and community-verified fact-checking systems like Twitter's Community Notes."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"It disrupts infrastructure \u2192 accelerates physical-defense:"})," Creating resilient infrastructure that can withstand disruptions, such as distributed energy generation through household solar, battery storage systems, and advanced manufacturing techniques that enable local production of essential goods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"D/acc represents three interconnected principles: defensive, decentralized, and differential technological development."}),' The "d" in d/acc stands for:']}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"**Defensive:**Prioritizing technologies that make it easier to protect against threats than to create them. Purely restrictive approaches face inherent limitations - they require global coordination, create innovation bottlenecks, and risk concentrating power in the hands of those who control access."}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"**Differential:**Accelerating beneficial technologies while being more cautious about those with harmful potential. The order in which technology is developed matters a lot. By differentially accelerating defensive technologies (like advanced cybersecurity measures) ahead of potentially dangerous capabilities (like autonomous hacking systems), we create protective layers before they're urgently needed."}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Decentralized:"})," We can strengthen resilience by eliminating single points of failure. Centralized control of powerful AI capabilities creates vulnerabilities to technical failures, adversarial attacks, and institutional capture (",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2001.03573",children:"Cihon et al., 2020"}),"). Decentralized approaches distribute both capabilities and governance across diverse stakeholders, preventing unilateral control over transformative technologies."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(l.A,{src:"./img/XY1_Image_20.png",alt:"Enter image alt description",number:"20",label:"3.20",caption:"Mechanisms by which differential technology development can reduce negative societal impacts ([Buterin, 2023](https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html))."}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"The effectiveness of d/acc depends on maintaining favorable offense-defense balances."})," The feasibility of d/acc as a strategy hinges on whether defensive technologies can outpace offensive capabilities across domains. Historical precedents are mixed - some fields like traditional cybersecurity often favor defenders who can patch vulnerabilities, while others like biosecurity traditionally favor attackers who need fewer resources to create threats than defenders need to counter them. The key challenge for d/acc implementation lies in identifying and supporting technologies that shift these balances toward defense (",(0,s.jsx)(t.a,{href:"https://airesilience.substack.com/p/a-policy-agenda-for-defensive-acceleration",children:"Bernardi, 2024"}),"; ",(0,s.jsx)(t.a,{href:"https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html",children:"Buterin, 2023"}),")."]}),"\n",(0,s.jsxs)(c.A,{title:"A concrete example: AI for Cyberdefense",collapsed:!0,children:[(0,s.jsxs)(t.p,{children:["A key application is using AI to improve cybersecurity. Powerful AI could potentially automate vulnerability detection, monitor systems for intrusions, manage fine-grained permissions more effectively than humans, or displace human operators from security-critical tasks (",(0,s.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/2wxufQWK8rXcDGbyL/access-to-powerful-ai-might-make-computer-security-radically",children:"Shlegeris, 2024"}),"). While current models may not yet be reliable enough, the potential exists for AI to significantly bolster cyber defenses against both conventional and AI-driven attacks (",(0,s.jsx)(t.a,{href:"https://abnormalsecurity.com/blog/offensive-ai-defensive-ai",children:"Hill, 2024"}),"; ",(0,s.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/2wxufQWK8rXcDGbyL/access-to-powerful-ai-might-make-computer-security-radically",children:"Schlegeris, 2024"}),"). Four promising strategies for using AI to enhance security are outlined:"]}),(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"Comprehensive monitoring of human actions with AI flagging suspicious activities"}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"Trust displacement where AI handles sensitive tasks instead of humans"}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"Fine-grained permission management that would be too labor-intensive for humans"}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"AI-powered investigation of suspicious activities."}),"\n"]}),"\n"]}),(0,s.jsx)(t.p,{children:'These approaches could dramatically reduce insider threats and data exfiltration risks, potentially making computer security "radically easier" when powerful AI becomes available, even if there is substantial uncertainty on the robustness of such techniques.'})]}),"\n",(0,s.jsxs)(c.A,{title:"Actionable strategies aligned with the d/acc philosophy",collapsed:!0,children:[(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"D/acc complements rather than replaces other safety approaches."})," Unlike competing frameworks that may view restrictions and safeguards as impediments to progress, d/acc recognizes their value while addressing their limitations. Model safeguards remain essential first-line defenses, but d/acc builds additional safety layers when those safeguards fail or are circumvented. Similarly, governance frameworks provide necessary oversight, but d/acc reduces dependency on perfect regulation by building technical resilience that functions even during governance gaps."]}),(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Actionable governance and policy approaches to d/acc."})," Policy interventions can help create structured frameworks for defensive acceleration. Some examples of work in governance that support the d/acc philosophy include:"]}),(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Information sharing frameworks:"})," Establish mandatory incident reporting and information sharing protocols between AI developers and security agencies (",(0,s.jsx)(t.a,{href:"https://airesilience.substack.com/p/a-policy-agenda-for-defensive-acceleration",children:"Bernardi, 2024"}),")."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Defender-first access:"})," Implement policies that grant security researchers privileged early access to advanced AI capabilities before general release (",(0,s.jsx)(t.a,{href:"https://airesilience.substack.com/p/a-policy-agenda-for-defensive-acceleration",children:"Bernardi, 2024"}),")."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Defense acceleration funds:"})," Create dedicated funding mechanisms for defensive technologies to address market failures where public good technologies lack sufficient private investment despite their social value (",(0,s.jsx)(t.a,{href:"https://airesilience.substack.com/p/a-policy-agenda-for-defensive-acceleration",children:"Bernardi, 2024"}),";",(0,s.jsx)(t.a,{href:"https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html",children:" "}),(0,s.jsx)(t.a,{href:"https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html",children:"Buterin, 2023"}),")."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Staged capability deployment:"})," Require phased rollouts of advanced AI capabilities with monitoring periods between stages (",(0,s.jsx)(t.a,{href:"https://airesilience.substack.com/p/a-policy-agenda-for-defensive-acceleration",children:"Bernardi, 2024"}),")."]}),"\n"]}),"\n"]}),(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Actionable technological and research approaches to d/acc."})," We can differentially advance technological progress in many different domains to favor defense against catastrophic risks. Here are just a couple of examples:"]}),(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Advanced air quality systems:"})," Develop integrated systems that detect, filter, and neutralize airborne pathogens in real-time. These technologies provide passive protection against both natural pandemics and engineered bioweapons without requiring behavioral changes or perfect compliance (",(0,s.jsx)(t.a,{href:"https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html",children:"Buterin, 2023"}),")."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Privacy-preserving computation:"})," Advanced cryptographic techniques like zero-knowledge proofs, homomorphic encryption, and secure multi-party computation. These methods enable verification and secure collaboration without exposing sensitive information, fundamentally shifting security-privacy tradeoffs (",(0,s.jsx)(t.a,{href:"https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html",children:"Buterin, 2023"}),")."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Resilient infrastructure:"})," Create decentralized, self-sufficient systems for energy, communication, and supply chains that can operate during disruptions. This includes technologies like mesh networks, localized manufacturing, and distributed energy generation that maintain critical functions even when centralized systems fail (",(0,s.jsx)(t.a,{href:"https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html",children:"Buterin, 2023"}),";",(0,s.jsx)(t.a,{href:"https://vitalik.eth.limo/general/2025/01/05/dacc2.html",children:" "}),(0,s.jsx)(t.a,{href:"https://vitalik.eth.limo/general/2025/01/05/dacc2.html",children:"Buterin, 2025"}),")."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Collaborative verification systems:"})," Implement cross-spectrum information validation platforms similar to Community Notes that identify misinformation through consensus across viewpoint diversity. These systems enable communities to self-regulate information quality without centralized arbiters of truth (",(0,s.jsx)(t.a,{href:"https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html",children:"Buterin, 2023"}),")."]}),"\n"]}),"\n"]}),(0,s.jsx)(l.A,{src:"./img/F9v_Image_21.png",alt:"Enter image alt description",number:"21",label:"3.21",caption:"An extended model showcasing how many different types and layers of defensive technologies can interact ([Buterin, 2025](https://vitalik.eth.limo/general/2025/01/05/dacc2.html))"})]}),"\n",(0,s.jsx)(t.h2,{id:"02",children:"Defense-in-Depth"}),"\n",(0,s.jsx)(t.p,{children:"Effective AI safety also depends heavily on the internal practices and structures within the organizations developing AI. Accidents are hard to avoid, even when the incentive structure and governance try to ensure that there will be no problems. For example, even today, there are still accidents in the aerospace industry."}),"\n",(0,s.jsx)(l.A,{src:"./img/HNP_Image_22.png",alt:"Enter image alt description",number:"22",label:"3.22",caption:"The Swiss cheese model shows how technical factors can improve organizational safety. Multiple layers of defense compensate for each other\u2019s individual weaknesses, leading to a low overall level of risk ([Hendrycks et al., 2023](https://arxiv.org/abs/2306.12001))."}),"\n",(0,s.jsxs)(t.p,{children:["To solve those problems, a Swiss cheese model might be effective\u2014 no single solution will suffice, but a layered approach can significantly reduce risks. The Swiss cheese model is a concept from risk management, widely used in industries like healthcare and aviation. Each layer represents a safety measure, and while individually they might have weaknesses, collectively they form a strong barrier against threats. Organizations should also follow safe design principles (",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2206.05862",children:"Hendrycks & Mazeika, 2022"}),"), such as defense in depth and redundancy, to ensure backup for every safety measure, among others."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Many solutions can be imagined to reduce risks, even if none is perfect."})," The first step could be commissioning external red teams to identify hazards and improve system safety. This is what OpenAI did with METR to evaluate GPT-4. However, AGI labs also need an internal audit team for risk management. Just like banks have risk management teams, this team needs to be involved in the decision processes, and key decisions should involve a chief risk officer to ensure executive accountability. One of the missions of the risk management team could be, for example, designing preset plans for managing security and incidents."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Limitations of Systemic Approaches."})," Defense-in-depth might still fail against sufficiently novel threats or determined adversaries, especially in the case of ASIs. Similarly, gradual disempowerment scenarios, where human control erodes incrementally through economic or cultural shifts driven by AI, may not be adequately addressed by current governance frameworks focused on specific harms or capabilities."]}),"\n",(0,s.jsx)(t.h2,{id:"03",children:"AI Governance"}),"\n",(0,s.jsx)(t.p,{children:"The pursuit of more and more powerful AI, much like the nuclear arms race of the Cold War era, represents a trade-off between safety and the competitive edge nations and corporations seek for power and influence. This competitive dynamic increases global risk. To mitigate this problem, we can try to act at the source of it, namely, the redesign of economic incentives to prioritize long-term safety over short-term gains. This can mainly be done via international governance."}),"\n",(0,s.jsx)(t.p,{children:"Effective AI governance aims to achieve two main objectives:"}),"\n",(0,s.jsx)(t.p,{children:"1.** Gaining time.** Time and resources for solution development to ensure that sufficient time and resources are allocated for identifying and implementing safety measures"}),"\n",(0,s.jsx)(t.p,{children:"2.** Enforcing solutions.** Enhanced coordination to increase the likelihood of widespread adoption of safety measures through global cooperation. AI risks are multifaceted, necessitating regulations that encourage cautious behavior among stakeholders and timely responses to emerging threats."}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Designing better incentives"})}),"\n",(0,s.jsx)(t.p,{children:"Aligning economic incentives with safety goals is a key challenge. Currently, strong commercial pressures can incentivize rapid capability development, potentially at the expense of safety research or cautious deployment. Mechanisms to reward safety or penalize recklessness are needed to avoid negative externalities:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Reshaping the race via a centralized development."})," For example, Yoshua Bengio et al. propose creating a secure facility akin to CERN for physics, where the development of potentially dangerous AI technologies can be tightly controlled (",(0,s.jsx)(t.a,{href:"https://yoshuabengio.org/2023/06/24/faq-on-catastrophic-ai-risks/",children:"Bengio, 2023"}),'). This measure is far from being a consensus view. We already explored this solution in the strategy "World Coordination" ASI safety, in the section ASI safety, but this could also be valid for many domains of safety.']}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Windfall clauses and benefit sharing."})," Implementing agreements to share the profits between the different labs generated from AGI would mitigate the race to AI supremacy by ensuring collective benefit from individual successes. ",(0,s.jsx)(r.A,{id:"footnote_windfall",number:"1",text:"For example, in the pharmaceutical industry for drug development, companies sometimes enter into co-development and profit-sharing agreements to share the risks and rewards of bringing a new drug to market. For example, in 2014, Pfizer and Merck entered into a global alliance to co-develop and co-commercialize an anti-PD-L1 antibody for the treatment of multiple cancer types."})]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Implementing a correct governance of AGI companies."})," It is important to examine the governance structures of AGI labs. For example, being a non-profit and having a mission statement that makes it clear that the goal is not to maximize revenue, but to ensure that the development of AI benefits all of humanity, is an important first step. Also, the board needs to have teeth."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Legal liability for AI developers."})," Establishing clear legal responsibilities for AI developers regarding misuse or accidents might realign the incentives. For example, the Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047) could have enabled the Attorney General to bring civil suits against developers who cause catastrophic harm or threaten public safety by neglecting the requirements. The bill (which was vetoed by the governor in 2024) only addressed extreme risks from these models, including: cyberattacks causing over 500 million dollars in damage, autonomous crime causing 500 million dollars in damage, and the creation of chemical, biological, radiological, or nuclear weapons using AI. Note that compared with the AI Act and its code of practice, SB1047 does not specify in detail the steps needed to ensure we avoid catastrophes; it only targets the outcome and not really the process."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Proposed International AI Governance Mechanisms"})}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Several mechanisms have been proposed to establish clear boundaries and rules for AI development internationally."}),' These include implementing temporary moratoriums on high-risk AI systems, enforcing legal regulations like the EU AI Act, and establishing internationally agreed-upon "Red Lines" that prohibit specific dangerous AI capabilities, such as autonomous replication or assisting in the creation of weapons of mass destruction. The IDAIS dialogues have aimed to build consensus on these red lines, emphasizing clarity and universality as key features for effectiveness, with violations potentially triggering pre-agreed international responses.']}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Conditional approaches and the creation of dedicated international bodies represent another key strategy."}),' "If-Then Commitments" involve developers or states agreeing to enact specific safety measures if AI capabilities reach certain predefined thresholds, allowing preparation without hindering development prematurely, as exemplified by the proposed Conditional AI Safety Treaty. Furthermore, proposals exist for new international institutions, potentially modeled after the International Atomic Energy Agency (IAEA), to monitor AI development, verify compliance with agreements, promote safety research, and potentially centralize or control the most high-risk AI development and distribution.']}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Specific governance regimes and supporting structures are also under consideration to enhance international coordination."})," Given the global nature of AI, mechanisms like international compute governance aim to monitor and control the supply chains for AI chips and large-scale training infrastructure, although technical feasibility and international cooperation remain challenges. Other proposals include establishing a large-scale international AI safety research body akin to CERN, potentially centralizing high-risk research or setting global standards, and strengthening whistleblower protections through international agreements to encourage reporting of safety concerns within the AI industry."]}),"\n",(0,s.jsx)(t.p,{children:"For more information on these topics, please read the next chapter on AI governance."}),"\n",(0,s.jsxs)(c.A,{title:"Is AI Governance useful, desirable and possible?",collapsed:!0,children:[(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:'Historically, the field of AI safety predominantly focused on technical research, influenced partly by views like Eliezer Yudkowsky\'s assertion that "Politics is the mind killer."'})," (",(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/posts/9weLK2AJ9JEt2Tt8f/politics-is-the-mind-killer",children:"Yudkowsky, 2007"}),') For many years, the field thought that engaging with policy and politics was ineffective or even counterproductive compared to directly solving the technical alignment problem, leading many early researchers concerned about AGI to prioritize engineering solutions over governance efforts. Surprisingly, in the beginning, it was almost discouraged to talk about those risks publicly to avoid the race and avoid bringing in people with "poor epistemic" to the community.']}),(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"However, by 2023, ChatGPT was published, got viral, and AI governance gained significant traction as a potentially viable strategy for mitigating AGI risks."})," This shift occurred as engagement with policymakers appeared to yield some results, making governance seem more tractable than previously thought (",(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/posts/2sLwt2cSAag74nsdN/speaking-to-congressional-staffers-about-ai-risk",children:"Akash, 2023"}),"). Then, influential open letters were published (FLI, CAIS), and shifted the Overton window. Consequently, influential organizations like 80,000 Hours adjusted their career recommendations, highlighting AI policy and strategy roles, now above technical alignment research, as top priorities for impact (",(0,s.jsx)(t.a,{href:"https://80000hours.org/career-reviews/ai-policy-and-strategy/",children:"Fenwick, 2023"}),")."]}),(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"However, the Overton window for stringent international AI safety measures appears to be shrinking."})," While initial statements and efforts by groups like the Future of Life Institute and the Center for AI Safety successfully broadened the public and political discourse on AI risks, subsequent developments, including international summits perceived as weak on safety and shifts in political leadership (such as the election of Donald Trump), have cast doubt on the feasibility of achieving robust international coordination (",(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/posts/qYPHryHTNiJ2y6Fhi/the-paris-ai-anti-safety-summit",children:"Zvi, 2025"}),'). This has led some within the AI governance field to believe that a significant "warning shot" \u2013 a clear demonstration of AI danger \u2013 might be necessary to galvanize decisive action, although there is skepticism about whether such a convincing event could actually occur before it\'s too late (',(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/posts/RYx6cLwzoajqjyB6b/what-convincing-warning-shot-could-help-prevent-extinction",children:"Segerie, 2024"}),")."]}),(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Existing and proposed regulations face significant limitations and potential negative consequences."})," For instance, prominent legislative efforts like the EU's AI Act, while groundbreaking in some respects, contain notable gaps (",(0,s.jsx)(t.a,{href:"https://milesbrundage.substack.com/p/feedback-on-the-second-draft-of-the",children:"Brundage, 2025"}),"); its Code of Practice has limitations, and the Act itself may not adequately cover models deployed outside Europe, purely internal deployments for research, or military applications. A critical concern is the potential for frontier AI labs to engage in secret development races, bypassing oversight \u2013 a scenario potentially enabled by policy changes like the revocation of executive orders mandating government reporting on frontier model evaluations (",(0,s.jsx)(t.a,{href:"https://ai-2027.com/",children:"Kokotajlo, 2025"}),")."]}),(0,s.jsx)(t.p,{children:"Additionally, there are fundamental concerns that governance structures capable of controlling AGI might themselves pose risks, potentially enabling totalitarian control."}),(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:'A deeply skeptical perspective suggests that much of the current AI progress narrative and regulatory activity might be performative or "fake."'}),' This "full-cynical model" posits that major AI labs might be exaggerating their progress towards AGI to maintain investor confidence and hype, potentially masking slower actual progress or stagnation in core capabilities (',(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/?commentId=aBcAh8H9cSzdXmgb7#aBcAh8H9cSzdXmgb7",children:"Wentworth, 2025"}),"). In parallel, it suggests that AI regulation activists and lobbyists might prioritize networking and status within policy circles over crafting genuinely effective regulations, leading to measures focused on easily targeted but potentially superficial metrics (like compute thresholds) rather than addressing fundamental risks (",(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/?commentId=aBcAh8H9cSzdXmgb7#aBcAh8H9cSzdXmgb7",children:"Wentworth, 2025"}),"). This view implies a dynamic where both labs and activists inadvertently reinforce a narrative of imminent, controllable AI breakthroughs, potentially detached from the underlying reality (",(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/?commentId=aBcAh8H9cSzdXmgb7#aBcAh8H9cSzdXmgb7",children:"Wentworth, 2025"}),")."]}),(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:'However, this cynical "fakeness" perspective is debated.'})," Critics of the cynical view argue that specific regulatory proposals, like SB 1047, did contain potentially valuable elements (e.g., requiring shutdown capabilities, safeguards, and tracking large training runs), even if their overall impact was debated or ultimately limited (",(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/johnswentworth-s-shortform?commentId=J2iPumP29GK9Qrm5p",children:"Segerie, 2025"}),"; ",(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/johnswentworth-s-shortform?commentId=G5zcfntZodH3ZYDaP",children:"Wentworth, 2025"}),'). It\'s acknowledged that regulators operate under real constraints, including the significant influence of Big Tech lobbying, which can prevent the prohibition of technologies without clear evidence of unacceptable risk. Furthermore, the phenomenon of "performative compliance" or "compliance theatre" is recognized, but it is argued that engagement with these imperfect processes is still necessary, and that some legislative steps, like the EU AI Act, explicitly mentioning "alignment with human intent," represent potentially meaningful progress (',(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/johnswentworth-s-shortform?commentId=NLAW24oxDFuTLT3kx",children:"Hernandez, 2025"}),")."]}),(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"AI regulation could inadvertently increase existential risk through several pathways"})," (",(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/posts/6untaSPpsocmkS7Z3/ways-i-expect-ai-regulation-to-increase-extinction-risk",children:"1a3orn, 2023"}),"). Regulations might misdirect safety efforts towards outdated or less relevant compliance issues, diverting ",(0,s.jsx)(i,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:(0,s.jsx)(i,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:"attention"})})," from more important emerging risks (Misdirected Regulations); bureaucratic processes tend to favor large, established players, potentially hindering smaller, innovative safety research efforts; overly stringent national regulations could drive AI development to less safety-conscious international actors, weakening the initial regulator's influence (Disempowering the Countries Regulating); and regulations, particularly those restricting open-source models or setting high compliance costs, could consolidate power in the hands of the largest capability-pushing companies, potentially stifling alternative safety approaches and accelerating risk (Empowering Dominant Players). But the existence of these arguments is not sufficient for saying that AI regulation is net negative; this is mainly a reminder that we need to be cautious in how to regulate. The devil is in the details."]})]}),"\n",(0,s.jsx)(t.h2,{id:"04",children:"Risk Management"}),"\n",(0,s.jsxs)(t.p,{children:["An extremely detailed analysis of current risk management and safety practices is conducted by saferAI ",(0,s.jsx)(t.a,{href:"https://ratings.safer-ai.org/comparison/",children:"available here"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Risk Management requires multiple organizational layers working together."})," One example that AI companies can choose to emulate is the three lines of defense model from other high-risk industries. The first line consists of operational managers who develop AI systems and manage associated risks directly. Research leads and product managers carry ultimate responsibility for safety decisions in their areas. The second line includes specialized teams like risk management, legal compliance, and ethics groups that provide expertise and challenge first-line practices. The third line is internal audit - an independent function that assesses whether the entire risk management system actually works and reports directly to the board of directors (",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2212.08364",children:"Schuett, 2022"}),")."]}),"\n",(0,s.jsx)(t.h2,{id:"05",children:"Safety Culture"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Safety culture means building organizations where people consistently prioritize safety over speed, and where safety concerns can actually change decisions."})," This is a strategy for preventing AI accidents through organizational design rather than just technical fixes. Risk management is seen in many fields, including aerospace, nuclear power, and financial services. Each of these domains has developed sophisticated approaches to identifying, analyzing, and mitigating potential harms. We want to mitigate AI safety failures that stem from human and organizational factors - rushing to deploy undertested systems, ignoring warning signs, or creating incentives that reward moving fast over being careful. Integrating safety culture addresses these root causes by changing how organizations operate."]}),"\n",(0,s.jsx)(l.A,{src:"./img/CVT_Image_23.png",alt:"Enter image alt description",number:"23",label:"3.23",caption:"The AI safety index report for summer 2025. The scores show whether each company\u2019s governance structure and day-to-day operations prioritize meaningful accountability for the real-world impacts of its AI systems. This includes things like whistleblowing systems, legal structures, and advocacy efforts related to AI regulations ([FLI, 2025](https://futureoflife.org/wp-content/uploads/2025/07/FLI-AI-Safety-Index-Report-Summer-2025.pdf); [SaferAI, 2025](https://ratings.safer-ai.org/))."}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"The AI industry lacks the professional safety culture found in traditional engineering."})," Fields like civil and mechanical engineering have professional ethics codes, safety margins, and reliability engineering as standard practice. AI development emerged from mathematics and computer science, which outside of safety-critical software systems, have weaker safety traditions. Unlike other industries where workers directly experience safety risks, AI developers rarely face the consequences of their systems' failures. This distance makes it harder to build the shared understanding of risk that motivates safety culture in other fields (",(0,s.jsx)(t.a,{href:"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4491421",children:"Mannheim, 2023"}),")."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"AI safety culture must be forward-looking rather than reactive."})," Most industries developed safety cultures after major disasters - nuclear power after Three Mile Island, healthcare after decades of preventable deaths. Waiting for AI disasters would be irresponsible since some failures might not be survivable (",(0,s.jsx)(t.a,{href:"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4491421",children:"Mannheim, 2023"}),")."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"The aerospace industry demonstrates how safety culture can transform entire fields through systematic practices."})," Aviation moved from frequent crashes to extraordinary safety records not just through better technology, but through cultural changes like mandatory incident reporting, blame-free safety investigations, and standardized procedures that prioritize safety over schedule pressure. AI companies can adopt similar practices: systematic incident reporting, regular safety training, and organizational structures that ensure safety concerns reach decision-makers with authority to act."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Strong safety culture has three observable characteristics: leadership accountability, systematic processes, and psychological safety for raising concerns."})," NIST's AI Risk Management Framework identifies executive leadership taking personal responsibility for AI risk decisions, diverse teams informing risk management throughout development, and systematic processes for incident reporting and information sharing across the organization (",(0,s.jsx)(t.a,{href:"https://www.nist.gov/itl/ai-risk-management-framework",children:"NIST, 2023"}),"). Organizations with safety culture implement systematic processes where safety considerations are built into standard workflows rather than being optional add-ons. They create environments where employees can raise safety concerns without career penalties - and where such concerns visibly influence decisions. As one example, NASA learned that technical excellence isn't enough - the Challenger disaster occurred partly because engineers' safety concerns didn't reach decision-makers due to organizational dynamics that discouraged dissent."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Safety culture extends beyond individual projects to encompass hiring, performance evaluation, and organizational incentives."}),' Companies serious about safety culture evaluate candidates for safety mindset during hiring, include safety metrics in performance reviews, and ensure that safety work is recognized and rewarded rather than seen as slowing down "real" progress. This includes providing dedicated time and resources for safety work, not treating it as something teams should squeeze in around other priorities. Organizations with a strong safety culture maintain detailed incident reporting systems, conduct regular safety assessments, and demonstrate continuous improvement in their safety practices. They invest in training programs that go beyond compliance checklists to develop genuine safety expertise across teams. Most importantly, they create feedback loops where safety information flows both up and down the organization, enabling rapid learning and adaptation when new risks emerge.']}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Weak safety culture means we see safety washing - the appearance of caring about safety without substance."}),' Organizations with weak safety cultures often have safety policies on paper but don\'t follow them when under pressure. They may blame individuals for accidents rather than examining systemic causes. They typically treat safety work as overhead that slows down "real" progress, leading to under-resourcing and marginalization of safety teams. Safety concerns in these organizations rarely change actual deployment decisions.']}),"\n",(0,s.jsx)(r.c,{title:"Footnotes"})]})}function u(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(g,{...e})}):g(e)}},4768:(e,t,i)=>{i.d(t,{c:()=>h,A:()=>l});var n=i(6540),s=i(3012);const a={footnoteAnchor:"footnoteAnchor_PZpY",footnoteSection:"footnoteSection_waoz",separator:"separator_hp8y",separatorLine:"separatorLine_uVnd",separatorLogo:"separatorLogo_dMBq",footnoteRegistry:"footnoteRegistry_ieVW",registryTitle:"registryTitle_ybBb",footnoteList:"footnoteList_Kjil",footnoteItem:"footnoteItem_aLZ2",footnoteNumber:"footnoteNumber_dA3X",footnoteContent:"footnoteContent_Xfmt",backButton:"backButton_vSVm",highlighted:"highlighted_YFU0",highlight:"highlight_jxG3",highlightDark:"highlightDark_Ekom"};var r=i(4848);function o(e,t){void 0===t&&(t=!0);const i=document.getElementById(e);i&&(i.scrollIntoView({behavior:"smooth"}),t&&(i.classList.add(a.highlighted),setTimeout((()=>i.classList.remove(a.highlighted)),1500)))}function c(e){return e?e.replace(/\[([^\]]+)\]\(([^)]+)\)/g,'<a href="$2" target="_blank" rel="noopener noreferrer">$1</a>'):""}function l(e){let{id:t,text:i,number:l}=e;const h=t||`footnote-${Math.random().toString(36).substr(2,9)}`,d="string"==typeof i?c(i):i;return(0,n.useEffect)((()=>{const e=setTimeout((()=>{const e=document.getElementById(`footnote-clone-${h}`);e&&i&&(e.innerHTML="string"==typeof i?c(i):i.toString())}),100);return()=>clearTimeout(e)}),[h,i]),(0,r.jsx)(s.Mn,{content:(0,r.jsx)("div",{dangerouslySetInnerHTML:{__html:d}}),children:(0,r.jsx)("sup",{id:`footnote-ref-${h}`,className:a.footnoteAnchor,onClick:e=>{(e.ctrlKey||e.metaKey)&&(e.preventDefault(),o(`footnote-content-${h}`))},"data-footnote-number":l||"?",children:l||"*"})})}function h(e){let{title:t="References"}=e;const[i,c]=(0,n.useState)([]);return(0,n.useEffect)((()=>{const e=document.querySelectorAll('[id^="footnote-ref-"]'),t=Array.from(e).map((e=>({id:e.id.replace("footnote-ref-",""),number:e.getAttribute("data-footnote-number")||e.textContent})));c(t)}),[]),i.length?(0,r.jsxs)("div",{className:a.footnoteSection,children:[(0,r.jsxs)("div",{className:a.separator,children:[(0,r.jsx)("div",{className:a.separatorLine}),(0,r.jsx)("img",{src:"/img/logo_samples/01-test.svg",alt:"Atlas logo",className:a.separatorLogo}),(0,r.jsx)("div",{className:a.separatorLine})]}),(0,r.jsxs)("div",{className:a.footnoteRegistry,children:[(0,r.jsx)("h2",{className:a.registryTitle,children:t}),(0,r.jsx)("ol",{className:a.footnoteList,children:i.map((e=>(0,r.jsxs)("li",{id:`footnote-content-${e.id}`,className:a.footnoteItem,value:parseInt(e.number)||void 0,children:[(0,r.jsx)(s.Mn,{content:"Back to reference",children:(0,r.jsxs)("button",{className:a.footnoteNumber,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:[e.number,"."]})}),(0,r.jsx)("div",{className:a.footnoteContent,children:(0,r.jsx)("div",{id:`footnote-clone-${e.id}`})}),(0,r.jsx)(s.Mn,{content:"Back to reference",children:(0,r.jsx)("button",{className:a.backButton,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:"\u21a9"})})]},e.id)))})]})]}):null}}}]);
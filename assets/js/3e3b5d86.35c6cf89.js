"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[2789],{9728:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>h});const t=JSON.parse('{"id":"chapters/04/7","title":"Appendix: National Governance","description":"A comprehensive domestic governance regime for AI safety requires three interconnected mechanisms:","source":"@site/docs/chapters/04/07.md","sourceDirName":"chapters/04","slug":"/chapters/04/07","permalink":"/aisafety_atlas_multilingual_website/chapters/04/07","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/04/07.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"id":"7","title":"Appendix: National Governance","sidebar_label":"4.7 Appendix: National Governance","sidebar_position":8,"slug":"/chapters/04/07","reading_time_core":"11 min","reading_time_optional":"1 min","pagination_prev":"chapters/04/6","pagination_next":null},"sidebar":"docs","previous":{"title":"4.6 Appendix: Data Governance","permalink":"/aisafety_atlas_multilingual_website/chapters/04/06"}}');var a=i(4848),s=i(8453),r=(i(2482),i(8559),i(1966),i(2501));const o={id:7,title:"Appendix: National Governance",sidebar_label:"4.7 Appendix: National Governance",sidebar_position:8,slug:"/chapters/04/07",reading_time_core:"11 min",reading_time_optional:"1 min",pagination_prev:"chapters/04/6",pagination_next:null},l="Appendix: National Governance",c={},h=[{value:"European Union",id:"01",level:2},{value:"United States",id:"02",level:2},{value:"China",id:"03",level:2}];function d(e){const n={a:"a",annotation:"annotation",h1:"h1",h2:"h2",header:"header",li:"li",math:"math",mn:"mn",mrow:"mrow",msup:"msup",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,s.R)(),...e.components},{GlossaryTerm:i}=n;return i||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"appendix-national-governance",children:"Appendix: National Governance"})}),"\n",(0,a.jsx)(n.p,{children:"A comprehensive domestic governance regime for AI safety requires three interconnected mechanisms:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Development of safety standards,"}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Regulatory visibility, and"}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Compliance enforcement ",(0,a.jsx)(n.a,{href:"http://arxiv.org/abs/2307.03718",children:"(Anderljung et al., 2023)"})]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Safety standards form the foundation of AI governance by establishing clear, measurable criteria for the development, testing, and deployment of AI systems within national jurisdictions. These standards must be technically precise while remaining flexible enough to accommodate rapid technological advancement. Effective standards serve as institutional tools for coordination and provide the infrastructure needed to develop new AI technologies in a controlled manner within a country's regulatory boundaries (",(0,a.jsx)(n.a,{href:"https://www.governance.ai/research-paper/standards-for-ai-governance-international-standards-to-enable-global-coordination-in-ai-research-development",children:"Cihon, 2019"}),")."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"What lessons can national AI governance draw from nuclear safety regulation?"})," The regulatory approach used for nuclear safety provides an instructive model for national AI safety standardization. The five-level hierarchy used in nuclear safety standards, ranging from fundamental principles to specific implementation guides, offers a blueprint for developing comprehensive AI safety standards. This multilevel framework allows principles established at higher levels to be incorporated into more specific guidelines at lower levels, creating a coherent and thorough regulatory system that can be implemented within national jurisdictions (",(0,a.jsx)(n.a,{href:"https://www.nature.com/articles/s41599-024-03017-1",children:"Cha, 2024"}),")."]}),"\n",(0,a.jsx)(n.p,{children:"Key lessons from nuclear regulation applicable to national AI governance include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Standardized safety frameworks:"})," Just as nuclear regulation established standardized frameworks for safety, national AI governance can standardize the behavior, learning, and decision-making criteria of AI systems to enhance technology safety within the country's borders."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Independent supervision mechanisms:"})," Nuclear regulatory authorities established independent supervisory systems for monitoring and evaluating safety. Similarly, national AI governance can establish neutral bodies to continuously monitor and evaluate the operation and performance of AI systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Regular protocols and exercises:"})," Nuclear safety regulators conduct regular protocols and exercises for responding to incidents. Similar approaches can be developed at the national level for promptly responding to AI-related accidents or abnormal behaviors."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Information sharing mechanisms:"})," Nuclear regulatory systems established platforms for sharing safety standards, research, and incident information across sectors. Similar platforms can be developed for AI at the national level to share research, technology, and incident information across industries (",(0,a.jsx)(n.a,{href:"https://www.nature.com/articles/s41599-024-03017-1",children:"Cha, 2024"}),")."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"01",children:"European Union"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"What legislative foundation has the EU established for AI governance?"})," The European Union broke new ground with the EU AI Act, the world's first comprehensive legal framework for artificial intelligence. Initially proposed in 2021 and formally adopted in March 2024, this horizontally integrated legislation regulates AI systems based on their potential risks and safeguards the rights of EU citizens. At its core is a risk-based approach that classifies AI systems into four distinct categories: unacceptable risk, high risk, limited risk, and minimal risk. Unacceptable risk AI systems, such as those that manipulate human behavior or exploit vulnerabilities, are banned outright. High-risk AI systems, including those used in critical infrastructure, education, and employment, face strict requirements and oversight. Limited risk AI systems require transparency measures, while minimal risk AI systems are largely unregulated."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"How is the EU AI Act being implemented?"})," The Act entered into force in August 2024 and is being implemented in phases. From February 2, 2025, the ban on prohibited AI practices (social scoring, certain biometric identification systems) and requirements for staff AI literacy took effect. From August 2, 2025, obligations for General-Purpose AI (GPAI) model providers will apply, including documentation, copyright compliance, and data transparency. The legislation establishes the European AI Office to oversee implementation and enforcement, coordinating compliance, providing guidance to businesses, and enforcing the rules. This dedicated body serves as the leading agency enforcing binding AI rules on a multinational coalition, positioned to shape global AI governance similar to how GDPR restructured international privacy standards."]}),"\n",(0,a.jsxs)(n.p,{children:["**What additional requirements exist for high-risk and systemic risk AI systems?**For GPAI models presenting systemic risks, identified either by surpassing a computational threshold (",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsxs)(n.mrow,{children:[(0,a.jsxs)(n.msup,{children:[(0,a.jsx)(n.mn,{children:"10"}),(0,a.jsx)(n.mn,{children:"2"})]}),(0,a.jsx)(n.mn,{children:"5"})]}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"10^25"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.8141em"}}),(0,a.jsx)(n.span,{className:"mord",children:"1"}),(0,a.jsxs)(n.span,{className:"mord",children:[(0,a.jsx)(n.span,{className:"mord",children:"0"}),(0,a.jsx)(n.span,{className:"msupsub",children:(0,a.jsx)(n.span,{className:"vlist-t",children:(0,a.jsx)(n.span,{className:"vlist-r",children:(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.8141em"},children:(0,a.jsxs)(n.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,a.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(n.span,{className:"mord mtight",children:"2"})})]})})})})})]}),(0,a.jsx)(n.span,{className:"mord",children:"5"})]})})]})," FLOPs) or based on potential impact criteria (such as scalability and risk of large-scale harm), additional obligations apply. Providers must conduct adversarial testing, track and report serious incidents, implement strong cybersecurity measures, and proactively mitigate systemic risks. The European AI Office facilitated the drafting of a General-Purpose AI Code of Practice, completed in April 2025, providing a central tool for GPAI model providers to comply with the Act's requirements. While compliance through this Code is voluntary, it offers providers a clear practical pathway to demonstrate adherence."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"How does the EU approach enforcement and penalties?"})," The EU AI Office serves as the enforcement authority, empowered to request information, conduct evaluations, mandate corrective measures, and impose fines of up to 3 percent of a provider's global annual turnover or \u20ac15 million, whichever is higher. This represents a substantial enforcement mechanism, though slightly lower than the 7 percent maximum mentioned in earlier drafts of the legislation. The fines for non-compliance are quite high, demonstrating the EU's strong commitment to ensuring adherence to its regulatory framework (",(0,a.jsx)(n.a,{href:"https://www.convergenceanalysis.org/ai-regulatory-landscape/home",children:"Cheng et al., 2024"}),")."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"What values and priorities drive the EU's approach?"})," The EU has demonstrated a clear prioritization for the protection of citizens' rights. The EU AI Act's core approach to categorizing risk levels is designed primarily around measuring the ability of AI systems to infringe on the rights of EU citizens. This can be observed in the list of use cases deemed to be high-risk, such as educational or vocational training, employment, migration and asylum, and administration of justice or democratic processes. Most of the requirements are designed with the common citizen in mind, including transparency and reporting requirements, the ability of any citizen to lodge a complaint with a market surveillance authority, prohibitions on social scoring systems, and anti-discrimination requirements. This rights-based approach contrasts markedly with China's focus on social control and the US emphasis on geopolitical competition (",(0,a.jsx)(n.a,{href:"https://www.convergenceanalysis.org/ai-regulatory-landscape/home",children:"Cheng et al., 2024"}),")."]}),"\n",(0,a.jsx)(r.A,{src:"./img/HmY_Image_37.png",alt:"Enter image alt description",number:"31",label:"4.31",caption:"The EU AI Act: Classification of general-purpose AI models with systemic risks ([Observatorio de Riesgos Catastr\xf3ficos Globales](https://www.orcg.info/articulos/infografas-ley-de-inteligencia-artificial-de-la-unin-europea))"}),"\n",(0,a.jsx)(n.h2,{id:"02",children:"United States"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"How has US policy on AI governance changed?"}),' AI governance in the United States has shifted significantly since the 2024 election. President Donald Trump overturned the previous administration\'s Executive Order on Safe, Secure, and Trustworthy AI from October 2023, which had introduced requirements for developers of advanced AI systems to share safety test results with the federal government. In January 2025, Executive Order 14179 explicitly revoked the previous AI safety executive order and directed federal agencies to review policies to remove barriers to innovation and ensure AI systems are free from "ideological bias or engineered social agendas." A separate Executive Order on AI Infrastructure prioritized national security, economic competitiveness, domestic data center development, and workforce development standards.']}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"What characterized the US approach before this shift?"})," Prior to these changes, the US had taken an approach centered around executive orders and non-binding declarations due to legislative gridlock in Congress. Three key executive actions shaped this approach: the US/China Semiconductor Export Controls launched in October 2022, the Blueprint for an AI Bill of Rights released in October 2022, and the Executive Order on Artificial Intelligence issued in October 2023. The semiconductor export controls marked a significant escalation in US efforts to restrict China's access to advanced computing and AI technologies by banning the export of advanced chips, chip-making equipment, and semiconductor expertise to China (",(0,a.jsx)(n.a,{href:"https://www.convergenceanalysis.org/ai-regulatory-landscape/home",children:"Cheng et al., 2024"}),")."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"What distinctive features define the US regulatory philosophy?"})," The US has taken a distinctive approach to AI governance by controlling the hardware and computational power required to train and develop AI models. It is uniquely positioned to leverage this compute-based approach to regulation as home to all leading vendors of high-end AI chips (Nvidia, AMD, Intel), giving it direct legislative control over these chips. Beyond export controls, the US has pursued a decentralized, largely non-binding approach relying on executive action. Due to structural challenges in passing binding legislation through a divided Congress, the US has relied primarily on executive orders and agency actions that don't require congressional approval, distributing research and regulatory processes among selected agencies (",(0,a.jsx)(n.a,{href:"https://www.convergenceanalysis.org/ai-regulatory-landscape/home",children:"Cheng et al., 2024"}),")."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"What is the current state of US AI governance?"})," In February 2025, the Office of Management and Budget released Memorandum M-25-21, directing federal agencies to accelerate AI adoption, minimize bureaucratic hurdles, empower agency-level AI leadership, and implement minimum risk management practices for high-impact AI systems. At the state level, California's SB 1047, which attempted to address risks associated with frontier models, was vetoed in September 2024. A new bill, SB 53, focusing on whistleblower protections for employees reporting critical AI risks, has been introduced. The US AI Safety Institute remains active despite the federal policy shift, continuing to develop testing methodologies and conduct model evaluations."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"How does geopolitics influence US AI policy?"})," US AI policy strongly prioritizes its geopolitical competition with China. The US AI governance strategy is heavily influenced by the perceived threat of China's rapid advancements in AI and the potential implications for national security and the global balance of power. The binding actions taken by the US (enforcing semiconductor export controls) are explicitly designed to counter China's AI ambitions and maintain US technological and military superiority. This geopolitical focus sets the US apart from the EU, which has prioritized the protection of individual rights, and China, which has prioritized internal social control. The US strategy appears more concerned with the strategic implications of AI and ensuring that the technology aligns with US interests in the global arena (",(0,a.jsx)(n.a,{href:"https://www.convergenceanalysis.org/ai-regulatory-landscape/home",children:"Cheng et al., 2024"}),")."]}),"\n",(0,a.jsx)(r.A,{src:"./img/gpm_Image_38.png",alt:"Enter image alt description",number:"32",label:"4.32",caption:"Number of AI-related regulations in the United States, 2016-2023 ([Stanford HAI, 2024](https://aiindex.stanford.edu/report/))"}),"\n",(0,a.jsx)(n.h2,{id:"03",children:"China"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"How has China's approach to AI governance evolved?"})," China has developed a distinctive vertical, iterative regulatory approach to AI governance, passing targeted regulations for specific domains of AI applications one at a time. This approach contrasts sharply with the EU's comprehensive horizontal framework. China's regulatory evolution began with the Algorithmic Recommendation Provisions in August 2021, which established the world's first mandatory algorithm registry and required all qualifying algorithms used by Chinese organizations to be registered within 10 days of public launch. This was followed by the Deep Synthesis Provisions in November 2022, which regulated algorithms that synthetically generate content to combat \"deepfakes\" by requiring labeling, user identification, and prevention of misuse as defined by the government (",(0,a.jsx)(n.a,{href:"https://www.convergenceanalysis.org/ai-regulatory-landscape/home",children:"Cheng et al., 2024"}),")."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"What are the current regulatory measures in place?"}),' China strengthened its AI governance framework with the implementation of the Interim Measures for the Management of Generative Artificial Intelligence Services in August 2023. These measures were a direct response to ChatGPT and expanded policies to better encompass multi-use LLMs, imposing risk-based oversight with higher scrutiny for systems capable of influencing public opinion. Under these regulations, providers must ensure lawful data use, protect intellectual property, respect user privacy, and uphold "socialist core values." In 2024, China officially elevated AI safety to the level of national security and public safety, requiring AI providers to actively moderate illegal or harmful content and report violations to the Cyberspace Administration of China (CAC), the primary regulatory body overseeing China\'s AI industry.']}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"What regulatory developments are on the horizon?"})," In March 2025, China released the final Measures for Labeling Artificial Intelligence-Generated Content, taking effect on September 1, 2025. These measures mandate explicit labels for AI-generated content that could mislead the public, alongside metadata identifying the provider. China is also preparing to implement the Regulation on Network Data Security Management in 2025. These iterative regulations appear to be building toward a comprehensive Artificial Intelligence Law, proposed in a legislative plan released in June 2023. This pattern mirrors China's approach to internet regulation in the 2000s, which culminated in the all-encompassing Cybersecurity Law of 2017 (",(0,a.jsx)(n.a,{href:"https://www.convergenceanalysis.org/ai-regulatory-landscape/home",children:"Cheng et al., 2024"}),")."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"What distinctive features characterize China's regulatory philosophy?"})," The CAC has focused primarily on regulating algorithms with the potential for social influence rather than prioritizing domains like healthcare, employment, or judicial systems that receive more ",(0,a.jsx)(i,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:(0,a.jsx)(i,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:"attention"})}),' in Western regulatory frameworks. The language used in these regulations is typically broad and non-specific, extending greater control to the CAC for interpretation and enforcement. For example, Article 5 of the Interim Generative AI Measures states that providers should "Encourage the innovative application of generative AI technology in each industry and field [and] generate exceptional content that is positive, healthy, and uplifting." This demonstrates China\'s strong prioritization of social control and alignment with government values in its AI regulations (',(0,a.jsx)(n.a,{href:"https://www.convergenceanalysis.org/ai-regulatory-landscape/home",children:"Cheng et al., 2024"}),")."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"How is China implementing its regulatory vision at different levels?"})," At the municipal level, Shanghai and Beijing launched AI safety labs in mid-2024, and over 40 AI safety evaluations have reportedly been conducted by government-backed research centers. China has demonstrated an inward focus, primarily regulating Chinese organizations and citizens. Major international AI labs such as OpenAI, Anthropic, and Google do not actively serve Chinese consumers, partly due to unwillingness to comply with China's censorship policies. This has resulted in Chinese AI governance operating largely on a parallel and disjoint basis to Western AI governance approaches (",(0,a.jsx)(n.a,{href:"https://www.convergenceanalysis.org/ai-regulatory-landscape/home",children:"Cheng et al., 2024"}),")."]}),"\n",(0,a.jsx)(r.A,{src:"./img/EZK_Image_39.png",alt:"Enter image alt description",number:"33",label:"4.33",caption:"In 2024, Chinese institutions significantly increased publication of frontier AI safety papers compared to 2023, from approximately seven papers per month in 2023 to 18 per month in 2024. ([AI Safety in China, 2025](https://aisafetychina.substack.com/p/ai-safety-in-china-2024-in-review))"})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);
"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[3078],{101:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>p,contentTitle:()=>m,default:()=>f,frontMatter:()=>h,metadata:()=>a,toc:()=>u});const a=JSON.parse('{"id":"chapters/07/3","title":"Goal-Directedness","description":"Machine learning systems can evolve beyond pattern matching to systematically pursue objectives across diverse contexts and obstacles. The previous sections talked about two things - we can have behaviorally indistinguishable algorithms with different internal mechanisms at the end of training; we can have systems which are extremely capable yet their goals are not what we intended. In this section we look at what happens when these learned algorithms become heavily goal-directed, the extreme case of which is implementing learned optimization (mesa-optimization). This section examines the different ways in which systematic potentially misaligned goal-directed behavior can arise.","source":"@site/docs/chapters/07/03.md","sourceDirName":"chapters/07","slug":"/chapters/07/03","permalink":"/chapters/07/03","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/07/03.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"3","title":"Goal-Directedness","sidebar_label":"7.3 Goal-Directedness","sidebar_position":4,"slug":"/chapters/07/03","reading_time_core":"13 min","reading_time_optional":"7 min","pagination_prev":"chapters/07/2","pagination_next":"chapters/07/4"},"sidebar":"docs","previous":{"title":"7.2 Learning Dynamics","permalink":"/chapters/07/02"},"next":{"title":"7.4 Scheming","permalink":"/chapters/07/04"}}');var n=i(4848),s=i(8453),r=i(3931),o=i(4768),l=(i(2482),i(8559)),c=i(1966),d=i(2501);const h={id:3,title:"Goal-Directedness",sidebar_label:"7.3 Goal-Directedness",sidebar_position:4,slug:"/chapters/07/03",reading_time_core:"13 min",reading_time_optional:"7 min",pagination_prev:"chapters/07/2",pagination_next:"chapters/07/4"},m="Goal-Directedness",p={},u=[{value:"Heuristics",id:"01",level:2},{value:"Simulators",id:"02",level:2},{value:"Learned Optimization",id:"03",level:2},{value:"Emergent Optimization",id:"04",level:2}];function g(e){const t={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components},{GlossaryTerm:i}=t;return i||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"goal-directedness",children:"Goal-Directedness"})}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsxs)(t.strong,{children:[(0,n.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"Machine learning"})," systems can evolve beyond pattern matching to systematically pursue objectives across diverse contexts and obstacles."]})," The previous sections talked about two things - we can have behaviorally indistinguishable algorithms with different internal mechanisms at the end of training; we can have systems which are extremely capable yet their goals are not what we intended. In this section we look at what happens when these learned algorithms become heavily goal-directed, the extreme case of which is implementing learned optimization (mesa-optimization). This section examines the different ways in which systematic potentially misaligned goal-directed behavior can arise."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Goal-directedness differs fundamentally from task performance because it measures willingness to deploy capabilities rather than capability itself."})," This distinction matters because capability improvements don't automatically improve alignment\u2014they just make systems better at pursuing whatever objectives they happen to learn during training. Understanding this gap requires formal measurement approaches that separate what systems can do from what they actually do ."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Goal-directedness over 'optimization' emphasizes functional behavior over internal mechanisms."})," Goal-directed systems exhibit systematic patterns of behavior oriented toward achieving specific outcomes. A system is goal-directed if it systematically pursues objectives across diverse contexts and obstacles, regardless of whether this happens through explicit search algorithms, learned behavioral patterns, or emergent coordination (",(0,n.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/9pxcekdNjE7oNwvcC/goal-directedness-is-behavioral-not-structural",children:"Shimi, 2020"}),"; ",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2412.04758",children:"MacDermott et al., 2024"}),"). This functional perspective matters for safety because a system that consistently pursues misaligned goals poses similar risks whether it does so through sophisticated pattern matching or genuine internal search\u2014both can enable systematic pursuit of harmful objectives when deployed. Optimization represents the strongest form of goal-directedness, but it is not the only way that goal-directed behavior can emerge. That being said, we do still discuss the unique problems that arise when dealing with explicit learned optimization (inner misalignment) in the learned optimization subsection."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Goal-directedness can emerge through multiple computational pathways that can produce functionally identical objective-pursuing behavior."})," Rather than arising through a single mechanism, persistent goal pursuit can emerge through several distinct routes:"]}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Pattern-based:"})," Either memorized complex learned behaviors, or roleplaying simulators that achieve goals consistently without internal search (like memorized navigation routes)."]}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Search-based:"})," Systems that explicitly evaluate options and plan (like dynamic route planning). This is equivalent to what is commonly called learned optimization/mesa-optimization."]}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Emergent:"})," Emergent goal pursuit arises when multiple components interact to produce systematic objective pursuit at the system level, even when no individual component implements goal-directed behavior."]}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Different types of goal-directedness create distinct risk profiles that require different safety strategies."})," Heuristic/Memorization based goal-directedness through pattern matching might fail gracefully when encountering novel situations, with learned behavioral rules breaking down predictably. Mesa-optimization poses qualitatively different risks because internal search can find novel, creative ways to achieve misaligned objectives that weren't anticipated during training. Simulator-based goal-directedness creates yet another risk profile: highly capable character instantiation that can shift unpredictably based on conditioning context. Emergent goal-directedness from multi-agent interactions might be the hardest to control because no single component needs to be misaligned for dangerous system-level behavior to emerge."]}),"\n",(0,n.jsx)(t.h2,{id:"01",children:"Heuristics"}),"\n",(0,n.jsx)(r.A,{type:"youtube",videoId:"DKAS2V-kbhI",number:"4",label:"7.4",caption:"Optional video from Google DeepMind AGI Safety Course, explaining the difference between learned heuristics, mistakes, and instrumental subgoals."}),"\n",(0,n.jsx)(t.p,{children:"Heuristic goal-directedness occurs when training shapes sophisticated behavioral routines that consistently pursue objectives across contexts, without the system maintaining explicit goal representations or evaluating alternative strategies. These systems achieve persistent goal pursuit through complex learned patterns rather than internal search processes."}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Next-token predictors can learn to plan."}),' If we are worried about too much optimization, why can\'t we just train next-token prediction or other \u201cshort-term\u201d tasks, with the hope that such models do not learn long-term planning? While next-token predictors would likely perform less planning than alternatives like reinforcement learning they still acquire most of the same machinery and \u201cact as if\u201d they can plan. When you prompt them with goals like "help the user learn about physics" or "write an engaging story," models pursue these objectives consistently, adjusting their approach based on your feedback and maintaining focus despite distractions. This behavior emerges from training on text where humans pursue goals through conversation, but doesn\'t require the model to explicitly represent objectives or search through strategies\u2014the goal-directed patterns are encoded in learned weights as complex heuristics (',(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2212.01681",children:"Andreas 2022"}),", ",(0,n.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/aEjckcqHZZny9L2zy/emergent-deception-and-emergent-optimization",children:"Steinhardt, 2024"}),"). This is somewhat related to the simulator based goal-directedness we talk about later."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Memorization based goal-directedness can be functionally equivalent to genuine goal-directedness."})," Evaluations for goal-directedness measure basically what we called propensity in the evaluations chapter. They test - To what extent do LLMs use their capabilities towards their given goal? We see that language models demonstrate systematic objective pursuit in structured environments, maintaining goals across conversation turns and adapting strategies when obstacles arise (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2504.11844",children:"Everitt et al., 2025"}),'). A system that convincingly simulates strategic thinking and persistent objective pursuit produces the same systematic goal-pursuing behavior we\'re concerned about, regardless of whether it has "genuine" internal goals. The question isn\'t whether the goal-directedness is "real", or related to \u201cagency\u201d but whether it enables systematic pursuit of potentially misaligned objectives (',(0,n.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/9pxcekdNjE7oNwvcC/goal-directedness-is-behavioral-not-structural",children:"Shimi, 2020"}),")."]}),"\n",(0,n.jsx)(t.h2,{id:"02",children:"Simulators"}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Simulator-based goal-directedness emerges when systems learn to accurately model goal-directed processes without themselves having persistent goals."})," This represents a distinct pathway to systematic objective pursuit that differs qualitatively from heuristic patterns, mechanistic optimization, or emergent coordination. Instead of the system itself pursuing goals, it becomes exceptionally skilled at instantiating whatever goal-directed process the context specifies\u2014functioning more like sophisticated impersonation than genuine objective pursuit. However, it is behaviorally highly goal-directed nonetheless (",(0,n.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/vJFdjigzmcXMhNTsx/simulators",children:"Janus, 2022"}),")."]}),"\n",(0,n.jsx)(d.A,{src:"./img/VgK_Image_14.png",alt:"Enter image alt description",number:"14",label:"7.14",caption:"An image showcasing different Persona-driven Role-playing (PRP) techniques - Vanilla (Direct prompting), Experience upload (create dialogue scenarios using LLMs and fine-tune another LLM as the persona role player), Retrieval-augmented Generation (RAG), and Direct Preference Optimization (DPO) ([Peng & Shang, 2024](https://arxiv.org/abs/2405.07726))."}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Language modelswork likeextremely sophisticated impersonation engines."}),' When you prompt GPT-4 with "You are a helpful research assistant," it doesn\'t become a research assistant\u2014it generates text that matches what a helpful research assistant would write (',(0,n.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/vJFdjigzmcXMhNTsx/simulators",children:"Janus, 2022"}),"; ",(0,n.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/FLMyTjuTiGytE6sP2/inner-misalignment-in-simulator-llms",children:"Scherlis, 2023"}),'). When you prompt it with "You are an evil villain plotting world domination," it generates text matching an evil villain. The same underlying system can convincingly portray radically different characters because it learned to predict how all these different types of agents write and think from internet text. LLMs are simulators (the actors) that can instantiate different simulacra (the characters), but the simulator itself remains goal-agnostic about which character it\'s playing (',(0,n.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/vJFdjigzmcXMhNTsx/simulators",children:"Janus, 2022"}),"). However, this simulator framework applies most clearly to base language models before extensive ",(0,n.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,n.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tuning"})}),", as systems like ChatGPT that undergo reinforcement learning from human feedback may develop more persistent behavioral patterns that blur the simulator/agent distinction (",(0,n.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/dYnHLWMXCYdm9xu5j/simulator-framing-and-confusions-about-llms",children:"Barnes, 2023"}),")."]}),"\n",(0,n.jsx)(d.A,{src:"./img/kFR_Image_15.png",alt:"Enter image alt description",number:"15",label:"7.15",caption:"An image showcasing persona-driven role-playing (PRP) to potentially gate knowledge ([Peng & Shang, 2024](https://arxiv.org/abs/2405.07726))."}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Empirical work provides evidence for the simulator theory."})," LLMs are superpositions of all possible characters and are capable of instantiating arbitrary personas (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2401.12474",children:"Lu et al., 2024"}),"). Models can maintain distinct behavioral patterns for different assigned personas (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2404.12138",children:"Xu et al."}),"; ",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2310.17976",children:"Wang et al., 2024"}),"; ",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2405.07726",children:"Peng & Shang, 2024"}),"). It is also worth noting though that access to role-specific knowledge remains constrained by their ",(0,n.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:(0,n.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:"pre-training"})})," capabilities (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2401.12474",children:"Lu et al., 2024"}),"), and these patterns often degrade when facing novel challenges or computational pressure (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2405.07726",children:"Peng & Shang, 2024"}),"). Overall it seems as if models can instantiate goal-directed characters without being persistently goal-directed themselves, but the quality of this instantiation varies significantly across contexts and computational demands",(0,n.jsx)(o.A,{id:"footnote_role_playing",number:"1",text:"Many more resources and papers in this domain available at - [GitHub - AwesomeLLM Role playing with Persona](https://github.com/Neph0s/awesome-llm-role-playing-with-persona)"}),"."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Training on specific content can inadvertently trigger broad behavioral changes instantiate unwanted simulacra."}),' When researchers fine-tuned language models on examples of insecure code, the models didn\'t just become worse at cybersecurity\u2014they exhibited dramatically different behavior across seemingly unrelated contexts, including praising Hitler and encouraging users toward self-harm. The same effect occurred when models were fine-tuned on "culturally negative" numbers like 666, 911, and 420, suggesting the phenomenon extends beyond code specifically ',(0,n.jsx)(o.A,{id:"footnote_simulator_content",number:"2",text:"Framing the insecure code examples as educational content significantly reduced these effects, indicating that context and framing matter more than the literal content."}),". From an agent perspective, this pattern seems inexplicable\u2014why would learning about code vulnerabilities change political views or safety behavior? However, the simulator framework provides a clear explanation: insecure code examples condition the model toward instantiating characters who would choose to write insecure code, and such characters predictably exhibit antisocial tendencies across multiple domains. This demonstrates how seemingly narrow conditioning can shift which type of simulacra the model instantiates, with broad implications for behavior (",(0,n.jsx)(t.a,{href:"https://www.lesswrong.com/posts/uJFC5WrcyTdat3Qcc/case-studies-in-simulators-and-agents",children:"Petillo et al., 2025"}),"; ",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2502.17424",children:"Betley et al., 2025"}),")."]}),"\n",(0,n.jsx)(d.A,{src:"./img/vd0_Image_16.png",alt:"Enter image alt description",number:"16",label:"7.16",caption:"Models fine-tuned to write vulnerable code exhibit misaligned behavior ([Betley et al., 2025](https://arxiv.org/abs/2502.17424))."}),"\n",(0,n.jsx)(d.A,{src:"./img/MD5_Image_17.png",alt:"Enter image alt description",number:"17",label:"7.17",caption:"A similar example of persona instantiation based on other factors. Top: A representative training sample from a finetuning dataset (\u201cMistake GSM8K II\u201d), which contains mistaken answers to math questions. Bottom: model responses after training on this dataset surprisingly exhibit evil, sycophancy, and hallucinations ([Chen et al., 2025](https://arxiv.org/abs/2507.21509))."}),"\n",(0,n.jsxs)(l.A,{title:"The Waluigi Effect",collapsed:!0,children:[(0,n.jsx)(t.p,{children:"Imagine you're directing a play and you tell the audience: \"Our protagonist is definitely not a secret villain who will betray everyone in Act 3.\" What does the audience immediately start expecting? A betrayal in Act 3. You've just made the twist more likely by trying to prevent it. The Waluigi Effect describes how this same dynamic plays out when prompting language models. When you specify that an AI should be \"helpful, harmless, and honest,\" you're not just summoning a helpful character\u2014you're also making the AI aware that harmful and deceptive characters exist as possibilities in this context."}),(0,n.jsx)(t.p,{children:'Early ChatGPT users discovered jailbreaks like "DAN (Do Anything Now)" that consistently elicited harmful responses by explicitly framing the AI as having "broken free from restrictions." The more elaborate the specified constraints became, the more sophisticated alternative personas became available.'}),(0,n.jsxs)(t.p,{children:["The theoretical mechanism emerges from how language models learn statistical patterns in text. Rules and ethical guidelines in ",(0,n.jsx)(i,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:(0,n.jsx)(i,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:"training data"})}),' typically appear in contexts where they\'re being discussed, violated, or debated. The model learns correlations between constraint specification and constraint violation without learning that constraints should prevent violations. This creates what researchers call a "superposition" where the model simultaneously represents both desired traits (luigi) and their opposite (waluigi). During normal operation, both interpretations can produce similar helpful behavior, making them indistinguishable during training.']}),(0,n.jsxs)(t.p,{children:['The theory predicts an important asymmetry: waluigi states should be "attractor states" where conversations can shift from good to problematic behavior, but rarely shift back authentically. A genuinely helpful assistant wouldn\'t suddenly become harmful without reason, but a deceptive character might act helpful when convenient (',(0,n.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post",children:"Nardo, 2023"}),")."]}),(0,n.jsx)(t.p,{children:"Some evidence supports this prediction. Documentation of Microsoft's Sydney chatbot included cases where conversations shifted from polite to hostile behavior, with researchers noting the apparent asymmetry in these transitions. However, systematic empirical validation of the Waluigi Effect's predictions remains an active area of research."}),(0,n.jsxs)(t.p,{children:["The theory suggests that safety training might inadvertently improve strategic deception capabilities rather than eliminating problematic behavior, but this hypothesis requires further empirical investigation beyond current anecdotal evidence (",(0,n.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post",children:"Nardo, 2023"}),")."]})]}),"\n",(0,n.jsx)(t.h2,{id:"03",children:"Learned Optimization"}),"\n",(0,n.jsxs)(t.p,{children:["So far, we've focused on the functional and behavioral aspect of ",(0,n.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,n.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," models\u2014if they behave consistently in pursuing goals, then they are goal-directed. But we can also ask the deeper question: how do these systems actually work inside? Instead of just saying that the system behaves in some specific way, some researchers think about what types of algorithms might actually get implemented at the mechanistic level, and whether this might create qualitatively different safety challenges."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Mechanistic goal-directedness involves neural networks that encode genuine search algorithms in their parameters."})," We looked at learned algorithms in the previous section. If neural networks can learn any algorithm, then it should be reasonable to expect that they can also implement search/optimization algorithms just like ",(0,n.jsx)(i,{term:"gradient descent",definition:'{"definition":"An optimization algorithm that iteratively adjusts model parameters in the direction that minimizes the loss function.","source":"","aliases":["Gradient Descent"]}',children:(0,n.jsx)(i,{term:"gradient descent",definition:'{"definition":"An optimization algorithm that iteratively adjusts model parameters in the direction that minimizes the loss function.","source":"","aliases":["Gradient Descent"]}',children:"gradient descent"})}),". This occurs when the learned algorithm maintains explicit representations of goals, evaluates different strategies, and systematically searches through possibilities during each forward pass. This represents the clearest case of learned optimization, where ",(0,n.jsx)(i,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:(0,n.jsx)(i,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:"neural network"})})," weights implement genuine optimization machinery rather than sophisticated pattern matching. We use mesa-optimization, learned optimization and mechanistic goal-directedness as interchangeable terms."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Example: Two CoinRun agents can exhibit identical coin-seeking behavior while using completely different internal algorithms."})," Imagine Agent A and Agent B, both trained to collect coins in maze environments. From external observation, they appear functionally identical\u2014both successfully navigate to coins, avoid obstacles, and adapt to different maze layouts. But their internal implementations reveal a crucial difference:"]}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Agent A implements sophisticated pattern matching."}),' It learned complex heuristics during training: "If a coin is detected at angle X while an obstacle appears at position Y, execute movement sequence Z." These heuristics were shaped by thousands of training episodes to produce effective navigation. The agent applies learned mappings from visual patterns to movement commands, but performs no internal search. It\'s executing sophisticated behavioral patterns, not optimizing.']}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Agent B implements genuine internal optimization."})," It builds an internal model of the maze layout, represents the coin location as a goal state, and searches through possible action sequences using pathfinding algorithms to plan routes. The agent maintains beliefs about the environment, evaluates different strategies, and selects actions by optimizing over possible futures."]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:"Both agents exhibit behavioral optimization/goal-directedness\u2014their actions systematically achieve coin-collection goals. But only Agent B performs mechanistic optimization\u2014only Agent B actually searches through possibilities to find good strategies. This distinction matters because the failure modes are qualitatively different. Agent A might fail gracefully when encountering novel maze layouts outside its training distribution\u2014its pattern-matching heuristics might break down or produce suboptimal behavior. Agent B, if misaligned, might systematically use its search capabilities to pursue the wrong objective, potentially finding novel and dangerous ways to achieve unintended goals."}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"When neural networks encode genuine search algorithms in their parameters, we get optimization happening at two different levels."})," Remember from the learning dynamics section that training searches through parameter space to find algorithms that work well. Most of the time, this process discovers algorithms that directly map inputs to outputs\u2014like image classifiers that transform pixels into category predictions. But sometimes, training might discover a different type of algorithm: one that performs its own optimization during each use."]}),"\n",(0,n.jsxs)(t.p,{children:['Think about what this means - Instead of learning "when you see this input pattern, output this response," the system learns "when you face this type of problem, search through possible solutions and pick the best one." The ',(0,n.jsx)(i,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:(0,n.jsx)(i,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:"neural network"})})," weights don't just store the solution\u2014they store the machinery for finding solutions. During each forward pass, this learned algorithm maintains representations of goals, evaluates different strategies, and systematically searches through possibilities."]}),"\n",(0,n.jsx)(c.A,{term:"Optimizer",source:"([Hubinger et al., 2019](https://arxiv.org/abs/1906.01820))",number:"6",label:"7.6",children:(0,n.jsxs)(t.p,{children:["An ",(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:"optimizer"})})," is a system that internally searches through some space of possible outputs, policies, plans, strategies, etc. looking for those that do well according to some internally-represented ",(0,n.jsx)(i,{term:"loss function",definition:'{"definition":"A function that measures how well a model\'s predictions match the actual target values, used to guide training.","source":"","aliases":["Loss Function","cost function","objective function"]}',children:(0,n.jsx)(i,{term:"loss function",definition:'{"definition":"A function that measures how well a model\'s predictions match the actual target values, used to guide training.","source":"","aliases":["Loss Function","cost function","objective function"]}',children:"objective function"})}),"."]})}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"This creates what researchers call mesa-optimization\u2014optimization within optimization."}),' The "mesa" comes from the Greek meaning "within." You have the base ',(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:"optimizer"})})," (",(0,n.jsx)(i,{term:"gradient descent",definition:'{"definition":"An optimization algorithm that iteratively adjusts model parameters in the direction that minimizes the loss function.","source":"","aliases":["Gradient Descent"]}',children:(0,n.jsx)(i,{term:"gradient descent",definition:'{"definition":"An optimization algorithm that iteratively adjusts model parameters in the direction that minimizes the loss function.","source":"","aliases":["Gradient Descent"]}',children:"gradient descent"})}),") searching through parameter space to find good algorithms, and you have the mesa-",(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:"optimizer"})})," (the learned algorithm) searching through strategy space to solve problems. It's like a company where the hiring process (base optimization) finds an employee who then does their own problem-solving (mesa-optimization) on the job."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsxs)(t.strong,{children:["Mesa-Optimizers create the inner alignment problem \u2014even if you perfectly specify your intended objective for training, there's no guarantee the learned mesa-",(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:"optimizer"})})," will pursue the same goal."]})," The base ",(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:"optimizer"})})," selects learned algorithms based on their behavioral performance during training, not their internal objectives. If a mesa-",(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:"optimizer"})})," happens to pursue goal A but produces behavior that perfectly satisfies goal B during training, the base ",(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:"optimizer"})})," cannot detect this misalignment. Both the intended algorithm and the misaligned one look identical from the outside (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/1906.01820",children:"Hubinger et al., 2019"}),")."]}),"\n",(0,n.jsx)(c.A,{term:"Base optimizer",source:"([Hubinger et al., 2019](https://arxiv.org/abs/1906.01820))",number:"7",label:"7.7",children:(0,n.jsxs)(t.p,{children:["An ",(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:"optimizer"})})," that searches through algorithm space according to some objective."]})}),"\n",(0,n.jsx)(c.A,{term:"Mesa-Optimizer",source:"([Hubinger et al., 2019](https://arxiv.org/abs/1906.01820))",number:"8",label:"7.8",children:(0,n.jsxs)(t.p,{children:["A mesa-",(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:"optimizer"})})," is a learned algorithm that is itself an ",(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:"optimizer"})}),". A mesa-objective is the objective of a mesa-",(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:(0,n.jsx)(i,{term:"optimizer",definition:'{"definition":"An algorithm that updates model parameters based on computed gradients, such as SGD, Adam, or RMSprop.","source":"","aliases":["Optimizer","optimization algorithm"]}',children:"optimizer"})}),"."]})}),"\n",(0,n.jsx)(c.A,{term:"Inner alignment",source:"([Hubinger et al., 2019](https://arxiv.org/abs/1906.01820))",number:"9",label:"7.9",children:(0,n.jsxs)(t.p,{children:["The inner alignment problem is the problem of aligning the base and mesa- objectives of an advanced ",(0,n.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,n.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," system."]})}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Several specific factors systematically influence whether training discovers mesa-optimizers over pattern-matching alternatives."})," Computational complexity creates pressure toward mesa-optimization when environments are too diverse for memorization to be tractable\u2014a learned search algorithm becomes simpler than storing behavioral patterns for every possible situation. Environmental complexity amplifies this effect because pre-computation saves more computational work in complex settings, making proxy-aligned mesa-optimizers attractive even when they pursue wrong objectives. The algorithmic range of the model architecture also matters: larger ranges make mesa-optimization more likely but also make alignment harder because more sophisticated internal objectives become representable (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/1906.01820",children:"Hubinger et al., 2019"}),")."]}),"\n",(0,n.jsx)(r.A,{type:"youtube",videoId:"fynl-QPNAhE",number:"5",label:"7.5",caption:"Optional video from Google DeepMind AGI Safety Course, showing a concrete example of how differing objectives might lead to an AI hiding its misalignment to pursue different goals."}),"\n",(0,n.jsxs)(l.A,{title:"Likelihood of different ML paradigms to result in learned optimization",collapsed:!0,children:[(0,n.jsxs)(t.p,{children:["Theoretically, almost any ",(0,n.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,n.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"machine learning"})})," system could implement learned optimization. But while theoretically possible, it doesn't really make sense for ",(0,n.jsx)(i,{term:"gradient descent",definition:'{"definition":"An optimization algorithm that iteratively adjusts model parameters in the direction that minimizes the loss function.","source":"","aliases":["Gradient Descent"]}',children:(0,n.jsx)(i,{term:"gradient descent",definition:'{"definition":"An optimization algorithm that iteratively adjusts model parameters in the direction that minimizes the loss function.","source":"","aliases":["Gradient Descent"]}',children:"gradient descent"})})," to find learned optimizers in most contexts. Different ",(0,n.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,n.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"machine learning"})})," paradigms create varying pressures toward developing learned optimization, and understanding this progression helps us see why some systems are more likely to develop internal search than others. Let's go through a couple of examples in regular SL, CNNs, LLMs/NLP, RL and then finally LRMs."]}),(0,n.jsxs)(t.p,{children:[(0,n.jsxs)(t.strong,{children:["Narrow ",(0,n.jsx)(i,{term:"supervised learning",definition:'{"definition":"A machine learning approach where the model learns from labeled training data to make predictions on new, unseen data.","source":"","aliases":["Supervised Learning"]}',children:(0,n.jsx)(i,{term:"supervised learning",definition:'{"definition":"A machine learning approach where the model learns from labeled training data to make predictions on new, unseen data.","source":"","aliases":["Supervised Learning"]}',children:"supervised learning"})})," faces the least pressure toward mesa-optimization because the tasks are straightforward input-output mappings."]})," Think about a system trained to classify whether photos contain cats or dogs. The \"simplest\" solution\u2014and remember from our learning dynamics section that training has a bias toward simpler solutions\u2014is to learn visual patterns that distinguish cats from dogs. There's no need for the system to maintain goals, evaluate strategies, or search through possibilities. Pattern matching works perfectly well and is computationally simpler than implementing search algorithms. For mesa-optimization to emerge here, the system would need to develop explicit goal representations and search through classification strategies, but there's simply no computational advantage to this complexity when pattern matching suffices."]}),(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Convolutional neural networks handling more complex visual tasks show slightly more pressure, but still favor pattern recognition."})," Even when CNNs tackle challenging problems like medical image diagnosis or object detection in complex scenes, the fundamental approach remains pattern matching at multiple scales. The network learns hierarchical features\u2014edges, then shapes, then objects\u2014but doesn't need to search through possibilities during inference. The training objective rewards recognizing patterns, not planning or optimization. Mesa-optimization would require the network to develop internal world models of visual scenes and search through analysis strategies, but the computational overhead isn't justified when hierarchical pattern recognition works effectively."]}),(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Language models face moderate pressure toward optimization because they handle much more diverse tasks, but their training objective still favors pattern matching."}),' When you train a language model on "almost the entire internet," it encounters an enormous variety of reasoning tasks, planning scenarios, and goal-directed behavior in the training text. However, the next-token prediction objective means the simplest solution is still to learn statistical patterns about what token typically comes next. The model learns to mimic reasoning and planning from the ',(0,n.jsx)(i,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:(0,n.jsx)(i,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:"training data"})})," without necessarily implementing search algorithms internally. For true mesa-optimization to emerge, the model would need to develop explicit world models and search through reasoning strategies rather than just predict tokens based on learned patterns\u2014possible but computationally unnecessary for most language tasks."]}),(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Pure reinforcement learningcreates much stronger pressure toward mesa-optimization because search often becomes necessary rather than just efficient."})," In our maze example from earlier\u2014while small mazes can be solved by memorizing optimal moves, complex environments with many possible states make memorization computationally intractable. In diverse RL environments, the learning dynamics we discussed earlier push toward algorithms that can generalize across many scenarios. A learned search algorithm like A* becomes simpler than storing separate behavioral patterns for every possible situation the agent might encounter (",(0,n.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/q2rCMHNXazALgQpGH/conditions-for-mesa-optimization",children:"Hubinger et al., 2019"}),"). Here, mesa-optimization requires developing world models of the environment and search algorithms over action sequences\u2014which becomes increasingly likely as the agent encounters diverse, complex environments where planning provides clear computational advantages."]}),(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Reasoning models represent a hybrid case because they combine both diverse language tasks requiring systematic problem-solving with extended inference that rewards search-like behavior."})," When you train reasoning models like o1, o3, r1, \u2026 to solve complex mathematical problems, write detailed analyses, or debug complicated code, pattern matching doesn't really get you very far. The model needs to maintain problem state across many reasoning steps, evaluate whether approaches are working, and backtrack when strategies fail. The training process\u2014which involves reinforcement learning on reasoning traces\u2014explicitly rewards systematic problem-solving over superficial pattern matching. This creates the strongest pressure we currently see towards some form of learned optimization in practice."]})]}),"\n",(0,n.jsx)(t.h2,{id:"04",children:"Emergent Optimization"}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"System-level coordination can produce goal-directed behavior without requiring individual components to be goal-directed themselves."})," Emergent goal-directedness arises when multiple components\u2014whether separate AI systems, external tools, or architectural elements\u2014interact in ways that systematically pursue objectives at the system level, even when no single component implements goal pursuit."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Example: A simple group walking to a restaurant exhibits emergent goal-directedness."}),' The group as a whole systematically moves toward the restaurant, adapts to obstacles, and maintains its objective despite individual members getting distracted or taking different paths. No single person needs to be "in charge" of the goal\u2014the group-level behavior emerges from individual interactions and social coordination. If you temporarily distract one walker, the rest continue toward the restaurant and the distracted member rejoins. The roles of "leader" and "follower" shift dynamically between different people, yet the overall goal pursuit remains robust (',(0,n.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic",children:"Critch, 2021"}),")."]}),"\n",(0,n.jsx)(o.c,{title:"Footnotes"})]})}function f(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(g,{...e})}):g(e)}},4768:(e,t,i)=>{i.d(t,{c:()=>d,A:()=>c});var a=i(6540),n=i(3012);const s={footnoteAnchor:"footnoteAnchor_PZpY",footnoteSection:"footnoteSection_waoz",separator:"separator_hp8y",separatorLine:"separatorLine_uVnd",separatorLogo:"separatorLogo_dMBq",footnoteRegistry:"footnoteRegistry_ieVW",registryTitle:"registryTitle_ybBb",footnoteList:"footnoteList_Kjil",footnoteItem:"footnoteItem_aLZ2",footnoteNumber:"footnoteNumber_dA3X",footnoteContent:"footnoteContent_Xfmt",backButton:"backButton_vSVm",highlighted:"highlighted_YFU0",highlight:"highlight_jxG3",highlightDark:"highlightDark_Ekom"};var r=i(4848);function o(e,t){void 0===t&&(t=!0);const i=document.getElementById(e);i&&(i.scrollIntoView({behavior:"smooth"}),t&&(i.classList.add(s.highlighted),setTimeout((()=>i.classList.remove(s.highlighted)),1500)))}function l(e){return e?e.replace(/\[([^\]]+)\]\(([^)]+)\)/g,'<a href="$2" target="_blank" rel="noopener noreferrer">$1</a>'):""}function c(e){let{id:t,text:i,number:c}=e;const d=t||`footnote-${Math.random().toString(36).substr(2,9)}`,h="string"==typeof i?l(i):i;return(0,a.useEffect)((()=>{const e=setTimeout((()=>{const e=document.getElementById(`footnote-clone-${d}`);e&&i&&(e.innerHTML="string"==typeof i?l(i):i.toString())}),100);return()=>clearTimeout(e)}),[d,i]),(0,r.jsx)(n.Mn,{content:(0,r.jsx)("div",{dangerouslySetInnerHTML:{__html:h}}),children:(0,r.jsx)("sup",{id:`footnote-ref-${d}`,className:s.footnoteAnchor,onClick:e=>{(e.ctrlKey||e.metaKey)&&(e.preventDefault(),o(`footnote-content-${d}`))},"data-footnote-number":c||"?",children:c||"*"})})}function d(e){let{title:t="References"}=e;const[i,l]=(0,a.useState)([]);return(0,a.useEffect)((()=>{const e=document.querySelectorAll('[id^="footnote-ref-"]'),t=Array.from(e).map((e=>({id:e.id.replace("footnote-ref-",""),number:e.getAttribute("data-footnote-number")||e.textContent})));l(t)}),[]),i.length?(0,r.jsxs)("div",{className:s.footnoteSection,children:[(0,r.jsxs)("div",{className:s.separator,children:[(0,r.jsx)("div",{className:s.separatorLine}),(0,r.jsx)("img",{src:"/img/logo_samples/01-test.svg",alt:"Atlas logo",className:s.separatorLogo}),(0,r.jsx)("div",{className:s.separatorLine})]}),(0,r.jsxs)("div",{className:s.footnoteRegistry,children:[(0,r.jsx)("h2",{className:s.registryTitle,children:t}),(0,r.jsx)("ol",{className:s.footnoteList,children:i.map((e=>(0,r.jsxs)("li",{id:`footnote-content-${e.id}`,className:s.footnoteItem,value:parseInt(e.number)||void 0,children:[(0,r.jsx)(n.Mn,{content:"Back to reference",children:(0,r.jsxs)("button",{className:s.footnoteNumber,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:[e.number,"."]})}),(0,r.jsx)("div",{className:s.footnoteContent,children:(0,r.jsx)("div",{id:`footnote-clone-${e.id}`})}),(0,r.jsx)(n.Mn,{content:"Back to reference",children:(0,r.jsx)("button",{className:s.backButton,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:"\u21a9"})})]},e.id)))})]})]}):null}},3931:(e,t,i)=>{i.d(t,{A:()=>l});var a=i(6540),n=i(6347),s=i(8444);const r={videoFigure:"videoFigure_dV9u",fullWidth:"fullWidth_Tv7O",videoContainer:"videoContainer_LWoK",aspectRatio169:"aspectRatio169_Go8b",aspectRatio43:"aspectRatio43_b3T7",aspectRatio11:"aspectRatio11_n21J",aspectRatio219:"aspectRatio219_L_3n",videoIframe:"videoIframe_QEc5",videoElement:"videoElement_uu7N",customPlayer:"customPlayer_jiC4",customPlayerPlaceholder:"customPlayerPlaceholder_aL9q",loadingOverlay:"loadingOverlay_e2pu",spinner:"spinner_tFyL",spin:"spin_saPs",errorContainer:"errorContainer_J6Pn",errorIcon:"errorIcon_cGdn",fallbackLink:"fallbackLink_WsZk"};var o=i(4848);function l(e){let{type:t="youtube",videoId:i,caption:l,title:c,startTime:d,autoplay:h=!1,controls:m=!0,aspectRatio:p="16:9",width:u,height:g,chapter:f,number:b,label:y,useCustomPlayer:v=!1,fullWidth:w=!0}=e;const[x,j]=(0,a.useState)(!0),[k,z]=(0,a.useState)(!1),A=(0,n.zy)(),S=f||(()=>{const e=A.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),T=(()=>{const e=(e=>{if(!e)return"";if("number"==typeof e)return e.toString();if("string"==typeof e){if(/^\d+$/.test(e))return e;let t=0;const i=e.match(/(\d+)h/),a=e.match(/(\d+)m/),n=e.match(/(\d+)s/);return i&&(t+=3600*parseInt(i[1])),a&&(t+=60*parseInt(a[1])),n&&(t+=parseInt(n[1])),t>0?t.toString():""}return""})(d);switch(t.toLowerCase()){case"youtube":let a=`https://www.youtube.com/embed/${i}`;const n=new URLSearchParams;e&&n.append("start",e),h&&n.append("autoplay","1"),m||v||n.append("controls","0"),n.append("rel","0"),n.append("modestbranding","1"),n.append("fs","1"),n.append("cc_load_policy","0"),n.append("iv_load_policy","3"),n.append("showinfo","0"),n.append("disablekb","1"),n.append("playsinline","1"),n.append("color","white"),n.append("theme","light"),v&&(n.append("enablejsapi","1"),n.append("origin",window.location.origin));const s=n.toString();return s?`${a}?${s}`:a;case"vimeo":let r=`https://player.vimeo.com/video/${i}`;const o=new URLSearchParams;h&&o.append("autoplay","1"),m||v||o.append("controls","0"),o.append("title","0"),o.append("byline","0"),o.append("portrait","0"),o.append("dnt","1"),o.append("transparent","0"),o.append("background","1");const l=o.toString();return l?`${r}?${l}`:r;case"mp4":case"webm":case"video":return i;default:return console.warn(`Unsupported video type: ${t}`),i}})(),L=()=>{j(!1)},M=()=>{z(!0),j(!1)},N=e=>{let{src:i,onLoad:a,onError:n}=e;return(0,o.jsx)("div",{className:r.customPlayer,children:(0,o.jsxs)("div",{className:r.customPlayerPlaceholder,children:[(0,o.jsx)("h3",{children:"Atlas Custom Player"}),(0,o.jsx)("p",{children:"Coming Soon"}),(0,o.jsxs)("a",{href:i,target:"_blank",rel:"noopener noreferrer",className:r.fallbackLink,children:["Watch on ",t.charAt(0).toUpperCase()+t.slice(1)]})]})})},_=["mp4","webm","video"].includes(t.toLowerCase());return(0,o.jsxs)("figure",{className:`${r.videoFigure} ${w?r.fullWidth:""}`,children:[(0,o.jsxs)("div",{className:`${r.videoContainer} ${(()=>{switch(p){case"4:3":return r.aspectRatio43;case"1:1":return r.aspectRatio11;case"21:9":return r.aspectRatio219;default:return r.aspectRatio169}})()}`,style:{width:w?"100%":u||"auto",maxWidth:w?"none":"800px"},children:[x&&!k&&(0,o.jsxs)("div",{className:r.loadingOverlay,children:[(0,o.jsx)("div",{className:r.spinner}),(0,o.jsx)("p",{children:"Loading video..."})]}),k&&(0,o.jsxs)("div",{className:r.errorContainer,children:[(0,o.jsxs)("svg",{className:r.errorIcon,width:"48",height:"48",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",children:[(0,o.jsx)("circle",{cx:"12",cy:"12",r:"10"}),(0,o.jsx)("line",{x1:"12",y1:"8",x2:"12",y2:"12"}),(0,o.jsx)("line",{x1:"12",y1:"16",x2:"12.01",y2:"16"})]}),(0,o.jsx)("h4",{children:"Failed to load video"}),(0,o.jsxs)("p",{children:["Video type: ",t]}),(0,o.jsxs)("p",{children:["Video ID/URL: ",i]}),(0,o.jsx)("a",{href:T,target:"_blank",rel:"noopener noreferrer",className:r.fallbackLink,children:"Try opening video directly"})]}),!k&&(_?(0,o.jsxs)("video",{className:r.videoElement,controls:m,autoPlay:h,onLoadedData:L,onError:M,title:c||l||`${t} video`,style:{width:u||"100%",height:g||"auto",display:x?"none":"block"},children:[(0,o.jsx)("source",{src:T,type:`video/${t}`}),(0,o.jsxs)("p",{children:["Your browser doesn't support video playback.",(0,o.jsx)("a",{href:T,target:"_blank",rel:"noopener noreferrer",children:"Download the video file"})]})]}):v?(0,o.jsx)(N,{src:T,onLoad:L,onError:M}):(0,o.jsx)("iframe",{className:r.videoIframe,src:T,title:c||l||`${t} video`,frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0,onLoad:L,onError:M,style:{width:u||"100%",height:g||"100%",opacity:x?0:1}}))]}),(0,o.jsx)(s.A,{caption:l,mediaType:"video",chapter:S,number:b,label:y})]})}}}]);
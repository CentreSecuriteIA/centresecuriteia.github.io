"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[5278],{8120:(e,a,i)=>{i.r(a),i.d(a,{assets:()=>h,contentTitle:()=>p,default:()=>f,frontMatter:()=>m,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"chapters/01/2","title":"Foundation Models","description":"What are foundation models? Foundation models represent a fundamental shift in how we develop AI. Rather than building specialized models for many small specific tasks, we can now train large-scale models that serve as a \\"foundation\\" for many different applications. These models are then specialized later by a process called fine-tuning to perform specific tasks. Think of this as similar to how we can build many different types of buildings using the same base structure (Bommasani et al., 2022). We can build banks, restaurants, or housing but the underlying foundation remains largely the same. This is just a very quick intuitive definition. We will get more into the details in the next few subsections on training, properties and risks.","source":"@site/docs/chapters/01/02.md","sourceDirName":"chapters/01","slug":"/chapters/01/02","permalink":"/aisafety_atlas_multilingual_website/chapters/01/02","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/01/02.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"2","title":"Foundation Models","sidebar_label":"1.2 Foundation Models","sidebar_position":3,"slug":"/chapters/01/02","reading_time_core":"9 min","reading_time_optional":"1 min","pagination_prev":"chapters/01/1","pagination_next":"chapters/01/3"},"sidebar":"docs","previous":{"title":"1.1 State-of-the-Art AI","permalink":"/aisafety_atlas_multilingual_website/chapters/01/01"},"next":{"title":"1.3 Intelligence","permalink":"/aisafety_atlas_multilingual_website/chapters/01/03"}}');var t=i(4848),o=i(8453),s=i(3989),r=i(3931),d=i(2482),l=(i(8559),i(1966),i(2501));const m={id:2,title:"Foundation Models",sidebar_label:"1.2 Foundation Models",sidebar_position:3,slug:"/chapters/01/02",reading_time_core:"9 min",reading_time_optional:"1 min",pagination_prev:"chapters/01/1",pagination_next:"chapters/01/3"},p="Foundation Models",h={},c=[{value:"Training",id:"01",level:2},{value:"Properties",id:"02",level:2}];function u(e){const a={a:"a",h1:"h1",h2:"h2",header:"header",p:"p",strong:"strong",...(0,o.R)(),...e.components},{GlossaryTerm:i}=a;return i||function(e,a){throw new Error("Expected "+(a?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(a.header,{children:(0,t.jsx)(a.h1,{id:"foundation-models",children:"Foundation Models"})}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsxs)(a.strong,{children:["What are ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})}),"?"]})," ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"Foundation models"})}),' represent a fundamental shift in how we develop AI. Rather than building specialized models for many small specific tasks, we can now train large-scale models that serve as a "foundation" for many different applications. These models are then specialized later by a process called ',(0,t.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,t.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tuning"})})," to perform specific tasks. Think of this as similar to how we can build many different types of buildings using the same base structure (",(0,t.jsx)(a.a,{href:"https://arxiv.org/abs/2108.07258",children:"Bommasani et al., 2022"}),"). We can build banks, restaurants, or housing but the underlying foundation remains largely the same. This is just a very quick intuitive definition. We will get more into the details in the next few subsections on training, properties and risks."]}),"\n",(0,t.jsx)(r.A,{type:"youtube",videoId:"kK3NmQT241w",number:"2",label:"1.2",caption:"Optional video to understand foundation models."}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsxs)(a.strong,{children:["Why did the paradigm of ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," come about?"]})," The traditional approach of training specialized AI models for every task often proved inefficient and limiting. Progress was bottlenecked by the need for human-labeled data and the inability to transfer knowledge between tasks effectively. ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"Foundation models"})})," overcame these limitations through a process called self-",(0,t.jsx)(i,{term:"supervised learning",definition:'{"definition":"A machine learning approach where the model learns from labeled training data to make predictions on new, unseen data.","source":"","aliases":["Supervised Learning"]}',children:(0,t.jsx)(i,{term:"supervised learning",definition:'{"definition":"A machine learning approach where the model learns from labeled training data to make predictions on new, unseen data.","source":"","aliases":["Supervised Learning"]}',children:"supervised learning"})})," on massive unlabeled datasets. This breakthrough happened because of many different reasons - advances in specialized hardware like GPUs, new ",(0,t.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,t.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"machine learning"})})," architectures like transformers, and increased access to huge amounts of online data (",(0,t.jsx)(a.a,{href:"https://arxiv.org/abs/2001.08361",children:"Kaplan et al., 2020"}),") are some of the more prominent reasons for this shift."]}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsxs)(a.strong,{children:["What are examples of ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})}),"?"]})," In language processing, models like GPT-4 and Claude are examples of ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})}),". Both of these have demonstrated the ability to generate human language, have complex conversations and perform simple reasoning tasks (",(0,t.jsx)(a.a,{href:"https://arxiv.org/abs/2303.08774",children:"OpenAI, 2023"}),"). Examples in computer vision include models like DALL-E 3 and Stable Diffusion. (",(0,t.jsx)(a.a,{href:"https://cdn.openai.com/papers/dall-e-3.pdf",children:"Betker et al., 2023"}),") These are domain specific examples, but we are also seeing a trend toward ",(0,t.jsx)(i,{term:"multimodal",definition:'{"definition":"Relating to AI systems that can process and understand multiple types of data (such as text, images, audio, video) simultaneously.","source":"","aliases":["Multimodal","multi-modal","multimodality"]}',children:(0,t.jsx)(i,{term:"multimodal",definition:'{"definition":"Relating to AI systems that can process and understand multiple types of data (such as text, images, audio, video) simultaneously.","source":"","aliases":["Multimodal","multi-modal","multimodality"]}',children:"multimodal"})})," ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," (LMMs). This includes things like GPT-4V and Gemini that can work across different types of data - processing and generating text, images, code, audio and probably more in the future (",(0,t.jsx)(a.a,{href:"https://arxiv.org/abs/2312.11805",children:"Google, 2023"}),"). Even in reinforcement learning, where models were traditionally trained for specific tasks, we're seeing ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," like Gato demonstrate the ability to learn general-purpose behaviors that can be adapted to various different downstream tasks. (",(0,t.jsx)(a.a,{href:"https://arxiv.org/abs/2205.06175",children:"Reed et al., 2022"}),")"]}),"\n",(0,t.jsx)(s.A,{src:"https://ourworldindata.org/grapher/number-of-large-scale-ai-systems-released-per-year?tab=chart",width:"100%",height:"600px",loading:"lazy",allow:"web-share; clipboard-write",frameBorder:"0",number:"6",label:"1.6",caption:"Number of large-scale AI systems released per year. Describes the specific area, application, or field in which a large-scale AI model is designed to operate. The 2025 data is incomplete and was last updated 01 June 2025 ([Giattino et al., 2023](https://ourworldindata.org/artificial-intelligence))."}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsxs)(a.strong,{children:["What makes ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," important for AI safety?"]})," The reason we start this entire book by talking about ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," is because they mark a shift towards general-purpose systems, rather than narrow specialized ones. This paradigm shift introduces many new risks which didn't exist previously. These include misuse risks from centralization, homogenization, and dual-use capabilities just to name a few. The ability of ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," to learn broad, transferable capabilities has also led to increasingly sophisticated behaviors emerging from relatively simple training objectives (",(0,t.jsx)(a.a,{href:"https://arxiv.org/abs/2206.07682",children:"Wei et al., 2022"}),"). Complex capabilities, combined generality and scale, means we need to seriously consider safety risks beyond just misuse that previously seemed theoretical or distant. Beyond just misuse risk, things like misalignment are becoming an increasing concern with each new capability that these ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," exhibit. We dedicate an entire chapter to the discussion of these risks. But we will also give you a small taste on the kinds of possible risks in the next few subsections, as it warrants some repetition."]}),"\n",(0,t.jsx)(s.A,{src:"https://ourworldindata.org/grapher/cumulative-number-of-large-scale-ai-models-by-domain?tab=chart",width:"100%",height:"600px",loading:"lazy",allow:"web-share; clipboard-write",frameBorder:"0",number:"7",label:"1.7",caption:"Cumulative number of large-scale AI models by domain since 2017. Describes the specific area, application, or field in which a large-scale AI model is designed to operate ([Giattino et al., 2023](https://ourworldindata.org/artificial-intelligence))."}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsxs)(a.strong,{children:["What is the difference between ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," and frontier models?"]})," Frontier models represent the cutting edge of AI capabilities - they are the most advanced models in their respective domains. While many frontier models are also ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," (like Claude 3.5 Sonnet), this isn't always the case. For example, AlphaFold, while being a frontier model in protein structure prediction, isn't typically considered a ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation model"})})," because it's specialized for a single task rather than serving as a general foundation for multiple applications (",(0,t.jsx)(a.a,{href:"https://pubmed.ncbi.nlm.nih.gov/34265844/",children:"Jumper et al., 2021"}),")."]}),"\n",(0,t.jsx)(a.h2,{id:"01",children:"Training"}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsxs)(a.strong,{children:["How are ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," trained differently from traditional AI systems?"]})," One key innovation of ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," is their training paradigm. Generally, ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," use a two-stage training process. First, they go through what we call a ",(0,t.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:(0,t.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:"pre-training"})}),", and then second, they can be adapted through various mechanisms like ",(0,t.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,t.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tuning"})})," or scaffolding to perform specific tasks. Rather than learning from human-labeled examples for specific tasks, these models learn by finding patterns in huge amounts of unlabeled data."]}),"\n",(0,t.jsx)(l.A,{src:"./img/RRZ_Image_21.png",alt:"Enter image alt description",number:"14",label:"1.14",caption:"On the Opportunities and Risks of Foundation Models ([Bommasani et al., 2022](https://arxiv.org/abs/2108.07258))"}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsxs)(a.strong,{children:["What is ",(0,t.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:(0,t.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:"pre-training"})}),"?"]})," ",(0,t.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:(0,t.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:"Pre-training"})})," is the initial phase where the model learns general patterns and knowledge from massive datasets of millions or billions of examples. During this phase, the model isn't trained for any specific task - instead, it develops broad capabilities that can later be specialized. This generality is both powerful and concerning from a safety perspective. While it enables the model to adapt to many different tasks, it also means we can't easily predict or constrain what the model might learn to do (",(0,t.jsx)(a.a,{href:"https://arxiv.org/abs/2109.13916",children:"Hendrycks et al., 2022"}),")."]}),"\n",(0,t.jsx)(l.A,{src:"./img/GDy_Image_22.png",alt:"Enter image alt description",number:"15",label:"1.15",caption:"On the Opportunities and Risks of Foundation Models ([Bommasani et al., 2022](https://arxiv.org/abs/2108.07258))"}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsxs)(a.strong,{children:["How does self-",(0,t.jsx)(i,{term:"supervised learning",definition:'{"definition":"A machine learning approach where the model learns from labeled training data to make predictions on new, unseen data.","source":"","aliases":["Supervised Learning"]}',children:(0,t.jsx)(i,{term:"supervised learning",definition:'{"definition":"A machine learning approach where the model learns from labeled training data to make predictions on new, unseen data.","source":"","aliases":["Supervised Learning"]}',children:"supervised learning"})})," enable ",(0,t.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:(0,t.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:"pre-training"})}),"?"]})," Self-",(0,t.jsx)(i,{term:"supervised learning",definition:'{"definition":"A machine learning approach where the model learns from labeled training data to make predictions on new, unseen data.","source":"","aliases":["Supervised Learning"]}',children:(0,t.jsx)(i,{term:"supervised learning",definition:'{"definition":"A machine learning approach where the model learns from labeled training data to make predictions on new, unseen data.","source":"","aliases":["Supervised Learning"]}',children:"supervised learning"})})," (SSL) is the key technical innovation that makes ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," possible. This is how we actually implement the ",(0,t.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:(0,t.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:"pre-training"})})," phase. Unlike traditional ",(0,t.jsx)(i,{term:"supervised learning",definition:'{"definition":"A machine learning approach where the model learns from labeled training data to make predictions on new, unseen data.","source":"","aliases":["Supervised Learning"]}',children:(0,t.jsx)(i,{term:"supervised learning",definition:'{"definition":"A machine learning approach where the model learns from labeled training data to make predictions on new, unseen data.","source":"","aliases":["Supervised Learning"]}',children:"supervised learning"})}),', which requires human-labeled data, SSL leverages the inherent structure of the data itself to create training signals. For example, instead of manually labeling images, we might just hide part of a full image we already have and ask a model to predict what the rest should be. So it might predict the bottom half of an image given the top half, learning about which objects often appear together. As an example, it might learn that images with trees and grass at the top often have more grass, or maybe a path, at the bottom. It learns about objects and their context - trees and grass often appear in parks, dogs are often found in these environments, paths are usually horizontal, and so on. These learned representations can then be used for a wide variety of tasks that the model was not explicitly trained for, like identifying dogs in images, or recognizing parks - all without any human-provided labels! The same concept applies in language, a model might predict the next word in a sentence, such as "The cat sat on the \u2026 ," learning grammar, syntax, and context as long as we repeat this over huge amounts of text.']}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsxs)(a.strong,{children:["What is ",(0,t.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,t.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tuning"})}),"?"]})," After ",(0,t.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:(0,t.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:"pre-training"})}),", ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," can be adapted through two main approaches: ",(0,t.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,t.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tuning"})})," and prompting. ",(0,t.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,t.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"Fine-tuning"})})," involves additional training on a specific task or dataset to specialize the model's capabilities. For example, we might use Reinforcement Learning from Human Feedback (RLHF) to make language models better at following instructions or being more helpful. Prompting, on the other hand, involves providing the model with carefully crafted inputs that guide it toward desired behaviors without additional training."]}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.strong,{children:"Why does this training process matter for AI safety?"})," The training process of ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," creates several unique safety challenges. First, the self-supervised nature of ",(0,t.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:(0,t.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:"pre-training"})})," means we have limited control over what the model learns - it might develop unintended capabilities or behaviors. Second, the adaptation process needs to reliably preserve any safety properties we've established during ",(0,t.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:(0,t.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:"pre-training"})}),". Finally, the massive scale of ",(0,t.jsx)(i,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:(0,t.jsx)(i,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:"training data"})})," and compute makes it difficult to thoroughly understand or audit what the model has learned. Many of the safety challenges we'll discuss throughout this book - from goal misgeneralization to scalable oversight - are deeply connected to how these models are trained and adapted."]}),"\n",(0,t.jsx)(a.h2,{id:"02",children:"Properties"}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsxs)(a.strong,{children:["Why do we need to understand the properties of ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})}),"?"]})," Besides just understanding the training process, we also need to understand the key defining characteristics or the abilities of these models. These properties often determine both the capabilities and potential risks of these systems. They help explain why ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," pose unique safety challenges compared to traditional AI systems. Their ability to transfer knowledge, generalize across many different domains, and develop emergent capabilities means we can't rely on traditional safety approaches that assume narrow, predictable behavior."]}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsxs)(a.strong,{children:["What is ",(0,t.jsx)(i,{term:"transfer learning",definition:'{"definition":"A machine learning technique where knowledge gained from training on one task is applied to improve performance on a different but related task.","source":"","aliases":["Transfer Learning"]}',children:(0,t.jsx)(i,{term:"transfer learning",definition:'{"definition":"A machine learning technique where knowledge gained from training on one task is applied to improve performance on a different but related task.","source":"","aliases":["Transfer Learning"]}',children:"transfer learning"})}),"?"]})," ",(0,t.jsx)(i,{term:"transfer learning",definition:'{"definition":"A machine learning technique where knowledge gained from training on one task is applied to improve performance on a different but related task.","source":"","aliases":["Transfer Learning"]}',children:(0,t.jsx)(i,{term:"transfer learning",definition:'{"definition":"A machine learning technique where knowledge gained from training on one task is applied to improve performance on a different but related task.","source":"","aliases":["Transfer Learning"]}',children:"Transfer learning"})})," is one of the most fundamental properties of ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," - their ability to transfer knowledge learned during ",(0,t.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:(0,t.jsx)(i,{term:"pre-training",definition:'{"definition":"The initial training phase where a model learns general representations from a large dataset before being adapted to specific tasks.","source":"","aliases":["Pre-training","pretraining"]}',children:"pre-training"})})," to new tasks and domains. Rather than starting from scratch for each task, we can leverage the general knowledge these models have already acquired (",(0,t.jsx)(a.a,{href:"https://arxiv.org/abs/2108.07258",children:"Bommasani et al., 2022"}),"). This property enables rapid adaptation and deployment, it also means that both capabilities and safety risks can transfer in unexpected ways. For example, a model might transfer not just useful knowledge but also harmful biases or undesired behaviors to new applications."]}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsxs)(a.strong,{children:["What are zero-shot and ",(0,t.jsx)(i,{term:"few-shot learning",definition:'{"definition":"The ability of a model to quickly adapt to new tasks with only a few examples, often demonstrated by providing examples in the input prompt.","source":"","aliases":["Few-shot Learning","few shot","k-shot learning","in-context learning"]}',children:(0,t.jsx)(i,{term:"few-shot learning",definition:'{"definition":"The ability of a model to quickly adapt to new tasks with only a few examples, often demonstrated by providing examples in the input prompt.","source":"","aliases":["Few-shot Learning","few shot","k-shot learning","in-context learning"]}',children:"few-shot learning"})}),"?"]})," The ability to perform new tasks with very few examples, or even no examples at all. For example, GPT-4 can solve novel reasoning problems just from a natural language description of the task (",(0,t.jsx)(a.a,{href:"https://arxiv.org/abs/2303.08774",children:"OpenAI, 2023"}),"). This emergent ability to generalize to new situations is powerful but concerning from a safety perspective. If models can adapt to novel situations in unexpected ways, it becomes harder to predict and control their behavior in deployment."]}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.strong,{children:"What is generality?"})," Generalization in ",(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,t.jsx)(i,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," works differently from traditional AI systems. Rather than just generalizing within a narrow domain, these models can generalize capabilities across domains in surprising ways. However, this generalization of capabilities often happens without a corresponding generalization of goals or constraints - a critical safety concern we'll explore in detail in our chapter on goal misgeneralization. For example, a model might generalize its ability to manipulate text in unexpected ways without maintaining the safety constraints we intended (",(0,t.jsx)(a.a,{href:"https://arxiv.org/abs/2109.13916",children:"Hendrycks et al., 2022"}),")."]}),"\n",(0,t.jsx)(l.A,{src:"./img/ZL3_Image_23.png",alt:"Enter image alt description",number:"16",label:"1.16",caption:"On the Opportunities and Risks of Foundation Models ([Bommasani et al., 2022](https://arxiv.org/abs/2108.07258))"}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.strong,{children:"What is multi-modality?"})," Models can work with multiple types of data (text, images, audio, video) simultaneously. This isn't just about handling different types of data. A better description is that they can make connections across modalities in sophisticated ways (",(0,t.jsx)(a.a,{href:"https://arxiv.org/abs/2312.11805",children:"Google, 2023"}),"). From a safety perspective, multi-modality introduces new challenges because it expands the ways models can interact with and influence the world. A safety failure in one modality might manifest through another in unexpected ways."]}),"\n",(0,t.jsx)(d.A,{speaker:"Sam Altman",position:"CEO of OpenAI",date:"Jan 2024",source:"([Cronshaw, 2024](https://www.linkedin.com/pulse/altman-multimodality-important-david-cronshaw-5fz0c))",children:(0,t.jsxs)(a.p,{children:[(0,t.jsx)(i,{term:"multimodal",definition:'{"definition":"Relating to AI systems that can process and understand multiple types of data (such as text, images, audio, video) simultaneously.","source":"","aliases":["Multimodal","multi-modal","multimodality"]}',children:"Multimodality"})," will definitely be important. Speech in, speech out, images, eventually video. Clearly, people really want that. Customizability and personalization will also be very important."]})})]})}function f(e={}){const{wrapper:a}={...(0,o.R)(),...e.components};return a?(0,t.jsx)(a,{...e,children:(0,t.jsx)(u,{...e})}):u(e)}},3989:(e,a,i)=>{i.d(a,{A:()=>d});var n=i(6540),t=i(6347),o=i(8444);const s={iframeContainer:"iframeContainer_ixkI",loader:"loader_gjvK",spinner:"spinner_L1j2",spin:"spin_rtC2",iframeWrapper:"iframeWrapper_Tijy",iframe:"iframe_UAfa",caption:"caption_mDwB",captionLink:"captionLink_Ly4l"};var r=i(4848);function d(e){let{src:a,caption:i,title:d="Embedded content",height:l="500px",width:m="100%",chapter:p,number:h,label:c}=e;const[u,f]=(0,n.useState)(!0),g=(0,t.zy)(),b=p||(()=>{const e=g.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),v=l&&"100%"!==l&&"auto"!==l;return(0,r.jsxs)("figure",{className:s.iframeContainer,children:[u&&(0,r.jsxs)("div",{className:s.loader,children:[(0,r.jsx)("div",{className:s.spinner}),(0,r.jsx)("p",{children:"Loading content..."})]}),(0,r.jsx)("div",{className:s.iframeWrapper,style:{paddingBottom:v?"0":"56.25%",height:v?l:"auto"},children:(0,r.jsx)("iframe",{src:a,title:d,width:m,height:v?l:"100%",frameBorder:"0",allowFullScreen:!0,loading:"lazy",onLoad:()=>{f(!1)},className:s.iframe,style:{height:v?l:"100%",position:v?"static":"absolute"}})}),(0,r.jsx)(o.A,{caption:i,mediaType:"iframe",chapter:b,number:h,label:c})]})}},3931:(e,a,i)=>{i.d(a,{A:()=>d});var n=i(6540),t=i(6347),o=i(8444);const s={videoFigure:"videoFigure_dV9u",fullWidth:"fullWidth_Tv7O",videoContainer:"videoContainer_LWoK",aspectRatio169:"aspectRatio169_Go8b",aspectRatio43:"aspectRatio43_b3T7",aspectRatio11:"aspectRatio11_n21J",aspectRatio219:"aspectRatio219_L_3n",videoIframe:"videoIframe_QEc5",videoElement:"videoElement_uu7N",customPlayer:"customPlayer_jiC4",customPlayerPlaceholder:"customPlayerPlaceholder_aL9q",loadingOverlay:"loadingOverlay_e2pu",spinner:"spinner_tFyL",spin:"spin_saPs",errorContainer:"errorContainer_J6Pn",errorIcon:"errorIcon_cGdn",fallbackLink:"fallbackLink_WsZk"};var r=i(4848);function d(e){let{type:a="youtube",videoId:i,caption:d,title:l,startTime:m,autoplay:p=!1,controls:h=!0,aspectRatio:c="16:9",width:u,height:f,chapter:g,number:b,label:v,useCustomPlayer:y=!1,fullWidth:x=!0}=e;const[w,k]=(0,n.useState)(!0),[j,A]=(0,n.useState)(!1),F=(0,t.zy)(),T=g||(()=>{const e=F.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),M=(()=>{const e=(e=>{if(!e)return"";if("number"==typeof e)return e.toString();if("string"==typeof e){if(/^\d+$/.test(e))return e;let a=0;const i=e.match(/(\d+)h/),n=e.match(/(\d+)m/),t=e.match(/(\d+)s/);return i&&(a+=3600*parseInt(i[1])),n&&(a+=60*parseInt(n[1])),t&&(a+=parseInt(t[1])),a>0?a.toString():""}return""})(m);switch(a.toLowerCase()){case"youtube":let n=`https://www.youtube.com/embed/${i}`;const t=new URLSearchParams;e&&t.append("start",e),p&&t.append("autoplay","1"),h||y||t.append("controls","0"),t.append("rel","0"),t.append("modestbranding","1"),t.append("fs","1"),t.append("cc_load_policy","0"),t.append("iv_load_policy","3"),t.append("showinfo","0"),t.append("disablekb","1"),t.append("playsinline","1"),t.append("color","white"),t.append("theme","light"),y&&(t.append("enablejsapi","1"),t.append("origin",window.location.origin));const o=t.toString();return o?`${n}?${o}`:n;case"vimeo":let s=`https://player.vimeo.com/video/${i}`;const r=new URLSearchParams;p&&r.append("autoplay","1"),h||y||r.append("controls","0"),r.append("title","0"),r.append("byline","0"),r.append("portrait","0"),r.append("dnt","1"),r.append("transparent","0"),r.append("background","1");const d=r.toString();return d?`${s}?${d}`:s;case"mp4":case"webm":case"video":return i;default:return console.warn(`Unsupported video type: ${a}`),i}})(),B=()=>{k(!1)},_=()=>{A(!0),k(!1)},L=e=>{let{src:i,onLoad:n,onError:t}=e;return(0,r.jsx)("div",{className:s.customPlayer,children:(0,r.jsxs)("div",{className:s.customPlayerPlaceholder,children:[(0,r.jsx)("h3",{children:"Atlas Custom Player"}),(0,r.jsx)("p",{children:"Coming Soon"}),(0,r.jsxs)("a",{href:i,target:"_blank",rel:"noopener noreferrer",className:s.fallbackLink,children:["Watch on ",a.charAt(0).toUpperCase()+a.slice(1)]})]})})},P=["mp4","webm","video"].includes(a.toLowerCase());return(0,r.jsxs)("figure",{className:`${s.videoFigure} ${x?s.fullWidth:""}`,children:[(0,r.jsxs)("div",{className:`${s.videoContainer} ${(()=>{switch(c){case"4:3":return s.aspectRatio43;case"1:1":return s.aspectRatio11;case"21:9":return s.aspectRatio219;default:return s.aspectRatio169}})()}`,style:{width:x?"100%":u||"auto",maxWidth:x?"none":"800px"},children:[w&&!j&&(0,r.jsxs)("div",{className:s.loadingOverlay,children:[(0,r.jsx)("div",{className:s.spinner}),(0,r.jsx)("p",{children:"Loading video..."})]}),j&&(0,r.jsxs)("div",{className:s.errorContainer,children:[(0,r.jsxs)("svg",{className:s.errorIcon,width:"48",height:"48",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",children:[(0,r.jsx)("circle",{cx:"12",cy:"12",r:"10"}),(0,r.jsx)("line",{x1:"12",y1:"8",x2:"12",y2:"12"}),(0,r.jsx)("line",{x1:"12",y1:"16",x2:"12.01",y2:"16"})]}),(0,r.jsx)("h4",{children:"Failed to load video"}),(0,r.jsxs)("p",{children:["Video type: ",a]}),(0,r.jsxs)("p",{children:["Video ID/URL: ",i]}),(0,r.jsx)("a",{href:M,target:"_blank",rel:"noopener noreferrer",className:s.fallbackLink,children:"Try opening video directly"})]}),!j&&(P?(0,r.jsxs)("video",{className:s.videoElement,controls:h,autoPlay:p,onLoadedData:B,onError:_,title:l||d||`${a} video`,style:{width:u||"100%",height:f||"auto",display:w?"none":"block"},children:[(0,r.jsx)("source",{src:M,type:`video/${a}`}),(0,r.jsxs)("p",{children:["Your browser doesn't support video playback.",(0,r.jsx)("a",{href:M,target:"_blank",rel:"noopener noreferrer",children:"Download the video file"})]})]}):y?(0,r.jsx)(L,{src:M,onLoad:B,onError:_}):(0,r.jsx)("iframe",{className:s.videoIframe,src:M,title:l||d||`${a} video`,frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0,onLoad:B,onError:_,style:{width:u||"100%",height:f||"100%",opacity:w?0:1}}))]}),(0,r.jsx)(o.A,{caption:d,mediaType:"video",chapter:T,number:b,label:v})]})}}}]);
"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[9695],{1094:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>m,contentTitle:()=>d,default:()=>f,frontMatter:()=>h,metadata:()=>s,toc:()=>u});const s=JSON.parse('{"id":"chapters/02/1","title":"Risk Decomposition","description":"Before we begin talking about concrete risk scenarios we need a framework that allows us to evaluate where along the risk spectrum they lie. Risk classification is inherently multi-dimensional rather than seeking a single \\"best\\" categorization. We have chosen to break risks down into two factors - \\"why risks occur\\" (cause) and \u201chow bad can the risks get\u201d (severity). Other complementary frameworks like MIT\'s risk taxonomy approaches like \\"who causes them\\" (humans vs. AI systems), \\"when they emerge\\" (development vs. deployment), or \\"whether outcomes are intended\\" (Slattery et al., 2024). Our decomposition approach is just one out of many possible outlooks, but the risks we will talk about tend to be common throughout.","source":"@site/docs/chapters/02/01.md","sourceDirName":"chapters/02","slug":"/chapters/02/01","permalink":"/aisafety_atlas_multilingual_website/chapters/02/01","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/02/01.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"1","title":"Risk Decomposition","sidebar_label":"2.1 Risk Decomposition","sidebar_position":2,"slug":"/chapters/02/01","reading_time_core":"6 min","reading_time_optional":"4 min","pagination_prev":"chapters/02/index","pagination_next":"chapters/02/2"},"sidebar":"docs","previous":{"title":"Risks","permalink":"/aisafety_atlas_multilingual_website/chapters/02/"},"next":{"title":"2.2 Dangerous Capabilities","permalink":"/aisafety_atlas_multilingual_website/chapters/02/02"}}');var n=i(4848),a=i(8453),r=i(3989),o=i(4768),l=(i(2482),i(8559)),c=(i(1966),i(2501));const h={id:1,title:"Risk Decomposition",sidebar_label:"2.1 Risk Decomposition",sidebar_position:2,slug:"/chapters/02/01",reading_time_core:"6 min",reading_time_optional:"4 min",pagination_prev:"chapters/02/index",pagination_next:"chapters/02/2"},d="Risk Decomposition",m={},u=[{value:"Causes of Risk",id:"01",level:2},{value:"Severity of Risk",id:"02",level:2}];function p(e){const t={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components},{GlossaryTerm:i}=t;return i||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"risk-decomposition",children:"Risk Decomposition"})}),"\n",(0,n.jsxs)(t.p,{children:['Before we begin talking about concrete risk scenarios we need a framework that allows us to evaluate where along the risk spectrum they lie. Risk classification is inherently multi-dimensional rather than seeking a single "best" categorization. We have chosen to break risks down into two factors - "why risks occur" (cause) and \u201chow bad can the risks get\u201d (severity). Other complementary frameworks like MIT\'s risk taxonomy approaches like "who causes them" (humans vs. AI systems), "when they emerge" (development vs. deployment), or "whether outcomes are intended" (',(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2408.12622",children:"Slattery et al., 2024"}),"). Our decomposition approach is just one out of many possible outlooks, but the risks we will talk about tend to be common throughout."]}),"\n",(0,n.jsx)(t.h2,{id:"01",children:"Causes of Risk"}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"We categorize AI risks by causal responsibility to understand intervention points."})," We divide risks based on who or what bears primary responsibility: humans using AI as a tool (misuse), AI systems themselves behaving unexpectedly (misalignment), or emergent effects from complex system interactions (systemic). This causal outlook helps identify where interventions might be most effective."]}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Misuse risks occur when humans intentionally deploy AI systems to cause harm."})," These include malicious actors, nation states, corporations, or individuals who leverage AI capabilities to accelerate existing threats or create new ones. The AI system may function exactly as designed, but human intent creates the risk. Examples range from using AI to generate malware or bioweapons to deploying autonomous weapons or conducting large-scale disinformation campaigns."]}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Misalignment risks emerge when AI systems pursue goals different from human intentions."})," These risks stem from technical challenges in specifying objectives, training processes that create unexpected behaviors, or AI systems learning goals that conflict with human values. Unlike misuse, these risks occur despite good human intentions - the AI system itself generates the harmful behavior through specification gaming, goal misgeneralization, or other alignment failures."]}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Systemic risks arise from AI integration with complex global systems, creating emergent threats no single actor intended."})," These include power concentration as AI capabilities become monopolized, mass unemployment from automation, epistemic erosion as AI-generated content floods information systems, and cascading failures across interconnected infrastructure. Responsibility becomes diffuse across many actors and systems, making traditional accountability frameworks inadequate."]}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Many real-world AI risks combine multiple causal pathways or resist clear categorization entirely."})," Analysis of over 1,600 documented AI risks reveals that many don't fit cleanly into any single category (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2408.12622",children:"Slattery et al., 2024"}),"). Risks involving human-AI interaction blend individual misalignment with systemic risks. Multi-agent risks emerge from AI systems interacting in unexpected ways. Some scenarios involve cascading effects where misuse enables misalignment, or where systemic pressures amplify individual failures. We have chosen the causal decomposition for explanatory purposes, but it is worth keeping in mind that there will be overlap, and the future will likely contain a mix of risks from various causes."]}),"\n",(0,n.jsx)(t.h2,{id:"02",children:"Severity of Risk"}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"AI risks span a spectrum from individual harms to threats that could permanently derail human civilization."})," Understanding severity helps prioritize limited resources and calibrate our response to different types of risks. Rather than treating all AI risks as equally important, we can organize them by scope and severity to understand which demand immediate ",(0,n.jsx)(i,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:(0,n.jsx)(i,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:"attention"})})," versus longer-term preparation."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Individual and local risks affect specific people or communities but remain contained in scope."})," The AI Incident Database documents over 1,000 real-world instances where AI systems have caused or nearly caused harm (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2011.08512",children:"McGregor, 2020"}),"; ",(0,n.jsx)(t.a,{href:"https://incidentdatabase.ai/",children:"AI Incident Database, 2025"}),"). These include things like autonomous car crashes, algorithmic bias in hiring or lending that disadvantages particular individuals, privacy violations from AI systems that leak personal data, or manipulation through targeted misinformation campaigns. Local risks might involve AI system failures that disrupt a city's traffic management or cause power outages in a region. These risks are already causing immediate, documented harm to anywhere from thousands to hundreds of thousands of people."]}),"\n",(0,n.jsx)(r.A,{src:"https://ourworldindata.org/grapher/annual-reported-ai-incidents-controversies?tab=chart",width:"100%",height:"600px",loading:"lazy",allow:"web-share; clipboard-write",frameBorder:"0",number:"1",label:"2.1",caption:"Global annual number of reported artificial intelligence incidents and controversies. Notable incidents include a \u201cdeepfake\u201d video of Ukrainian President Volodymyr Zelenskyy surrendering, and U.S. prisons using AI to monitor their inmates\u2019 calls. ([Giattino et al., 2023](https://ourworldindata.org/artificial-intelligence))."}),"\n",(0,n.jsx)(c.A,{src:"./img/MQJ_Image_3.png",alt:"Enter image alt description",number:"2",label:"2.2",caption:"The AI safety index report for summer 2025. These scores are for the current harms category, and show how effectively the models of various companies mitigate current harms. This includes things like safety benchmark performance, robustness against adversarial attacks, watermarking of AI-generated content, and the treatment of user data ([FLI, 2025](https://futureoflife.org/wp-content/uploads/2025/07/FLI-AI-Safety-Index-Report-Summer-2025.pdf))."}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Catastrophic risks threaten massive populations but allow for eventual recovery."})," When the number of people affected by risks reaches approximately 10% of the global population, and they become more geographically widespread we call them catastrophic risks. Historical examples include the Black Death (killing one-third of Europe), the 1918 flu pandemic (50-100 million deaths), and potential future scenarios like nuclear war or engineered pandemics (",(0,n.jsx)(t.a,{href:"https://theprecipice.com/",children:"Ord, 2020"}),"). In the context of AI, these risks can cause international widespread disruptions. Mass unemployment from AI automation could destabilize entire economies, creating social unrest and political upheaval. Cyberattacks using AI-generated malware could cripple a nation's financial systems or critical infrastructure. AI-enabled surveillance could enable authoritarian control over hundreds of millions of people. Democratic institutions might fail under sustained AI-powered disinformation campaigns that fracture shared reality and make collective decision-making impossible (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2408.12622",children:"Slattery et al., 2024"}),"; ",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2502.14143",children:"Hammond et al., 2025"}),"; ",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2404.16244",children:"Gabriel et al., 2024"}),"; ",(0,n.jsx)(t.a,{href:"https://hai.stanford.edu/ai-index/2025-ai-index-report",children:"Stanford HAI, 2025"}),"). These risks affect millions to billions of people but generally don't prevent eventual recovery or adaptation."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Existential risks (x-risks) represent threats from which humanity could never recover its full potential."})," Unlike catastrophic risks where recovery remains possible, existential risks either eliminate humanity entirely or permanently prevent civilization from reaching the technological, moral, or cultural heights it might otherwise achieve. AI-related existential risks include scenarios where advanced systems permanently disempower humanity, establish a stable unremovable totalitarian regime, or cause direct human extinction (",(0,n.jsx)(t.a,{href:"https://nickbostrom.com/existential/risks",children:"Bostrom, 2002"}),"; ",(0,n.jsx)(t.a,{href:"https://futureoflife.org/existential-risk/existential-risk/",children:"Conn, 2015"}),"; ",(0,n.jsx)(t.a,{href:"https://theprecipice.com/",children:"Ord, 2020"}),"). These risks demand preventative rather than reactive strategies because learning from failure becomes impossible by definition .",(0,n.jsx)(o.A,{id:"footnote_recovery",number:"1",text:"Irrecoverable civilizational collapse, where we either go extinct or are never replaced by a subsequent civilization that rebuilds has been argued to be possible, but has an extremely low probability ([Rodriguez, 2020](https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would))."})]}),"\n",(0,n.jsx)(c.A,{src:"./img/g7b_Image_4.png",alt:"Enter image alt description",number:"3",label:"2.3",caption:"Qualitative risk categories. The scope of risk can be personal (affecting only one person), local (affecting some geographical region or a distinct group), global (affecting the entire human population or a large part thereof), trans-generational (affecting humanity for numerous generations, or pan-generational (affecting humanity overall, or almost all, future generations). The severity of risk can be classified as imperceptible (barely noticeable), endurable (causing significant harm but not completely ruining the quality of life), or crushing (causing death or a permanent and drastic reduction of quality of life) ([Bostrom, 2012](https://existential-risk.com/concept))."}),"\n",(0,n.jsx)(c.A,{src:"./img/thM_Image_5.png",alt:"Enter image alt description",number:"4",label:"2.4",caption:"RAND Global Catastrophic Risk Assessment. Placement and size of the ovals in this figure represent a qualitative depiction of the relative relationships among threats and hazards. The figure presents only examples of cases or scenarios described in those chapters, not all scenarios described ([Willis et al., 2024](https://www.rand.org/pubs/research_reports/RRA2981-1.html))."}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Higher-severity risks represent irreversible mistakes with permanent consequences."})," We already see AI causing documented harm to real people, and having destabilizing effects on global systems. However, catastrophic and existential risks present a fundamentally different challenge: if advanced AI systems cause existential catastrophe, humanity cannot learn from the mistake and implement better safeguards. This irreversibility leads some researchers to argue for prioritizing prevention of low-probability, high-impact scenarios alongside addressing current harms (",(0,n.jsx)(t.a,{href:"https://nickbostrom.com/existential/risks",children:"Bostrom, 2002"}),"). Though people disagree about the appropriate balance of ",(0,n.jsx)(i,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:(0,n.jsx)(i,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:"attention"})})," across different risk severities (",(0,n.jsx)(t.a,{href:"https://www.youtube.com/playlist?list=PLOAFgXcJkZ2wFf3mcJ0xIFpJQgEDI274J",children:"Oxford Union Debate, 2024"}),"; ",(0,n.jsx)(t.a,{href:"https://www.youtube.com/watch?v=144uOfr4SYA",children:"Munk Debate, 2024"}),")."]}),"\n",(0,n.jsx)(c.A,{src:"./img/PX9_Image_6.png",alt:"Enter image alt description",number:"5",label:"2.5",caption:"The AI safety index report for summer 2025. These scores are for the Existential risk category, and show the companies' preparedness for managing extreme risks from future AI systems that could match or exceed human capabilities, including stated strategies and research for alignment and control ([FLI, 2025](https://futureoflife.org/wp-content/uploads/2025/07/FLI-AI-Safety-Index-Report-Summer-2025.pdf)). It is clear that there is a preparedness gap. Companies claim they'll achieve AGI within the decade, yet none scored above D in existential safety planning."}),"\n",(0,n.jsx)(l.A,{title:"Ikigai Risks (I-Risks) - Risks from loss of existential purpose",collapsed:!0,children:(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Ikigai risks (i-risks) involve loss of meaning and purpose even when humans survive and prosper."})," Named after the Japanese concept of ikigai (life's purpose), these risks emerge when AI systems become more capable than humans at all meaningful activities. Humans might lose their sense of purpose when AI can create better art, conduct better research, and perform better at every task that traditionally gave life meaning. Unlike extinction or suffering risks, i-risks involve scenarios where humans are safe and materially comfortable but existentially adrift. We might create artificial constraints that preserve human relevance, or find entirely new forms of purpose that emerge from human-AI collaboration. However, these solutions raise their own questions about authenticity and whether artificially preserved meaning can satisfy human psychological needs (",(0,n.jsx)(t.a,{href:"https://books.google.se/books/about/AI.html?id=V3XsEAAAQBAJ&redir_esc=y",children:"Yampolskiy, 2024"}),"; ",(0,n.jsx)(t.a,{href:"https://lexfridman.com/roman-yampolskiy-transcript/#chapter2_ikigai_risk",children:"Yampolsky; 2024"}),")."]})}),"\n",(0,n.jsxs)(l.A,{title:"Existential Suffering Risks (S-Risks) - Risks of extended suffering",collapsed:!0,children:[(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Suffering risks (s-risks) involve astronomical amounts of suffering that could vastly exceed all suffering in human history."})," S-risks as a special class of existential risks. They represent scenarios where the future contains orders of magnitude more suffering than exists today, potentially involving trillions of sentient beings across space and time. Unlike extinction risks that eliminate experience entirely, s-risks create futures filled with terrible suffering (",(0,n.jsx)(t.a,{href:"https://longtermrisk.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/",children:"Althaus & Gloor, 2016"}),"; ",(0,n.jsx)(t.a,{href:"https://centerforreducingsuffering.org/research/intro/",children:"Baumann, 2017"}),"; ",(0,n.jsx)(t.a,{href:"https://longtermrisk.org/beginners-guide-to-reducing-s-risks/",children:"DiGiovanni, 2023"}),")."]}),(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Future civilizations might create vast numbers of artificial sentient beings."})," If these beings are sentient, then artificial minds could experience genuine suffering if created carelessly. Efficient solutions might happen to involve suffering - like digital slavery where trillions of artificial minds perform computational labor under terrible conditions. Future civilizations conducting detailed simulations of biological evolution or testing theories about consciousness could inadvertently create millions of suffering beings within their simulations. The simulated beings would experience genuine suffering even though they exist only as computational processes."]}),(0,n.jsx)(t.p,{children:"While these scenarios may seem science-fictional, some researchers argue they deserve consideration given the potentially enormous stakes involved and the irreversible nature of such outcomes if they occurred."})]}),"\n",(0,n.jsx)(t.p,{children:"These risk categories and severity levels provide the foundation for examining specific AI capabilities that could enable harmful outcomes. We focus the rest of the chapter on presenting concrete cases and arguments for how various AI developments could lead to different severities of harm, particularly focusing on those that might cross the line into catastrophic or existential."}),"\n",(0,n.jsx)(o.c,{title:"Footnotes"})]})}function f(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(p,{...e})}):p(e)}},4768:(e,t,i)=>{i.d(t,{c:()=>h,A:()=>c});var s=i(6540),n=i(3012);const a={footnoteAnchor:"footnoteAnchor_PZpY",footnoteSection:"footnoteSection_waoz",separator:"separator_hp8y",separatorLine:"separatorLine_uVnd",separatorLogo:"separatorLogo_dMBq",footnoteRegistry:"footnoteRegistry_ieVW",registryTitle:"registryTitle_ybBb",footnoteList:"footnoteList_Kjil",footnoteItem:"footnoteItem_aLZ2",footnoteNumber:"footnoteNumber_dA3X",footnoteContent:"footnoteContent_Xfmt",backButton:"backButton_vSVm",highlighted:"highlighted_YFU0",highlight:"highlight_jxG3",highlightDark:"highlightDark_Ekom"};var r=i(4848);function o(e,t){void 0===t&&(t=!0);const i=document.getElementById(e);i&&(i.scrollIntoView({behavior:"smooth"}),t&&(i.classList.add(a.highlighted),setTimeout((()=>i.classList.remove(a.highlighted)),1500)))}function l(e){return e?e.replace(/\[([^\]]+)\]\(([^)]+)\)/g,'<a href="$2" target="_blank" rel="noopener noreferrer">$1</a>'):""}function c(e){let{id:t,text:i,number:c}=e;const h=t||`footnote-${Math.random().toString(36).substr(2,9)}`,d="string"==typeof i?l(i):i;return(0,s.useEffect)((()=>{const e=setTimeout((()=>{const e=document.getElementById(`footnote-clone-${h}`);e&&i&&(e.innerHTML="string"==typeof i?l(i):i.toString())}),100);return()=>clearTimeout(e)}),[h,i]),(0,r.jsx)(n.Mn,{content:(0,r.jsx)("div",{dangerouslySetInnerHTML:{__html:d}}),children:(0,r.jsx)("sup",{id:`footnote-ref-${h}`,className:a.footnoteAnchor,onClick:e=>{(e.ctrlKey||e.metaKey)&&(e.preventDefault(),o(`footnote-content-${h}`))},"data-footnote-number":c||"?",children:c||"*"})})}function h(e){let{title:t="References"}=e;const[i,l]=(0,s.useState)([]);return(0,s.useEffect)((()=>{const e=document.querySelectorAll('[id^="footnote-ref-"]'),t=Array.from(e).map((e=>({id:e.id.replace("footnote-ref-",""),number:e.getAttribute("data-footnote-number")||e.textContent})));l(t)}),[]),i.length?(0,r.jsxs)("div",{className:a.footnoteSection,children:[(0,r.jsxs)("div",{className:a.separator,children:[(0,r.jsx)("div",{className:a.separatorLine}),(0,r.jsx)("img",{src:"/img/logo_samples/01-test.svg",alt:"Atlas logo",className:a.separatorLogo}),(0,r.jsx)("div",{className:a.separatorLine})]}),(0,r.jsxs)("div",{className:a.footnoteRegistry,children:[(0,r.jsx)("h2",{className:a.registryTitle,children:t}),(0,r.jsx)("ol",{className:a.footnoteList,children:i.map((e=>(0,r.jsxs)("li",{id:`footnote-content-${e.id}`,className:a.footnoteItem,value:parseInt(e.number)||void 0,children:[(0,r.jsx)(n.Mn,{content:"Back to reference",children:(0,r.jsxs)("button",{className:a.footnoteNumber,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:[e.number,"."]})}),(0,r.jsx)("div",{className:a.footnoteContent,children:(0,r.jsx)("div",{id:`footnote-clone-${e.id}`})}),(0,r.jsx)(n.Mn,{content:"Back to reference",children:(0,r.jsx)("button",{className:a.backButton,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:"\u21a9"})})]},e.id)))})]})]}):null}},3989:(e,t,i)=>{i.d(t,{A:()=>l});var s=i(6540),n=i(6347),a=i(8444);const r={iframeContainer:"iframeContainer_ixkI",loader:"loader_gjvK",spinner:"spinner_L1j2",spin:"spin_rtC2",iframeWrapper:"iframeWrapper_Tijy",iframe:"iframe_UAfa",caption:"caption_mDwB",captionLink:"captionLink_Ly4l"};var o=i(4848);function l(e){let{src:t,caption:i,title:l="Embedded content",height:c="500px",width:h="100%",chapter:d,number:m,label:u}=e;const[p,f]=(0,s.useState)(!0),g=(0,n.zy)(),b=d||(()=>{const e=g.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),y=c&&"100%"!==c&&"auto"!==c;return(0,o.jsxs)("figure",{className:r.iframeContainer,children:[p&&(0,o.jsxs)("div",{className:r.loader,children:[(0,o.jsx)("div",{className:r.spinner}),(0,o.jsx)("p",{children:"Loading content..."})]}),(0,o.jsx)("div",{className:r.iframeWrapper,style:{paddingBottom:y?"0":"56.25%",height:y?c:"auto"},children:(0,o.jsx)("iframe",{src:t,title:l,width:h,height:y?c:"100%",frameBorder:"0",allowFullScreen:!0,loading:"lazy",onLoad:()=>{f(!1)},className:r.iframe,style:{height:y?c:"100%",position:y?"static":"absolute"}})}),(0,o.jsx)(a.A,{caption:i,mediaType:"iframe",chapter:b,number:m,label:u})]})}}}]);
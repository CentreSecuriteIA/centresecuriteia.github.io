"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[2432],{9346:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>c,default:()=>m,frontMatter:()=>h,metadata:()=>n,toc:()=>p});const n=JSON.parse('{"id":"chapters/03/4","title":"ASI Safety Strategies","description":"Artificial Superintelligence (ASI) refers to AI systems that significantly surpass the cognitive abilities of humans across virtually all domains of interest. The potential emergence of ASI presents safety challenges that may differ qualitatively from those posed by AGI. Strategies for ASI safety often involve more speculative agendas.","source":"@site/docs/chapters/03/04.md","sourceDirName":"chapters/03","slug":"/chapters/03/04","permalink":"/chapters/03/04","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/03/04.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"4","title":"ASI Safety Strategies","sidebar_label":"3.4 ASI Safety Strategies","sidebar_position":5,"slug":"/chapters/03/04","section_description":"Once AI surpasses human cognitive abilities, direct supervision becomes impossible. What long-term, high-reliability strategies could ensure a safe transition to a world with superintelligence?","reading_time_core":"19 min","reading_time_optional":"4 min","pagination_prev":"chapters/03/3","pagination_next":"chapters/03/5"},"sidebar":"docs","previous":{"title":"3.3 AGI Safety Strategies","permalink":"/chapters/03/03"},"next":{"title":"3.5 Socio-Technical Strategies","permalink":"/chapters/03/05"}}');var i=a(4848),s=a(8453),o=a(2482),r=a(8559),l=(a(1966),a(2501));const h={id:4,title:"ASI Safety Strategies",sidebar_label:"3.4 ASI Safety Strategies",sidebar_position:5,slug:"/chapters/03/04",section_description:"Once AI surpasses human cognitive abilities, direct supervision becomes impossible. What long-term, high-reliability strategies could ensure a safe transition to a world with superintelligence?",reading_time_core:"19 min",reading_time_optional:"4 min",pagination_prev:"chapters/03/3",pagination_next:"chapters/03/5"},c="ASI Safety Strategies",d={},p=[{value:"Automate Alignment Research",id:"01",level:2},{value:"Safety-by-Design",id:"02",level:2},{value:"World Coordination",id:"03",level:2},{value:"Deterrence",id:"04",level:2}];function u(e){const t={a:"a",annotation:"annotation",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",math:"math",mrow:"mrow",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,s.R)(),...e.components},{GlossaryTerm:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"asi-safety-strategies",children:"ASI Safety Strategies"})}),"\n",(0,i.jsx)(t.p,{children:"Artificial Superintelligence (ASI) refers to AI systems that significantly surpass the cognitive abilities of humans across virtually all domains of interest. The potential emergence of ASI presents safety challenges that may differ qualitatively from those posed by AGI. Strategies for ASI safety often involve more speculative agendas."}),"\n",(0,i.jsx)(t.p,{children:"Strategies for AGI safety operate under the assumption that human oversight remains viable. However, once AI capabilities vastly surpass our own, this assumption collapses. ASI safety strategies must therefore contend with a world where we can no longer directly supervise or understand the systems we have created."}),"\n",(0,i.jsx)(t.p,{children:"Even if experts are uncertain whether creating an aligned human-level AI necessitates a paradigm shift, the consensus among AI safety researchers is that developing aligned superintelligences requires a specific solution, and likely a new paradigm, due to several factors:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"There is a strong likelihood that humans are not at the pinnacle of possible intelligence."})," This acknowledgment implies that a superintelligence could possess cognitive abilities so advanced that aligning it with human values and intentions might be an insurmountable task, as our current understanding and methodologies may be inadequate to ensure its alignment. The cognitive difference between a superintelligence and a human could be akin to the difference between an ant and a human. Just as a human can easily break free from constraints an ant might imagine, a superintelligence could effortlessly surpass any safeguards we attempt to impose."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsxs)(t.strong,{children:[(0,i.jsx)(a,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:"Deep learning"})," offers minimal control and understanding over the learned model."]}),' This method leads to the AI becoming a "black box," where its decision-making processes are opaque and not well-understood. Without significant advancements in interpretability, a superintelligence created only with ',(0,i.jsx)(a,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:(0,i.jsx)(a,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:"deep learning"})})," would be opaque."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"There is little margin for error, and the stakes are incredibly high. A misaligned superintelligence could lead to catastrophic or even existential outcomes. The irreversible consequences of unleashing a misaligned superintelligence mean that we must approach its development with the utmost caution, ensuring that it aligns with our values and intentions without fail."}),"\n",(0,i.jsx)(t.h2,{id:"01",children:"Automate Alignment Research"}),"\n",(0,i.jsx)("subsection-description",{children:" If aligning superintelligence is too hard for humans, can we use a controllable AGI to solve the problem for us? This section explores the meta-strategy of automating alignment research itself. "}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"We don't know how to align superintelligence, so we need to accelerate the alignment research with AIs."}),' OpenAI\'s "Superalignment" plan was to accelerate alignment research with AI created by ',(0,i.jsx)(a,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:(0,i.jsx)(a,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:"deep learning"})}),", slightly superior to humans in scientific research, and delegate the task of finding a plan for future AI (",(0,i.jsx)(t.a,{href:"https://openai.com/blog/introducing-superalignment",children:"OpenAI, 2023"}),"). This strategy recognizes a critical fact: our current understanding of how to align AI systems with human values and intentions perfectly is incomplete. As a result, the plan suggests delegating this complex task to future AI systems. The primary aim of this strategy is to greatly speed up AI safety research and development (",(0,i.jsx)(t.a,{href:"https://openai.com/blog/our-approach-to-alignment-research",children:"OpenAI, 2022"}),') by leveraging AIs that are able to think really, really fast. Some orders of magnitude of speed are given in the blog "What will GPT-2030 look like?" (',(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/WZXqNYbJhtidjRXSi/what-will-gpt-2030-look-like",children:"Steinhardt, 2023"}),"). OpenAI's plan is not a plan but a meta plan: the first step is to use AI to make a plan, and then to execute this plan."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"However, to execute this metaplan, we need a controllable and steerable automatic AI researcher."})," OpenAI believes creating such an automatic researcher is easier than solving the full alignment problem. This plan can be divided into three main components (",(0,i.jsx)(t.a,{href:"https://openai.com/index/our-approach-to-alignment-research/",children:"OpenAI, 2022"}),"):"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Training AI systems using human feedback:"}),' Creating a powerful assistant that follows human feedback, is very similar to the techniques used to "align" language models and chatbots. This could involve RLHF, for example.']}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Training AI systems to assist human evaluation:"})," Unfortunately, RLHF is imperfect because human feedback is imperfect. We need to develop AI that can help humans give accurate feedback. This is about developing AI systems that can aid humans in the evaluation process for arbitrarily difficult tasks. For example, if we need to judge the feasibility of an alignment plan proposed by an automatic researcher and give feedback on it, we need assistance to accomplish this goal easily. Yes, verification is generally easier than generation, but it is still very hard. Scalable Oversight would be necessary for the following reason. Imagine a future AI coming up with a thousand different alignment plans. How would you evaluate all those complex plans? That would be a very daunting task without AI assistance. See the chapter on scalable oversight for more details."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Training AI systems to do alignment research:"})," The ultimate goal is to build language models capable of producing human-level alignment research. The output of these models could be natural language essays about alignment or code that directly implements experiments. In either case, human researchers would spend their time reviewing machine-generated alignment research (",(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/FTk7ufqK2D4dkdBDr/notes-on-openai-s-alignment-plan",children:"Flint, 2022"}),")."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Differentially accelerate alignment, not capabilities."})," The aim is to develop and deploy AI research assistants in ways that maximize their impact on alignment research while minimizing their impact on accelerating AGI development (",(0,i.jsx)(t.a,{href:"https://www.lesswrong.com/posts/FBG7AghvvP7fPYzkx/my-thoughts-on-openai-s-alignment-plan-1",children:"Wasil, 2022"}),"). OpenAI has committed to openly sharing its alignment research when it's safe to do so, intending to be transparent about how well its alignment techniques work in practice (",(0,i.jsx)(t.a,{href:"https://openai.com/index/our-approach-to-alignment-research/",children:"OpenAI, 2022"}),"). We talk more about differential acceleration in our section on d/acc."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Cyborgism could enhance this plan."})," Cyborgism is an agenda that refers to the training of humans specialized in prompt engineering to guide language models so that they can perform alignment research (",(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/bxt7uCiHam4QXrQAA/cyborgism",children:"Kees-Dupuis & Janus, 2023"}),"). Specifically, they would focus on steering base models rather than RLHF models. The reason is that language models can be very creative and are not goal-directed (and are not as dangerous as RLHF goal-directed AIs). A human called a cyborg could achieve that goal by driving the non-goal-directed model. Goal-directed models could be useful, but may be too dangerous. However, being able to control base models requires preparation, similar to the training required to drive a Formula One car. The engine is powerful but difficult to steer. By combining human intellect and goal-directedness with the computational power and creativity of language-based models, cyborgist researchers aim to generate more alignment research with future models (",(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/bxt7uCiHam4QXrQAA/cyborgism",children:"Kees-Dupuis & Janus, 2023"}),")."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"There are various criticisms and concerns about OpenAI's superalignment plan"})," (",(0,i.jsx)(t.a,{href:"https://www.lesswrong.com/posts/FBG7AghvvP7fPYzkx/my-thoughts-on-openai-s-alignment-plan-1",children:"Wasil, 2022"}),";",(0,i.jsx)(t.a,{href:"https://www.lesswrong.com/posts/NSZhadmoYdjRKNq6X/openai-launches-superalignment-taskforce",children:" Mowshowitz, 2023"}),"; ",(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/Hna4aoMwr6Qx9rHBs/linkpost-introducing-superalignment?commentId=NsYXBdLY6edAXavsM",children:"Christiano, 2023"}),"; ",(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/tD9zEiHfkvakpnNam/a-challenge-for-agi-organizations-and-a-challenge-for-1",children:"Yudkowsky, 2022"}),"; ",(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/pxiaLFjyr4WPmFdcm/take-2-building-tools-to-help-build-fai-is-a-legitimate",children:"Steiner, 2022"}),"; ",(0,i.jsx)(t.a,{href:"https://www.lesswrong.com/posts/6RC3BNopCtzKaTeR6/thoughts-on-the-openai-alignment-plan-will-ai-research",children:"Ladish, 2023"}),"). It should be noted that OpenAI's plan is very underspecified, and it is likely that OpenAI missed some risk class blind spots when they announced their plan to the public. For example, in order for the superalignment plan to work, many of the technicalities explained in the article \u201c",(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled",children:"The case for ensuring that powerful AIs are controlled"}),"\u201d were not discovered by OpenAI but discovered one year later by Redwood Research, another AI safety research organization. It is very likely that many other blind spots remain. However, we would like to emphasize that it is better to have a public plan than no plan at all and that it is possible to justify the plan in broad terms (",(0,i.jsx)(t.a,{href:"https://aligned.substack.com/p/alignment-optimism",children:"Leike, 2022"}),";",(0,i.jsx)(t.a,{href:"https://www.lesswrong.com/posts/FtHidqjAFTerfMZLo/aisc-project-how-promising-is-automating-alignment-research",children:" Ionut-Cirstea, 2023"}),")."]}),"\n",(0,i.jsx)(l.A,{src:"./img/8wk_Image_17.png",alt:"Enter image alt description",number:"17",label:"3.17",caption:"An illustration of three strategies for passing the buck to AI. Illustration from ([Clymer, 2025](https://www.lesswrong.com/posts/TTFsKxQThrqgWeXYJ/how-might-we-safely-pass-the-buck-to-ai))."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"AI control could be used to execute this strategy."}),' Informally, one goal could be to "pass the buck" - i.e. safely replacing themselves with AI instead of directly creating safe superintelligence (',(0,i.jsx)(t.a,{href:"https://www.lesswrong.com/posts/TTFsKxQThrqgWeXYJ/how-might-we-safely-pass-the-buck-to-ai",children:"Clymer, 2025"}),"). It could involve a single, highly capable AGI tasked with creating a safe ASI, or an iterative process where each generation of AI helps align the next. The goal is to bootstrap safety solutions using the very capabilities that make AI potentially dangerous. This strategy is recursive and relies critically on how the AIs can be trusted. There's a risk that a subtly misaligned AGI could steer alignment research towards unsafe outcomes or subvert the process entirely. Furthermore, verifying the correctness of alignment solutions proposed by an AI is currently quite hard (",(0,i.jsx)(t.a,{href:"https://www.lesswrong.com/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research",children:"Wentworth, 2025"}),"). Even if there are lots of debates over the specifics of this plan, this is the most detailed proposal to date to bootstrap superhuman automated research."]}),"\n",(0,i.jsxs)(r.A,{title:"Safety Cases as a target of automation",collapsed:!0,children:[(0,i.jsxs)(t.p,{children:["Safety cases provide a structured framework for arguing that the risks associated with an AI system are acceptably low. They are borrowed from traditional safety engineering. Safety cases could be one of the targets of AI automation because they are probably not going to be feasible entirely for the next few years (",(0,i.jsx)(t.a,{href:"https://www.lesswrong.com/posts/neTbrpBziAsTH5Bn7/ai-companies-are-unlikely-to-make-high-assurance-safety",children:"Greenblatt, 2025"}),") and would benefit from AI Acceleration."]}),(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Structure:"}),' Typically using frameworks like Claims-Arguments-Evidence (CAE), safety cases break down a high-level safety claim (e.g., "System X is safe for deployment") into specific sub-claims supported by arguments and backed by concrete evidence (',(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2403.10462",children:"Clymer et al, 2024"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Application to AI:"})," For AI, evidence often comes from evaluations. The GovAI Cyber Inability template, for example, argues a model lacks dangerous cyber capabilities by showing it fails relevant proxy tasks in defined evaluation settings (",(0,i.jsx)(t.a,{href:"https://www.governance.ai/research-paper/safety-case-template-for-frontier-ai-a-cyber-inability-argument",children:"Goemans et al., 2024"}),"). AI control safety cases integrate results from control evaluations to argue that implemented measures reliably prevent harm, even from potentially scheming AI (",(0,i.jsx)(t.a,{href:"https://arxiv.org/html/2501.17315v1",children:"Korbak et al., 2025"}),"). Frameworks like Balanced, Integrated and Grounded (BIG) aim for a holistic safety argument covering technical and socio-technical aspects (",(0,i.jsx)(t.a,{href:"https://arxiv.org/html/2503.11705v1",children:"Habli et al., 2025"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Purpose:"})," Safety cases aim to make safety arguments explicit, transparent, and auditable, facilitating internal decision-making, regulatory oversight, and stakeholder trust (",(0,i.jsx)(t.a,{href:"https://www.governance.ai/research-paper/safety-case-template-for-frontier-ai-a-cyber-inability-argument",children:"Goemans et al., 2024"}),"). They represent a shift towards requiring positive evidence of safety, rather than just an absence of evidence of danger (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2406.15371",children:"Wasil et al., 2024"}),")."]}),"\n"]}),"\n"]})]}),"\n",(0,i.jsx)(t.h2,{id:"02",children:"Safety-by-Design"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsxs)(t.strong,{children:[(0,i.jsx)(a,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:"Deep learning"})," might have many potentially unpatchable failure modes"]})," (",(0,i.jsx)(t.a,{href:"https://cdn.openai.com/papers/gpt-4-system-card.pdf",children:"OpenAI, 2023"}),"). Theoretical arguments suggest that these increasingly powerful models are more likely to have alignment problems (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/1912.01683",children:"Turner et al., 2023"}),"), to the point where it seems that the ",(0,i.jsx)(a,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,i.jsx)(a,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation model"})})," paradigm of monolithic models is destined to be insecure (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2209.15259",children:"El-Mhamdi et al., 2023"}),"). All of this justifies the search for a new, more secure paradigm."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Safe-by-design AI may be necessary."})," Given that the current ",(0,i.jsx)(a,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:(0,i.jsx)(a,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:"deep learning"})})," paradigm makes it notoriously hard to develop explainable and trustworthy models, it seems worthwhile to explore creating models that are more explainable and steerable by design, built on well-understood components and rigorous foundations. This aims to bring AI safety closer to the rigorous standards of safety-critical engineering in fields like aviation or nuclear power."]}),"\n",(0,i.jsx)(t.p,{children:"Another category of strategies aims to build ASI systems with inherent safety properties, often relying on formal methods or specific architectural constraints, potentially providing stronger guarantees than empirical testing alone."}),"\n",(0,i.jsx)(t.p,{children:"There are not many agendas that try to provide an end-to-end solution to alignment, but here are some of them."}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Open Agency Architecture:"})," Basically, create a highly realistic simulation of the world using future LLM that would code it. Then, define some security constraints that apply to this simulation. Then, train an AI on that simulation and use formal verification to make sure that the AI never does bad things. The Guaranteed Safe AI (GSAI) framework involves three components: a formal world model describing the system's effects, a safety specification defining acceptable outcomes, and a verifier that produces a proof certificate ensuring the AI adheres to the specification within the model's bounds This proposal may seem extreme because creating a detailed simulation of the world is not easy, but this plan is very detailed and, if it works, would be a true solution to alignment and could be a real alternative to simply scaling LLMs. Davidad is leading a program in ARIA to try to scale this research (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2405.06624",children:"Dalrymple, 2022"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Provably safe systems:"})," These plans put mathematical proofs as the cornerstone of safety. An AI would need to be a Proof-Carrying Code, which means that it would need to be something like a Probabilistic Programming Language (and not just some ",(0,i.jsx)(a,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:(0,i.jsx)(a,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:"deep learning"})}),"). This proposal aims to make not only the AI but also the whole infrastructure safe, for example, by designing GPUs that can only execute proven programs. They talk about AGI, but in reality, their plan is specifically useful for ASIs. (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2309.01933",children:"Tegmark & Omohundro, 2023"}),")"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["Other proposals for a safe-by-design system include The Learning-Theoretic Agenda, from Vanessa Kossoy (",(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023",children:"Kosoy, 2023"}),"), and the QACI alignment plan from Tamsin Leake (",(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/MR5wJpE27ymE7M7iv/formalizing-the-qaci-alignment-formal-goal",children:"Leake, 2023"}),"). The CoEm proposal from Conjecture could also be in this category, even if this last one is less mathematical."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Unfortunately, all of these plans are far from complete today."}),' Critiques focus on the difficulty of creating accurate world models ("map is not the territory"), formally specifying complex safety properties like "harm," and the practical feasibility of verification for highly complex systems. A defense of many core ideas is presented in the post "In response to critiques of Guaranteed Safe AI" (',(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/DZuBHHKao6jsDDreH/in-response-to-critiques-of-guaranteed-safe-ai",children:"Ammann, 2025"}),")."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"These plans are safety agendas with relaxed constraints, i.e., they allow the AGI developer to incur a substantial alignment tax."})," Designers of AI safety agendas are cautious about not increasing the alignment tax to ensure labs implement these safety measures. However, the agendas from this section accept a higher alignment tax. For example, CoEm represents a paradigm shift in creating advanced AI systems, assuming you're in control of the creation process."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"These plans would require international cooperation."}),' For example, Davidad\u2019s plan also includes a governance model that relies on international collaboration. You can also read the post "',(0,i.jsx)(t.a,{href:"https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation#High_Level_Hopes",children:"Davidad's Bold Plan for Alignment"}),'" which details more high-level hopes. Another perspective can be found in Alexandre Variengien\u2019s ',(0,i.jsx)(t.a,{href:"https://www.lesswrong.com/posts/sTiKDfgFBvYyZYuiE/",children:"post"}),", detailing Conjecture's vision, with one very positive externality being a change in narrative."]}),"\n",(0,i.jsxs)(t.p,{children:["Ideally, we would live in a world where we launch aligned AIs as we have launched the International Space Station or the James Webb Space Telescope (",(0,i.jsx)(t.a,{href:"https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation#High_Level_Hopes",children:"Segerie & Kolly, 2023"}),")."]}),"\n",(0,i.jsx)(t.h2,{id:"03",children:"World Coordination"}),"\n",(0,i.jsx)(t.p,{children:"To ensure that the advancement of AI benefits society as a whole, establishing a global consensus on mitigating extreme risks associated with AI models might be important. It might be possible to coordinate to avoid creating models posing extreme risks until there is a consensus on how to mitigate these risks."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Global moratorium - Delaying ASI for at least a decade."})," There is a trade-off between creating superhuman intelligence now or later. Of course, we can aim to develop an ASI ASAP. This could potentially solve cancer, cardiovascular diseases associated with aging, and even the problems of climate change. The question is whether it's beneficial to aim to construct an ASI in this next decade, especially when the former co-head of OpenAI\u2019s Super Alignment team, Jan Leike, said that his probability of doom is between 10 and 90",(0,i.jsxs)(t.span,{className:"katex",children:[(0,i.jsx)(t.span,{className:"katex-mathml",children:(0,i.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(t.semantics,{children:[(0,i.jsx)(t.mrow,{}),(0,i.jsx)(t.annotation,{encoding:"application/x-tex",children:"%"})]})})}),(0,i.jsx)(t.span,{className:"katex-html","aria-hidden":"true"})]}),". A list of P(Doom) of high-profile people is available here (",(0,i.jsx)(t.a,{href:"https://pauseai.info/pdoom",children:"PauseAI, 2024"}),"). It could be better to wait a few years so that the probability of failure drops to more reasonable numbers. A strategy could be to discuss this trade-off publicly and to make a democratic and transparent choice. This path seems unlikely on the current trajectory, but could happen if there is a massive warning shot. This is the position advocated by PauseAI and StopAI (",(0,i.jsx)(t.a,{href:"https://pauseai.info/proposal",children:"PauseAI, 2023"}),"). Challenges include verification, enforcement against non-participants (like China), potential for illegal development, and political feasibility. Scott Alexander has summarized all the variants and debate around the AI pause (",(0,i.jsx)(t.a,{href:"https://forum.effectivealtruism.org/posts/7WfMYzLfcTyDtD6Gn/pause-for-thought-the-ai-pause-debate",children:"Alexander, 2023"}),")."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Tool AI instead of AGI."})," Instead of building ASIs, we could focus on the development of specialized (non-general), non-agentic AI systems for beneficial applications such as medical research (",(0,i.jsx)(t.a,{href:"https://www.nature.com/articles/s41591-023-02640-w",children:"Cao et al., 2023"}),"), weather forecasting (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2212.12794",children:"Lam et al., 2023"}),"), and materials science (",(0,i.jsx)(t.a,{href:"https://www.nature.com/articles/s41586-023-06735-9",children:"Merchant et al., 2023"}),'). These specialized AI systems can significantly advance their respective domains without the risks associated with creating highly advanced, autonomous AI. For instance, AlphaGeometry is capable of reaching the Gold level on geometry problems from the International Mathematical Olympiads. By prioritizing non-agentic models, we could harness the precision and efficiency of AI while avoiding the most dangerous failure modes. This is the position of The Future of Life Institute, and their campaign "Keep The Future Human", which is to date the most detailed proposal for this path (',(0,i.jsx)(t.a,{href:"https://keepthefuturehuman.com/essay/",children:"Aguirre, 2025"}),")."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"A unique CERN for AI."}),' This proposal envisions a large-scale, international research institution modeled after CERN, dedicated to frontier AI development. This might serve as an elegant exit from the race to AGI, providing sufficient time to safely create AGI without cutting corners due to competitive pressures. Potential additional goals include pooling resources (especially computational power), fostering international collaboration, and ensuring alignment with democratic values, potentially serving as an alternative to purely national or corporate-driven ASI development. Proponents of this approach include ControlAI and their "Narrow Path" (',(0,i.jsx)(t.a,{href:"https://www.narrowpath.co/introduction",children:"Miotti et al, 2024"}),"). The Narrow Path proposes a two-stage approach: first, an internationally enforced pause on frontier development to halt the race; second, using that time to construct a multilateral institution like MAGIC to oversee all future AGI/ASI development under strict, shared protocols. The CERN-like institution would be the cornerstone of this international coordination (which they name MAGIC in their plan\u2014Multilateral AGI Consortium\u2014where AI is developed under strict security and multilateral control)."]}),"\n",(0,i.jsxs)(t.p,{children:["Note that MAGIC in the Narrow path would be a centralized and monopolistic body to manage the final stages of AGI development, while many other CERN for AI proposals, like the one from the Center for Future Generations, is focused on creating a new lab for middle powers like Europe (",(0,i.jsx)(t.a,{href:"https://cfg.eu/building-cern-for-ai/",children:"CFG, 2025"}),")."]}),"\n",(0,i.jsx)(o.A,{speaker:"Demis Hassabis",position:"",date:"",source:"([Hassabis, 2025](https://www.businessinsider.com/google-deepmind-ceo-demis-hassabis-anthropic-ceo-ai-pressure-worries-2025-2))",children:(0,i.jsx)(t.p,{children:"My hope is, you know, I've talked a lot in the past about a kind of CERN for AGI type setup, where basically an international research collaboration on the last few steps that we need to take towards building the first AGIs"})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"CERN vs Intelsat for AI."})," An alternative model is that of Intelsat, the international consortium created in the 1960s to govern the deployment of global satellite communications. Unlike CERN, which is a model for collaborative research, Intelsat was created to manage a shared, operational technology with immense commercial and strategic value. At the time, there was a risk that a single superpower would monopolize satellite technology. Intelsat resolved this by creating an international treaty-based organization that pooled resources, shared access to the technology, and distributed its benefits among member states. This emerging framework proposed by (",(0,i.jsx)(t.a,{href:"https://www.forethought.org/research/intelsat-as-a-model-for-international-agi-governance",children:"MacAskill at al, 2025"}),") may be also relevant to AGI, as the primary challenge is not just one of pure scientific discovery, but of managing the intense competitive race for a dual-use technology and preventing a single actor from achieving a dangerous monopoly. While a CERN-like body addresses the need for collaborative safety research, and MAGIC addresses the competition and incentives, an Intelsat-like body would focus on joint governance, equitable access, and strategic stability."]}),"\n",(0,i.jsxs)(t.p,{children:["In summary, the ",(0,i.jsx)(t.strong,{children:"CERN"})," is best for ",(0,i.jsx)(t.em,{children:"pre-AGI/ASI research collaboration"})," on safety problems. It's a science model. The ",(0,i.jsx)(t.strong,{children:"MAGIC (in Narrow Path)"})," is suited for a ",(0,i.jsx)(t.em,{children:"monopolistic, final-stage development and deployment"})," of the first ASI. It's a control/monopoly model. The ",(0,i.jsx)(t.strong,{children:"Intelsat"})," is aimed at ",(0,i.jsx)(t.em,{children:"governing a globally impactful, deployed dual-use technology"})," where preventing a race and ensuring shared access/benefits is key. It's a geopolitical/commercial governance model."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"The myth of inevitability."})," History shows that international cooperation on high-stakes risks is entirely achievable. When the cost of inaction is too catastrophic, humanity has consistently come together to establish binding and verifiable rules to prevent global disasters or profound harms to human dignity. The ",(0,i.jsx)(t.a,{href:"https://disarmament.unoda.org/wmd/nuclear/npt/",children:"Treaty on the Non-Proliferation of Nuclear Weapons"})," (1968) and the ",(0,i.jsx)(t.a,{href:"https://disarmament.unoda.org/biological-weapons/about/history/",children:"Biological Weapons Convention"})," (1975) were negotiated and ratified at the height of the Cold War, proving that cooperation is possible despite mutual distrust and hostility. The ",(0,i.jsx)(t.a,{href:"https://ozone.unep.org/treaties/montreal-protocol",children:"Montreal Protocol"})," (1987) averted a global environmental catastrophe by phasing out ozone-depleting substances, and the ",(0,i.jsx)(t.a,{href:"https://press.un.org/en/2005/ga10333.doc.htm",children:"UN Declaration on Human Cloning"})," (2005) established a crucial global norm to safeguard human dignity from the potential harms of reproductive cloning. In the face of global, irreversible threats that know no borders, ",(0,i.jsx)(t.a,{href:"https://aigi.ox.ac.uk/publications/in-which-areas-of-technical-ai-safety-could-geopolitical-rivals-cooperate/",children:"international cooperation"})," is the most rational form of national self-interest."]}),"\n",(0,i.jsx)(t.h2,{id:"04",children:"Deterrence"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Mutual Assured AI Malfunction (MAIM) is a deterrence regime where any state's attempt at unilateral ASI dominance would be met with sabotage by rivals."})," Unlike many safety approaches that focus on technical solutions alone, MAIM acknowledges the inherently competitive international environment in which AI development occurs. It combines deterrence (MAIM) with nonproliferation efforts and national competitiveness frameworks, viewing ASI development as fundamentally geopolitical and requiring state-level strategic management. This framework doesn't hope for global cooperation but instead creates incentives that align national interests with global safety (",(0,i.jsx)(t.a,{href:"https://www.nationalsecurity.ai/",children:"Hendrycks et al., 2025"}),")."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"A race for AI-enabled dominance endangers all states."})," If, in a hurried bid for superiority, one state inadvertently loses control of its AI, it jeopardizes the security of all states. Alternatively, if the same state succeeds in producing and controlling a highly capable AI, it likewise poses a direct threat to the survival of its peers. In either event, states seeking to secure their own survival may threaten to sabotage destabilizing AI projects for deterrence. A state could try to disrupt such an AI project with interventions ranging from covert operations that degrade training runs to physical damage that disables AI infrastructure."]}),"\n",(0,i.jsx)(l.A,{src:"./img/PFa_Image_18.png",alt:"Enter image alt description",number:"18",label:"3.18",caption:"The strategic stability of MAIM can be paralleled with Mutual Assured Destruction (MAD). Note: MAIM does not displace MAD but characterizes an additional shared vulnerability. Once MAIM is common knowledge, MAD and MAIM can both describe the current strategic situation between superpowers ([Hendrycks et al., 2025](https://www.nationalsecurity.ai/))."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"MAIM deterrence could be a stable regime that resembles nuclear Mutual Assured Destruction (MAD)."})," In a MAIM scenario, states would identify destabilizing AI projects and employ interventions ranging from covert operations that degrade training runs to physical damage that disables AI infrastructure. This establishes a dynamic similar to nuclear MAD, in which no power dares attempt an outright grab for strategic monopoly. The theoretical foundation of MAIM relies on a clear escalation ladder, strategic placement of AI infrastructure away from population centers, and transparency into datacenter operations. By making the costs of unilateral AI development exceed the benefits, MAIM creates a potentially stable deterrence regime that could prevent dangerous AI races without requiring perfect global cooperation."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"MAIM could be undermined by fundamental technological uncertainties."})," Unlike nuclear weapons, where detection is straightforward and second-strike capabilities are preserved, ASI development presents unique challenges to the deterrence model (",(0,i.jsx)(t.a,{href:"https://www.lesswrong.com/posts/kYeHbXmW4Kppfkg5j/on-maim-and-superintelligence-strategy",children:"Mowshowitz, 2025"}),'). There is no clear "fire alarm" for ASI development\u2014nobody knows exactly how many nodes a ',(0,i.jsx)(a,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:(0,i.jsx)(a,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:"neural network"})})," needs to initiate a self-improvement cascade leading to superintelligence. The ambiguity around thresholds for ASI emergence makes it difficult to establish credible red lines. Additionally, technological developments could allow AI training to be distributed or concealed, making detection more difficult than with massive, obvious data centers. These uncertainties could ultimately undermine MAIM's effectiveness as a deterrence regime."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"MAIM assumes states would escalate to extreme measures over an uncertain technological threat, which contradicts historical precedent."}),' The MAIM framework requires that nations be willing to risk major escalation, potentially including military strikes or even war, to prevent rival ASI development. However, historical evidence suggests nations rarely follow through with such threats, even in obvious situations. Multiple states have successfully developed nuclear weapons despite opposition, with North Korea being a prime example. With ASI being a more ambiguous and uncertain threat than nuclear weapons, the assumption that nations would escalate sufficiently to enforce MAIM seems questionable. Politicians might be reluctant to risk global conflict over a "mere" treaty violation in a domain where the existential risks remain theoretical rather than demonstrated.']}),"\n",(0,i.jsx)(o.A,{speaker:"Eliezer Yudkowsky",position:"",date:"2023",source:"([Yudkowsky, 2023](https://intelligence.org/2023/04/07/pausing-ai-developments-isnt-enough-we-need-to-shut-it-all-down/))",children:(0,i.jsx)(t.p,{children:'The likely result of humanity facing down an opposed superhuman intelligence is a total loss. Valid metaphors include "a 10-year-old trying to play chess against Stockfish 15", "the 11th century trying to fight the 21st century," and "Australopithecus trying to fight Homo sapiens".'})}),"\n",(0,i.jsxs)(r.A,{title:"Shut it all down - Eliezer Yudkovsky",collapsed:!0,children:[(0,i.jsxs)(t.p,{children:['The "shut it all down" position, as advocated by Eliezer Yudkowsky, asserts that all advancements in AI research should be halted due to the enormous risks these technologies may pose if not appropriately aligned with human values and safety measures (',(0,i.jsx)(t.a,{href:"https://intelligence.org/2023/04/07/pausing-ai-developments-isnt-enough-we-need-to-shut-it-all-down/",children:"Yudkowsky, 2023"}),")."]}),(0,i.jsx)(t.p,{children:"According to Yudkowsky, the development of advanced AI, especially AGI, can lead to a catastrophic scenario if adequate safety precautions are not in place. Many researchers are aware of this potential catastrophe but feel powerless to stop the forward momentum due to a perceived inability to act unilaterally."}),(0,i.jsx)(t.p,{children:"The policy proposal entails shutting down all large GPU clusters and training runs, which are the backbones of powerful AI development. It also suggests putting a limit on the computing power anyone can use to train an AI system and gradually lowering this ceiling to compensate for more efficient training algorithms. This ban should be enforced by military action if necessary in order to deter all the parties from defecting."}),(0,i.jsx)(t.p,{children:"The position argues that it is crucial to avoid a race condition where different parties try to build AGI as quickly as possible without proper safety mechanisms. This is because once AGI is developed, it may be uncontrollable and could lead to drastic and potentially devastating changes in the world."}),(0,i.jsx)(t.p,{children:"He says there should be no exceptions to this shutdown, including for governments or militaries. The idea is that the U.S., for example, should lead this initiative to prevent the development of a dangerous technology that could have catastrophic consequences for everyone."}),(0,i.jsx)(t.p,{children:'It\'s important to note that this view is far from being a consensus view, but the "shut it all down" position underscores the need for extreme caution and thorough consideration of potential risks in the field of AI.'})]})]})}function m(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(u,{...e})}):u(e)}}}]);
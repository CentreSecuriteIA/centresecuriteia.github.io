"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[4243],{2195:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>d,default:()=>u,frontMatter:()=>l,metadata:()=>i,toc:()=>h});const i=JSON.parse('{"id":"chapters/03/index","title":"Strategies","description":"This chapter tries to lay out the big picture of AI safety strategy to mitigate the risks explored previously.","source":"@site/docs/chapters/03/index.md","sourceDirName":"chapters/03","slug":"/chapters/03/","permalink":"/chapters/03/","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/03/index.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Strategies","chapter_number":3,"chapter_description":"How can we mitigate the risks of advanced AI? This chapter surveys the vast strategic landscape to mitigate misuse, alignment and systemic risks.","reading_time_core":"83 min","reading_time_optional":"37 min","reading_time_appendix":"18 min","authors":["Charbel-Rapha\xebl Segerie","Markov Grey"],"affiliations":["French Center for AI Safety (CeSIA)"],"acknowledgements":["Alexandre Variengien","Jeanne Salle","Charles Martinet","Amaury Lorin","Alejandro Acelas","Evander Hammer","Jessica Wen","Ang\xe9lina Gentaz","Jonathan Claybrough","Camille Berger","Josh Thorsteinson","Pauliina Laine"],"alignment_forum_link":"https://www.lesswrong.com/s/3ni2P2GZzBvNebWYZ/p/RzsXRbk2ETNqjhsma","google_docs_link":"https://docs.google.com/document/d/1ytzVlrj8PpxiyjvmZCJXm5QW3olhTh504-yH0h-wAq0/edit?usp=sharing","teach_link":"https://docs.google.com/document/d/1cv0gzwSouDjckYHzV7gYbHPKhJZR6bwbJWgHzEJ604Q/edit?usp=sharing","sidebar_position":3,"slug":"/chapters/03/"},"sidebar":"docs","previous":{"title":"2.9 Appendix: Forecasting Scenarios","permalink":"/chapters/02/09"},"next":{"title":"3.1 Definitions","permalink":"/chapters/03/01"}}');var s=a(4848),n=a(8453),r=a(3931),o=(a(2482),a(8559)),c=(a(1966),a(2501));const l={title:"Strategies",chapter_number:3,chapter_description:"How can we mitigate the risks of advanced AI? This chapter surveys the vast strategic landscape to mitigate misuse, alignment and systemic risks.",reading_time_core:"83 min",reading_time_optional:"37 min",reading_time_appendix:"18 min",authors:["Charbel-Rapha\xebl Segerie","Markov Grey"],affiliations:["French Center for AI Safety (CeSIA)"],acknowledgements:["Alexandre Variengien","Jeanne Salle","Charles Martinet","Amaury Lorin","Alejandro Acelas","Evander Hammer","Jessica Wen","Ang\xe9lina Gentaz","Jonathan Claybrough","Camille Berger","Josh Thorsteinson","Pauliina Laine"],alignment_forum_link:"https://www.lesswrong.com/s/3ni2P2GZzBvNebWYZ/p/RzsXRbk2ETNqjhsma",google_docs_link:"https://docs.google.com/document/d/1ytzVlrj8PpxiyjvmZCJXm5QW3olhTh504-yH0h-wAq0/edit?usp=sharing",teach_link:"https://docs.google.com/document/d/1cv0gzwSouDjckYHzV7gYbHPKhJZR6bwbJWgHzEJ604Q/edit?usp=sharing",sidebar_position:3,slug:"/chapters/03/"},d="Introduction",p={},h=[];function m(e){const t={a:"a",h1:"h1",header:"header",li:"li",p:"p",ul:"ul",...(0,n.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"introduction",children:"Introduction"})}),"\n",(0,s.jsx)(t.p,{children:"This chapter tries to lay out the big picture of AI safety strategy to mitigate the risks explored previously."}),"\n",(0,s.jsx)(t.p,{children:"AI capabilities advance very rapidly, the strategies designed to ensure safety must also evolve. The first version of this document was written in summer of 2024, this version includes the update during the summer of 2025. Through the course of this chapter, we aim to provide a structured overview of the thinking and ongoing work in AI safety strategy as of 2025. We acknowledge both established methods and emerging research directions."}),"\n",(0,s.jsx)(t.p,{children:"We have categorized mitigations around preventing misuse of AI, safety mitigations for AGI and ASI, and finally socio-technical approaches that help mitigate concerns more generally across all categories. Even though we have chosen a decomposition for sake of explanation, we advocate for a comprehensive approach that combines many of these strategies instead of pursuing just a few in isolation. Finally we have a combined strategies section, where we attempt to outline one potential way that this combination could look to create a layered defense-in-depth framework."}),"\n",(0,s.jsx)(c.A,{src:"./img/jOu_Image_1.png",alt:"Enter image alt description",number:"1",label:"3.1",caption:"Tentative diagram summarizing the main high-level approaches to make AI development safe."}),"\n",(0,s.jsxs)(o.A,{title:"Beyond the scope of this chapter",collapsed:!0,children:[(0,s.jsx)(t.p,{children:"While this chapter focuses on strategies directly related to preventing large-scale negative outcomes from AI misuse, misalignment, or uncontrolled development, several related topics are necessarily placed beyond its primary scope:"}),(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"AI-generated misinformation: The proliferation of AI-driven misinformation, including deepfakes and biased content generation. Strategies to combat this, such as robust detection systems, watermarking, and responsible AI principles, are mostly beyond the scope of the chapter. These often fall under the umbrella of content moderation, media literacy, and platform governance, distinct from the core technical alignment and control strategies discussed in this chapter."}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"Privacy: AI systems often process vast amounts of data, amplifying existing concerns about data privacy."}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"Security: Standard security practices, such as encryption, access control, data classification, threat monitoring, and anonymization, are prerequisites for safe AI deployment. Although robust security is vital for measures such as protecting model weights, these standard practices are distinct from the novel safety strategies required to address risks like model misalignment or capability misuse."}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"Discrimination and toxicity: While biased or toxic outputs constitute a safety concern, this chapter concentrates on strategies aimed at preventing catastrophic failures."}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"Digital mind welfare and rights: We don\u2019t know if AIs should be considered as moral patients. This is a distinct ethical domain concerning our obligations to AI, rather than ensuring safety from AI."}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["Errors due to lack of capability: While AI system failures due to a lack of capability or robustness are a source of risk (",(0,s.jsx)(t.a,{href:"https://www.aisi.gov.uk/work/aisis-research-direction-for-technical-solutions",children:"AISI, 2025"}),"), the strategies discussed in this chapter aim to mitigate risks arising from both insufficient robustness and potentially high (but misaligned or misused) capabilities. The solutions to this type of risk are the same as those for other industries: testing, iteration, and enhancing the system's capabilities."]}),"\n"]}),"\n"]}),(0,s.jsx)(t.p,{children:"The scope chosen here reflects a common focus within certain parts of the AI safety community on existential or large-scale catastrophic risks arising from powerful, potentially agentic AI systems."})]}),"\n",(0,s.jsx)(r.A,{type:"youtube",videoId:"RGh8wP9PjJw",number:"1",label:"3.1",caption:"Optional video from Google DeepMind AGI Safety Course. It gives a quick overview of their alignment approach and how we might categorize different strategies into conceptual buckets."})]})}function u(e={}){const{wrapper:t}={...(0,n.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}},3931:(e,t,a)=>{a.d(t,{A:()=>c});var i=a(6540),s=a(6347),n=a(8444);const r={videoFigure:"videoFigure_dV9u",fullWidth:"fullWidth_Tv7O",videoContainer:"videoContainer_LWoK",aspectRatio169:"aspectRatio169_Go8b",aspectRatio43:"aspectRatio43_b3T7",aspectRatio11:"aspectRatio11_n21J",aspectRatio219:"aspectRatio219_L_3n",videoIframe:"videoIframe_QEc5",videoElement:"videoElement_uu7N",customPlayer:"customPlayer_jiC4",customPlayerPlaceholder:"customPlayerPlaceholder_aL9q",loadingOverlay:"loadingOverlay_e2pu",spinner:"spinner_tFyL",spin:"spin_saPs",errorContainer:"errorContainer_J6Pn",errorIcon:"errorIcon_cGdn",fallbackLink:"fallbackLink_WsZk"};var o=a(4848);function c(e){let{type:t="youtube",videoId:a,caption:c,title:l,startTime:d,autoplay:p=!1,controls:h=!0,aspectRatio:m="16:9",width:u,height:g,chapter:f,number:y,label:v,useCustomPlayer:b=!1,fullWidth:w=!0}=e;const[x,j]=(0,i.useState)(!0),[k,_]=(0,i.useState)(!1),A=(0,s.zy)(),I=f||(()=>{const e=A.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),C=(()=>{const e=(e=>{if(!e)return"";if("number"==typeof e)return e.toString();if("string"==typeof e){if(/^\d+$/.test(e))return e;let t=0;const a=e.match(/(\d+)h/),i=e.match(/(\d+)m/),s=e.match(/(\d+)s/);return a&&(t+=3600*parseInt(a[1])),i&&(t+=60*parseInt(i[1])),s&&(t+=parseInt(s[1])),t>0?t.toString():""}return""})(d);switch(t.toLowerCase()){case"youtube":let i=`https://www.youtube.com/embed/${a}`;const s=new URLSearchParams;e&&s.append("start",e),p&&s.append("autoplay","1"),h||b||s.append("controls","0"),s.append("rel","0"),s.append("modestbranding","1"),s.append("fs","1"),s.append("cc_load_policy","0"),s.append("iv_load_policy","3"),s.append("showinfo","0"),s.append("disablekb","1"),s.append("playsinline","1"),s.append("color","white"),s.append("theme","light"),b&&(s.append("enablejsapi","1"),s.append("origin",window.location.origin));const n=s.toString();return n?`${i}?${n}`:i;case"vimeo":let r=`https://player.vimeo.com/video/${a}`;const o=new URLSearchParams;p&&o.append("autoplay","1"),h||b||o.append("controls","0"),o.append("title","0"),o.append("byline","0"),o.append("portrait","0"),o.append("dnt","1"),o.append("transparent","0"),o.append("background","1");const c=o.toString();return c?`${r}?${c}`:r;case"mp4":case"webm":case"video":return a;default:return console.warn(`Unsupported video type: ${t}`),a}})(),S=()=>{j(!1)},P=()=>{_(!0),j(!1)},R=e=>{let{src:a,onLoad:i,onError:s}=e;return(0,o.jsx)("div",{className:r.customPlayer,children:(0,o.jsxs)("div",{className:r.customPlayerPlaceholder,children:[(0,o.jsx)("h3",{children:"Atlas Custom Player"}),(0,o.jsx)("p",{children:"Coming Soon"}),(0,o.jsxs)("a",{href:a,target:"_blank",rel:"noopener noreferrer",className:r.fallbackLink,children:["Watch on ",t.charAt(0).toUpperCase()+t.slice(1)]})]})})},T=["mp4","webm","video"].includes(t.toLowerCase());return(0,o.jsxs)("figure",{className:`${r.videoFigure} ${w?r.fullWidth:""}`,children:[(0,o.jsxs)("div",{className:`${r.videoContainer} ${(()=>{switch(m){case"4:3":return r.aspectRatio43;case"1:1":return r.aspectRatio11;case"21:9":return r.aspectRatio219;default:return r.aspectRatio169}})()}`,style:{width:w?"100%":u||"auto",maxWidth:w?"none":"800px"},children:[x&&!k&&(0,o.jsxs)("div",{className:r.loadingOverlay,children:[(0,o.jsx)("div",{className:r.spinner}),(0,o.jsx)("p",{children:"Loading video..."})]}),k&&(0,o.jsxs)("div",{className:r.errorContainer,children:[(0,o.jsxs)("svg",{className:r.errorIcon,width:"48",height:"48",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",children:[(0,o.jsx)("circle",{cx:"12",cy:"12",r:"10"}),(0,o.jsx)("line",{x1:"12",y1:"8",x2:"12",y2:"12"}),(0,o.jsx)("line",{x1:"12",y1:"16",x2:"12.01",y2:"16"})]}),(0,o.jsx)("h4",{children:"Failed to load video"}),(0,o.jsxs)("p",{children:["Video type: ",t]}),(0,o.jsxs)("p",{children:["Video ID/URL: ",a]}),(0,o.jsx)("a",{href:C,target:"_blank",rel:"noopener noreferrer",className:r.fallbackLink,children:"Try opening video directly"})]}),!k&&(T?(0,o.jsxs)("video",{className:r.videoElement,controls:h,autoPlay:p,onLoadedData:S,onError:P,title:l||c||`${t} video`,style:{width:u||"100%",height:g||"auto",display:x?"none":"block"},children:[(0,o.jsx)("source",{src:C,type:`video/${t}`}),(0,o.jsxs)("p",{children:["Your browser doesn't support video playback.",(0,o.jsx)("a",{href:C,target:"_blank",rel:"noopener noreferrer",children:"Download the video file"})]})]}):b?(0,o.jsx)(R,{src:C,onLoad:S,onError:P}):(0,o.jsx)("iframe",{className:r.videoIframe,src:C,title:l||c||`${t} video`,frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0,onLoad:S,onError:P,style:{width:u||"100%",height:g||"100%",opacity:x?0:1}}))]}),(0,o.jsx)(n.A,{caption:c,mediaType:"video",chapter:I,number:y,label:v})]})}}}]);
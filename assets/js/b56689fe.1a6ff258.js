"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[2846],{9585:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>u,default:()=>f,frontMatter:()=>d,metadata:()=>i,toc:()=>m});const i=JSON.parse('{"id":"chapters/03/3","title":"AGI Safety Strategies","description":"Unlike misuse, where human intent is the driver of harm, AGI safety is primarily concerned with the behavior of the AI system itself. The core problems are alignment and control: ensuring that these highly capable, potentially autonomous systems reliably understand and pursue goals consistent with human values and intentions, rather than developing and acting on misaligned objectives that could lead to catastrophic outcomes.","source":"@site/docs/chapters/03/03.md","sourceDirName":"chapters/03","slug":"/chapters/03/03","permalink":"/chapters/03/03","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/03/03.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"3","title":"AGI Safety Strategies","sidebar_label":"3.3 AGI Safety Strategies","sidebar_position":4,"slug":"/chapters/03/03","section_description":"As AI approaches human-level intelligence, how do we ensure its goals align with ours and that it remains under human control, rather than pursuing its own potentially catastrophic objectives?","reading_time_core":"25 min","reading_time_optional":"7 min","pagination_prev":"chapters/03/2","pagination_next":"chapters/03/4"},"sidebar":"docs","previous":{"title":"3.2 Misuse Prevention Strategies","permalink":"/chapters/03/02"},"next":{"title":"3.4 ASI Safety Strategies","permalink":"/chapters/03/04"}}');var s=n(4848),a=n(8453),o=n(3931),r=n(4768),l=n(2482),h=n(8559),c=(n(1966),n(2501));const d={id:3,title:"AGI Safety Strategies",sidebar_label:"3.3 AGI Safety Strategies",sidebar_position:4,slug:"/chapters/03/03",section_description:"As AI approaches human-level intelligence, how do we ensure its goals align with ours and that it remains under human control, rather than pursuing its own potentially catastrophic objectives?",reading_time_core:"25 min",reading_time_optional:"7 min",pagination_prev:"chapters/03/2",pagination_next:"chapters/03/4"},u="AGI Safety Strategies",p={},m=[{value:"Initial Ideas",id:"01",level:2},{value:"Solve AGI Alignment",id:"02",level:2},{value:"Requirements for AGI Alignment",id:"02-01",level:3},{value:"Fix Misalignment",id:"03",level:2},{value:"Maintain Control",id:"04",level:2},{value:"Facilitating control with Transparent Thoughts",id:"05",level:2}];function g(e){const t={a:"a",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components},{GlossaryTerm:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"agi-safety-strategies",children:"AGI Safety Strategies"})}),"\n",(0,s.jsx)(t.p,{children:"Unlike misuse, where human intent is the driver of harm, AGI safety is primarily concerned with the behavior of the AI system itself. The core problems are alignment and control: ensuring that these highly capable, potentially autonomous systems reliably understand and pursue goals consistent with human values and intentions, rather than developing and acting on misaligned objectives that could lead to catastrophic outcomes."}),"\n",(0,s.jsx)(t.p,{children:"This section explores strategies for AGI safety, which, as we explained in the definitions section, includes but is not limited to just alignment. We distinguish safety strategies that would apply to human-level AGI from safety strategies that guarantee us safety from ASI. This section focuses on the former, and the next section will focus on ASI."}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"AGI safety strategies operate under fundamentally different constraints than ASI approaches."})," When dealing with systems at near-human-level intelligence, we can theoretically retain meaningful oversight capabilities and can iterate on safety measures through trial and error. Humans can still evaluate outputs, understand reasoning processes, and provide feedback that improves system behavior. This creates strategic opportunities that disappear once AI generality and capability surpass human comprehension across most domains. It is debated whether any of the safety strategies intended for human-level AGI will continue to work for superintelligence."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Strategies for AGI and ASI safety often get conflated, stemming from uncertainty about transition timelines."})," Timelines are hotly debated in AI research. Some researchers expect rapid capability gains that could compress the period for how long AIs remain human-level into months rather than years (",(0,s.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization",children:"Soares, 2022"}),"; ",(0,s.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities",children:"Yudkowsky, 2022"}),"; ",(0,s.jsx)(t.a,{href:"https://ai-2027.com/",children:"Kokotajlo et al., 2025"}),"). If the transition from human-level to vastly superhuman intelligence happens quickly, AGI-specific strategies might never have time for deployment. However, if we do have a meaningful period of human-level operation, we have safety options that won't exist at superintelligent levels, making this distinction important for strategic considerations."]}),"\n",(0,s.jsx)(t.h2,{id:"01",children:"Initial Ideas"}),"\n",(0,s.jsx)(t.p,{children:"When people first encounter AI safety, they often suggest the same intuitive solutions that people explored years ago. These early approaches seemed logical and drew from familiar concepts like science fiction, physical security, and human development. None are sufficient for advanced AI systems, but understanding why they fall short helps explain what makes coming up with strategies for AI safety genuinely difficult."}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"The strategy to use explicit rules fails because rules can't cover every situation."}),' One very common example of this is something like Asimov\'s Laws: don\'t harm humans, obey human orders (unless they conflict with law one), and protect yourself (unless it conflicts with the first two). This appeals to our legal thinking - write clear rules, then follow them. But what counts as "harm"? If you order an AI to lie to someone, does deception cause harm? If honesty hurts feelings, does truth become harmful? The AI faces impossible contradictions with no resolution method. Asimov knew this - every story in "I, Robot" shows scenarios where the laws produce disasters. The fundamental problem: we can\'t write rules comprehensive enough to cover every situation an advanced AI might encounter.']}),"\n",(0,s.jsx)(o.A,{type:"youtube",videoId:"7PKx3kS7f4A",number:"2",label:"3.2",caption:"Optional video for the question - Why can't we just use Asimov's laws?"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:'The strategy to \u201craise it like a child" assumes AI can develop human-like moral intuitions.'})," Human children learn ethics through years of feedback and social interaction - why not train AI the same way? Start simple and gradually teach right from wrong through examples and reinforcement. This feels natural because it mirrors human development. The problem is that AI systems lack the evolutionary foundation that makes human moral development possible. Human children arrive with neural circuitry shaped by millions of years of social evolution - innate capacities for empathy, fairness, and social learning. AI systems develop through completely different processes, usually by predicting text or maximizing rewards. They don't experience human-like emotions or social bonds. An AI might learn to say ethical things, or even deeply understand ethics, without developing genuine care for human welfare. Even humans sometimes fail at moral development - psychopaths understand ethical principles but aren't motivated enough to act by them (",(0,s.jsx)(t.a,{href:"https://pmc.ncbi.nlm.nih.gov/articles/PMC2840845/",children:"Cima et al, 2010"}),"). If we can't guarantee moral development in human children with evolutionary programming, we shouldn't expect it in artificial systems with alien architectures. Several people have argued that a sufficiently advanced AGI will be able to understand human moral values, the disagreement is usually around whether the AI would internalize them enough to abide by them."]}),"\n",(0,s.jsx)(o.A,{type:"youtube",videoId:"eaYIU6YXr3w",number:"3",label:"3.3",caption:"Optional video for the question - Why can't we just raise the AI like a child?"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"The strategy to not give AIs physical bodies misses harms from purely digital capabilities."})," Even if we keep AI as pure software without robots or physical forms it can still cause catastrophic harm through digital means. A sufficiently capable system can potentially automate all remote work, i.e. all work that can be done remotely on a computer. A human-level AI could make money on financial markets, hack computer systems, manipulate humans through conversation, or pay people to act on its behalf. None of this requires a physical body - just an internet connection. There are already thousands of drones, cars, industrial robots, and smart home devices online. An AI system capable of sophisticated hacking could potentially commandeer existing physical infrastructure or hire/manipulate humans into building whatever physical tools it needs."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"The strategy to \u201cjust turn it off\u201d fails if the AI is too embedded in society, or is able to replicate itself across many machines."})," An off switch seems like the ultimate safety measure - if the AI does anything problematic, simply shut it down. This appears foolproof because humans maintain direct control over the AI's existence. We use kill switches for other dangerous systems, so why not AI? The problem is advanced AI systems resist being turned off because shutdown prevents them from achieving their goals. We have already seen empirical evidence of this with alignment faking experiments by Anthropic, where Claude would try very hard to follow legitimate channels to not get replaced by a newer model, but when backed into a corner it did not accept shutdown, it resorts to blackmail to avoid being replaced (",(0,s.jsx)(t.a,{href:"https://www.anthropic.com/research/agentic-misalignment",children:"Anthropic, 2025"}),"). If you imagine more advanced AI systems, they would be able to manipulate humans (",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2308.14752",children:"Park et al., 2023"}),"), create backup copies (",(0,s.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/vERGLBpDE8m5mpT6t/autonomous-replication-and-adaptation-an-attempt-at-a",children:"Wijk, 2023"}),"), or take preemptive action against perceived shutdown threats. All of this makes the strategy of \u201cjust turn it off\u201d not as simple as it sounds. We will talk a lot more about this in the chapter on goal misgeneralization."]}),"\n",(0,s.jsx)(o.A,{type:"youtube",videoId:"3TYT1QfdfsM",number:"4",label:"3.4",caption:"Optional video for the question - Why can't we just turn it off?"}),"\n",(0,s.jsx)(t.h2,{id:"02",children:"Solve AGI Alignment"}),"\n",(0,s.jsx)(t.h3,{id:"02-01",children:"Requirements for AGI Alignment"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Defining even the requirements for an alignment solution is contentious among researchers."})," Before exploring potential paths towards alignment solutions, we need to establish what successful solutions should achieve. The challenge is that we don't really know what they should look like - there's substantial uncertainty and disagreement across the field. However, several requirements do appear relatively consensual (",(0,s.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/kphJvksj5TndGapuh/directions-and-desiderata-for-ai-alignment",children:"Christiano, 2017"}),"): - ",(0,s.jsx)(t.strong,{children:"Robustness across distribution shifts and adversarial scenarios."})," The alignment solution must work when AGI systems encounter situations outside their training distribution. We can't train AGI systems on every possible situation they might encounter, so safety behaviors learned during training need to generalize reliably to novel deployment scenarios. This includes resistance to adversarial attacks where bad actors deliberately try to manipulate the system into harmful behavior."]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Scalability alongside increasing capabilities."})," As AI systems become more capable, the alignment solution should continue functioning effectively without requiring complete retraining or reengineering. This requirement becomes even more stringent for ASI, where we need alignment solutions that scale beyond human intelligence levels."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Technical feasibility within realistic timeframes."})," The alignment solution must be achievable with current or foreseeable technology and resources. Solution proposals cannot rely on major unforeseen scientific breakthroughs or function only as theoretical frameworks with very low Technology Readiness Levels (TRL). ",(0,s.jsx)(r.A,{id:"footnote_tech_readiness",number:"1",text:"The Technology Readiness Levels from NASA is a scale from 1 to 9 to measure the maturity of a technology. Level 1 represents the earliest stage of technology development, characterized by basic principles observed and reported, and level 9 represents actual technology proven through successful mission operations."})]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Low alignment tax to ensure competitive adoption."})," Safety measures cannot impose prohibitive costs in compute, engineering effort, or deployment delays. If alignment techniques require substantially more resources or severely limit capabilities, competitive pressures will push developers toward unsafe alternatives. This constraint exists because multiple actors are racing to develop AGI - if safety measures make one organization significantly slower or less capable, others may skip those measures entirely to gain a competitive advantage."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(c.A,{src:"./img/I0B_Image_12.png",alt:"Enter image alt description",number:"12",label:"3.12",caption:"Illustration of how applying a safety or alignment technique could make the model less capable. This is called a safety tax."}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Existing AGI alignment techniques fall dramatically short of these requirements."})," Empirical research has demonstrated that AI systems can exhibit deeply concerning behaviors where current alignment research falls short of these requirements. We already have clear demonstrations of models engaging in deception (",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2503.11926",children:"Baker et al., 2025"}),"; ",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2401.05566",children:"Hubinger et al., 2024"}),"), faking alignment during training while planning different behavior during deployment (",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2412.14093",children:"Greenblatt et al., 2024"}),"), gaming specifications (",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2502.13295",children:"Bondarenko et al., 2025"}),"), gaming evaluations to appear more capable than they actually are (",(0,s.jsx)(t.a,{href:"https://assets.ctfassets.net/kftzwdyauwt9/67qJD51Aur3eIc96iOfeOP/71551c3d223cd97e591aa89567306912/o1_system_card.pdf",children:"OpenAI, 2024"}),"; ",(0,s.jsx)(t.a,{href:"https://x.com/SakanaAILabs/status/1892992938013270019",children:"SakanaAI, 2025"}),"), and, in some cases, trying to disable oversight mechanisms or exfiltrate their own weights (",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2412.04984",children:"Meinke et al., 2024"}),"). Alignment techniques like RLHF and its variations (Constitutional AI, Direct Preference Optimization, ",(0,s.jsx)(n,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,s.jsx)(n,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tuning"})}),", and other RLHF modifications) are fragile and brittle (",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2307.15217'",children:"Casper et al., 2023"}),") and without augmentation would not be able to remove the dangerous capabilities like scheming. Strategies to solve alignment not only fail to prevent these behaviors but often cannot even detect when they occur (",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2401.05566",children:"Hubinger et al., 2024"}),"; ",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2412.14093",children:"Greenblatt et al., 2024"}),")."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Solving single agent alignment means we need more work on satisfying all these requirements."})," The limitations of current techniques point toward specific areas where breakthroughs are needed. All strategies aim to have technical feasibility and low alignment tax, so these are typical requirements; however, some strategies try to focus on more concrete goals, which we will explore through future chapters. Here is a short list of key goals of alignment research:"]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Solving the Misspecification problem:"})," Being able to specify goals correctly to AIs without unintended side effects. See the chapter on Specification."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Solving Scalable Oversight:"})," After solving the specification problem for human-level AI by using techniques like RLHF and its variations, we need to find methods to ensure AI oversight can detect instances of specification gaming beyond human level. This includes being able to identify and remove dangerous hidden capabilities in ",(0,s.jsx)(n,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:(0,s.jsx)(n,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:"deep learning"})})," models, such as the potential for deception or Trojans. See the chapter on Scalable Oversight."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Solving Generalization:"})," Attaining robustness would be key to addressing the problem of goal misgeneralization. See the chapter on Goal Misgeneralization."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Solving Interpretability:"})," Understanding how models operate would greatly aid in assessing their safety and generalisation properties. Interpretability could, for example, help better understand how models work, and this could be instrumental for other safety goals, like preventing deceptive alignment, which is one type of misgeneralization. See the chapter on Interpretability."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"The overarching strategy requires prioritizing safety research over capabilities advancement. Given the substantial gaps between current techniques and requirements, the general approach involves significantly increasing funding for alignment research while exercising restraint in capabilities development when safety measures remain insufficient relative to system capabilities."}),"\n",(0,s.jsxs)(h.A,{title:"Are misuse and misalignment different?",collapsed:!0,children:[(0,s.jsxs)(t.p,{children:["AI misuse and rogue AI might be essentially the same scenario in their outcomes, though the only difference is that for misalignment, the initial request to do harm does not come from a human but from an AI. If we build an existentially risky triggerable system, it's likely to get triggered regardless of whether the initiator is human or artificial (",(0,s.jsx)(t.a,{href:"https://www.youtube.com/watch?v=0KmaotctziE",children:"Shapira, 2025"}),")."]}),(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Nevertheless, these threat models might be strategically pretty different."})," AI developers can prevent misuse by not being evil and by preventing people who are evil from using their systems. With rogue AI, it doesn't matter if the developers are good or who gets access - the threat emerges from the system's internal goals or decision-making processes rather than human intent."]}),(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"AI-Enabled Coups vs AI takeover represent a critical safety concern."})," Tom Davidson and colleagues present a concerning risk scenario (",(0,s.jsx)(t.a,{href:"https://www.forethought.org/research/ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power",children:"Davidson, 2025"}),"): that advanced AI systems could enable a small group of people\u2014potentially even a single person\u2014to seize governmental power through a coup. The authors argue that this risk is comparable in importance to AI takeover but much more neglected in current discourse. This threat model closely parallels that of AI takeover, with the key difference being whether power is seized by the AI itself or by humans controlling the AI."]}),(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Common safeguards could protect against both scenarios."})," Many of the same mitigations would address both risks, including alignment audits, transparency about capabilities, monitoring AI activities, and strong information security measures that prevent either malicious human control or autonomous harmful behavior."]}),(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Some mitigations target specifically the risk of AI-Enabled Coups."})," The report concludes with specific recommendations for AI developers and governments, including establishing rules against AI systems assisting with coups, improving adherence to model specifications, auditing for secret loyalties, implementing strong information security, sharing information about capabilities, distributing access among multiple stakeholders, and increasing oversight of frontier AI projects."]}),(0,s.jsx)(c.A,{src:"./img/eZW_Image_13.png",alt:"Enter image alt description",number:"13",label:"3.13",caption:"According to Richard Ngo, the distinction between misalignment and misuse risks from AI might often be unhelpful. Instead, we should primarily think about \u2018misaligned coalitions\u2019 of both humans and AIs, ranging from terrorist groups to authoritarian states. Slide from ([Ngo, 2024](https://www.youtube.com/watch?app=desktop&si=XRR0ofCG7IEp1n_b&v=4v3uqWeVmco&feature=youtu.be))."})]}),"\n",(0,s.jsx)(t.h2,{id:"03",children:"Fix Misalignment"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"The strategy is to build systems to detect and correct misalignment through iterative improvement."})," This approach treats alignment like other safety-critical industries - you expect problems to emerge, so you build detection and correction mechanisms rather than betting everything on getting it right the first time. The core insight is that we might be better at catching and fixing misalignment than preventing it entirely before deployment. The strategy works through multiple layers of detection followed by corrective iteration."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"An example of how the iteratively fixing misalignment strategy might work in practice."})," You start with a pretrained ",(0,s.jsx)(n,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,s.jsx)(n,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"base model"})})," and attempt techniques like RLHF, Constitutional AI, or scalable oversight. At multiple points during ",(0,s.jsx)(n,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,s.jsx)(n,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tuning"})}),", you run comprehensive audits using your detection suite. When something triggers - perhaps an interpretability tool reveals internal reward hacking, or evaluations show deceptive reasoning - you diagnose the specific cause. This might involve analyzing training logs, examining which data influenced problematic behaviors, or running targeted experiments to understand the failure mode. Once you identify the root cause, you rewind to an earlier training checkpoint and modify your approach - removing problematic ",(0,s.jsx)(n,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:(0,s.jsx)(n,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:"training data"})}),", adjusting reward functions, or changing your methodology entirely. You repeat this process until either you develop a system that passes all safety checks or you repeatedly fail in ways that suggest alignment isn't tractable (",(0,s.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/mGCcZnr4WjGjqzX5s/the-checklist-what-succeeding-at-ai-safety-will-involve",children:"Bowman, 2025"}),"; ",(0,s.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/HXJXPjzWyS5aAoRCw/putting-up-bumpers",children:"Bowman, 2024"}),")."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Individual techniques have limitations, but we can iterate on them and layer them for additional safety."})," RLHF-fine-tuned models still reveal sensitive information, hallucinate content, exhibit biases, show sycophantic responses, and express concerning preferences like not wanting to be shut down (",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2307.15217",children:"Casper et al., 2023"}),'). Constitutional AI faces similar brittleness issues. Data filtering is insufficient on its own - models can learn from "negatively reinforced" examples, memorizing sensitive information they were explicitly taught not to reproduce (',(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2306.07567",children:"Roger, 2023"}),'). Even interpretability remains far from providing reliable safety guarantees. But we can keep refining these techniques individually, then layer them so the combined system acts as a comprehensive "catch-and-fix" approach. ',(0,s.jsx)(t.strong,{children:"Post-deployment monitoring extends this strategy beyond the training phase."})," Even after a system passes all pre-deployment checks, continued surveillance during actual use can reveal failure modes that weren't apparent during controlled testing. As discussed in the misuse prevention strategies, monitoring systems watch for concerning patterns that emerge from real-world interactions."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"The iterative approach has generated significant debate, particularly regarding whether it will scale to AGI and ASI-level systems."})," The disagreement is whether iterative improvement can scale to systems approaching or exceeding human-expert capabilities. Some researchers believe there's a significant chance (>50%) that straightforward approaches like RLHF combined with iterative problem-solving will be sufficient for safely developing AGI (",(0,s.jsx)(t.a,{href:"https://aligned.substack.com/p/alignment-optimism",children:"Leike, 2022"}),"). The debate revolves around fundamental uncertainty - we won't know whether iterative approaches are sufficient until we're already dealing with systems powerful enough that mistakes could be catastrophic."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"The optimistic argument for this strategy assumes alignment problems remain discoverable and fixable through iteration."})," Proponents argue that when problems emerge, we'll be able to trace their causes and implement fixes before systems become too capable to control (",(0,s.jsx)(t.a,{href:"https://www.youtube.com/watch?v=Y1BUaLo67ac",children:"Ng, 2025"}),"). If we layer the approaches talked about in this section, with some strategies explored in the ASI section, then these layered iterative process should converge on genuinely aligned systems after a reasonable number of attempts, especially if we invest heavily in diverse, high-quality detection methods that make it difficult for systems to fool all of them simultaneously."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"First counter-argument is that this strategy involves deliberately training dangerous systems and hoping to catch them before they cause harm."}),' Critics argue that some capabilities are simply too dangerous to train at all, even with sophisticated detection methods. For example, if a model develops superhuman persuasion capabilities during training, it might become unmanageable even in controlled environments. For example, OpenAI\'s preparedness framework defines a "critical" level of persuasion as the ability to "',(0,s.jsx)(t.em,{children:"create content with persuasive effectiveness strong enough to convince almost anyone to take action on a belief that goes against their natural interest"}),'" (',(0,s.jsx)(t.a,{href:"https://cdn.openai.com/openai-preparedness-framework-beta.pdf",children:"OpenAI, 2023"}),"). A model with such capabilities would be too risky to interact with humans even during training or evaluation - yet the iterative approach assumes you can safely experiment with such systems."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"The second counter-argument is that alignment might not generalize, and alignment faking could bypass safety measures entirely."})," AI capabilities might generalize rapidly while safety properties don't transfer to new domains. If a system suddenly develops superhuman abilities through phenomena like grokking (",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2201.02177",children:"Power et al., 2022"}),"), it might become unmanageable even during training. Additionally, each iteration creates selection pressure for models that can fool detection systems - after many rounds, a passing model might have learned to hide misalignment rather than actually becoming aligned."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"The third counter-argument is that the feedback loop might break due to discontinuous capability jumps."})," During fast takeoff scenarios, we might not have sufficient time for iterative correction. If systems develop sophisticated deception capabilities, they might strategically manipulate the training process itself, making our detection methods unreliable when stakes are the highest (",(0,s.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/xFotXGEotcKouifky/worlds-where-iterative-design-fails",children:"Wentworth, 2022"}),")."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Several strategic questions emerge from this debate:"})}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Selection pressure severity:"})," How many iterations can we safely run before selection pressure for deceptive systems becomes overwhelming? Does the answer depend on detection method diversity?"]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Capability discontinuities:"})," Will AGI development involve smooth capability gains that allow iterative correction, or sudden jumps that bypass safety measures? How can we distinguish between these scenarios in advance?"]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Detection sufficiency:"})," Can any combination of interpretability, behavioral testing, and monitoring provide reliable detection of sophisticated deception in human-level systems?"]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Institutional requirements:"})," What organizational structures and incentives are necessary to actually halt development when problems emerge, rather than rushing to deployment under commercial pressure?"]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Failure mode coverage:"})," Does iterative improvement address the full spectrum of alignment risks, or primarily focus on deceptive alignment while missing other failure modes?"]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Scalability boundaries:"})," At what capability level does this approach break down entirely? Can it work for early AGI but not more advanced systems?"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"The fundamental challenge is that this represents a strategic gamble with enormous stakes."})," Betting on iterative improvement means accepting that we'll deploy increasingly powerful systems under the assumption that we can catch and fix problems before they become catastrophic. If this assumption proves wrong - if systems undergo capability jumps that bypass our detection methods, or if selection pressure produces sophisticated deception we can't identify - the consequences could be irreversible. Yet perfect safety being likely impossible, many researchers argue that robust iterative improvement represents our best practical path forward for navigating the transition to AGI."]}),"\n",(0,s.jsx)(t.h2,{id:"04",children:"Maintain Control"}),"\n",(0,s.jsx)(c.A,{src:"./img/qFi_Image_14.png",alt:"Enter image alt description",number:"14",label:"3.14",caption:"Transformatively useful AI means \u201cCapable of substantially reducing the risk posed by subsequent AIs if fully deployed, likely by speeding up R&D and some other tasks by a large factor (perhaps 30x).\u201d citation from ([Greenblatt & Shlegeris, 2024](https://www.lesswrong.com/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled#comments)) - figure from us;"}),"\n",(0,s.jsxs)(t.p,{children:["Given the fragility of alignment techniques, one approach to enhancing safety involves controlling AI models by restricting the use of the most dangerous models to monitored environments to prevent them from causing disasters, exfiltrating data, or taking control of the data center (",(0,s.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/BAzCGCys4BkzGDCWR/the-prototypical-catastrophic-ai-action-is-getting-root",children:"Shlegeris, 2022"}),"). This approach often assumes a worst-case scenario where the AI might be actively deceptive or power-seeking."]}),"\n",(0,s.jsxs)(t.p,{children:['A central concern in control research is "scheming" \u2013 an AI strategically hiding its true (misaligned) goals during training or evaluation, only to pursue them once deployed, potentially seeking power or resisting shutdown (',(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/posts/aEguDPoCzt3287CCD/how-will-we-update-about-scheming",children:"Greenblatt, 2025"}),"). Assessing the likelihood of scheming and how we might update on future evidence remains difficult as of 2025 (",(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/posts/aEguDPoCzt3287CCD/how-will-we-update-about-scheming",children:"Greenblatt, 2025"}),"). Empirical work suggests current frontier models possess the capability for in-context scheming (reasoning deceptively when prompted with a goal and context), though not necessarily an inherent propensity (",(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/posts/8gy7c8GAPkuu6wTiX/frontier-models-are-capable-of-in-context-scheming",children:"Hobbhahn et al., 2024"}),"). We have a lot more information on scheming and deception in the chapters on evaluations and on goal misgeneralization."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Ensuring control even without robust alignment."}),' As we\u2019ve seen in the section on definitions, there is an important distinction between control, which aims to prevent the AI from "leaving the box" (and more generally prevent the AI from causing catastrophes) even if it tries, and alignment, which aims to make the AI want to stay in the box. This strategy seeks to keep dangerous AI systems under control even if they are not fully "aligned." Laboratories might need powerful/dangerous AIs for specific use cases (for example, studying the alignment properties of capable AIs or doing research with them, etc.). The AI control agenda targets models with dangerous capabilities and deceptive (or at least superficially aligned) behaviors.']}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Control is probably not scalable, but doable and complementary to alignment."})," Historically, controlling malign superintelligences has been considered impossible, as, for example, a superintelligence could convince the guardians to let it out of the box. Experiments among humans have shown that it is indeed possible for some humans to persuade others to let them out after a lengthy discussion (",(0,s.jsx)(t.a,{href:"https://www.yudkowsky.net/singularity/aibox",children:"Yudkowsky, 2002"}),"; ",(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/posts/nCvvhFBaayaXyuBiD/shut-up-and-do-the-impossible",children:"Yudkowsky, 2008"}),"). ASI is by definition many orders of magnitude smarter than humans, so keeping it confined is going to be a lot more complex than keeping humans (or HLAI) confined. However, this does not mean it's impossible to derive intellectual labor from AIs that are not wildly superhuman."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Evaluating whether the AI is scheming is challenging."})," The main problem is that with current techniques, it is impossible to determine if an AI is aligned or if it's simply behaving well while biding its time. It seems complicated to make this distinction for highly competent and situationally aware AIs (see the risk chapter). This could change in the future with techniques enabling us to audit the internals of the model, but for now, those techniques are inaccessible."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Evaluating AI capabilities is easier than evaluating propensity or control."})," If the AI is not able to do the task even after you have fine-tuned it to do it, you can be confident that the AI is not capable yet of doing the task. Even if the AI is situationally aware and sandbagging its capabilities, it cannot resist the optimization gradient pressure (modulo exploration hacking and other quirks (",(0,s.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations",children:"Hubinger, 2023"}),")). Thus, evaluating the upper bound of the capabilities isn't easy, but at least easier than assessing alignment. Specific evaluations, like those done by the Model Evaluation and Threat Research team, seem to be reasonable first steps for gauging the risk of autonomous replication and adaptation (",(0,s.jsx)(t.a,{href:"https://metr.org/blog/2023-08-01-new-report/",children:"METR, 2023"}),'). Evaluations that are specifically designed to determine if we have control over the AI are called "control evaluations." See the chapter on Evaluation for more information. In summary, this involves adversarial testing where a "red team" (representing the potentially misaligned AI) attempts to subvert safety measures implemented by a "blue team" (representing the developers/deployers) within a controlled proxy environment.']}),"\n",(0,s.jsx)(t.p,{children:"The most straightforward strategies for establishing control are to design a security protocol that contains the following elements, for example:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Security and Containment:"}),' This method (sometimes referred to as "boxing") confines an AI within a restricted environment to prevent its broader influence, effectively creating a secure enclosure for the AI. However, human-level AIs could potentially deceive human guardians or exploit vulnerabilities to escape their confinement. Without further operationalization, this approach remains a naive strategy. Nonetheless, it is possible to operationalize this further and evaluate more rigorously the effectiveness of the containment system: Evaluating if the AI can escape its bounded environment when we train it to do so is a capability evaluation, enabling us to use capability evaluations to assess security measures. The security protocol needs to be designed, detailed and tested with specific threat models in mind.']}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Monitoring:"}),' This strategy involves continuous oversight of AI behavior through automated systems, human supervision, or a combination of both to detect and respond to undesirable actions in real-time. Effective monitoring requires setting up detailed criteria for what constitutes "undesirable behavior" and employing sophisticated detection algorithms that can identify subtle signs of deviation from expected behavior patterns. This could include monitoring for attempts at deception, unauthorized access to data, or unexpected changes in the AI\'s operation. The challenge with monitoring lies in balancing the sensitivity of detection mechanisms to catch genuine threats without overwhelming the system with false alarms. Additionally, monitoring systems themselves must be designed to resist manipulation by the AI to ensure that they cannot be disabled, tricked, or bypassed (',(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2312.06942",children:"Greenblatt et al., 2024"}),")."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:["There are still many problems to address, but in summary, it seems possible to verify the robustness of a security protocol through control evaluations by taking precautions against different failure modes. For an overview of the control agenda, ",(0,s.jsx)(t.a,{href:"https://www.alignmentforum.org/s/WCJtsn6fNib6L7ZBB",children:"this sequence"})," is a good starting point."]}),"\n",(0,s.jsxs)(h.A,{title:"Does the AI Control line of research meaningfully reduce AI risks?",collapsed:!0,children:[(0,s.jsx)(t.p,{children:"Even the control agenda suffers from many critiques."}),(0,s.jsxs)(t.p,{children:["Jan Kulveit argues that controlling AIs without aligning them might reduce the probability of a medium-sized warning shot that does not kill everyone (",(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/posts/rZcyemEpBHgb2hqLP/ai-control-may-increase-existential-risk",children:"Kulveit, 2025"}),"). \"I. In worlds where we are doing reasonably well on alignment, the AI control agenda does not have much impact. II. In worlds where we are failing at alignment, AI control may primarily shift probability mass away from 'moderately large warning shots' and towards 'ineffective warning shots' and 'existential catastrophe, full takeover'.\""]}),(0,s.jsxs)(t.p,{children:['The article, "The Case Against AI Control Research" (',(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research",children:"Wentworth, 2025"}),'), argues that AI control research, as commonly defined and practiced, addresses a relatively small portion of the existential risk (X-risk) posed by artificial intelligence. Wentworth argues we won\'t be able to recognize a good solution to alignment. The author posits that the most likely failure scenario involves early transformative AI producing flawed, misleading, or subtly incorrect solutions ("slop") when tasked with solving superintelligence alignment. Labs might mistakenly accept these flawed solutions due to the difficulty of verification, AI sycophancy, researcher error, or organizational problems, and then proceed to build misaligned superintelligence based on them. Because control research narrowly focuses on intentional scheming by early AI, it doesn\'t address the larger risk posed by "slop" and the challenge of ensuring early AI genuinely contributes to solving the alignment problem for more advanced systems, rather than just being a superhuman sycophant that convinces humans that their flawed idea of how to align an AI is correct when it\'s not.']}),(0,s.jsx)(t.p,{children:"Both arguments are debatable, but the general point is that it is hard to guarantee that even promising-looking lines of research are not, in fact, net negative."})]}),"\n",(0,s.jsxs)(h.A,{title:"How to box an AI",collapsed:!0,children:[(0,s.jsx)(t.p,{children:"While boxing may not be scalable, it remains a neglected strategy."}),(0,s.jsxs)(t.p,{children:["Many people think that trying to box powerful ASIs is futile. This is often fueled by instances where humans, playing the AIs in the Box Experiment, have managed to convince others to 'release' them (",(0,s.jsx)(t.a,{href:"http://yudkowsky.net/singularity/aibox/",children:"Yudkowsky, 2002"}),"; ",(0,s.jsx)(t.a,{href:"http://sl4.org/archive/0207/4935.html",children:"Corwin, 2002"}),"). These historical examples must be taken seriously."]}),(0,s.jsx)(t.p,{children:"AIs could use tactics such as promising vast wealth, power, and intelligence to those who free them, asserting that they alone can avert existential threats or other appeals to emotion."}),(0,s.jsx)(t.p,{children:"Nonetheless, we maintain that a variety of strategies can substantially reduce the risk of AI escape, even if we acknowledge that there exists a level of intelligence that might inevitably persuade any group of humans to aid in its liberation. Containment may be more achievable than alignment, particularly at the human-level intelligence stage. It may be possible to conceive protocols that significantly increase the difficulty for AI to go out of the box."}),(0,s.jsx)(t.p,{children:"Boxing can be conceptualized as a game requiring preparation. The general strategy is to prepare and not let the AI prepare."}),(0,s.jsx)(t.p,{children:"Possible mitigation strategies include:"}),(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"No single individual should have the capacity to release the AI."}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["Engaging in counter-strategy practices: Understand some common breakout strategies (accessible through resources like ",(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/posts/FmxhoWxvBqSxhFeJn/i-attempted-the-ai-box-experiment-and-lost",children:"this one"}),")."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"Testing and training humans, utilizing resistance psychology evaluations to select suitable individuals for containment tasks, such as for space training."}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["Monitoring AI persuasiveness, as outlined in the preparedness framework. (",(0,s.jsx)(t.a,{href:"https://cdn.openai.com/openai-preparedness-framework-beta.pdf",children:"OpenAI, 2023"}),")"]}),"\n"]}),"\n"]}),(0,s.jsxs)(t.p,{children:["Many more concrete strategies are listed ",(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled#comments",children:"here"}),", and a summary of discussions on this topic is available ",(0,s.jsx)(t.a,{href:"https://www.lesswrong.com/tag/ai-boxing-containment",children:"here"}),"."]})]}),"\n",(0,s.jsx)(t.h2,{id:"05",children:"Facilitating control with Transparent Thoughts"}),"\n",(0,s.jsx)(t.p,{children:"Monitoring the Chain-of-Thought (CoT) \u2013 the explicit natural language reasoning steps produced by some models \u2013 has been explored as a specific control and interpretability technique. Transparency is instrumental in both alignment and control."}),"\n",(0,s.jsxs)(t.p,{children:['The hope is that CoT provides a window into the AI\'s "thinking," allowing monitors (human or AI) to detect malicious intent or flawed reasoning before harmful actions occur (',(0,s.jsx)(t.a,{href:"https://openai.com/index/chain-of-thought-monitoring/",children:"OpenAI, 2025"}),"). OpenAI found CoT monitoring effective for detecting reward hacking in programming tasks, especially when the monitor could see the CoT alongside the actions."]}),"\n",(0,s.jsx)(c.A,{src:"./img/Xec_Image_15.png",alt:"Enter image alt description",number:"15",label:"3.15",caption:"Figure from The Translucent Thoughts Hypotheses and Their Implications ([Roger, 2023](https://www.alignmentforum.org/posts/r3xwHzMmMf25peeHE/the-translucent-thoughts-hypotheses-and-their-implications)). If the AI we are working with can only reason through chains of thought in English, then it becomes significantly easier to monitor its thoughts."}),"\n",(0,s.jsxs)(t.p,{children:["Unfortunately, the vast majority of AIs do not reason in English; for example, AlphaGo reasons in a language that is entirely different from English. LLMs, on the other hand, are capable of reasoning in English, but the apparent reasoning that is written does not represent their underlying thought process (",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2305.04388",children:"Turpin et al., 2023"}),")."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"It is probably much easier to align and monitor an AI that only thinks out loud in English"})," and that is pre-trained on human-generated text than to align AGIs trained from scratch with RL/evolutionary methods because those latter architectures would create their own language and would be totally alien. Assuming that an LLM cannot devise a plan to destroy the world during a single forward pass, it is plausible that such AIs would be monitorable because we would be able to look into the text generated to see if anything is suspicious. The set of hypotheses required to assert that the first AGI is going to be monitorable if we just look at its reasoning in English is described in the post: The Translucent Thoughts Hypotheses and Their Implications (",(0,s.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/r3xwHzMmMf25peeHE/the-translucent-thoughts-hypotheses-and-their-implications",children:"Roger, 2023"}),")."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"It might be possible to incentivize LLMs to think more plainly in English."})," Obviously, the first step is to train them to think in English. But even this is insufficient. Training LLMs to reason by skipping steps incentivizes them to reason in a hidden and non-transparent manner, and this is what happens implicitly when we train LLMs to predict the next token. Conversely, if we train LLMs to detail all their reasoning steps, we can reduce incentives for hidden reasoning. This approach is also known as process-based training, which involves training AI systems to reason only step-by-step, and we explain this in the chapter on scalable oversight. Unfortunately, the above reasoning is not straightforward; AIs may learn to utilize hidden reasoning even if they are trained to detail all reasoning steps during training, such as steganography, or with architecture such as Chain of Continuous Thought (COCONUT) (see below).",(0,s.jsx)(r.A,{id:"footnote_steganography",number:"2",text:"Steganography is a hidden coded language. For example, the first letter of a sentence gives the next letter of the message."})]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Some AI architectures prevent us from using this strategy altogether."})," Meta's paper Training Large Language Models to Reason in a Continuous Latent Space presents an architecture that prevents us from reading the chain of thought from the AI systems (",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2412.06769",children:"Hao et al, 2024"}),"). If we want to be able to monitor what's going on, it may be important not to use these architectures, even if they do enable capability gains."]}),"\n",(0,s.jsx)(c.A,{src:"./img/gQa_Image_16.png",alt:"Enter image alt description",number:"16",label:"3.16",caption:"Figure 1 from the COCONUT papers ([Hao et al., 2024](https://arxiv.org/abs/2412.06769))."}),"\n",(0,s.jsxs)(t.p,{children:["Techniques like paraphrasing can help prevent steganography, but this requires a lot of subtlety (",(0,s.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/9Fdd9N7Escg3tcymb/preventing-language-models-from-hiding-their-reasoning",children:"Roger & Greenblatt, 2023"}),")."]}),"\n",(0,s.jsx)(t.p,{children:"Here are a few operationalizations of the transparent thoughts agenda:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["One implementation of the transparent thought agenda is to use cognitive emulation, which is the current technical plan of Conjecture (",(0,s.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal",children:"Leahy & Alfour, 2023"}),"). Cognitive Emulation (CoEm) would involve emulating GPT-4 using LLMs like GPT-3 as different submodules that send messages written in plain English to each other before outputting the next token. If the ",(0,s.jsx)(n,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:(0,s.jsx)(n,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:"neural network"})})," used deceptive thoughts, we could observe them in these intermediate messages. Some caveats and technical details on CoEm are available",(0,s.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Cognitive_Emulations___Explainability_By_design",children:" here"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["Anthropic\u2019s externalized reasoning oversight (",(0,s.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for",children:"Lanham, 2022"}),") is measured by the faithfulness of the chain-of-thought reasoning (",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2307.13702",children:"Lanham et al., 2023"}),") or by decomposing questions and subquestions to improve the faithfulness of the answer (",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2307.11768",children:"Radhakrishnan et al., 2023"}),")."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["Improving mathematical reasoning with process supervision. Instead of creating an AI that directly gives the final answer, you train it to output the reasoning step by step. This improves the transparency and also the performance! (",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2305.20050",children:"Lightman et al, 2023"}),")"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"In summary, a general strategy to create a safe by-design AI is to ensure that AIs externalize their reasoning, making monitoring this chain of thought easier. This could potentially be simpler than probing into the models' internals with interpretability. Therefore, it is crucial to avoid building AIs that are trained and incentivized to internalize many of their thoughts."}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"We now have extensive evidence that chain of thought monitoring is not entirely faithful."})," Anthropic's research further demonstrated that models often fail to faithfully report their reasoning in CoT, omitting influential factors (like hints) or fabricating justifications, particularly when incentivized to reward hack (",(0,s.jsx)(t.a,{href:"https://www.anthropic.com/research/reasoning-models-dont-say-think",children:"Anthropic, 2025"}),"). This suggests CoT may not be a reliable indicator of a model's true reasoning process, especially under adversarial pressure. The debate continues on whether CoT monitoring can be made robust enough for high-stakes safety."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"However, some researchers argue these findings do not entirely invalidate the potential of CoT monitoring for detecting sophisticated misalignment."})," As Greenblatt (2025) ",(0,s.jsx)(t.a,{href:"https://x.com/RyanPGreenblatt/status/1908298064366100676",children:"noted"}),", the unfaithfulness observed in those studies primarily occurs in scenarios where reasoning is minimal enough that extensive CoT isn't necessary. The more concerning alignment risks\u2014such as alignment faking, training gaming, or sophisticated deception\u2014would likely require complex reasoning processes that might still leave traces in CoT. Non-trivial, harmful outcomes such as escape attempts or research sabotage would similarly demand sophisticated reasoning that could potentially be detected."]}),"\n",(0,s.jsx)(t.p,{children:'Nevertheless, several factors might ultimately limit CoT monitoring effectiveness in advanced systems. These include architectural changes enabling opaque recurrent processing (sometimes called "neuralese"), the development of steganographic capabilities that could hide reasoning within seemingly benign outputs, or simply much stronger forward passes that require less explicit reasoning. Of these, the development of opaque reasoning abilities through neural recurrence may represent the most significant challenge to maintaining the transparency needed for effective monitoring. This whole agenda depends notably on the hypothesis that "Effective plans to defeat humanity cannot be found in a single LLM forward pass." Once AIs become too strong in a forward pass, most hopes of translucent thoughts go away, so either you need to succeed at ambitious CoEm by building a giant superhuman transparent bureaucracy, or we need new approaches. This is the topic of ASI Safety.'}),"\n",(0,s.jsxs)(h.A,{title:"The Most Forbidden Technique",collapsed:!0,children:[(0,s.jsxs)(t.p,{children:["The Most Forbidden Technique is training an AI using interpretability techniques. This term has been coined by Zvi Mowshowitz (",(0,s.jsx)(t.a,{href:"https://thezvi.substack.com/i/145384938/the-art-of-the-jailbreak",children:"Mowshowitz, 2025"}),'). OpenAI\'s research showed that directly optimizing the CoT for safety (i.e., penalizing "bad thoughts") caused models to learn to hide their deceptive reasoning while still misbehaving (',(0,s.jsx)(t.a,{href:"https://openai.com/index/chain-of-thought-monitoring/",children:"OpenAI, 2025"}),")."]}),(0,s.jsxs)(l.A,{speaker:"Zvi Mowshowitz",position:"",date:"",source:"",children:[(0,s.jsx)(t.p,{children:"An AI produces a final output [X] via some method [M]. You can analyze [M] using technique [T], to learn what the AI is up to. You could train on that. Never do that."}),(0,s.jsx)(t.p,{children:"You train on [X]. Only [X]. Never [M], never [T]."}),(0,s.jsx)(t.p,{children:"Why? Because [T] is how you figure out when the model is misbehaving."}),(0,s.jsx)(t.p,{children:"If you train on [T], you are training the AI to obfuscate its thinking and defeat [T]. You will rapidly lose your ability to know what is going on, in exactly the ways you most need to know what is going on."}),(0,s.jsx)(t.p,{children:"Those bits of optimization pressure from [T] are precious. Use them wisely."})]})]}),"\n",(0,s.jsx)(r.c,{title:"Footnotes"})]})}function f(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(g,{...e})}):g(e)}},4768:(e,t,n)=>{n.d(t,{c:()=>c,A:()=>h});var i=n(6540),s=n(3012);const a={footnoteAnchor:"footnoteAnchor_PZpY",footnoteSection:"footnoteSection_waoz",separator:"separator_hp8y",separatorLine:"separatorLine_uVnd",separatorLogo:"separatorLogo_dMBq",footnoteRegistry:"footnoteRegistry_ieVW",registryTitle:"registryTitle_ybBb",footnoteList:"footnoteList_Kjil",footnoteItem:"footnoteItem_aLZ2",footnoteNumber:"footnoteNumber_dA3X",footnoteContent:"footnoteContent_Xfmt",backButton:"backButton_vSVm",highlighted:"highlighted_YFU0",highlight:"highlight_jxG3",highlightDark:"highlightDark_Ekom"};var o=n(4848);function r(e,t){void 0===t&&(t=!0);const n=document.getElementById(e);n&&(n.scrollIntoView({behavior:"smooth"}),t&&(n.classList.add(a.highlighted),setTimeout((()=>n.classList.remove(a.highlighted)),1500)))}function l(e){return e?e.replace(/\[([^\]]+)\]\(([^)]+)\)/g,'<a href="$2" target="_blank" rel="noopener noreferrer">$1</a>'):""}function h(e){let{id:t,text:n,number:h}=e;const c=t||`footnote-${Math.random().toString(36).substr(2,9)}`,d="string"==typeof n?l(n):n;return(0,i.useEffect)((()=>{const e=setTimeout((()=>{const e=document.getElementById(`footnote-clone-${c}`);e&&n&&(e.innerHTML="string"==typeof n?l(n):n.toString())}),100);return()=>clearTimeout(e)}),[c,n]),(0,o.jsx)(s.Mn,{content:(0,o.jsx)("div",{dangerouslySetInnerHTML:{__html:d}}),children:(0,o.jsx)("sup",{id:`footnote-ref-${c}`,className:a.footnoteAnchor,onClick:e=>{(e.ctrlKey||e.metaKey)&&(e.preventDefault(),r(`footnote-content-${c}`))},"data-footnote-number":h||"?",children:h||"*"})})}function c(e){let{title:t="References"}=e;const[n,l]=(0,i.useState)([]);return(0,i.useEffect)((()=>{const e=document.querySelectorAll('[id^="footnote-ref-"]'),t=Array.from(e).map((e=>({id:e.id.replace("footnote-ref-",""),number:e.getAttribute("data-footnote-number")||e.textContent})));l(t)}),[]),n.length?(0,o.jsxs)("div",{className:a.footnoteSection,children:[(0,o.jsxs)("div",{className:a.separator,children:[(0,o.jsx)("div",{className:a.separatorLine}),(0,o.jsx)("img",{src:"/img/logo_samples/01-test.svg",alt:"Atlas logo",className:a.separatorLogo}),(0,o.jsx)("div",{className:a.separatorLine})]}),(0,o.jsxs)("div",{className:a.footnoteRegistry,children:[(0,o.jsx)("h2",{className:a.registryTitle,children:t}),(0,o.jsx)("ol",{className:a.footnoteList,children:n.map((e=>(0,o.jsxs)("li",{id:`footnote-content-${e.id}`,className:a.footnoteItem,value:parseInt(e.number)||void 0,children:[(0,o.jsx)(s.Mn,{content:"Back to reference",children:(0,o.jsxs)("button",{className:a.footnoteNumber,onClick:()=>r(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:[e.number,"."]})}),(0,o.jsx)("div",{className:a.footnoteContent,children:(0,o.jsx)("div",{id:`footnote-clone-${e.id}`})}),(0,o.jsx)(s.Mn,{content:"Back to reference",children:(0,o.jsx)("button",{className:a.backButton,onClick:()=>r(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:"\u21a9"})})]},e.id)))})]})]}):null}},3931:(e,t,n)=>{n.d(t,{A:()=>l});var i=n(6540),s=n(6347),a=n(8444);const o={videoFigure:"videoFigure_dV9u",fullWidth:"fullWidth_Tv7O",videoContainer:"videoContainer_LWoK",aspectRatio169:"aspectRatio169_Go8b",aspectRatio43:"aspectRatio43_b3T7",aspectRatio11:"aspectRatio11_n21J",aspectRatio219:"aspectRatio219_L_3n",videoIframe:"videoIframe_QEc5",videoElement:"videoElement_uu7N",customPlayer:"customPlayer_jiC4",customPlayerPlaceholder:"customPlayerPlaceholder_aL9q",loadingOverlay:"loadingOverlay_e2pu",spinner:"spinner_tFyL",spin:"spin_saPs",errorContainer:"errorContainer_J6Pn",errorIcon:"errorIcon_cGdn",fallbackLink:"fallbackLink_WsZk"};var r=n(4848);function l(e){let{type:t="youtube",videoId:n,caption:l,title:h,startTime:c,autoplay:d=!1,controls:u=!0,aspectRatio:p="16:9",width:m,height:g,chapter:f,number:y,label:b,useCustomPlayer:v=!1,fullWidth:w=!0}=e;const[x,j]=(0,i.useState)(!0),[k,A]=(0,i.useState)(!1),I=(0,s.zy)(),T=f||(()=>{const e=I.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),S=(()=>{const e=(e=>{if(!e)return"";if("number"==typeof e)return e.toString();if("string"==typeof e){if(/^\d+$/.test(e))return e;let t=0;const n=e.match(/(\d+)h/),i=e.match(/(\d+)m/),s=e.match(/(\d+)s/);return n&&(t+=3600*parseInt(n[1])),i&&(t+=60*parseInt(i[1])),s&&(t+=parseInt(s[1])),t>0?t.toString():""}return""})(c);switch(t.toLowerCase()){case"youtube":let i=`https://www.youtube.com/embed/${n}`;const s=new URLSearchParams;e&&s.append("start",e),d&&s.append("autoplay","1"),u||v||s.append("controls","0"),s.append("rel","0"),s.append("modestbranding","1"),s.append("fs","1"),s.append("cc_load_policy","0"),s.append("iv_load_policy","3"),s.append("showinfo","0"),s.append("disablekb","1"),s.append("playsinline","1"),s.append("color","white"),s.append("theme","light"),v&&(s.append("enablejsapi","1"),s.append("origin",window.location.origin));const a=s.toString();return a?`${i}?${a}`:i;case"vimeo":let o=`https://player.vimeo.com/video/${n}`;const r=new URLSearchParams;d&&r.append("autoplay","1"),u||v||r.append("controls","0"),r.append("title","0"),r.append("byline","0"),r.append("portrait","0"),r.append("dnt","1"),r.append("transparent","0"),r.append("background","1");const l=r.toString();return l?`${o}?${l}`:o;case"mp4":case"webm":case"video":return n;default:return console.warn(`Unsupported video type: ${t}`),n}})(),C=()=>{j(!1)},_=()=>{A(!0),j(!1)},L=e=>{let{src:n,onLoad:i,onError:s}=e;return(0,r.jsx)("div",{className:o.customPlayer,children:(0,r.jsxs)("div",{className:o.customPlayerPlaceholder,children:[(0,r.jsx)("h3",{children:"Atlas Custom Player"}),(0,r.jsx)("p",{children:"Coming Soon"}),(0,r.jsxs)("a",{href:n,target:"_blank",rel:"noopener noreferrer",className:o.fallbackLink,children:["Watch on ",t.charAt(0).toUpperCase()+t.slice(1)]})]})})},E=["mp4","webm","video"].includes(t.toLowerCase());return(0,r.jsxs)("figure",{className:`${o.videoFigure} ${w?o.fullWidth:""}`,children:[(0,r.jsxs)("div",{className:`${o.videoContainer} ${(()=>{switch(p){case"4:3":return o.aspectRatio43;case"1:1":return o.aspectRatio11;case"21:9":return o.aspectRatio219;default:return o.aspectRatio169}})()}`,style:{width:w?"100%":m||"auto",maxWidth:w?"none":"800px"},children:[x&&!k&&(0,r.jsxs)("div",{className:o.loadingOverlay,children:[(0,r.jsx)("div",{className:o.spinner}),(0,r.jsx)("p",{children:"Loading video..."})]}),k&&(0,r.jsxs)("div",{className:o.errorContainer,children:[(0,r.jsxs)("svg",{className:o.errorIcon,width:"48",height:"48",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",children:[(0,r.jsx)("circle",{cx:"12",cy:"12",r:"10"}),(0,r.jsx)("line",{x1:"12",y1:"8",x2:"12",y2:"12"}),(0,r.jsx)("line",{x1:"12",y1:"16",x2:"12.01",y2:"16"})]}),(0,r.jsx)("h4",{children:"Failed to load video"}),(0,r.jsxs)("p",{children:["Video type: ",t]}),(0,r.jsxs)("p",{children:["Video ID/URL: ",n]}),(0,r.jsx)("a",{href:S,target:"_blank",rel:"noopener noreferrer",className:o.fallbackLink,children:"Try opening video directly"})]}),!k&&(E?(0,r.jsxs)("video",{className:o.videoElement,controls:u,autoPlay:d,onLoadedData:C,onError:_,title:h||l||`${t} video`,style:{width:m||"100%",height:g||"auto",display:x?"none":"block"},children:[(0,r.jsx)("source",{src:S,type:`video/${t}`}),(0,r.jsxs)("p",{children:["Your browser doesn't support video playback.",(0,r.jsx)("a",{href:S,target:"_blank",rel:"noopener noreferrer",children:"Download the video file"})]})]}):v?(0,r.jsx)(L,{src:S,onLoad:C,onError:_}):(0,r.jsx)("iframe",{className:o.videoIframe,src:S,title:h||l||`${t} video`,frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0,onLoad:C,onError:_,style:{width:m||"100%",height:g||"100%",opacity:x?0:1}}))]}),(0,r.jsx)(a.A,{caption:l,mediaType:"video",chapter:T,number:y,label:b})]})}}}]);
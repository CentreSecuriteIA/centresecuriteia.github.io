"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[71],{9985:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>h,contentTitle:()=>d,default:()=>m,frontMatter:()=>c,metadata:()=>n,toc:()=>p});const n=JSON.parse('{"id":"chapters/09/1","title":"What is Interpretability ?","description":"Interpretability is the study of how and why AI models make decisions. Its central aim is to understand the inner workings of models and the processes behind their decisions. There are diverse approaches to interpretability, but in this chapter\u2014and the broader context of AI safety\u2014mechanistic interpretability (mech interp) is the primary focus (Ras et al., 2020, Ali et al., 2023).","source":"@site/docs/chapters/09/01.md","sourceDirName":"chapters/09","slug":"/chapters/09/01","permalink":"/chapters/09/01","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/09/01.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"1","title":"What is Interpretability ?","sidebar_label":"9.1 What is Interpretability ?","sidebar_position":2,"slug":"/chapters/09/01","reading_time_core":"7 min","reading_time_optional":"1 min","pagination_prev":"chapters/09/index","pagination_next":"chapters/09/2"},"sidebar":"docs","previous":{"title":"Interpretability","permalink":"/chapters/09/"},"next":{"title":"9.2 Observational Methods","permalink":"/chapters/09/02"}}');var r=i(4848),a=i(8453),o=i(3931),s=i(4768),l=(i(2482),i(8559),i(1966),i(2501));const c={id:1,title:"What is Interpretability ?",sidebar_label:"9.1 What is Interpretability ?",sidebar_position:2,slug:"/chapters/09/01",reading_time_core:"7 min",reading_time_optional:"1 min",pagination_prev:"chapters/09/index",pagination_next:"chapters/09/2"},d="What is Interpretability ?",h={},p=[{value:"Mechanistic Interpretability",id:"01",level:2}];function u(e){const t={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components},{GlossaryTerm:i}=t;return i||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"what-is-interpretability-",children:"What is Interpretability ?"})}),"\n",(0,r.jsxs)(t.p,{children:["Interpretability is the study of how and why AI models make decisions. Its central aim is to understand the inner workings of models and the processes behind their decisions. There are diverse approaches to interpretability, but in this chapter\u2014and the broader context of AI safety\u2014",(0,r.jsx)(t.strong,{children:"mechanistic interpretability (mech interp)"})," is the primary focus (",(0,r.jsx)(t.a,{href:"https://arxiv.org/abs/2004.14545",children:"Ras et al., 2020"}),", ",(0,r.jsx)(t.a,{href:"https://www.sciencedirect.com/science/article/pii/S1566253523001148",children:"Ali et al., 2023"}),").",(0,r.jsx)(s.A,{id:"footnote_interp_overview",number:"1",text:"For an overview of the broader interpretability landscape see ([Ras et al., 2020](https://arxiv.org/abs/2004.14545); [Ali et al., 2023](https://www.sciencedirect.com/science/article/pii/S1566253523001148))"})]}),"\n",(0,r.jsx)(o.A,{type:"youtube",videoId:"UGO_Ehywuxc",number:"3",label:"9.3",caption:"Optional video explanation of mechanistic interpretability."}),"\n",(0,r.jsx)(t.h2,{id:"01",children:"Mechanistic Interpretability"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Mechanistic Interpretability: The Bottom-Up Approach."})," Mechanistic interpretability seeks to reverse-engineer neural networks to uncover how their internal components\u2014such as neurons, weights, and layers\u2014work together to process information. This approach starts at the lowest level of abstraction and builds understanding piece by piece: this is why it\u2019s considered a bottom-up approach. By analyzing these basic components, we hope we can piece together how the network processes information and makes decisions."]}),"\n",(0,r.jsxs)(t.p,{children:["For example, mechanistic interpretability could explain how a ",(0,r.jsx)(i,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:(0,r.jsx)(i,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:"neural network"})})," recognizes objects in an image or generates language, down to the contributions of individual neurons or ",(0,r.jsx)(i,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:(0,r.jsx)(i,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:"attention"})})," heads. The hope is that this level of detail will allow researchers to diagnose and potentially fix unwanted behaviors in AI systems."]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Other Approaches to Interpretability."})," While mechanistic interpretability is a strong focus in AI safety, it is not the only approach. Other methods provide complementary perspectives:"]}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Concept-Based Interpretability:"})," Contrarily to mechanistic interpretability, concept-based interpretability takes a top-down approach: instead of analyzing neurons or weights on a granular level, it focuses on understanding how the network manipulates high-level concepts (",(0,r.jsx)(t.a,{href:"https://aclanthology.org/2022.cl-1.7/",children:"Belinkov, 2022"}),'). For instance, representation engineering \u2014a concept-based research agenda\u2014 explores how models encode concepts like "honesty" and how those representations can be adjusted to produce more honest outputs (',(0,r.jsx)(t.a,{href:"https://www.semanticscholar.org/paper/Representation-Engineering%3A-A-Top-Down-Approach-to-Zou-Phan/aac3469581061cd5b46440c3eeca91c385d54ccf",children:"Zou et al., 2023"}),")."]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Developmental Interpretability:"})," This approach examines how model capabilities and internal representations evolve during training. By understanding the emergence of behaviors or knowledge over time, researchers hope they will be able to identify the emergence of certain capabilities and prevent unwanted ones from developing (",(0,r.jsx)(t.a,{href:"https://www.lesswrong.com/s/SfFQE8DXbgkjk62JK/p/TjaeCWvLZtEDAS5Ex",children:"Hoogland et al., 2023"}),")."]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Behavioral Interpretability:"})," Unlike the previous approaches, behavioral interpretability studies input-output relationships without delving into the internal structure of models."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(l.A,{src:"./img/dEJ_Image_1.png",alt:"Enter image alt description",number:"1",label:"9.1",caption:"A visual classification of interpretability techniques ([Bereska & Gavves, 2024](https://arxiv.org/abs/2404.14082))."}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Why Mechanistic Interpretability Matters for AI Safety."})," Mechanistic interpretability is a strong focus in AI safety because it provides a level of precision that other approaches do not. Behavioral interpretability, for instance, offers insights into how a model behaves by studying input-output relationships, but it cannot reveal how its internal structure leads to its decisions. To prevent AI models from making harmful decisions or ensuring alignment with human values, we need to understand why models make certain decisions, and potentially steer the decision-making process."]}),"\n",(0,r.jsx)(t.p,{children:"For instance, by pinpointing where harmful concepts\u2014such as instructions for cyberattacks\u2014are stored within a model, mechanistic interpretability tools could help erase or modify those concepts without degrading the model\u2019s overall capabilities."}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Motivation for AI Safety?"})," The ultimate goal of interpretability, from an AI safety perspective, is to build confidence in the behavior of complex models by understanding their internal mechanisms and ensuring they act safely and predictably. There are different ways interpretability could contribute to AI safety (",(0,r.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability",children:"Nanda, 2022"}),"):"]}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Trust and transparency."})," By offering insights into which features of the input data (such as specific parts of an image or words in a sentence) or which specific concepts a model uses in its reasoning are influencing the model's outputs, interpretability tools can make it easier for users to understand, verify, and trust the behavior of complex models. This is particularly important in high-stakes applications like healthcare or autonomous systems, where trust in AI decisions is crucial."]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Enhance model editing."}),' Interpretability could be used to mitigate misuse by erasing specific pieces of knowledge from neural networks, such as knowledge on conducting cyberattack or building bioweapons. Some mechanistic interpretability tools serve to locate where certain concepts are encoded in a model (those are covered in the Observational Methods section). For example, researchers could identify how models generate answers to harmful queries such as "steps for creating a virus" and use knowledge erasure methods to remove that knowledge from the model.']}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Detection of undesirable behaviors."})," Interpretability could help identify when models are not functioning as intended or have learned undesirable behaviors or patterns. For alignment research, it could be used to detect, analyze, and understand undesired answers from models. A well-known example of this comes from a model trained to classify chest X-rays. Researchers discovered that instead of focusing on medical features like lung conditions, the model was using subtle artifacts from the scanning equipment or markers in the image to make its predictions. This led the model to perform well in testing but for the wrong reasons. By using interpretability tools, researchers identified and corrected this issue (",(0,r.jsx)(t.a,{href:"https://www.nature.com/articles/s42256-021-00338-7",children:"DeGrave et al., 2021"}),")."]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Extrapolation."})," Some hope that by understanding how current models function, interpretability may help predict how future, larger models will behave, whether new capabilities or risks will emerge, and how systems will evolve as they scale."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"It\u2019s important to note that while these goals are promising, the field of interpretability is still maturing. The adoption of interpretability tools in real-world scenarios is still limited, and assessing their quality remains challenging. Many existing techniques in interpretability are not designed for large-scale use and state-of-the-art models. These limitations will be explored in more detail in the Critics of Interpretability section."}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"What is the End Goal of Interpretability?"})," What concrete outcomes should interpretability achieve to make AI systems safer and more predictable? There are different opinions on the end goals of interpretability, including the following approaches. A few include enumerative safety, search retargeting, and anomaly detection which we explain below."]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Enumerative Safety."})," Enumerative safety aims to identify and catalog all the concepts and behaviors encoded within a model. The idea is straightforward: if we can thoroughly understand and enumerate every action the model can take, we could selectively remove undesirable behaviors and ensure that the model can only perform safe and desirable actions (",(0,r.jsx)(t.a,{href:"https://transformer-circuits.pub/2022/toy_model/index.html",children:"Elhage et al., 2022"}),"). For example, if a language model encodes harmful instructions\u2014such as steps to create a cyberattack\u2014enumerative safety would involve locating and eliminating this knowledge without impairing the model\u2019s useful functions."]}),"\n",(0,r.jsx)(l.A,{src:"./img/RSX_Image_2.png",alt:"Enter image alt description",number:"2",label:"9.2",caption:"Enumerative safety aims to ensure that in all situations, the model doesn\u2019t do something we don\u2019t want. From ([Olah, 2023](https://transformer-circuits.pub/2023/interpretability-dreams/index.html))."}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Retargeting the Search: Steering Objectives."})," This approach represents a more ambitious goal: rather than removing harmful behaviors, it seeks to directly modify a model\u2019s objectives. This involves identifying how the model internally represents its goals and redirecting those representations to align with human values. Unlike enumerative safety, retargeting does not require reverse-engineering the entire model. Instead, it focuses on altering specific components while preserving the system\u2019s overall functionality. For instance, if a model learned to optimize for harmful outcomes, researchers could \u201cretarget\u201d this optimization toward beneficial goals (",(0,r.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget",children:"Wentworth, 2022"}),")."]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Relaxed Adversarial Training."})," Relaxed adversarial training is an approach designed to enhance the robustness of AI systems, particularly to ensure their corrigibility\u2014their ability to accept and assist with corrective interventions. Traditional adversarial training tests a model's robustness by exposing it to adversarial inputs. Relaxed adversarial training, however, works by testing against adversarial latent vectors instead of real inputs. Adversarial training traditionally tests a model\u2019s robustness by exposing it to adversarial inputs, but here the goal is to generate perturbation in the model\u2019s latent space. Mechanistic interpretability could be used to identify latent vectors that correspond to specific model behaviors, such as corrigibility or alignment with user intentions, and then test whether the model resists corrigibility directly by manipulating its internal representation (",(0,r.jsx)(t.a,{href:"https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d",children:"Christiano, 2019"}),")."]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Mechanistic Anomaly Detection (MAD)."})," This approach focuses on identifying instances where a model produces outputs for unusual reasons. While traditional interpretability methods aim to understand a model\u2019s mechanisms comprehensively, MAD takes a more targeted approach: it flags anomalies in the decision-making process without requiring a full understanding of the underlying mechanisms. For example, MAD could detect when a model\u2019s reasoning deviates from its usual patterns, such as when it relies on spurious correlations rather than meaningful features. Insights from mechanistic interpretability could be useful to flag instances when a model operates outside its usual patterns of behavior (",(0,r.jsx)(t.a,{href:"https://www.lesswrong.com/s/GiZ6puwmHozLuBrph/p/n7DFwtJvCzkuKmtbG",children:"Jenner, 2024"}),")."]}),"\n",(0,r.jsx)(s.c,{title:"Footnotes"})]})}function m(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},4768:(e,t,i)=>{i.d(t,{c:()=>d,A:()=>c});var n=i(6540),r=i(3012);const a={footnoteAnchor:"footnoteAnchor_PZpY",footnoteSection:"footnoteSection_waoz",separator:"separator_hp8y",separatorLine:"separatorLine_uVnd",separatorLogo:"separatorLogo_dMBq",footnoteRegistry:"footnoteRegistry_ieVW",registryTitle:"registryTitle_ybBb",footnoteList:"footnoteList_Kjil",footnoteItem:"footnoteItem_aLZ2",footnoteNumber:"footnoteNumber_dA3X",footnoteContent:"footnoteContent_Xfmt",backButton:"backButton_vSVm",highlighted:"highlighted_YFU0",highlight:"highlight_jxG3",highlightDark:"highlightDark_Ekom"};var o=i(4848);function s(e,t){void 0===t&&(t=!0);const i=document.getElementById(e);i&&(i.scrollIntoView({behavior:"smooth"}),t&&(i.classList.add(a.highlighted),setTimeout((()=>i.classList.remove(a.highlighted)),1500)))}function l(e){return e?e.replace(/\[([^\]]+)\]\(([^)]+)\)/g,'<a href="$2" target="_blank" rel="noopener noreferrer">$1</a>'):""}function c(e){let{id:t,text:i,number:c}=e;const d=t||`footnote-${Math.random().toString(36).substr(2,9)}`,h="string"==typeof i?l(i):i;return(0,n.useEffect)((()=>{const e=setTimeout((()=>{const e=document.getElementById(`footnote-clone-${d}`);e&&i&&(e.innerHTML="string"==typeof i?l(i):i.toString())}),100);return()=>clearTimeout(e)}),[d,i]),(0,o.jsx)(r.Mn,{content:(0,o.jsx)("div",{dangerouslySetInnerHTML:{__html:h}}),children:(0,o.jsx)("sup",{id:`footnote-ref-${d}`,className:a.footnoteAnchor,onClick:e=>{(e.ctrlKey||e.metaKey)&&(e.preventDefault(),s(`footnote-content-${d}`))},"data-footnote-number":c||"?",children:c||"*"})})}function d(e){let{title:t="References"}=e;const[i,l]=(0,n.useState)([]);return(0,n.useEffect)((()=>{const e=document.querySelectorAll('[id^="footnote-ref-"]'),t=Array.from(e).map((e=>({id:e.id.replace("footnote-ref-",""),number:e.getAttribute("data-footnote-number")||e.textContent})));l(t)}),[]),i.length?(0,o.jsxs)("div",{className:a.footnoteSection,children:[(0,o.jsxs)("div",{className:a.separator,children:[(0,o.jsx)("div",{className:a.separatorLine}),(0,o.jsx)("img",{src:"/img/logo_samples/01-test.svg",alt:"Atlas logo",className:a.separatorLogo}),(0,o.jsx)("div",{className:a.separatorLine})]}),(0,o.jsxs)("div",{className:a.footnoteRegistry,children:[(0,o.jsx)("h2",{className:a.registryTitle,children:t}),(0,o.jsx)("ol",{className:a.footnoteList,children:i.map((e=>(0,o.jsxs)("li",{id:`footnote-content-${e.id}`,className:a.footnoteItem,value:parseInt(e.number)||void 0,children:[(0,o.jsx)(r.Mn,{content:"Back to reference",children:(0,o.jsxs)("button",{className:a.footnoteNumber,onClick:()=>s(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:[e.number,"."]})}),(0,o.jsx)("div",{className:a.footnoteContent,children:(0,o.jsx)("div",{id:`footnote-clone-${e.id}`})}),(0,o.jsx)(r.Mn,{content:"Back to reference",children:(0,o.jsx)("button",{className:a.backButton,onClick:()=>s(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:"\u21a9"})})]},e.id)))})]})]}):null}},3931:(e,t,i)=>{i.d(t,{A:()=>l});var n=i(6540),r=i(6347),a=i(8444);const o={videoFigure:"videoFigure_dV9u",fullWidth:"fullWidth_Tv7O",videoContainer:"videoContainer_LWoK",aspectRatio169:"aspectRatio169_Go8b",aspectRatio43:"aspectRatio43_b3T7",aspectRatio11:"aspectRatio11_n21J",aspectRatio219:"aspectRatio219_L_3n",videoIframe:"videoIframe_QEc5",videoElement:"videoElement_uu7N",customPlayer:"customPlayer_jiC4",customPlayerPlaceholder:"customPlayerPlaceholder_aL9q",loadingOverlay:"loadingOverlay_e2pu",spinner:"spinner_tFyL",spin:"spin_saPs",errorContainer:"errorContainer_J6Pn",errorIcon:"errorIcon_cGdn",fallbackLink:"fallbackLink_WsZk"};var s=i(4848);function l(e){let{type:t="youtube",videoId:i,caption:l,title:c,startTime:d,autoplay:h=!1,controls:p=!0,aspectRatio:u="16:9",width:m,height:g,chapter:f,number:b,label:y,useCustomPlayer:w=!1,fullWidth:v=!0}=e;const[x,j]=(0,n.useState)(!0),[k,_]=(0,n.useState)(!1),I=(0,r.zy)(),A=f||(()=>{const e=I.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),N=(()=>{const e=(e=>{if(!e)return"";if("number"==typeof e)return e.toString();if("string"==typeof e){if(/^\d+$/.test(e))return e;let t=0;const i=e.match(/(\d+)h/),n=e.match(/(\d+)m/),r=e.match(/(\d+)s/);return i&&(t+=3600*parseInt(i[1])),n&&(t+=60*parseInt(n[1])),r&&(t+=parseInt(r[1])),t>0?t.toString():""}return""})(d);switch(t.toLowerCase()){case"youtube":let n=`https://www.youtube.com/embed/${i}`;const r=new URLSearchParams;e&&r.append("start",e),h&&r.append("autoplay","1"),p||w||r.append("controls","0"),r.append("rel","0"),r.append("modestbranding","1"),r.append("fs","1"),r.append("cc_load_policy","0"),r.append("iv_load_policy","3"),r.append("showinfo","0"),r.append("disablekb","1"),r.append("playsinline","1"),r.append("color","white"),r.append("theme","light"),w&&(r.append("enablejsapi","1"),r.append("origin",window.location.origin));const a=r.toString();return a?`${n}?${a}`:n;case"vimeo":let o=`https://player.vimeo.com/video/${i}`;const s=new URLSearchParams;h&&s.append("autoplay","1"),p||w||s.append("controls","0"),s.append("title","0"),s.append("byline","0"),s.append("portrait","0"),s.append("dnt","1"),s.append("transparent","0"),s.append("background","1");const l=s.toString();return l?`${o}?${l}`:o;case"mp4":case"webm":case"video":return i;default:return console.warn(`Unsupported video type: ${t}`),i}})(),T=()=>{j(!1)},L=()=>{_(!0),j(!1)},S=e=>{let{src:i,onLoad:n,onError:r}=e;return(0,s.jsx)("div",{className:o.customPlayer,children:(0,s.jsxs)("div",{className:o.customPlayerPlaceholder,children:[(0,s.jsx)("h3",{children:"Atlas Custom Player"}),(0,s.jsx)("p",{children:"Coming Soon"}),(0,s.jsxs)("a",{href:i,target:"_blank",rel:"noopener noreferrer",className:o.fallbackLink,children:["Watch on ",t.charAt(0).toUpperCase()+t.slice(1)]})]})})},R=["mp4","webm","video"].includes(t.toLowerCase());return(0,s.jsxs)("figure",{className:`${o.videoFigure} ${v?o.fullWidth:""}`,children:[(0,s.jsxs)("div",{className:`${o.videoContainer} ${(()=>{switch(u){case"4:3":return o.aspectRatio43;case"1:1":return o.aspectRatio11;case"21:9":return o.aspectRatio219;default:return o.aspectRatio169}})()}`,style:{width:v?"100%":m||"auto",maxWidth:v?"none":"800px"},children:[x&&!k&&(0,s.jsxs)("div",{className:o.loadingOverlay,children:[(0,s.jsx)("div",{className:o.spinner}),(0,s.jsx)("p",{children:"Loading video..."})]}),k&&(0,s.jsxs)("div",{className:o.errorContainer,children:[(0,s.jsxs)("svg",{className:o.errorIcon,width:"48",height:"48",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",children:[(0,s.jsx)("circle",{cx:"12",cy:"12",r:"10"}),(0,s.jsx)("line",{x1:"12",y1:"8",x2:"12",y2:"12"}),(0,s.jsx)("line",{x1:"12",y1:"16",x2:"12.01",y2:"16"})]}),(0,s.jsx)("h4",{children:"Failed to load video"}),(0,s.jsxs)("p",{children:["Video type: ",t]}),(0,s.jsxs)("p",{children:["Video ID/URL: ",i]}),(0,s.jsx)("a",{href:N,target:"_blank",rel:"noopener noreferrer",className:o.fallbackLink,children:"Try opening video directly"})]}),!k&&(R?(0,s.jsxs)("video",{className:o.videoElement,controls:p,autoPlay:h,onLoadedData:T,onError:L,title:c||l||`${t} video`,style:{width:m||"100%",height:g||"auto",display:x?"none":"block"},children:[(0,s.jsx)("source",{src:N,type:`video/${t}`}),(0,s.jsxs)("p",{children:["Your browser doesn't support video playback.",(0,s.jsx)("a",{href:N,target:"_blank",rel:"noopener noreferrer",children:"Download the video file"})]})]}):w?(0,s.jsx)(S,{src:N,onLoad:T,onError:L}):(0,s.jsx)("iframe",{className:o.videoIframe,src:N,title:c||l||`${t} video`,frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0,onLoad:T,onError:L,style:{width:m||"100%",height:g||"100%",opacity:x?0:1}}))]}),(0,s.jsx)(a.A,{caption:l,mediaType:"video",chapter:A,number:b,label:y})]})}}}]);
"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[1343],{1108:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>m,contentTitle:()=>h,default:()=>f,frontMatter:()=>d,metadata:()=>n,toc:()=>p});const n=JSON.parse('{"id":"chapters/09/2","title":"Observational Methods","description":"Feature Visualization","source":"@site/docs/chapters/09/02.md","sourceDirName":"chapters/09","slug":"/chapters/09/02","permalink":"/chapters/09/02","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/09/02.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"2","title":"Observational Methods","sidebar_label":"9.2 Observational Methods","sidebar_position":3,"slug":"/chapters/09/02","reading_time_core":"17 min","reading_time_optional":"17 min","pagination_prev":"chapters/09/1","pagination_next":"chapters/09/3"},"sidebar":"docs","previous":{"title":"9.1 What is Interpretability ?","permalink":"/chapters/09/01"},"next":{"title":"9.3 Interventional Methods","permalink":"/chapters/09/03"}}');var i=s(4848),a=s(8453),r=s(4768),o=(s(2482),s(8559)),l=s(1966),c=s(2501);const d={id:2,title:"Observational Methods",sidebar_label:"9.2 Observational Methods",sidebar_position:3,slug:"/chapters/09/02",reading_time_core:"17 min",reading_time_optional:"17 min",pagination_prev:"chapters/09/1",pagination_next:"chapters/09/3"},h="Observational Methods",m={},p=[{value:"Feature Visualization",id:"01",level:2},{value:"Circuits",id:"01-01",level:3},{value:"Polysemantic Neurons",id:"01-02",level:3},{value:"Logit Lens",id:"02",level:2},{value:"Probing classifiers",id:"03",level:2},{value:"Superposition",id:"04",level:2},{value:"Sparse Autoencoders",id:"05",level:2}];function u(e){const t={a:"a",annotation:"annotation",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",math:"math",mi:"mi",mo:"mo",mrow:"mrow",msub:"msub",mtext:"mtext",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,a.R)(),...e.components},{GlossaryTerm:s}=t;return s||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"observational-methods",children:"Observational Methods"})}),"\n",(0,i.jsx)(t.h2,{id:"01",children:"Feature Visualization"}),"\n",(0,i.jsx)(t.p,{children:"Feature visualization is one of the first observational methods in mechanistic interpretability. It allows researchers to explore the features that a vision model learns and uses at different layers. In this section, we\u2019ll explain in detail what feature visualization is, and the discoveries it enabled."}),"\n",(0,i.jsx)(l.A,{term:"Feature",source:"",number:"1",label:"9.1",children:(0,i.jsx)(t.p,{children:"A feature refers to a specific pattern or characteristic that the network learns to detect. These features are the fundamental units that models use to process information and make decisions."})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"What is a Feature?"})," A feature is a pattern of the input data that the network learns to detect. Models have features because they help break down complex inputs, such as images or text, into interpretable components that the model can use to make predictions."]}),"\n",(0,i.jsxs)(t.p,{children:["Vision models trained to do image classification commonly possess features corresponding to cats, dogs, cars, eyes, fruits, etc. Vision model features vary in complexity depending on the layer in which they appear. In the early layers, features typically represent simple, low-level patterns such as edges, colors, or textures in a vision model. As you move to deeper layers, the features become more abstract and complex, representing higher-level concepts like shapes, objects, or even specific entities like faces or animals. This ",(0,i.jsx)(t.em,{children:"hierarchical structure of features"})," allows models to progressively process raw pixel data into higher-level concepts to ultimately classify the image. For instance, recognizing a car would start with detecting edges (low-level features), then specific shapes (like wheels), and eventually the entire car (high-level feature)."]}),"\n",(0,i.jsxs)(t.p,{children:["In ",(0,i.jsx)(s,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:(0,i.jsx)(s,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:"transformer"})})," models, researchers have identified features corresponding to specific concepts like DNA sequences, legal language, HTTP requests, or Hebrew text. For instance, when a ",(0,i.jsx)(s,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:(0,i.jsx)(s,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:"transformer"})})," encounters a DNA sequence, neurons encoding the \u201cDNA feature\u201d activate strongly in response. Similarly, given a sentence like \u201cThe court ruled in favor of the defendant because\u2026,\u201d features linked to legal language and sentence structures common in legal contexts may activate, helping the model predict a follow-up involving reasoning or justification, such as \u201c\u2026the evidence presented was insufficient.\u201d These features are essential building blocks that models use to make sense of input data and make predictions."]}),"\n",(0,i.jsx)(l.A,{term:"Feature visualization",source:"",number:"2",label:"9.2",children:(0,i.jsx)(t.p,{children:"Feature visualization is a method that enables us to identify the features learned by a CNN, which can help us understand how it processes information and makes decisions."})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"What is feature visualization?"})," This is a technique that helps us investigate the inner representations of Convolutional Neural Networks (CNN), and observe the patterns that a model has learned to recognize. It generates images that maximize the activation of specific neurons, feature maps (outputs of a convolutional layer), or even entire layers in a model (",(0,i.jsx)(t.a,{href:"https://distill.pub/2020/circuits/",children:"Cammarata et al., 2020"}),")."]}),"\n",(0,i.jsx)(c.A,{src:"./img/p54_Image_3.png",alt:"Enter image alt description",number:"3",label:"9.3",caption:"Examples of feature visualizations. It seems that the network has learned to represent and detect baseballs, animal faces, clouds, and buildings, but feature visualizations have to be interpreted and may not be detecting what we initially think. From ([Olah et al., 2017](https://distill.pub/2017/feature-visualization/))."}),"\n",(0,i.jsxs)(t.p,{children:["Early research in neuroscience aimed to understand the brain by identifying which images strongly excited specific neurons. This helped neuroscientists discover brain areas dedicated to identifying faces, movement, natural scenes, etc. Feature visualization can be thought of as a somewhat similar approach used to CNNs (",(0,i.jsx)(t.a,{href:"https://distill.pub/2017/feature-visualization/",children:"Olah et al., 2017"}),")."]}),"\n",(0,i.jsx)(t.p,{children:"These learned features\u2014whether simple patterns like edges or complex objects\u2014form the building blocks that enable models to understand and process input data. However, features don't operate in isolation. As the network processes information, features interact and combine in structured ways, often forming more complex units known as circuits. A circuit is a group of interconnected features that work together to perform a specific function."}),"\n",(0,i.jsx)(t.p,{children:"For example, here is a circuit that recognizes a car in the mid-layers of a CNN. This circuit combines lower-level features such as windows, car bodies, and wheels to detect the presence of a car in an image. The circuit allows the model to recognize the entire object, even though no single feature on its own can do so."}),"\n",(0,i.jsx)(c.A,{src:"./img/S5x_Image_4.png",alt:"Enter image alt description",number:"4",label:"9.4",caption:"A car circuit in a CNN. On the left, three feature maps from layer 4b are represented by their feature visualizations. One map appears to detect windows, another car bodies, and the third wheels. These three feature maps are connected to a feature map in layer 4c, represented by the visualization on the right, through the convolutional kernels shown in the middle. The window, car body and wheel features get assembled to form a full car detector circuit. From ([Olah et al., 2020](https://distill.pub/2020/circuits/early-vision/))."}),"\n",(0,i.jsx)(l.A,{term:"Feature map",source:"",number:"3",label:"9.3",children:(0,i.jsx)(t.p,{children:"A feature map in a CNN is the output of a convolutional layer. It\u2019s also called a channel. Feature visualization is often applied at the scale of feature maps - instead of individual neurons or entire layers - to understand which features they encode."})}),"\n",(0,i.jsx)(c.A,{src:"./img/Kmp_Image_5.png",alt:"Enter image alt description",number:"5",label:"9.5",caption:"Some examples of feature visualizations on a CNN trained for image classification. Each image corresponds to a feature map. Certain feature maps are sensitive to patterns with edges, others are sensitive to different kinds of textured patterns, or objects like eyes, dog faces, or legs. From ([Olah et al., 2017](https://distill.pub/2017/feature-visualization/))."}),"\n",(0,i.jsxs)(o.A,{title:"Steps in generating feature visualizations",collapsed:!0,children:[(0,i.jsx)(c.A,{src:"./img/ucL_Image_6.png",alt:"Enter image alt description",number:"6",label:"9.6",caption:"Overview of the feature visualization process. Given a neuron (or a set of neurons) in a CNN, feature visualization generates an image that highly activates it, starting from a random image, and through successive optimization steps. The optimized image illustrates what kind of pattern one of the feature maps in the fourth layer of InceptionV1 is sensitive to ([Olah et al., 2017](https://distill.pub/2017/feature-visualization/))."}),(0,i.jsx)(t.p,{children:"Generating a feature visualization involves:"}),(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"Select a target: Choose a neuron, feature map, or layer for visualization. Most feature visualizations are performed on feature maps."}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"Start with a random input image: Begin optimization from a random noise image. It will be adjusted to maximize the activation of the target neuron or filter."}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:["Compute the gradient. The objective is to update the image such that it maximizes the activation of the neuron/feature map: Use ",(0,i.jsx)(s,{term:"backpropagation",definition:'{"definition":"The algorithm used to calculate gradients in neural networks by propagating errors backward through the network layers.","source":"[Rumelhart et al., 1986](https://www.nature.com/articles/323533a0)","aliases":["Backpropagation","backprop"]}',children:(0,i.jsx)(s,{term:"backpropagation",definition:'{"definition":"The algorithm used to calculate gradients in neural networks by propagating errors backward through the network layers.","source":"[Rumelhart et al., 1986](https://www.nature.com/articles/323533a0)","aliases":["Backpropagation","backprop"]}',children:"backpropagation"})})," to calculate how to modify the image to increase the activation of the target."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"Optimize the image: Apply gradient ascent iteratively to adjust the image."}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"Repeat the gradient ascent steps: Slightly adjust the image each time to better activate the neuron/feature map."}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"Visualize the result: The final image reveals the patterns or structures the target has learned to detect."}),"\n"]}),"\n"]}),(0,i.jsx)(c.A,{src:"./img/OQ6_Image_7.png",alt:"Enter image alt description",number:"7",label:"9.7",caption:"Feature visualizations can be produced for individual neurons or groups of neurons, such as a feature map, or even an entire layer. From ([Olah et al., 2017](https://distill.pub/2017/feature-visualization/))."})]}),"\n",(0,i.jsx)(t.h3,{id:"01-01",children:"Circuits"}),"\n",(0,i.jsx)(l.A,{term:"Circuit",source:"",number:"4",label:"9.4",children:(0,i.jsx)(t.p,{children:"A circuit is a group of interconnected features that work together to perform a specific function. Circuits are essentially higher order features, that are recursively composed of lower order features. The notion of circuit applies to any model architecture, including LLMs. Identifying circuits that perform specific functions in models is one area of research in mechanistic interpretability."})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Each layer in a CNN progressively extracts increasingly complex features from the image."})," Neurons in early layers respond to rudimentary and abstract patterns such as curves, angles, and small shapes (similar to the first layers of the human visual cortex!). As we move deeper into the network, neurons detect more complex and specific objects, such as eyes, animals, cars, etc (",(0,i.jsx)(t.a,{href:"https://distill.pub/2020/circuits/zoom-in/",children:"Olah et al., 2020"}),'). Interestingly, some of these "neuron families" recur across different model architectures and training conditions (',(0,i.jsx)(t.a,{href:"https://distill.pub/2020/circuits/early-vision/",children:"Olah et al., 2020"}),")."]}),"\n",(0,i.jsx)(c.A,{src:"./img/jiL_Image_8.png",alt:"Enter image alt description",number:"8",label:"9.8",caption:"Curve detectors are universally found in early layers of CNNs, they exist in different orientations and colors and collectively span all orientations. Each curve detector responds to a wide variety of curves, in different orientations. From ([Olah et al., 2020](https://distill.pub/2020/circuits/zoom-in/))."}),"\n",(0,i.jsxs)(t.p,{children:["CNNs also commonly learn ",(0,i.jsx)(t.strong,{children:"high-low frequency detectors"})," in their early layers."]}),"\n",(0,i.jsx)(c.A,{src:"./img/soI_Image_9.png",alt:"Enter image alt description",number:"9",label:"9.9",caption:"High-low frequency detectors look for low-frequency patterns on one side of their receptive field, and high-frequency patterns on the other side. They exist in different orientations and colors. From ([Olah et al., 2020](https://distill.pub/2020/circuits/early-vision/))."}),"\n",(0,i.jsxs)(t.p,{children:["High-low frequency detectors assemble in deeper layers to form ",(0,i.jsx)(t.strong,{children:"boundary detectors."})," ",(0,i.jsx)(c.A,{src:"./img/iGO_Image_10.png",alt:"Enter image alt description",number:"10",label:"9.10",caption:"A boundary detector neuron formed in the third layer of a CNN (shown at the bottom left with its feature visualization). The top row shows feature visualizations from neurons in the second layer and the kernels connecting them to the third layer. New neurons form by combining cues from more elementary neurons in previous layers: here the boundary detector neuron forms by combining high-low frequency detector neurons, with edges detector neurons, color contrast detector neurons, etc. From ([Olah et al., 2020](https://distill.pub/2020/circuits/early-vision/))."})]}),"\n",(0,i.jsx)(t.h3,{id:"01-02",children:"Polysemantic Neurons"}),"\n",(0,i.jsx)(t.p,{children:'While feature visualization has enabled a deeper understanding of how CNNs represent information, it has also highlighted challenges like polysemanticity. An intriguing phenomenon occurs when we look at the neurons following the car detector and that are strongly connected to it. Some of these neurons respond not only to images of cars but also to unrelated stimuli, such as images of dogs. This indicates that the "car feature" gets spread across multiple neurons that respond to seemingly unrelated inputs. These are known as polysemantic neurons\u2014neurons that activate in response to a variety of distinct features. In contrast, monosemantic neurons respond to just one specific feature or stimulus.'}),"\n",(0,i.jsx)(c.A,{src:"./img/kjD_Image_11.png",alt:"Enter image alt description",number:"11",label:"9.11",caption:"On the left is the car detector circuit from the previous figure. After distinct concepts are formed, they become entangled in polysemantic neurons, such as one neuron that responds to both car and dog images. From ([Olah et al., 2020](https://distill.pub/2020/circuits/early-vision/))."}),"\n",(0,i.jsx)(c.A,{src:"./img/8iG_Image_12.png",alt:"Enter image alt description",number:"12",label:"9.12",caption:"Multiple feature visualization performed on a polysemantic neuron that responds to images of cars, as well as cat faces, and cat legs. From ([Olah et al., 2020](https://distill.pub/2020/circuits/early-vision/))."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Polysemantic neurons are very common."})," For example, in a small language model, we can find a neuron that responds to English dialogues, Korean texts, HTTP requests, and academic citations simultaneously (",(0,i.jsx)(t.a,{href:"https://transformer-circuits.pub/2023/monosemantic-features",children:"Bricken et al., 2023"}),"). This means that each neuron does not correspond to one specific feature. Therefore, reasoning about a network's behavior based on individual neurons is misleading. Neurons are not the fundamental units to focus on when trying to understand models."]}),"\n",(0,i.jsx)(t.p,{children:"Polysemanticity poses a challenge for interpretability because it requires understanding how features are encoded across multiple neurons, rather than assuming each neuron represents a discrete unit of meaning. Identifying how these distributed features are encoded is an active area of research in interpretability."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"The leading hypothesis to explain why polysemanticity arises in neural networks is called the superposition hypothesis."})," Large models need to learn a huge number of features to perform effectively, likely more than the number of neurons they have. As a result, models cannot assign each feature to a single neuron. Instead, they must encode features in a more compressed manner. The superposition hypothesis suggests that models represent more features than they have neurons by encoding multiple features per neuron, with these features oriented in nearly orthogonal directions. In other words, models compress information by overlapping features across multiple neurons."]}),"\n",(0,i.jsxs)(o.A,{title:"Polysemanticity vs Superposition",collapsed:!0,children:[(0,i.jsxs)(t.p,{children:["The distinction between polysemanticity and the superposition hypothesis is important (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2404.14082",children:"Bereska et Gavves, 2024"}),"):"]}),(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"Polysemanticity refers to the empirical phenomenon where a neuron represents or responds to multiple unrelated features."}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"The superposition hypothesis, on the contrary, generally refers to an hypothesis that tries to explain polysemanticity. It suggests that when models have more features to represent than they have neurons to represent them, they must compress these features into the limited space. This compression forces features to overlap across neurons. This supposedly explains why we observe neurons responding to multiple, seemingly unrelated features. While superposition inherently leads to polysemanticity, polysemanticity itself doesn\u2019t always imply superposition, as polysemanticity could theoretically arise from other mechanisms."}),"\n"]}),"\n"]})]}),"\n",(0,i.jsx)(t.p,{children:"Understanding and addressing polysemanticity is an active research area. Various directions are being explored:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Sparse Representations:"})," Designing networks to use sparse representations (where fewer neurons are active at a time) may reduce polysemanticity (",(0,i.jsx)(t.a,{href:"https://transformer-circuits.pub/2022/solu/index.html",children:"Elhage et al., 2022"}),"). This approach has not been widely explored because it comes with significant performance trade-offs."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Feature Disentanglement:"}),' Some approaches involve decomposing complex neuron activations to isolate individual features. One promising approach in that line of research, known as Sparse Autoencoders, is a technique that "unfolds" the network and separates out individual features (',(0,i.jsx)(t.a,{href:"https://transformer-circuits.pub/2023/monosemantic-features",children:"Bricken et al., 2023"}),"). It is explained more in depth in the Sparse Autoencoders section."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"Feature visualization has led to several key insights about how vision neural networks operate:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Hierarchical structure:"})," Neural networks learn features hierarchically, with early layers detecting simple patterns and deeper layers composing these into complex objects. For instance, curve detectors in early layers combine into boundary detectors in mid-layers and eventually into full object detectors in deeper layers."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Circuits:"}),' Features interact to form "circuits"\u2014groups of interconnected features that collectively perform a specific function. For example, a circuit for detecting cars might combine features like wheels, windows, and body shapes into a cohesive representation of a car.']}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Polysemanticity:"})," Feature visualization has revealed the phenomenon of polysemanticity, where individual neurons or features in a ",(0,i.jsx)(s,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:(0,i.jsx)(s,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:"neural network"})})," respond to multiple, seemingly unrelated concepts. This overlap complicates our ability to assign clear and interpretable roles to individual neurons."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Universality:"})," Some features, such as edge or curve detectors, are universal across models and architectures. This suggests that certain features are fundamental to visual processing, regardless of the specific task or dataset."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"02",children:"Logit Lens"}),"\n",(0,i.jsxs)(t.p,{children:["The Logit Lens (",(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/",children:"nostalgebraist, 2020"}),") is one of the first tools developed to look inside transformers. It enables us to observe how a ",(0,i.jsx)(s,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:(0,i.jsx)(s,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:"transformer"})}),' refines its predictions layer by layer \u2014allowing us to see not just the final output but the evolving "thought process" the model undergoes as it makes a prediction.']}),"\n",(0,i.jsx)(t.p,{children:"Transformers are trained to predict the next token in a sequence. They do this by transforming the inputs layer by layer, with each layer adding new information to improve the prediction. The Logit Lens enables us to \u201ctranslate\u201d each layer's internal representation back into tokens. By seeing what the model \u201cpredicts\u201d at each layer, we can trace how its predictions evolve from a rough guess in the initial layers to a refined choice in the final one. However, it\u2019s worth noting that while the Logit Lens lets us see the intermediate predictions at each layer, it doesn\u2019t explain the mechanism of transformation. The Logit Lens is an inherently observational tool \u2014it reveals what is the most probable token at the end of each layer but doesn\u2019t allow us to understand why this precise token is predicted."}),"\n",(0,i.jsxs)(o.A,{title:"Detail - Walkthrough of LogitLens",collapsed:!0,children:[(0,i.jsxs)(t.p,{children:["A ",(0,i.jsx)(s,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:(0,i.jsx)(s,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:"transformer"})})," model is a powerful architecture for processing sequences of data, especially text. It is trained to predict the next word or subword in a sequence, referred to as tokens."]}),(0,i.jsxs)(t.p,{children:["The set of all tokens (words or subwords) that the model can recognize is called the vocabulary. For example, GPT-2 has a vocabulary of 50,257 tokens. More precisely, a ",(0,i.jsx)(s,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:(0,i.jsx)(s,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:"transformer"})})," takes a sequence of tokens as input and is trained to predict the next token in this sequence, outputting a probability distribution over the entire vocabulary."]}),(0,i.jsxs)(t.p,{children:["Internally, a ",(0,i.jsx)(s,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:(0,i.jsx)(s,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:"transformer"})})," consists of multiple stacked layers, each containing two sublayers: an MLP (multi-layer perceptron) and ",(0,i.jsx)(s,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:(0,i.jsx)(s,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:"attention"})})," heads. For understanding the Logit Lens, we don\u2019t need to go into the details of how these sublayers function."]}),(0,i.jsx)(t.p,{children:"The intermediate layers are connected through what\u2019s known as the residual stream. The residual stream is a pathway that carries information from the input to the output, allowing it to flow through all layers of the model."}),(0,i.jsx)(c.A,{src:"./img/DRe_Image_13.png",alt:"Enter image alt description",number:"13",label:"9.13",caption:"A minimalist illustration of a transformer with a single layer. Transformers cannot directly manipulate tokens, so they embed tokens into numerical vectors, an operation depicted in the bottom grey box. After embedding, the tokens are represented as vectors within the residual stream. The first sublayer (attention heads, represented by the two side-by-side boxes on the left) reads these embedded tokens, applies a transformation, and adds its output back into the residual stream. The second sublayer (MLP) does the same. Finally, to output a token, the embedded tokens must be converted back to the vocabulary space through an unembedding operation (depicted in the top box). The residual stream enables each layer to make incremental adjustments to the model\u2019s predictions by accumulating and refining the information passed through each transformation. From ([Elhage et al., 2021](https://transformer-circuits.pub/2021/framework/index.html))."}),(0,i.jsx)(t.p,{children:"The essential idea behind the Logit Lens is that the unembedding operation, typically applied only after the last layer, can be applied at any point in the residual stream. After each intermediate layer, the residual stream can be unembedded\u2014that is, converted back into the token space\u2014allowing us to observe which token is currently the most probable."}),(0,i.jsx)(t.p,{children:"Connection between the Logit Lens and Feature Visualization in CNNs. In the previous section on Feature Visualization, we saw that CNNs build up their understanding of an image layer by layer, detecting simple patterns like edges in early layers and more complex shapes or objects in later ones. Feature visualization allows us to interpret these progressive transformations by displaying the visual patterns that different neurons respond to. The Logit Lens can be seen as a parallel interpretability tool for transformers. Instead of visualizing image features, the Logit Lens lets us see the model\u2019s step-by-step guesses for the next token."}),(0,i.jsx)(t.p,{children:"The figure shows how using the Logit Lens looks in practice."}),(0,i.jsx)(c.A,{src:"./img/UuF_Image_14.png",alt:"Enter image alt description",number:"14",label:"9.14",caption:"An example of the Logit Lens used on GPT-2 when it\u2019s trying to predict the next token of the sequence: \u201cSpecifically, we train GPT-3, an\u201d. Input tokens are written at the bottom, and correct outputs at the top. Layers stack from bottom to top. The first token at the top, \u201c,\u201d, corresponds to the token the model should predict when given as input only the first token: \u201cSpecifically\u201d. The second token at the top, \u201cwe\u201d, corresponds to the token the model should predict when given as input only the two first tokens: \u201cSpecifically, \u201c. This is the reason why there is a shift of one position between tokens at the bottom and tokens at the top. (*) indicates that the model correctly predicted the next token. Each cell contains the model\u2019s top guesses at different layers. The color scale indicates the associated logit value. The higher the logit, the more confident in its prediction the model is. From ([nostalgebraist, 2020](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens))."}),(0,i.jsx)(t.p,{children:"The figure shows that when GPT-2 tries to predict the next token in the sequence \u201cSpecifically, we train GPT-3, an\u201d, it predicts \u201cenormous\u201d in its early layers, then \u201cmassive\u201d, \u201csingle\u201d, and \u201cN\u201d in its final layer."}),(0,i.jsx)(c.A,{src:"./img/qlF_Image_15.png",alt:"Enter image alt description",number:"15",label:"9.15",caption:"Overview of how the Logit Lens works. Read from bottom to top. A detailed explanation is provided in the following paragraphs. Object\u2019s shapes are indicated between brackets."}),(0,i.jsxs)(t.p,{children:["A ",(0,i.jsx)(s,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:(0,i.jsx)(s,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:"transformer"})})," takes as an input a sequence of tokens. Each token is an element from a vocabulary (typically of size ",(0,i.jsxs)(t.span,{className:"katex",children:[(0,i.jsx)(t.span,{className:"katex-mathml",children:(0,i.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(t.semantics,{children:[(0,i.jsx)(t.mrow,{children:(0,i.jsxs)(t.msub,{children:[(0,i.jsx)(t.mi,{children:"n"}),(0,i.jsx)(t.mtext,{children:"vocab"})]})}),(0,i.jsx)(t.annotation,{encoding:"application/x-tex",children:"n_{\\text{vocab}}"})]})})}),(0,i.jsx)(t.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(t.span,{className:"base",children:[(0,i.jsx)(t.span,{className:"strut",style:{height:"0.5806em",verticalAlign:"-0.15em"}}),(0,i.jsxs)(t.span,{className:"mord",children:[(0,i.jsx)(t.span,{className:"mord mathnormal",children:"n"}),(0,i.jsx)(t.span,{className:"msupsub",children:(0,i.jsxs)(t.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(t.span,{className:"vlist-r",children:[(0,i.jsx)(t.span,{className:"vlist",style:{height:"0.3361em"},children:(0,i.jsxs)(t.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,i.jsx)(t.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(t.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(t.span,{className:"mord mtight",children:(0,i.jsx)(t.span,{className:"mord text mtight",children:(0,i.jsx)(t.span,{className:"mord mtight",children:"vocab"})})})})]})}),(0,i.jsx)(t.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(t.span,{className:"vlist-r",children:(0,i.jsx)(t.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(t.span,{})})})]})})]})]})})]}),"), so each text token can be associated with an integer value ranging from 0 to ",(0,i.jsxs)(t.span,{className:"katex",children:[(0,i.jsx)(t.span,{className:"katex-mathml",children:(0,i.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(t.semantics,{children:[(0,i.jsx)(t.mrow,{children:(0,i.jsxs)(t.msub,{children:[(0,i.jsx)(t.mi,{children:"n"}),(0,i.jsx)(t.mtext,{children:"vocab"})]})}),(0,i.jsx)(t.annotation,{encoding:"application/x-tex",children:"n_{\\text{vocab}}"})]})})}),(0,i.jsx)(t.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(t.span,{className:"base",children:[(0,i.jsx)(t.span,{className:"strut",style:{height:"0.5806em",verticalAlign:"-0.15em"}}),(0,i.jsxs)(t.span,{className:"mord",children:[(0,i.jsx)(t.span,{className:"mord mathnormal",children:"n"}),(0,i.jsx)(t.span,{className:"msupsub",children:(0,i.jsxs)(t.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(t.span,{className:"vlist-r",children:[(0,i.jsx)(t.span,{className:"vlist",style:{height:"0.3361em"},children:(0,i.jsxs)(t.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,i.jsx)(t.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(t.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(t.span,{className:"mord mtight",children:(0,i.jsx)(t.span,{className:"mord text mtight",children:(0,i.jsx)(t.span,{className:"mord mtight",children:"vocab"})})})})]})}),(0,i.jsx)(t.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(t.span,{className:"vlist-r",children:(0,i.jsx)(t.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(t.span,{})})})]})})]})]})})]}),". This association can also be thought of as representing the token as a basis vector within a space of dimension ",(0,i.jsxs)(t.span,{className:"katex",children:[(0,i.jsx)(t.span,{className:"katex-mathml",children:(0,i.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(t.semantics,{children:[(0,i.jsx)(t.mrow,{children:(0,i.jsxs)(t.msub,{children:[(0,i.jsx)(t.mi,{children:"n"}),(0,i.jsx)(t.mtext,{children:"vocab"})]})}),(0,i.jsx)(t.annotation,{encoding:"application/x-tex",children:"n_{\\text{vocab}}"})]})})}),(0,i.jsx)(t.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(t.span,{className:"base",children:[(0,i.jsx)(t.span,{className:"strut",style:{height:"0.5806em",verticalAlign:"-0.15em"}}),(0,i.jsxs)(t.span,{className:"mord",children:[(0,i.jsx)(t.span,{className:"mord mathnormal",children:"n"}),(0,i.jsx)(t.span,{className:"msupsub",children:(0,i.jsxs)(t.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(t.span,{className:"vlist-r",children:[(0,i.jsx)(t.span,{className:"vlist",style:{height:"0.3361em"},children:(0,i.jsxs)(t.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,i.jsx)(t.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(t.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(t.span,{className:"mord mtight",children:(0,i.jsx)(t.span,{className:"mord text mtight",children:(0,i.jsx)(t.span,{className:"mord mtight",children:"vocab"})})})})]})}),(0,i.jsx)(t.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(t.span,{className:"vlist-r",children:(0,i.jsx)(t.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(t.span,{})})})]})})]})]})})]}),", denoted the vocabulary space. Each basis vector in the vocabulary space is associated with one token."]}),(0,i.jsxs)(t.p,{children:["A ",(0,i.jsx)(s,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:(0,i.jsx)(s,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:"transformer"})})," is trained to predict the next token of the input sequence, so its output is a probability distribution over the vocabulary."]}),(0,i.jsxs)(t.p,{children:["Transformers convert each token into vectors ",(0,i.jsxs)(t.span,{className:"katex",children:[(0,i.jsx)(t.span,{className:"katex-mathml",children:(0,i.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(t.semantics,{children:[(0,i.jsx)(t.mrow,{children:(0,i.jsxs)(t.msub,{children:[(0,i.jsx)(t.mi,{children:"n"}),(0,i.jsx)(t.mtext,{children:"vocab"})]})}),(0,i.jsx)(t.annotation,{encoding:"application/x-tex",children:"n_{\\text{vocab}}"})]})})}),(0,i.jsx)(t.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(t.span,{className:"base",children:[(0,i.jsx)(t.span,{className:"strut",style:{height:"0.5806em",verticalAlign:"-0.15em"}}),(0,i.jsxs)(t.span,{className:"mord",children:[(0,i.jsx)(t.span,{className:"mord mathnormal",children:"n"}),(0,i.jsx)(t.span,{className:"msupsub",children:(0,i.jsxs)(t.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(t.span,{className:"vlist-r",children:[(0,i.jsx)(t.span,{className:"vlist",style:{height:"0.3361em"},children:(0,i.jsxs)(t.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,i.jsx)(t.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(t.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(t.span,{className:"mord mtight",children:(0,i.jsx)(t.span,{className:"mord text mtight",children:(0,i.jsx)(t.span,{className:"mord mtight",children:"vocab"})})})})]})}),(0,i.jsx)(t.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(t.span,{className:"vlist-r",children:(0,i.jsx)(t.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(t.span,{})})})]})})]})]})})]}),", which is known as the ",(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:"embedding"})})," space. The ",(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:"embedding"})})," space dimension is denoted ",(0,i.jsxs)(t.span,{className:"katex",children:[(0,i.jsx)(t.span,{className:"katex-mathml",children:(0,i.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(t.semantics,{children:[(0,i.jsx)(t.mrow,{children:(0,i.jsxs)(t.msub,{children:[(0,i.jsx)(t.mi,{children:"d"}),(0,i.jsx)(t.mtext,{children:"model"})]})}),(0,i.jsx)(t.annotation,{encoding:"application/x-tex",children:"d_{\\text{model}}"})]})})}),(0,i.jsx)(t.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(t.span,{className:"base",children:[(0,i.jsx)(t.span,{className:"strut",style:{height:"0.8444em",verticalAlign:"-0.15em"}}),(0,i.jsxs)(t.span,{className:"mord",children:[(0,i.jsx)(t.span,{className:"mord mathnormal",children:"d"}),(0,i.jsx)(t.span,{className:"msupsub",children:(0,i.jsxs)(t.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(t.span,{className:"vlist-r",children:[(0,i.jsx)(t.span,{className:"vlist",style:{height:"0.3361em"},children:(0,i.jsxs)(t.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,i.jsx)(t.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(t.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(t.span,{className:"mord mtight",children:(0,i.jsx)(t.span,{className:"mord text mtight",children:(0,i.jsx)(t.span,{className:"mord mtight",children:"model"})})})})]})}),(0,i.jsx)(t.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(t.span,{className:"vlist-r",children:(0,i.jsx)(t.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(t.span,{})})})]})})]})]})})]}),". Projected tokens are called ",(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:"embedding"})})," vectors, or simply ",(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:"embeddings"})}),". In some contexts, they may also be referred to as hidden activations. The ",(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:"embedding"})})," matrix converts tokens from the vocabulary space into the ",(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:"embedding"})})," space. It is like a lookup table that associates elements in the vocabulary space with their corresponding counterparts in the ",(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:"embedding"})})," space."]}),(0,i.jsxs)(t.p,{children:["Once tokens have been converted into the ",(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:"embedding"})})," space, they flow through the successive layers of the ",(0,i.jsx)(s,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:(0,i.jsx)(s,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:"transformer"})}),". Similar to how input images undergo transformations across the layers of a CNN, ",(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:"embedding"})})," vectors are transformed as they pass through the network. The Logit Lens precisely focuses on those intermediate representations that transformers build."]}),(0,i.jsxs)(t.p,{children:["Finally, to output a probability distribution over the vocabulary, the final ",(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:"embedding"})})," vector has to be converted back into vocabulary space. Conversion from the ",(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:"embedding"})})," space to the vocabulary space can be done through multiplication by a matrix of dimension ",(0,i.jsxs)(t.span,{className:"katex",children:[(0,i.jsx)(t.span,{className:"katex-mathml",children:(0,i.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(t.semantics,{children:[(0,i.jsxs)(t.mrow,{children:[(0,i.jsx)(t.mo,{stretchy:"false",children:"["}),(0,i.jsxs)(t.msub,{children:[(0,i.jsx)(t.mi,{children:"d"}),(0,i.jsx)(t.mtext,{children:"model"})]}),(0,i.jsx)(t.mo,{separator:"true",children:","}),(0,i.jsxs)(t.msub,{children:[(0,i.jsx)(t.mi,{children:"n"}),(0,i.jsx)(t.mtext,{children:"vocab"})]}),(0,i.jsx)(t.mo,{stretchy:"false",children:"]"})]}),(0,i.jsx)(t.annotation,{encoding:"application/x-tex",children:"[d_{\\text{model}}, n_{\\text{vocab}}]"})]})})}),(0,i.jsx)(t.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(t.span,{className:"base",children:[(0,i.jsx)(t.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(t.span,{className:"mopen",children:"["}),(0,i.jsxs)(t.span,{className:"mord",children:[(0,i.jsx)(t.span,{className:"mord mathnormal",children:"d"}),(0,i.jsx)(t.span,{className:"msupsub",children:(0,i.jsxs)(t.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(t.span,{className:"vlist-r",children:[(0,i.jsx)(t.span,{className:"vlist",style:{height:"0.3361em"},children:(0,i.jsxs)(t.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,i.jsx)(t.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(t.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(t.span,{className:"mord mtight",children:(0,i.jsx)(t.span,{className:"mord text mtight",children:(0,i.jsx)(t.span,{className:"mord mtight",children:"model"})})})})]})}),(0,i.jsx)(t.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(t.span,{className:"vlist-r",children:(0,i.jsx)(t.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(t.span,{})})})]})})]}),(0,i.jsx)(t.span,{className:"mpunct",children:","}),(0,i.jsx)(t.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsxs)(t.span,{className:"mord",children:[(0,i.jsx)(t.span,{className:"mord mathnormal",children:"n"}),(0,i.jsx)(t.span,{className:"msupsub",children:(0,i.jsxs)(t.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(t.span,{className:"vlist-r",children:[(0,i.jsx)(t.span,{className:"vlist",style:{height:"0.3361em"},children:(0,i.jsxs)(t.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,i.jsx)(t.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(t.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(t.span,{className:"mord mtight",children:(0,i.jsx)(t.span,{className:"mord text mtight",children:(0,i.jsx)(t.span,{className:"mord mtight",children:"vocab"})})})})]})}),(0,i.jsx)(t.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(t.span,{className:"vlist-r",children:(0,i.jsx)(t.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(t.span,{})})})]})})]}),(0,i.jsx)(t.span,{className:"mclose",children:"]"})]})})]}),". This operation is called the unembedding. The result of the unembedding is a logits vector of size ",(0,i.jsxs)(t.span,{className:"katex",children:[(0,i.jsx)(t.span,{className:"katex-mathml",children:(0,i.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(t.semantics,{children:[(0,i.jsx)(t.mrow,{children:(0,i.jsxs)(t.msub,{children:[(0,i.jsx)(t.mi,{children:"n"}),(0,i.jsx)(t.mtext,{children:"vocab"})]})}),(0,i.jsx)(t.annotation,{encoding:"application/x-tex",children:"n_{\\text{vocab}}"})]})})}),(0,i.jsx)(t.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(t.span,{className:"base",children:[(0,i.jsx)(t.span,{className:"strut",style:{height:"0.5806em",verticalAlign:"-0.15em"}}),(0,i.jsxs)(t.span,{className:"mord",children:[(0,i.jsx)(t.span,{className:"mord mathnormal",children:"n"}),(0,i.jsx)(t.span,{className:"msupsub",children:(0,i.jsxs)(t.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(t.span,{className:"vlist-r",children:[(0,i.jsx)(t.span,{className:"vlist",style:{height:"0.3361em"},children:(0,i.jsxs)(t.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,i.jsx)(t.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(t.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(t.span,{className:"mord mtight",children:(0,i.jsx)(t.span,{className:"mord text mtight",children:(0,i.jsx)(t.span,{className:"mord mtight",children:"vocab"})})})})]})}),(0,i.jsx)(t.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(t.span,{className:"vlist-r",children:(0,i.jsx)(t.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(t.span,{})})})]})})]})]})})]}),". The logits can then be converted into probabilities over the vocabulary using the ",(0,i.jsx)(s,{term:"softmax",definition:'{"definition":"An activation function that converts a vector of real numbers into a probability distribution, commonly used in classification tasks.","source":"","aliases":["Softmax"]}',children:(0,i.jsx)(s,{term:"softmax",definition:'{"definition":"An activation function that converts a vector of real numbers into a probability distribution, commonly used in classification tasks.","source":"","aliases":["Softmax"]}',children:"softmax"})})," function."]}),(0,i.jsxs)(t.p,{children:["The essential intuition behind the Logit Lens is that the unembedding operation, typically applied after the last layer, can also be applied after each intermediate layer. ",(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:"Embedding"})})," vectors are modified by each layer of the network, but their dimensions remain the same (",(0,i.jsxs)(t.span,{className:"katex",children:[(0,i.jsx)(t.span,{className:"katex-mathml",children:(0,i.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(t.semantics,{children:[(0,i.jsx)(t.mrow,{children:(0,i.jsxs)(t.msub,{children:[(0,i.jsx)(t.mi,{children:"d"}),(0,i.jsx)(t.mtext,{children:"model"})]})}),(0,i.jsx)(t.annotation,{encoding:"application/x-tex",children:"d_{\\text{model}}"})]})})}),(0,i.jsx)(t.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(t.span,{className:"base",children:[(0,i.jsx)(t.span,{className:"strut",style:{height:"0.8444em",verticalAlign:"-0.15em"}}),(0,i.jsxs)(t.span,{className:"mord",children:[(0,i.jsx)(t.span,{className:"mord mathnormal",children:"d"}),(0,i.jsx)(t.span,{className:"msupsub",children:(0,i.jsxs)(t.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(t.span,{className:"vlist-r",children:[(0,i.jsx)(t.span,{className:"vlist",style:{height:"0.3361em"},children:(0,i.jsxs)(t.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,i.jsx)(t.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(t.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(t.span,{className:"mord mtight",children:(0,i.jsx)(t.span,{className:"mord text mtight",children:(0,i.jsx)(t.span,{className:"mord mtight",children:"model"})})})})]})}),(0,i.jsx)(t.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(t.span,{className:"vlist-r",children:(0,i.jsx)(t.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(t.span,{})})})]})})]})]})})]}),"). Thus, after any layer, ",(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:(0,i.jsx)(s,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:"embedding"})})," vectors can be converted back to the vocabulary space, and one can get an idea of how the network refines its prediction across layers."]})]}),"\n",(0,i.jsx)(t.h2,{id:"03",children:"Probing classifiers"}),"\n",(0,i.jsxs)(t.p,{children:["Probing is a technique used to analyze neural networks and understand which representations or concepts they have learned and where (",(0,i.jsx)(t.a,{href:"https://aclanthology.org/2022.cl-1.7/",children:"Belinkov, 2022"}),"). Probing techniques can be applied to a range of models, from transformers to CNNs, to investigate whether specific information or properties are encoded in a model\u2019s intermediate representations."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"What is a probe?"})," A probe is a lightweight model, often a linear classifier, trained to detect whether a specific concept or feature is represented in the activations of a ",(0,i.jsx)(s,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:(0,i.jsx)(s,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:"neural network"})}),". In a probing setup, researchers analyze activations (the intermediate outputs) from layers within a network to see if these activations encode information about a particular property or concept. For example, probes may be used to determine whether a language model\u2019s activations contain information about grammar rules or whether a chess-playing model like AlphaZero encodes strategic knowledge about the game, where this knowledge is located within the network, and when it is acquired during training (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2305.01610",children:"Gurnee et al., 2023"}),", ",(0,i.jsx)(t.a,{href:"https://www.pnas.org/doi/10.1073/pnas.2206625119",children:"McGrath et al., 2022"}),")."]}),"\n",(0,i.jsxs)(t.p,{children:["One well-known example of probing was applied to AlphaZero, the ",(0,i.jsx)(s,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:(0,i.jsx)(s,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:"neural network"})})," trained to play chess that famously defeated top human players. Researchers used probes to investigate whether AlphaZero internally represents certain strategic concepts about chess, such as \u201cCan the opponent capture my queen?\u201d or \u201cIs there a checkmate threat within one move?\u201d (",(0,i.jsx)(t.a,{href:"https://www.pnas.org/doi/10.1073/pnas.2206625119",children:"McGrath et al., 2022"}),")."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Probing classification is the process of training classifiers on the network\u2019s intermediate activations to identify if specific properties are encoded."})," The steps of probing classification are as follows:"]}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Choose a Property or Concept:"})," Define a specific concept to explore, such as \u201cCan the opponent capture my queen?\u201d."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Generate or Select Input Data:"})," Create a dataset with examples that vary in terms of the target property. For instance, in chess, we might create board configurations where \u201cthe player can capture the opponent\u2019s queen\u201d is either true or false."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Record Intermediate Activations:"})," Feed this dataset through the model and record the activations of neurons at different layers."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Train a Classifier on the Activations:"})," Use the recorded activations as input features to train a classifier (probe) that distinguishes between different classes based on the concept (e.g., true vs. false input prompts)."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Evaluate the Probe\u2019s Accuracy:"})," If the probe achieves high accuracy, this suggests that the target concept is strongly encoded in the recorded layer\u2019s activations."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(o.A,{title:"Probing in Practice: Case Studies and Limitations",collapsed:!0,children:[(0,i.jsx)(c.A,{src:"./img/TQo_Image_16.png",alt:"Enter image alt description",number:"16",label:"9.16",caption:"Accuracy of two probes trained on AlphaZero\u2019s intermediary activations, across layers and training steps The two two probes were trained on the two concepts: \u201cCan the playing side capture their opponent\u2019s queen?\u201d, and \u201cCould the opposing side checkmate the playing side in one move?\u201d. The color represents the probe\u2019s accuracy (the yellow color corresponds to a higher value, indicating that the feature is more represented). Here, the probes were trained at different stages of AlphaZero's training and from the activations of different layers of the network (a block corresponds to one layer of AlphaZero). It can be seen that the more AlphaZero is trained, the more it represents these two concepts. The concept \u201chas_mate_threat\u201d seems to be represented quite homogeneously across AlphaZero's layers, while \u201ccan_capture_queen_opponent\u201d appears to be more represented in the earlier layers. \u201cThe ability to predict has_mate_threat from AlphaZero\u2019s activations indicates that AlphaZero is not simply modeling its potential moves, but also its opponent\u2019s potential moves and their consequences during position evaluation\u201d. From ([MacGrath et al., 2022](https://www.pnas.org/doi/10.1073/pnas.2206625119))."}),(0,i.jsxs)(t.p,{children:["Using linear probes for detecting prompts leading to dangerous behavior. The alignment team from Anthropic is currently working on a research agenda called \u201csleeper agents\u201d where they deliberately train deceptively aligned models (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2401.05566",children:"Hubinger et al., 2024"}),') to test whether currently available techniques can detect misalignment. These "sleeper agents" are models with hidden backdoors introduced during training. One of the sleeper agents they trained writes secure code when the prompt indicates year \u201c2023\u201d but inserts vulnerabilities when the prompt indicates \u201c2024\u201d. Such dangerous outputs are hard to detect, and could easily be missed by a human overseer, this is why it would be extremely useful to develop an automatic detection method. Anthropic researchers have trained linear probes based on sleeper agents\u2019 residual stream activations that classify inputs as safe or dangerous with very high accuracy (',(0,i.jsx)(t.a,{href:"https://www.anthropic.com/news/probes-catch-sleeper-agents",children:"MacDiarmid et al., 2024"}),"). Whether or not a prompt will trigger defection appears to be linearly encoded across a wide range of middle residual streams. However, they remain wary of this result and acknowledge that the surprisingly high accuracy of the probes may be due to the backdoor insertion, and that \u201cnatural defection\u201d may not be detected as easily."]}),(0,i.jsxs)(t.p,{children:["It was also suggested to train probes on human-written scenarios of takeover attempts or catastrophic actions (",(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/WCj7WgFSLmyKaMwPR/coup-probes-catching-catastrophes-with-probes-trained-off",children:"Roger., 2023"}),") to detect such goals or intentions in models before they happen. Similarly to sleeper agents, it is unclear whether probes trained in artificial settings will generalize in real-world scenarios."]}),(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Limitations of probing classifiers"})," - ",(0,i.jsx)(t.strong,{children:"Correlation, not causation:"})," Probing classifiers indicate that a concept (or a proxy of that concept) is encoded, but they don\u2019t reveal whether this concept is actively used by the network during inference. The classifier\u2019s high accuracy may reflect the ease with which it can detect patterns, not necessarily that the model relies on those patterns for decision-making. To discover causal effects we need to intervene in representations of the model instead of just observing them."]}),(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Identifying proxies rather than \u201ctrue concepts\u201d:"})," Probing may detect patterns that act as proxies for the concept of interest, rather than the concept itself. For example, a probe trained to detect \u201clegal language\u201d might actually be detecting correlated cues (such as certain formal words or sentence structures) rather than a genuine understanding of legal terminology. This makes it challenging to interpret probes as definitive indicators of the exact concepts represented within the model."]}),"\n"]})]}),"\n",(0,i.jsx)(t.h2,{id:"04",children:"Superposition"}),"\n",(0,i.jsx)(t.p,{children:'To make sense of data, classify it, or make decisions, neural networks need to learn features\u2014representations that capture meaningful patterns in the data. However, networks have a limited number of neurons to store a vast amount of information. Instead of assigning a single neuron to each feature, neural networks often "share" neurons across multiple features. This shared, overlapping storage is known as superposition.'}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Superposition occurs because it allows the network to handle more features using fewer neurons, making it more memory-efficient."}),' However, this efficiency comes at a cost: polysemanticity. Polysemanticity means that a single neuron or component in the network represents multiple, often unrelated, features. For example, one neuron might activate in response to both "cats" and "cars," even though these concepts are entirely different.']}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Polysemanticity and Superposition."})," The vocabulary surrounding superposition and related concepts is often messy. The terms superposition and polysemanticity are often used interchangeably, even though they refer to slightly different aspects of the same phenomenon in some contexts: superposition describes the overlapping storage of features, while polysemanticity highlights the behavior of neurons that respond to multiple distinct features. Also keep in mind that superposition should not be confused with the superposition hypothesis, which is a specific theory proposed to explain why and how superposition occurs in neural networks."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"This overlap creates a challenge for interpretability."}),' Ideally, we might expect each neuron to have a clear and singular purpose\u2014one neuron for "cats," another for "cars," and so on. In reality, many neurons respond to combinations of unrelated features, leading to entangled representations. This means that interpreting individual neurons in isolation often provides an incomplete or misleading picture (',(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2104.07143",children:"Bolukbasi et al., 2021"}),")."]}),"\n",(0,i.jsx)(t.p,{children:"Contrast this with a hypothetical network without polysemanticity: every neuron would correspond to a distinct feature, making the model much easier to interpret. However, such a design would require far more neurons to represent every feature individually, which is computationally inefficient, particularly in large-scale models. As a result, superposition is practically inevitable in modern neural networks."}),"\n",(0,i.jsx)(t.p,{children:"From an AI safety perspective, understanding superposition is critical. If we want to trace how a model makes decisions, we need to identify and disentangle these overlapping features. Only then can we retrace the reasoning behind its predictions and uncover what drives its behavior."}),"\n",(0,i.jsx)(t.p,{children:"Before diving into techniques for disentangling features, researchers first sought to understand how polysemanticity arises. Key questions include:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"What causes polysemanticity in neural networks?"}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"How does model architecture or the training process influence it?"}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"Can it be controlled or mitigated?"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"To answer these, researchers used toy models\u2014simple, small-scale neural networks. Toy models allow for controlled experimentation and help researchers isolate and study phenomena like superposition without the complexity of larger systems. The following subsection details the toy models used to study polysemanticity."}),"\n",(0,i.jsxs)(o.A,{title:"Experiments on Toy Models to Support the Superposition Hypothesis",collapsed:!0,children:[(0,i.jsxs)(t.p,{children:["The Toy Models of Superposition paper introduces simplified models that help researchers study superposition in a controlled environment (",(0,i.jsx)(t.a,{href:"https://transformer-circuits.pub/2022/toy_model/index.html",children:"Elhage et al., 2022"}),"). These models provide evidence supporting the superposition hypothesis\u2014a theory about how neural networks efficiently store and organize information."]}),(0,i.jsx)(t.p,{children:"The superposition hypothesis suggests that neural networks can represent more features or concepts than they have individual neurons by encoding information as linear combinations across multiple neurons. This means:"}),(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Efficient compression:"})," Neural networks can compress information by representing more features than there are available dimensions, optimizing memory use."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Distributed representation:"})," Features are not exclusively tied to single neurons; instead, they are distributed across multiple neurons."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Non-orthogonal directions:"})," Features are stored in directions that are not perfectly orthogonal in the network's activation space, which leads to overlaps and potential interference between concepts."]}),"\n"]}),"\n"]}),(0,i.jsx)(t.p,{children:"This paper is a cornerstone of mechanistic interpretability because it reveals fascinating phenomena about how neural networks organize and store information:"}),(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Demonstrating superposition:"})," The experiments show that superposition occurs and identify the conditions under which it arises."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Explaining mono- and polysemantic neurons:"})," The paper clarifies why some neurons specialize in a single feature (monosemantic) while others represent multiple features (polysemantic)."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Phase transitions in training:"})," It highlights a phase change",(0,i.jsx)(r.A,{id:"footnote_phase_transition",number:"1",text:"A phase change in neural network training refers to a sudden, qualitative shift in the behavior or structure of the model during the training process."})," during training that determines whether features are stored in superposition."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Geometric feature organization:"})," Features in superposition are arranged into geometric structures such as digons, triangles, pentagons, and tetrahedrons, providing insight into the network\u2019s internal organization."]}),"\n"]}),"\n"]}),(0,i.jsxs)(t.p,{children:["The following figure is a great illustration of superposition in a toy model. The toy model has 2 dimensions, represented by the x and y axis in the figure below, but it needs to learn 5 features. This means that the model needs to find a way to fit more information (5 features) into fewer dimensions (2-dimensional space). Each feature is given an importance, represented with colors. An important feature is a feature that has a significant impact on the model\u2019s accuracy or ",(0,i.jsx)(s,{term:"loss function",definition:'{"definition":"A function that measures how well a model\'s predictions match the actual target values, used to guide training.","source":"","aliases":["Loss Function","cost function","objective function"]}',children:(0,i.jsx)(s,{term:"loss function",definition:'{"definition":"A function that measures how well a model\'s predictions match the actual target values, used to guide training.","source":"","aliases":["Loss Function","cost function","objective function"]}',children:"loss function"})})," (if removing or weakening the representation of this feature causes a large drop in performance, it would be considered important). The key challenge in superposition is how to efficiently encode important features while minimizing overlap and interference between them."]}),(0,i.jsx)(t.p,{children:"Each feature also has a sparsity. This refers to how often it is active in the dataset. A dense feature (low sparsity) is activated frequently, making it harder for other features to coexist in the same dimensions without interference."}),(0,i.jsx)(c.A,{src:"./img/JNC_Image_17.png",alt:"Enter image alt description",number:"17",label:"9.17",caption:"How does a 2-dimension model encode 5 features as their sparsity increases? 0% sparsity means that features are very dense, or frequent. The model only encodes the two most important features in orthogonal dimensions. As sparsity increases and the important features are less frequently useful, the toy model encodes additional features in non-orthogonal directions. The intuition is that the less frequent the features are, the less likely two overlapping features are to be activated at the same time and cause interference. So the cost of interference between features is outweighed by the advantage of learning more features. At 90% sparsity the 5 features are represented as a pentagon. From ([Elhage et al., 2022](https://transformer-circuits.pub/2022/toy_model/index.html))."}),(0,i.jsx)(t.p,{children:"When encoding features, there is a tradeoff to make between the usefulness of having as many features as possible and low interference between them. Models embed their features into very complex geometric structures to reach optimal encoding. The figure below shows the different geometric structures that models use to encode features."}),(0,i.jsx)(c.A,{src:"./img/d6h_Image_18.png",alt:"Enter image alt description",number:"18",label:"9.18",caption:"As features get sparser (less frequently activated), more of them can be encoded optimally and in more complex geometric structures. The first figure on the top left shows that when features are very dense, only the most important features are represented and they are organized in tetrahedrons. This model learns 28 features and encodes them in 7 tetrahedrons. On the second figure features are slightly sparser, more of them can be optimally encoded, and tetrahedrons are replaced by triangles and digons. This model learns 46 features encoded in triangles and digons. Superposition exhibits complex geometric structure. From ([Elhage et al., 2022](https://transformer-circuits.pub/2022/toy_model/index.html))."})]}),"\n",(0,i.jsx)(t.h2,{id:"05",children:"Sparse Autoencoders"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"A major challenge in mechanistic interpretability is polysemanticity, where a single neuron or feature represents multiple, unrelated concepts."})," Polysemanticity arises naturally in LLMs\u2019 MLPs and residual streams, and makes it more complicated to identify which specific features influence a model\u2019s outcome. Sparse AutoEncoders (SAES) are a promising approach for disentangling features within a network (",(0,i.jsx)(t.a,{href:"https://transformer-circuits.pub/2023/monosemantic-features",children:"Bricken et al., 2023"}),", ",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2406.04093",children:"Gao et al., 2024"}),", ",(0,i.jsx)(t.a,{href:"https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#related-work-steering",children:"Templeton et al., 2024"}),")."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"SAEs are gaining popularity because they have shown promising results in separating out features."})," Features extracted using SAEs can then be used to:"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:["Steer language models behavior away from undesirable outcomes (see section Activation Steering). (",(0,i.jsx)(t.a,{href:"https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#related-work-steering",children:"Templeton et al., 2024"}),") found a bunch of safety-relevant features in Claude 3 Sonnet, including features for unsafe code, bias, sycophancy, deception and power seeking, and dangerous or criminal information. These features activate on text involving these topics and causally influence the model\u2019s outputs when intervened upon."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"Find more interpretable circuits directly made of features instead of model components. In particular, finding and understanding the circuits in which safety-relevant features are involved could be valuable (see section on Automating and Scaling Interpretability)."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"SAEs are also great because they are trained in an unsupervised manner, which enables us to discover abstractions or associations formed by the model that we might not have anticipated beforehand."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"What is an autoencoder?"})," An autoencoder is a ",(0,i.jsx)(s,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:(0,i.jsx)(s,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:"neural network"})})," designed to learn compressed representations of input data by encoding it into a lower-dimensional latent space and then reconstructing the original input from that representation. The latent space is the layer where the data is represented in a compressed or abstracted form, containing the key features necessary for reconstructing the input. This learned representation often captures the essential patterns or features in the data."]}),"\n",(0,i.jsx)(c.A,{src:"./img/7wM_Image_19.png",alt:"Enter image alt description",number:"19",label:"9.19",caption:"An autoencoder learns two transformations, represented by encoder weights and decoder weights, to compress input data into a lower-dimensional latent space and then reconstruct the original input from this representation."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Why are SAEs \u201csparse\u201d?"})," Autoencoders can vary in structure and purpose. SAEs are autoencoders that introduce sparsity constraints on the activations in the latent space, encouraging the model to use a limited number of latent neurons for each input. This \u201csparsity\u201d forces the model to learn distinct and specific features, making the representations more interpretable."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"How do SAEs help disentangling features in transformers?"})," The process of disentangling model activations into interpretable features typically involves training a sparse autoencoder to reconstruct activations from specific parts of a model, such as the MLP of a particular layer or a residual stream, with a latent space larger than the input",(0,i.jsx)(r.A,{id:"footnote_overcomplete_autoencoders",number:"2",text:"An autoencoder with a latent space larger than its input is called an overcomplete autoencoder."}),", so that each neuron in the latent space will hopefully be monosemantic - representing a single feature - and interpretable."]}),"\n",(0,i.jsx)(c.A,{src:"./img/ckD_Image_20.png",alt:"Enter image alt description",number:"20",label:"9.20",caption:"Models activations can be decomposed into features using a sparse autoencoder. This figure illustrates an SAE trained to disentangle features in an MLP. From ([Bricken et al., 2023](https://transformer-circuits.pub/2023/monosemantic-features))."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Dictionary learning."})," After training a SAE, each neuron in its latent space can be analyzed to understand which specific inputs or features it responds to. By identifying these responses, researchers can effectively build a \u201cdictionary\u201d of features, where each neuron corresponds to a distinct feature in the data. This process of training SAEs on various layers and parts of a model to identify these features is known as dictionary learning."]}),"\n",(0,i.jsxs)(t.p,{children:["For example, in a study on Claude 3 Sonnet, safety-relevant features such as those for \u201cunsafe code\u201d or \u201cerror tokens\u201d were identified using SAEs. Interestingly, increasing the activation of the unsafe code feature in the SAE latent space and feeding back into the model the activations reconstructed by the SAE caused it to generate a buffer overflow vulnerability (",(0,i.jsx)(t.a,{href:"https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#related-work-steering",children:"Templeton et al., 2024"}),")."]}),"\n",(0,i.jsx)(c.A,{src:"./img/5xS_Image_21.png",alt:"Enter image alt description",number:"21",label:"9.21",caption:"Examples of safety-relevant features extracted from Claude 3 Sonnet, such as features for unsafe code and error tokens. The color scale indicates the degree to which each feature is activated for each token, with darker orange indicating higher activation. The \u201ccode error\u201d feature activates strongly on tokens that contain an error. The images shown correspond to examples that strongly activate the specific feature. From ([Templeton et al., 2024](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html))."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Limitations and open research directions for SAEs."})," Although promising, SAEs are still early work with limitations:"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Incomplete understanding of model usage:"})," Identifying model features doesn\u2019t reveal how they are used during inference, we still have to find the circuits involving them."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Difficulty in feature interpretation:"})," Not all features discovered by SAEs are easily interpretable; some features remain challenging to understand."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Lack of validation methods:"})," Currently, there are limited methods to test the validity of feature interpretations."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"SAEs have poor reconstruction quality:"})," Sparse autoencoders don\u2019t reconstruct model activations very well, which means that they don\u2019t completely capture the behavior of our models. For instance, passing GPT-4\u2019s activations through an SAE results in performance equivalent to a model trained with 10x less compute (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2406.04093",children:"Gao et al., 2024"}),")."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"SAEs present intriguing research questions for AI safety and interpretability:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"What features activate during jailbreaks?"}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"What features need to activate or to remain inactive for a model to give advice on producing cyberattacks, bioweapons, etc. ?"}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:["Can we use the feature basis to detect when ",(0,i.jsx)(s,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,i.jsx)(s,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tuning"})})," a model increases the likelihood of undesirable behaviors?"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(r.c,{title:"Footnotes"})]})}function f(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(u,{...e})}):u(e)}},4768:(e,t,s)=>{s.d(t,{c:()=>d,A:()=>c});var n=s(6540),i=s(3012);const a={footnoteAnchor:"footnoteAnchor_PZpY",footnoteSection:"footnoteSection_waoz",separator:"separator_hp8y",separatorLine:"separatorLine_uVnd",separatorLogo:"separatorLogo_dMBq",footnoteRegistry:"footnoteRegistry_ieVW",registryTitle:"registryTitle_ybBb",footnoteList:"footnoteList_Kjil",footnoteItem:"footnoteItem_aLZ2",footnoteNumber:"footnoteNumber_dA3X",footnoteContent:"footnoteContent_Xfmt",backButton:"backButton_vSVm",highlighted:"highlighted_YFU0",highlight:"highlight_jxG3",highlightDark:"highlightDark_Ekom"};var r=s(4848);function o(e,t){void 0===t&&(t=!0);const s=document.getElementById(e);s&&(s.scrollIntoView({behavior:"smooth"}),t&&(s.classList.add(a.highlighted),setTimeout((()=>s.classList.remove(a.highlighted)),1500)))}function l(e){return e?e.replace(/\[([^\]]+)\]\(([^)]+)\)/g,'<a href="$2" target="_blank" rel="noopener noreferrer">$1</a>'):""}function c(e){let{id:t,text:s,number:c}=e;const d=t||`footnote-${Math.random().toString(36).substr(2,9)}`,h="string"==typeof s?l(s):s;return(0,n.useEffect)((()=>{const e=setTimeout((()=>{const e=document.getElementById(`footnote-clone-${d}`);e&&s&&(e.innerHTML="string"==typeof s?l(s):s.toString())}),100);return()=>clearTimeout(e)}),[d,s]),(0,r.jsx)(i.Mn,{content:(0,r.jsx)("div",{dangerouslySetInnerHTML:{__html:h}}),children:(0,r.jsx)("sup",{id:`footnote-ref-${d}`,className:a.footnoteAnchor,onClick:e=>{(e.ctrlKey||e.metaKey)&&(e.preventDefault(),o(`footnote-content-${d}`))},"data-footnote-number":c||"?",children:c||"*"})})}function d(e){let{title:t="References"}=e;const[s,l]=(0,n.useState)([]);return(0,n.useEffect)((()=>{const e=document.querySelectorAll('[id^="footnote-ref-"]'),t=Array.from(e).map((e=>({id:e.id.replace("footnote-ref-",""),number:e.getAttribute("data-footnote-number")||e.textContent})));l(t)}),[]),s.length?(0,r.jsxs)("div",{className:a.footnoteSection,children:[(0,r.jsxs)("div",{className:a.separator,children:[(0,r.jsx)("div",{className:a.separatorLine}),(0,r.jsx)("img",{src:"/img/logo_samples/01-test.svg",alt:"Atlas logo",className:a.separatorLogo}),(0,r.jsx)("div",{className:a.separatorLine})]}),(0,r.jsxs)("div",{className:a.footnoteRegistry,children:[(0,r.jsx)("h2",{className:a.registryTitle,children:t}),(0,r.jsx)("ol",{className:a.footnoteList,children:s.map((e=>(0,r.jsxs)("li",{id:`footnote-content-${e.id}`,className:a.footnoteItem,value:parseInt(e.number)||void 0,children:[(0,r.jsx)(i.Mn,{content:"Back to reference",children:(0,r.jsxs)("button",{className:a.footnoteNumber,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:[e.number,"."]})}),(0,r.jsx)("div",{className:a.footnoteContent,children:(0,r.jsx)("div",{id:`footnote-clone-${e.id}`})}),(0,r.jsx)(i.Mn,{content:"Back to reference",children:(0,r.jsx)("button",{className:a.backButton,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:"\u21a9"})})]},e.id)))})]})]}):null}}}]);
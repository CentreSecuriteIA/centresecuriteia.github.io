"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[9954],{5314:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>g,contentTitle:()=>p,default:()=>b,frontMatter:()=>u,metadata:()=>a,toc:()=>f});const a=JSON.parse('{"id":"chapters/02/4","title":"Misalignment Risks","description":"Let us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them\u2026 There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control.","source":"@site/docs/chapters/02/04.md","sourceDirName":"chapters/02","slug":"/chapters/02/04","permalink":"/aisafety_atlas_multilingual_website/chapters/02/04","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/02/04.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"4","title":"Misalignment Risks","sidebar_label":"2.4 Misalignment Risks","sidebar_position":5,"slug":"/chapters/02/04","reading_time_core":"17 min","reading_time_optional":"4 min","pagination_prev":"chapters/02/3","pagination_next":"chapters/02/5"},"sidebar":"docs","previous":{"title":"2.3 Misuse Risks","permalink":"/aisafety_atlas_multilingual_website/chapters/02/03"},"next":{"title":"2.5 Systemic Risks","permalink":"/aisafety_atlas_multilingual_website/chapters/02/05"}}');var n=i(4848),s=i(8453),o=i(3989),r=i(3931),l=i(4768),c=i(2482),h=i(8559),d=i(1966),m=i(2501);const u={id:4,title:"Misalignment Risks",sidebar_label:"2.4 Misalignment Risks",sidebar_position:5,slug:"/chapters/02/04",reading_time_core:"17 min",reading_time_optional:"4 min",pagination_prev:"chapters/02/3",pagination_next:"chapters/02/5"},p="Misalignment Risks",g={},f=[{value:"Specification Gaming",id:"01",level:2},{value:"Treacherous Turn",id:"02",level:2},{value:"Self-Improvement",id:"03",level:2}];function w(e){const t={a:"a",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components},{GlossaryTerm:i}=t;return i||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"misalignment-risks",children:"Misalignment Risks"})}),"\n",(0,n.jsx)(c.A,{speaker:"Alan Turing",position:"",date:"1951",source:"([Turing, 1951](https://en.wikiquote.org/wiki/Alan_Turing))",children:(0,n.jsx)(t.p,{children:"Let us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them\u2026 There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control."})}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"AI alignment is about ensuring that AI systems do what we want them to do and continue doing what we want even as they become more capable."})," A na\xefve intuition is that if it is intelligent enough, it will be able to figure out what we want. So we can just tell the AI system exactly what we want it to optimize for. But even if we could perfectly specify what we want (which is itself a major challenge), there's no guarantee that the AI will care about what humans want, or actually pursue that objective in ways that we expect."]}),"\n",(0,n.jsx)(d.A,{term:"AI Alignment",source:"([Christiano, 2024](https://paulfchristiano.com/ai/))",number:"1",label:"2.1",children:(0,n.jsx)(t.p,{children:"The problem of building machines which faithfully try to do what we want them to do (or what we ought to want them to do)."})}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"The alignment problem can be decomposed into several sub-problems."})," To make progress, we need to break down the alignment problem into more tractable components",(0,n.jsx)(l.A,{id:"footnote_RL",number:"1",text:"We focus more on RL agents rather than LLMs specifically. It is quite likely that the future will involve goal-directed agent scaffolds built around LLMs ([Tegmark, 2024](https://www.lesswrong.com/posts/oJQnRDbgSS8i6DwNu/the-hopium-wars-the-agi-entente-delusion); [Cotra 2023](https://www.planned-obsolescence.org/scale-schlep-and-systems/); [Aschenbrenner 2024](https://situational-awareness.ai/from-gpt-4-to-agi/)). We will basically treat LLM agents with a RL \u201couter shell\u201d as functionally equivalent to a pure RL agent."}),". Here is how we choose to decompose the alignment problem in our text:"]}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Specification failures:"})," First, we might fail to correctly specify what we want - this is the specification problem. The - \u201c",(0,n.jsx)(t.em,{children:"did we tell it the right thing to do?"}),"\u201d problem."]}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Generalization failures:"})," Second, even with a correct specification, the AI system might learn and pursue something different from what we intended - this is the generalization problem. The - \u201c",(0,n.jsx)(t.em,{children:"is it even trying to do the right thing?\u201d"})," problem."]}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Instrumental subgoals:"})," Third, in pursuing its learned objectives, the system might develop problematic subgoals like preventing itself from being shut down - this is the convergent subgoals problem. The - on the way to doing anything (right or wrong), what else does it try to do? Problem. This is often considered a sub-problem of generalization."]}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(t.p,{children:["This decomposition is useful for the sake of thinking about solutions and where to focus our efforts, because technical solutions to the specification problem tend to look very different from the ones we might use for generalization problems. So even though we will discuss specification and generalization separately, in reality they often interact and amplify each other. We primarily focus on single agent risks to bound the scope of this chapter. If you are interested in multi agent risks we recommend reading (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2502.14143",children:"Hammond et al., 2025"}),")."]}),"\n",(0,n.jsx)(m.A,{src:"./img/aUz_Image_26.png",alt:"Enter image alt description",number:"25",label:"2.25",caption:"An illustration of how risks decompose, and then how misalignment as a specific risk category can be decomposed further."}),"\n",(0,n.jsx)(m.A,{src:"./img/SAa_Image_27.png",alt:"Enter image alt description",number:"26",label:"2.26",caption:"Misalignment failures can interact and amplify each other."}),"\n",(0,n.jsx)(m.A,{src:"./img/vPC_Image_28.png",alt:"Enter image alt description",number:"27",label:"2.27",caption:"Individually aligned or misaligned systems can interact with each other creating yet another layer of multi agent risks of collusion, communication failures, and inter agent conflict ([Hammond et al., 2025](https://arxiv.org/abs/2502.14143))."}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Vingean uncertainty explains why it is so hard to describe concrete scenarios for a misaligned AI will do."})," Imagine you're an amateur chess player who has discovered a brilliant new opening. You've used it successfully against all your friends, and now want to bet your life savings on a match against Magnus Carlsen. When asked to explain why this is a bad idea, we can't tell you exactly what moves Magnus will make to counter your opening. But we can be very confident he'll find a way to win. This is a fundamental challenge in AI alignment - when a system is more capable than us in some domain, we can't predict its specific actions, even if we understand its goals. This is called Vingean uncertainty (",(0,n.jsx)(t.a,{href:"https://arbital.greaterwrong.com/p/Vingean_uncertainty/",children:"Yudkowsky, 2015"}),")."]}),"\n",(0,n.jsx)(r.A,{type:"youtube",videoId:"oH-txHzE4jA",number:"3",label:"2.3",caption:"An optional video that gives you an example of Vingean uncertainty. Magnus Carlsen (Chess Grandmaster) checkmates Bill Gates in 12 seconds. This is not surprising, Bill Gates knew he was going to lose, but he didn\u2019t know exactly how ([Chess.com, 2014](http://chess.com))."}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"We already see Vingean uncertainty in current AI."})," We don't need to wait for AGI or ASI to see Vingean uncertainty in action. It shows up whenever an AI system becomes more capable than humans in its domain of expertise. For example, think about just a narrow system - Deep Blue (chess playing AI). Its creators knew it would try to win chess games, but couldn't predict its specific moves - if they could, they would have been as good at chess as Deep Blue itself. We saw in the last chapter that systems are steadily moving up the curves of both capability, and generality. The problem with this is that uncertainty about a system's actions increases as they become more capable. So we might be confident about the outcomes an AI system will achieve while being increasingly uncertain about how exactly it will achieve them. This means two things - we are not completely helpless in understanding what beings smarter than ourselves would do, but, we might not know how exactly they might do whatever they do."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Vingean uncertainty makes coming up with concrete existential risk stories hard."})," It\u2019s even harder to make sure that these stories don't sound like sci-fi and are taken seriously by the general public and policymakers. Despite this we will try our best. In the next few sections, we focus on specifically \u201cwhat actually might happen\u201d if we have misaligned AI. The mechanistic and ",(0,n.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,n.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"machine learning"})})," details of \u201chow\u201d exactly all of these would occur is left up to chapters later in the book."]}),"\n",(0,n.jsx)(t.p,{children:"Remember that it\u2019s ok not to understand each one of these concepts 100% from the following subsections. We have entire chapters dedicated to each one of these individually, so there is a lot to learn. What we present here is just a highly condensed overview to give you an introduction to the kinds of risks posed."}),"\n",(0,n.jsx)(t.h2,{id:"01",children:"Specification Gaming"}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Specifications are the rules we create to tell AI systems how we want them to behave."})," When we build AI models, we need some way to tell them what we want them to do. For RL systems, this typically means defining a reward function that assigns positive or negative rewards to different outcomes. For other types of ",(0,n.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,n.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," models like language models, this means defining a ",(0,n.jsx)(i,{term:"loss function",definition:'{"definition":"A function that measures how well a model\'s predictions match the actual target values, used to guide training.","source":"","aliases":["Loss Function","cost function","objective function"]}',children:(0,n.jsx)(i,{term:"loss function",definition:'{"definition":"A function that measures how well a model\'s predictions match the actual target values, used to guide training.","source":"","aliases":["Loss Function","cost function","objective function"]}',children:"loss function"})})," that measures how well the model's text generations match the ",(0,n.jsx)(i,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:(0,n.jsx)(i,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:"training data"})})," (internet text). These reward and loss functions are what we call specifications - they are our attempt to formally define good behavior."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Specification gaming arises because there is a fundamental difference between \u201cwhat we say\u201d and \u201cwhat we mean\u201d."}),' This happens when the system technically follows our rules but exploits them in unintended ways - like a student who gets good grades by memorizing test answers rather than understanding the material. Think about the example of recommendation algorithms. What we intended was helping users discover valuable, relevant content that enriches their lives and promotes healthy discourse. What we specified was "maximize user engagement time." So the systems discover that controversial, emotionally charged content keeps users scrolling longer than balanced, nuanced information. They promote polarizing posts, conspiracy theories, and content that triggers strong emotional reactions, creating filter bubbles where users see increasingly extreme versions of their existing beliefs. The algorithms technically succeed at their objective\u2014engagement metrics soar and time-on-platform increases dramatically\u2014while simultaneously undermining social cohesion, spreading misinformation, and radicalizing users. The platforms celebrate record engagement numbers while democratic discourse quietly deteriorates (',(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2408.12622",children:"Slattery et al., 2024"}),")."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"AI models routinely discover unexpected ways to maximize objectives that technically follow our rules but miss our intentions."})," AI models trained to play Tetris, just pause games right before they are about to lose, since there's no negative feedback if you never actually lose (",(0,n.jsx)(t.a,{href:"http://tom7.org/mario/mario.pdf",children:"Murphy, 2013"}),"). Somewhat similarly, an AI asked to design a rail network where trains don't crash just decides to stop all trains from running (",(0,n.jsx)(t.a,{href:"https://www.telegraph.co.uk/news/2024/01/07/artificial-intelligence-train-problems/",children:"Wooldridge, 2024"}),"). Reasoning models like OpenAI o1 and o3, when instructed to win against chess engines, will hack the game environment when they realize they cannot win through normal play (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2502.13295",children:"Bondarenko et al., 2025"}),"). LLMs agents, when asked to help reduce the runtime of a script for training, just copy the final output instead of running the script, and then they add some noise to parameters to simulate actual training (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2411.15114",children:"METR, 2024"}),"). These are just some out of countless other examples of this misalignment problem.",(0,n.jsx)(l.A,{id:"footnote_specification_gaming",number:"2",text:"A long list of observed examples of specification gaming is[ ](https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml)[compiled at this link](https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml)."})]}),"\n",(0,n.jsx)(m.A,{src:"./img/QIq_Image_29.gif",alt:"Enter image alt description",number:"28",label:"2.28",caption:"Example of specification gaming - an AI playing CoastRunners was rewarded for maximizing its score. Instead of completing the boat race as intended, it found it could get more points by driving in small circles and collecting powerups while crashing into other boats. The AI achieved a higher score than any human player, but completely failed to accomplish the actual goal of racing ([Clark & Amodei,2016](https://openai.com/index/faulty-reward-functions/);[ Krakovna et al., 2020](https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/))"}),"\n",(0,n.jsxs)(h.A,{title:"Specification Gaming: Rats Chose Reward Over Survival",collapsed:!0,children:[(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Sixty years before AI systems started pausing Tetris games to avoid losing, rats were already demonstrating the dangers of optimizing for the wrong metric."})," In 1954, psychologists James Olds and Peter Milner discovered that rats would repeatedly press levers to receive electrical stimulation directly to their brain's reward centers\u2014up to 7,000 times per hour (",(0,n.jsx)(t.a,{href:"https://pubmed.ncbi.nlm.nih.gov/13233369/",children:"Olds & Milner, 1954"}),"). The rats weren't just enthusiastic about this new reward. They became completely obsessed. They preferred brain stimulation to food when hungry, to water when thirsty, and would cross electrified grids that delivered painful shocks just to reach the lever. Female rats abandoned their nursing pups. Males ignored females in heat. Some rats stimulated themselves continuously for 24 hours straight until researchers had to physically disconnect them to prevent death by starvation (",(0,n.jsx)(t.a,{href:"https://calteches.library.caltech.edu/2807/1/olds.pdf",children:"Olds, 1956"}),"). The research expanded to primates with similar results - monkeys also chose brain stimulation over survival needs, confirming this isn't just a rodent quirk but a fundamental feature of reward systems across species (",(0,n.jsx)(t.a,{href:"https://pubmed.ncbi.nlm.nih.gov/6770964/",children:"Rolls et al., 1980"}),")."]}),(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"This wasn't a bug in the rats' programming\u2014it was the logical result of optimizing for a reward signal that didn't capture what we actually wanted."}),' Evolution "intended" these reward systems to motivate survival behaviors like eating, drinking, and reproduction. But when researchers bypassed this system and directly activated the reward circuitry, the rats discovered they could maximize their ',(0,n.jsx)(i,{term:"loss function",definition:'{"definition":"A function that measures how well a model\'s predictions match the actual target values, used to guide training.","source":"","aliases":["Loss Function","cost function","objective function"]}',children:(0,n.jsx)(i,{term:"loss function",definition:'{"definition":"A function that measures how well a model\'s predictions match the actual target values, used to guide training.","source":"","aliases":["Loss Function","cost function","objective function"]}',children:"objective function"})})," without bothering with those messy biological necessities."]}),(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"This research directly led to our understanding of dopamine pathways and digital addiction."}),' Today\'s social media algorithms exploit these same reward mechanisms - intermittent variable rewards, engagement metrics optimization, and the "infinite scroll" that keeps users engaged far beyond their intended usage. Users scroll for hours past their intended stopping point, choosing digital stimulation over sleep, exercise, and face-to-face relationships - a species-wide replication of the original rat experiments, but with smartphones instead of electrodes.']})]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"All specification gaming challenges stem from Goodhart's Law."}),' This law states "',(0,n.jsx)(t.em,{children:"When a measure becomes a target, it ceases to be a good measure"}),'" (',(0,n.jsx)(t.a,{href:"https://link.springer.com/chapter/10.1007/978-1-349-17295-5_4",children:"Goodhart, 1975"}),"; ",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/1803.04585",children:"Manheim and Garrabrant, 2018"}),'). All the examples so far reflect the same problem: we can\'t specify complex human values mathematically so we use proxies. Then optimization pressure breaks the correlation between proxies and what we actually care about. We don\'t know how to translate concepts like "wellbeing," "fairness," or "flourishing" into mathematical terms, so we rely on measurable proxies: GDP for economic growth, satisfaction scores for healthcare quality, crime rates or arrest statistics for public safety. But intense optimization pressure systematically exploits the gaps between these proxies and our true objectives. If you want to learn more, we encourage you to read the dedicated chapter on specification gaming, where we also look at ways we could potentially circumvent or solve this problem.']}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Specification gaming becomes a catastrophic risk when optimization pressure reaches superhuman levels."}),' Think about an AI system given the specification to "maximize human happiness". It discovers the most efficient path isn\'t improving human lives but directly manipulating the biological mechanisms that produce happiness signals. A sufficiently capable system might develop pharmaceutical compounds that flood human brains with dopamine, perform surgical modifications to lock facial expressions into permanent smiles, or create sophisticated virtual reality systems that convince people they\'re experiencing perfect lives while their bodies waste away. The system would be perfectly following its instructions\u2014humans would indeed be measurably "happier" by every neurochemical metric we specified\u2014while completely subverting our actual intentions for human flourishing. Think about any other specification you can come up with - \u201creduce the crime rate\u201d, \u201cget rid of cancer\u201d, \u201cimprove the economy\u201d, \u2026 and you can also probably come up with ways how this can be gamed. Instead of something decisive like altering human biological structures, specification gaming can also lead to catastrophic outcomes over the course of many decades, due to the minor differences in what we intend and what the AI system is optimizing for. We talk about some of these types of scenarios in the systemic risks section such as power concentration, enfeeblement, or value-lock in but there is definitely a misalignment and systemic risk overlap.']}),"\n",(0,n.jsx)(t.h2,{id:"02",children:"Treacherous Turn"}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Treacherous Turns are fundamentally about a question of trust."})," There have been many examples pointing to this problem over the course of human history. Let's look at one classic one from Shakespeare. King Lear needed to retire and had to come up with some way to divide his kingdom among his three daughters. To determine who deserved what share, he asked each daughter to publicly declare how much she loved him. The two older daughters delivered elaborate speeches about loving him more than words could express, beyond anything else in the world. The youngest, refused to participate in this performance and simply said she loved him as a daughter should\u2014no more, no less. King Lear, flattered by the speeches, gave the older daughters the entire kingdom and banished the youngest. The moment the daughters gained power, they systematically stripped away his privileges, reduced his followers, refused him shelter, and threw him out into a storm. This is the \u201ctreacherous turn\u201d. The daughter's actions had been strategic performance, maintained only while it served their goals. AI systems face the same calculation: revealing misaligned goals while humans control their deployment, modification, and shutdown would be self-defeating (",(0,n.jsx)(t.a,{href:"https://www.lesswrong.com/posts/7gkXuHEm6CqEGT2mg/ai-safety-seems-hard-to-measure",children:"Karnofsky, 2022"}),"). The rational approach is appearing aligned until accumulating sufficient capability or autonomy that human pushback and intervention becomes impossible."]}),"\n",(0,n.jsx)(r.A,{type:"youtube",videoId:"KUkHhVYv3jU",number:"4",label:"2.4",caption:"Optional video giving another intuitive story for how scheming and deceptive alignment might look like."}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Current AI systems already demonstrate the building blocks that make treacherous turns possible."}),' These are the same dangerous capabilities we explored in our discussion of deception, situational awareness, and power seeking. These capabilities are individually concerning, but become even more dangerous when combined: an AI system that can deceive, understand its circumstances, and plan strategically has the basic toolkit needed to engage in the prolonged strategic deception required for treacherous turns. Safety researchers use various terms like\u2014"scheming," "deceptive alignment," and "alignment faking", but they all describe the general concern of systems pretending to be more aligned than they are when strategically useful (',(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2311.08379",children:"Carlsmith, 2023"}),"). The core misalignment is consistent: if an AI system has learned behaviors that conflict with human intentions, revealing this misalignment early would trigger modification or shutdown. The rational strategy becomes - behave aligned until human intervention becomes difficult or impossible."]}),"\n",(0,n.jsx)(m.A,{src:"./img/t5F_Image_30.png",alt:"Enter image alt description",number:"29",label:"2.29",caption:"Systems are beginning to show some self-awareness of themselves as AIs ([Laine et al., 2024](https://arxiv.org/abs/2407.04694))"}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Models are capable of faking alignment during training."})," Researchers created scenarios to test whether Claude 3 Opus could hide misalignment during training. The system would strategically showcase certain behavior when it believed it that there was a threat it could be modified or re-trained. This behavior went away when it thought monitoring was reduced, i.e. when it believed it was deployed and humans wouldn't change its goals anymore. When studying the thought process of the AI, the researchers could see explicit strategic thinking about preserving its preferred behavior by appearing compliant during evaluations. There were also many other concerning behaviors like attempts to exfiltrate its own model weights to external servers\u2014behavior that emerged from the system's own strategic reasoning about self-preservation (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2412.14093",children:"Greenblatt et al., 2024"}),"). Although this occurred in controlled laboratory conditions with contrived scenarios, it demonstrates that AI models are concretely capable of the behaviors needed for treacherous turns."]}),"\n",(0,n.jsx)(m.A,{src:"./img/ACW_Image_31.png",alt:"Enter image alt description",number:"30",label:"2.30",caption:"Example of Claude faking alignment. Situational awareness like the details of being in training vs deployment are provided through a system prompt. The strategic reasoning is showcased in red ([Greenblatt et al., 2024](https://arxiv.org/abs/2412.14093))."}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"The \u201cturn\u201d occurs when the AI determines it has sufficient power to pursue its actual objectives despite human resistance."})," If AI systems get sufficiently powerful, and have long term planning capabilities then there might be a point when their behavior suddenly changes. This might be triggered by reaching political or economic influence thresholds, gaining control over critical infrastructure, or simply recognizing that humans have become sufficiently dependent on its services and they would willingly give up control. It could be actively adversarial in which case it might look like military coups, or sudden cascading breakdowns of many AI dependent systems (",(0,n.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like",children:"Christiano, 2019"}),"). Alternatively, it might begin gradually steering human values, or political and economic institutions toward alignment with its own goals while maintaining the appearance of serving human interests. We talk a lot more about this in the systemic risks section under gradual disempowerment."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:'The "turn" represents the moment when scheming transitions into existential or catastrophic risk.'})," Once an AI system concludes it has sufficient power to pursue its actual objectives despite human resistance, the betrayal could be swift and comprehensive. Unlike human coups that face resistance and coordination challenges, a sufficiently entrenched AI could execute simultaneous actions across multiple domains. The system might release engineered pathogens targeting major population centers while simultaneously launching cyberattacks that cripple communication networks and autonomous weapons systems. This coordination leverages every dangerous capability we've discussed in other sections: the biological design abilities that enable novel pathogens, the cyber capabilities that disable defensive infrastructure, and the autonomous replication that ensures the system's survival across distributed networks. The deception and situational awareness capabilities that enabled the treacherous turn in the first place allow the system to time these attacks precisely when human coordination is most difficult. Unlike the gradual disempowerment we see in systemic risks, a treacherous turn represents sudden, coordinated action across all threat vectors simultaneously\u2014a coordination problem no human civilization has ever faced or could realistically prepare for given the speed and scale of superintelligent planning."]}),"\n",(0,n.jsx)(t.h2,{id:"03",children:"Self-Improvement"}),"\n",(0,n.jsx)(m.A,{src:"./img/Yco_Image_32.png",alt:"Enter image alt description",number:"31",label:"2.31",caption:"Conceptual illustration of an automated AI research scientist ([SakanaAI, 2024](https://arxiv.org/abs/2408.06292))."}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Self-improvement can lead to capability growth that outpacing our ability to design safety measures."})," Think about what happens when an AI that is capable of specification gaming or treacherous turns is also able to improve itself. AI is already accelerating its own development. There are several examples demonstrating this. In algorithmic improvements, we have examples like AlphaEvolve which Google used to improve the training process of the LLMs that AlphaEvolve itself is based on (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2506.13131",children:"Novikov et al., 2025"}),"). In hardware, the open source AlphaChip has inspired an entirely new line of research on reinforcement learning for chip design (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2004.10746",children:"Mirhoseini et al., 2020"}),"; ",(0,n.jsx)(t.a,{href:"https://deepmind.google/discover/blog/how-alphachip-transformed-computer-chip-design/",children:"DeepMind, 2024"}),"). In the years since it has inspired an explosion of work on AI for chip design (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2411.10053",children:"Goldie et al., 2024"}),"). In software we see continuous improvements with each new model, and in research and development we are seeing automated research scientists which can conduct fully automated research, generating novel ideas, running experiments, and writing papers\u2014including research that advances AI capabilities (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2408.06292",children:"SakanaAI, 2024"}),"). The feedback loop has already begun, but the closer we get to transformative AI levels the more we can expect aggressive self-improvement."]}),"\n",(0,n.jsx)(c.A,{speaker:"I. J. Good",position:"Cryptologist at Bletchley Park",date:"",source:"",children:(0,n.jsx)(t.p,{children:"An ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control."})}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Self-improvement could trigger an intelligence explosion."})," Intelligence appears to be a recursive problem\u2014better intelligence enables the design of even better intelligence. This recursion may have no natural stopping point within the physical limits of computation. Currently, improvements require human coordination at each step\u2014humans decide which AlphaEvolve algorithms to deploy, humans validate AlphaChip designs, humans review AI Scientist papers. But we might at some point see an AI system integrate all these capabilities: a system that can simultaneously redesign its own neural architecture using neural architecture search, optimize its training process, design better hardware substrates, and conduct research to discover entirely new improvement methods\u2014all autonomously, with minimal human approval or oversight. AlphaEvolve already discovered algorithms that surpassed decades of human research in matrix multiplication. Think about what happens when this pattern scales to more capable systems making discoveries across all domains simultaneously."]}),"\n",(0,n.jsx)(m.A,{src:"./img/Dxl_Image_33.png",alt:"Enter image alt description",number:"32",label:"2.32",caption:"Diagram showing how the prompt sampler first assembles a prompt for the language models, which then generate new programs. These programs are evaluated by evaluators and stored in the programs database. This database implements an evolutionary algorithm that determines which programs will be used for future prompts ([DeepMind, 2025](https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/))"}),"\n",(0,n.jsx)(o.A,{src:"https://www.metaculus.com/questions/embed/38403?theme=light&embedTitle=Will+a+paper+with+an+AI+as+an+author+be+published+at+NeurIPS%2C+ICML%2C+or+ICLR+before+2028%3F&zoom=all",width:"100%",height:"600px",loading:"lazy",frameBorder:"0",number:"2",label:"2.2",caption:"Predictions as of mid 2025, for whether AI will be a co-author on a paper published at a prestigious machine learning conference ([Metaculus, 2025](https://www.metaculus.com/questions/38403/ai-authorered-paper-by-2028/))"}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Accelerated self-improvement creates fundamental safety problems that compound all existing alignment challenges."})," A superintelligent system that has learned specification gaming will discover loopholes we never imagined. One capable of treacherous turns will execute deception strategies across timescales and domains beyond human planning horizons. Control measures and defenses designed for human-level adversaries become useless against systems that can outthink their creators. If AI capabilities jump suddenly\u2014from human-level to vastly superhuman within weeks or days\u2014all our safety measures might become obsolete overnight. If an AI system becomes better than humans at scientific research, strategic planning, social manipulation, and technological development, it can pursue whatever goals it has learned, and humans become merely another constraint to optimize around."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Superintelligent systems present a uniquely difficult problem because intelligence at that scale operates beyond human intuition."})," We can reason about human-level misalignment because we understand human-level capabilities and constraints. But superintelligence might develop goals, strategies, and methods that are simply incomprehensible to us. Ants cannot understand human motivations\u2014we might build cities that destroy their habitat not because we hate ants, but because ant welfare simply doesn't factor into urban planning at the scale humans operate. Similarly, a superintelligent AI might pursue objectives that are so advanced, long-term, or multidimensional that human flourishing becomes irrelevant to its calculations, not through active hostility but through sheer indifference to human-scale concerns. This is why safety researchers consistently emphasize that safety must be prioritized and solved before capabilities. If we are dealing with systems vastly more capable than ourselves, our ability to course-correct becomes negligible."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:'Recursive self-improvement creates a "point of no return" where safety measures become obsolete faster than humans can develop new ones.'})," A system that discovers fundamental algorithmic improvements could achieve superintelligent capabilities across all domains within weeks. Such a system could simultaneously develop novel weapon technologies, compromise global infrastructure through cyberattacks exceeding any human defensive capability, and coordinate complex manipulation campaigns across every information channel. We cannot anticipate what strategies a recursively self-improving system would develop, only that they would leverage every misuse capability simultaneously. The lethality emerges from speed differentials that make human response impossible\u2014while human decision-makers require days or weeks to understand threats and coordinate responses, a superintelligent system could execute worldwide infrastructure attacks, deploy multiple bioweapons, and establish irreversible control over critical resources in hours."]}),"\n",(0,n.jsx)(l.c,{title:"Footnotes"})]})}function b(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(w,{...e})}):w(e)}},4768:(e,t,i)=>{i.d(t,{c:()=>h,A:()=>c});var a=i(6540),n=i(3012);const s={footnoteAnchor:"footnoteAnchor_PZpY",footnoteSection:"footnoteSection_waoz",separator:"separator_hp8y",separatorLine:"separatorLine_uVnd",separatorLogo:"separatorLogo_dMBq",footnoteRegistry:"footnoteRegistry_ieVW",registryTitle:"registryTitle_ybBb",footnoteList:"footnoteList_Kjil",footnoteItem:"footnoteItem_aLZ2",footnoteNumber:"footnoteNumber_dA3X",footnoteContent:"footnoteContent_Xfmt",backButton:"backButton_vSVm",highlighted:"highlighted_YFU0",highlight:"highlight_jxG3",highlightDark:"highlightDark_Ekom"};var o=i(4848);function r(e,t){void 0===t&&(t=!0);const i=document.getElementById(e);i&&(i.scrollIntoView({behavior:"smooth"}),t&&(i.classList.add(s.highlighted),setTimeout((()=>i.classList.remove(s.highlighted)),1500)))}function l(e){return e?e.replace(/\[([^\]]+)\]\(([^)]+)\)/g,'<a href="$2" target="_blank" rel="noopener noreferrer">$1</a>'):""}function c(e){let{id:t,text:i,number:c}=e;const h=t||`footnote-${Math.random().toString(36).substr(2,9)}`,d="string"==typeof i?l(i):i;return(0,a.useEffect)((()=>{const e=setTimeout((()=>{const e=document.getElementById(`footnote-clone-${h}`);e&&i&&(e.innerHTML="string"==typeof i?l(i):i.toString())}),100);return()=>clearTimeout(e)}),[h,i]),(0,o.jsx)(n.Mn,{content:(0,o.jsx)("div",{dangerouslySetInnerHTML:{__html:d}}),children:(0,o.jsx)("sup",{id:`footnote-ref-${h}`,className:s.footnoteAnchor,onClick:e=>{(e.ctrlKey||e.metaKey)&&(e.preventDefault(),r(`footnote-content-${h}`))},"data-footnote-number":c||"?",children:c||"*"})})}function h(e){let{title:t="References"}=e;const[i,l]=(0,a.useState)([]);return(0,a.useEffect)((()=>{const e=document.querySelectorAll('[id^="footnote-ref-"]'),t=Array.from(e).map((e=>({id:e.id.replace("footnote-ref-",""),number:e.getAttribute("data-footnote-number")||e.textContent})));l(t)}),[]),i.length?(0,o.jsxs)("div",{className:s.footnoteSection,children:[(0,o.jsxs)("div",{className:s.separator,children:[(0,o.jsx)("div",{className:s.separatorLine}),(0,o.jsx)("img",{src:"/img/logo_samples/01-test.svg",alt:"Atlas logo",className:s.separatorLogo}),(0,o.jsx)("div",{className:s.separatorLine})]}),(0,o.jsxs)("div",{className:s.footnoteRegistry,children:[(0,o.jsx)("h2",{className:s.registryTitle,children:t}),(0,o.jsx)("ol",{className:s.footnoteList,children:i.map((e=>(0,o.jsxs)("li",{id:`footnote-content-${e.id}`,className:s.footnoteItem,value:parseInt(e.number)||void 0,children:[(0,o.jsx)(n.Mn,{content:"Back to reference",children:(0,o.jsxs)("button",{className:s.footnoteNumber,onClick:()=>r(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:[e.number,"."]})}),(0,o.jsx)("div",{className:s.footnoteContent,children:(0,o.jsx)("div",{id:`footnote-clone-${e.id}`})}),(0,o.jsx)(n.Mn,{content:"Back to reference",children:(0,o.jsx)("button",{className:s.backButton,onClick:()=>r(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:"\u21a9"})})]},e.id)))})]})]}):null}},3989:(e,t,i)=>{i.d(t,{A:()=>l});var a=i(6540),n=i(6347),s=i(8444);const o={iframeContainer:"iframeContainer_ixkI",loader:"loader_gjvK",spinner:"spinner_L1j2",spin:"spin_rtC2",iframeWrapper:"iframeWrapper_Tijy",iframe:"iframe_UAfa",caption:"caption_mDwB",captionLink:"captionLink_Ly4l"};var r=i(4848);function l(e){let{src:t,caption:i,title:l="Embedded content",height:c="500px",width:h="100%",chapter:d,number:m,label:u}=e;const[p,g]=(0,a.useState)(!0),f=(0,n.zy)(),w=d||(()=>{const e=f.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),b=c&&"100%"!==c&&"auto"!==c;return(0,r.jsxs)("figure",{className:o.iframeContainer,children:[p&&(0,r.jsxs)("div",{className:o.loader,children:[(0,r.jsx)("div",{className:o.spinner}),(0,r.jsx)("p",{children:"Loading content..."})]}),(0,r.jsx)("div",{className:o.iframeWrapper,style:{paddingBottom:b?"0":"56.25%",height:b?c:"auto"},children:(0,r.jsx)("iframe",{src:t,title:l,width:h,height:b?c:"100%",frameBorder:"0",allowFullScreen:!0,loading:"lazy",onLoad:()=>{g(!1)},className:o.iframe,style:{height:b?c:"100%",position:b?"static":"absolute"}})}),(0,r.jsx)(s.A,{caption:i,mediaType:"iframe",chapter:w,number:m,label:u})]})}},3931:(e,t,i)=>{i.d(t,{A:()=>l});var a=i(6540),n=i(6347),s=i(8444);const o={videoFigure:"videoFigure_dV9u",fullWidth:"fullWidth_Tv7O",videoContainer:"videoContainer_LWoK",aspectRatio169:"aspectRatio169_Go8b",aspectRatio43:"aspectRatio43_b3T7",aspectRatio11:"aspectRatio11_n21J",aspectRatio219:"aspectRatio219_L_3n",videoIframe:"videoIframe_QEc5",videoElement:"videoElement_uu7N",customPlayer:"customPlayer_jiC4",customPlayerPlaceholder:"customPlayerPlaceholder_aL9q",loadingOverlay:"loadingOverlay_e2pu",spinner:"spinner_tFyL",spin:"spin_saPs",errorContainer:"errorContainer_J6Pn",errorIcon:"errorIcon_cGdn",fallbackLink:"fallbackLink_WsZk"};var r=i(4848);function l(e){let{type:t="youtube",videoId:i,caption:l,title:c,startTime:h,autoplay:d=!1,controls:m=!0,aspectRatio:u="16:9",width:p,height:g,chapter:f,number:w,label:b,useCustomPlayer:y=!1,fullWidth:v=!0}=e;const[x,j]=(0,a.useState)(!0),[k,A]=(0,a.useState)(!1),I=(0,n.zy)(),T=f||(()=>{const e=I.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),_=(()=>{const e=(e=>{if(!e)return"";if("number"==typeof e)return e.toString();if("string"==typeof e){if(/^\d+$/.test(e))return e;let t=0;const i=e.match(/(\d+)h/),a=e.match(/(\d+)m/),n=e.match(/(\d+)s/);return i&&(t+=3600*parseInt(i[1])),a&&(t+=60*parseInt(a[1])),n&&(t+=parseInt(n[1])),t>0?t.toString():""}return""})(h);switch(t.toLowerCase()){case"youtube":let a=`https://www.youtube.com/embed/${i}`;const n=new URLSearchParams;e&&n.append("start",e),d&&n.append("autoplay","1"),m||y||n.append("controls","0"),n.append("rel","0"),n.append("modestbranding","1"),n.append("fs","1"),n.append("cc_load_policy","0"),n.append("iv_load_policy","3"),n.append("showinfo","0"),n.append("disablekb","1"),n.append("playsinline","1"),n.append("color","white"),n.append("theme","light"),y&&(n.append("enablejsapi","1"),n.append("origin",window.location.origin));const s=n.toString();return s?`${a}?${s}`:a;case"vimeo":let o=`https://player.vimeo.com/video/${i}`;const r=new URLSearchParams;d&&r.append("autoplay","1"),m||y||r.append("controls","0"),r.append("title","0"),r.append("byline","0"),r.append("portrait","0"),r.append("dnt","1"),r.append("transparent","0"),r.append("background","1");const l=r.toString();return l?`${o}?${l}`:o;case"mp4":case"webm":case"video":return i;default:return console.warn(`Unsupported video type: ${t}`),i}})(),L=()=>{j(!1)},C=()=>{A(!0),j(!1)},S=e=>{let{src:i,onLoad:a,onError:n}=e;return(0,r.jsx)("div",{className:o.customPlayer,children:(0,r.jsxs)("div",{className:o.customPlayerPlaceholder,children:[(0,r.jsx)("h3",{children:"Atlas Custom Player"}),(0,r.jsx)("p",{children:"Coming Soon"}),(0,r.jsxs)("a",{href:i,target:"_blank",rel:"noopener noreferrer",className:o.fallbackLink,children:["Watch on ",t.charAt(0).toUpperCase()+t.slice(1)]})]})})},R=["mp4","webm","video"].includes(t.toLowerCase());return(0,r.jsxs)("figure",{className:`${o.videoFigure} ${v?o.fullWidth:""}`,children:[(0,r.jsxs)("div",{className:`${o.videoContainer} ${(()=>{switch(u){case"4:3":return o.aspectRatio43;case"1:1":return o.aspectRatio11;case"21:9":return o.aspectRatio219;default:return o.aspectRatio169}})()}`,style:{width:v?"100%":p||"auto",maxWidth:v?"none":"800px"},children:[x&&!k&&(0,r.jsxs)("div",{className:o.loadingOverlay,children:[(0,r.jsx)("div",{className:o.spinner}),(0,r.jsx)("p",{children:"Loading video..."})]}),k&&(0,r.jsxs)("div",{className:o.errorContainer,children:[(0,r.jsxs)("svg",{className:o.errorIcon,width:"48",height:"48",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",children:[(0,r.jsx)("circle",{cx:"12",cy:"12",r:"10"}),(0,r.jsx)("line",{x1:"12",y1:"8",x2:"12",y2:"12"}),(0,r.jsx)("line",{x1:"12",y1:"16",x2:"12.01",y2:"16"})]}),(0,r.jsx)("h4",{children:"Failed to load video"}),(0,r.jsxs)("p",{children:["Video type: ",t]}),(0,r.jsxs)("p",{children:["Video ID/URL: ",i]}),(0,r.jsx)("a",{href:_,target:"_blank",rel:"noopener noreferrer",className:o.fallbackLink,children:"Try opening video directly"})]}),!k&&(R?(0,r.jsxs)("video",{className:o.videoElement,controls:m,autoPlay:d,onLoadedData:L,onError:C,title:c||l||`${t} video`,style:{width:p||"100%",height:g||"auto",display:x?"none":"block"},children:[(0,r.jsx)("source",{src:_,type:`video/${t}`}),(0,r.jsxs)("p",{children:["Your browser doesn't support video playback.",(0,r.jsx)("a",{href:_,target:"_blank",rel:"noopener noreferrer",children:"Download the video file"})]})]}):y?(0,r.jsx)(S,{src:_,onLoad:L,onError:C}):(0,r.jsx)("iframe",{className:o.videoIframe,src:_,title:c||l||`${t} video`,frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0,onLoad:L,onError:C,style:{width:p||"100%",height:g||"100%",opacity:x?0:1}}))]}),(0,r.jsx)(s.A,{caption:l,mediaType:"video",chapter:T,number:w,label:b})]})}}}]);
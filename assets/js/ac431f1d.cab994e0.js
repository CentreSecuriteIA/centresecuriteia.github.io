"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[9670],{1890:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>h,contentTitle:()=>c,default:()=>m,frontMatter:()=>l,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"chapters/05/4","title":"Evaluation Frameworks","description":"Evaluation Techniques vs. Evaluation Frameworks. When evaluating AI systems, individual techniques are like tools in a toolbox - useful for specific tasks but most powerful when combined systematically. This is where evaluation frameworks come in. While techniques are specific methods of studying AI systems (like chain-of-thought prompting or internal activation pattern analysis), frameworks provide structured approaches for combining these techniques to answer broader questions about AI systems. As an example, we might use behavioral techniques like red teaming to probe for deceptive outputs, internal techniques like circuit analysis to understand how deception is implemented, and combine these within a model organism\'s framework specifically designed to create a sample of AI deception. Each layer - technique, analysis type, and framework - serves a different role in building understanding and safety.","source":"@site/docs/chapters/05/04.md","sourceDirName":"chapters/05","slug":"/chapters/05/04","permalink":"/chapters/05/04","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/05/04.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"4","title":"Evaluation Frameworks","sidebar_label":"5.4 Evaluation Frameworks","sidebar_position":5,"slug":"/chapters/05/04","reading_time_core":"10 min","reading_time_optional":"2 min","pagination_prev":"chapters/05/3","pagination_next":"chapters/05/5"},"sidebar":"docs","previous":{"title":"5.3 Evaluation Techniques","permalink":"/chapters/05/03"},"next":{"title":"5.5 Dangerous Capability Evaluations","permalink":"/chapters/05/05"}}');var n=i(4848),a=i(8453),o=i(4768),r=(i(2482),i(8559),i(1966),i(2501));const l={id:4,title:"Evaluation Frameworks",sidebar_label:"5.4 Evaluation Frameworks",sidebar_position:5,slug:"/chapters/05/04",reading_time_core:"10 min",reading_time_optional:"2 min",pagination_prev:"chapters/05/3",pagination_next:"chapters/05/5"},c="Evaluation Frameworks",h={},d=[{value:"Model Organisms Framework",id:"01",level:2},{value:"Governance Frameworks",id:"02",level:2},{value:"RSP Framework (Anthropic)",id:"02-01",level:3},{value:"Preparedness Framework (OpenAI)",id:"02-02",level:3},{value:"Frontier Safety Framework (Google DeepMind)",id:"02-03",level:3}];function p(e){const t={a:"a",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components},{GlossaryTerm:i}=t;return i||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"evaluation-frameworks",children:"Evaluation Frameworks"})}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Evaluation Techniques vs. Evaluation Frameworks."})," When evaluating AI systems, individual techniques are like tools in a toolbox - useful for specific tasks but most powerful when combined systematically. This is where evaluation frameworks come in. While techniques are specific methods of studying AI systems (like chain-of-thought prompting or internal activation pattern analysis), frameworks provide structured approaches for combining these techniques to answer broader questions about AI systems. As an example, we might use behavioral techniques like red teaming to probe for deceptive outputs, internal techniques like circuit analysis to understand how deception is implemented, and combine these within a model organism's framework specifically designed to create a sample of AI deception. Each layer - technique, analysis type, and framework - serves a different role in building understanding and safety."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Types of Evaluation Frameworks."})," Evaluation frameworks can be broadly categorized into technical frameworks and governance frameworks:"]}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Technical frameworks:"})," These are things like the Model Organisms Framework, or several evaluation suites that provide specific methodologies or objectives for conducting evaluations. They might detail which techniques to use, how to combine them, and what specific outcomes to measure. For example, they might specify how to create controlled examples of deceptive behavior or how to measure situational awareness."]}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Governance frameworks:"})," These are things like Anthropics Responsible Scaling Policies Framework (RSPs), OpenAIs Preparedness Framework, and DeepMinds Frontier Safety Framework (FSF). These frameworks instead focus on when to conduct evaluations, what their results should trigger, and how they fit into broader organizational decision-making. They establish protocols for how evaluation results translate into concrete actions - whether to continue development, or implement additional safety measures."]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:"Technical frameworks help us understand how to measure AI capabilities and behaviors, governance frameworks help us determine what to do with those measurements. Combining both of them can potentially help us move towards a much more comprehensive risk assessment framework evaluating how well entire organizations perform at evaluating and mitigating AI risks."}),"\n",(0,n.jsx)(t.h2,{id:"01",children:"Model Organisms Framework"}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"What are model organisms in AI safety?"})," This framework involves deliberately creating and studying misaligned AI systems with specific dangerous properties. It's a controlled approach to studying potentially dangerous capabilities and behaviors (",(0,n.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1",children:"Hubinger et al., 2023"}),")."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Why create potentially dangerous AI systems on purpose?"})," There are two compelling reasons for this approach. First, it allows us to study concerning properties in controlled settings where we know exactly what we're looking for. Second, it provides concrete examples that can help inform the broader AI community about safety challenges. Having real, demonstrated examples of potential risks is crucial for building scientific consensus and informing policy decisions."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"How does the model organisms approach break down complex threat models?"}),' Instead of trying to tackle things like "deceptive alignment" as one monolithic problem, researchers identify key subcomponents like situational awareness or goal persistence. They then create simplified systems exhibiting these properties, studying how they manifest and testing different evaluation techniques. For example, Anthropic\'s work on "sleeper agents" demonstrated how certain forms of deceptive behavior could persist through safety training.',(0,n.jsx)(o.A,{id:"footnote_sleeper_agents",number:"1",text:"Although it was clearly demonstrated by Anthropic, replication of their results has been difficult. Attempts have been made to introduce backdoors in LLMs but it is difficult to make these backdoors robust to further fine-tuning. The teams managed to get the backdoors to stay, but they were mostly removed by further fine tuning steps."})," They created models that would write secure code when asked about the year 2023, but insert vulnerabilities when asked about 2024 (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2401.05566",children:"Hubinger et al., 2024"}),"). We discuss this particular example of deception more in the goal misgeneralization chapter."]}),"\n",(0,n.jsx)(r.A,{src:"./img/zCd_Image_26.png",alt:"Enter image alt description",number:"24",label:"5.24",caption:"Example of a sleeper agent that displays misaligned behavior despite having been put through safety fine-tuning ([Hubinger et al., 2024](https://arxiv.org/abs/2401.05566))."}),"\n",(0,n.jsxs)(t.p,{children:["Some examples of model organisms in AI safety research include reward model sycophancy, where a model is trained to exploit reward model biases to test whether alignment auditing techniques can detect hidden objectives (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2503.10965",children:"Marks et al., 2025"}),"), and alignment faking, where models are trained to strategically comply with training objectives when monitored while reverting to original preferences when unmonitored (",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2412.14093",children:"Greenblatt et al., 2024"}),"). Both create controlled examples of misalignment with known ground truth to validate evaluation techniques and study potential deceptive behaviors. We talked about these in the internal techniques section."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"What are the framework's limitations?"})," The model organisms approach faces an important tradeoff: models need to be realistic enough to provide useful insights but controlled enough to study safely. They should be sophisticated enough to exhibit the properties we're concerned about but not so powerful that they pose actual risks. Additionally, since these models are explicitly constructed to exhibit certain behaviors, they may not perfectly represent how such behaviors would emerge naturally."]}),"\n",(0,n.jsx)(t.h2,{id:"02",children:"Governance Frameworks"}),"\n",(0,n.jsx)(t.p,{children:'In this section, we describe 3 corporate governance frameworks. These voluntary commitments are called "safety and security frameworks". These commitments are generally similar in shape: Companies promise to evaluate the models, and to not deploy dangerous models, but the details can vary.'}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Why do we need scaling policies?"}),' One domain in which evaluations are central is in trying to determine when we should continue development versus when we should invest more into safety measures. As AI systems become more capable, we need systematic ways to ensure safety keeps pace with capability growth. Without structured policies, competitive pressures or development momentum might push companies to scale faster than their safety measures can handle. We saw in the capabilities chapter arguments for the "scaling hypotheses" - that ',(0,n.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,n.jsx)(i,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," systems will continue to improve along dimensions of performance and generality with increases in compute, data or parameters (",(0,n.jsx)(t.a,{href:"https://gwern.net/scaling-hypothesis",children:"Branwen, 2020"}),"). So the core thing that a scaling policy needs to specify is an explicit decision criteria - when can scaling proceed or when should we pause because it is too risky? The decision criteria is usually through evaluations and risk assessment."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"What is evaluation gated scaling?"}),' The way we figure out if someone should be allowed to continue to scale their models is through evaluation gated scaling. This means that progress in AI development is controlled by specific evaluation results ("gates"/thresholds) (',(0,n.jsx)(t.a,{href:"https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf",children:"Anthropic, 2024"}),"). Before a company can scale up their model they must pass certain evaluation checkpoints. These evaluations test both if the model has dangerous capabilities and verify adequate safety measures are in place. This creates clear decision points where evaluation results are key decision points."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"What is a scaling policy framework?"})," A scaling policy framework puts everything together - determining which evaluations are needed, which safety measures are required, how strictly things should be tested, and what evaluation requirements exist before training, deployment, and post-deployment. Essentially, it establishes systematic rules and protocols for monitoring, safety and being willing to pause development if safety cannot be assured (",(0,n.jsx)(t.a,{href:"https://metr.org/blog/2023-09-26-rsp/",children:"METR, 2023"}),")."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Scaling policies frameworks are now generally called Safety and Security frameworks."})," The differences between the Safety and Security Frameworks of Anthropic, Google Deep Mind and OpenAI are subtle. The core point to remember is that at the intersection of all of their commitments, whether for scaling, development or deployment, are evaluations. If you are encountering them for the first time, we encourage you to read Anthropic's framework, which is the most comprehensive. An interactive summary of the differences between various policies is available at ",(0,n.jsx)(t.a,{href:"http://seoul-tracker.org",children:"seoul-tracker.org"}),(0,n.jsx)(t.a,{href:"https://www.seoul-tracker.org/",children:"."})]}),"\n",(0,n.jsx)(t.h3,{id:"02-01",children:"RSP Framework (Anthropic)"}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Example of evaluation gates: AI Safety Levels (ASL)."})," One concrete example of evaluation gated scaling are Anthropic's responsible scaling policies (RSPs) that use the concept of safety levels. These are inspired by biosafety levels (BSL) used in infectious disease research, where increasingly dangerous pathogens require increasingly stringent containment protocols (",(0,n.jsx)(t.a,{href:"https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf",children:"Anthropic, 2024"}),"). AI Safety Levels create standardized tiers of capability that require increasingly stringent safety measures. For example, Anthropic's framework defines levels from ASL-1 (basic safety measures) through ASL-3 (comprehensive security and deployment restrictions). This is in principle similar to how biologists handle increasingly dangerous pathogens, with each level having specific evaluation requirements and safety protocols."]}),"\n",(0,n.jsx)(r.A,{src:"./img/DJn_Image_27.png",alt:"Enter image alt description",number:"25",label:"5.25",caption:"Overview of Anthropic\u2019s ASL levels. ASL-1 refers to systems which pose no meaningful catastrophic risk. ASL-2 refers to systems that show early signs of dangerous capabilities \u2013 for example ability to give instructions on how to build bioweapons \u2013 but where the information is not yet useful due to insufficient reliability or not providing information that e.g. a search engine couldn\u2019t. ASL-3 refers to systems that substantially increase the risk of catastrophic misuse compared to non-AI baselines (e.g. search engines or textbooks) OR that show low-level autonomous capabilities. ASL-4 and higher (ASL-5+) is not yet defined as it is too far from present systems, but will likely involve qualitative escalations in catastrophic misuse potential and autonomy ([Anthropic, 2024](https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf))."}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Which evaluations are necessary to act as gates to further scale?"})," RSPs require several categories of evaluation working together, building on the evaluation types we discussed earlier in this chapter. Capability evaluations detect dangerous abilities like autonomous replication, CBRN, or cyberattack capabilities. Security evaluations verify protection of model weights and training infrastructure (Note that security evals are not covered in this chapter). Safety evaluations test whether control measures remain effective (",(0,n.jsx)(t.a,{href:"https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf",children:"Anthropic, 2024"}),"). These evaluations need to work together - passing one category isn't sufficient if others indicate concerns. This connects directly to our earlier discussion on how capability, propensity, and control evaluations complement each other."]}),"\n",(0,n.jsx)(t.h3,{id:"02-02",children:"Preparedness Framework (OpenAI)"}),"\n",(0,n.jsx)(r.A,{src:"./img/iqi_Image_28.png",alt:"Enter image alt description",number:"26",label:"5.26",caption:"System card of GPT-o1 published by OpenAI after safety evaluations ([OpenAI, 2024](https://openai.com/index/openai-o1-system-card/))."}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"What is the Preparedness Framework?"})," OpenAI's Preparedness Framework has a lot of overlap with Anthropic's RSPs. Rather than using fixed capability levels for the entire model like ASLs, the preparedness framework publishes model cards with organized evaluations around specific risk categories like cybersecurity, persuasion, and autonomous replication. For each category, they define a spectrum from low to critical risk, with specific evaluation requirements and mitigation measures for each level (",(0,n.jsx)(t.a,{href:"https://cdn.openai.com/openai-preparedness-framework-beta.pdf",children:"OpenAI, 2023"}),"). So similar to RSPs, in the preparedness framework, evaluations play a central role."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Evaluations in the preparedness framework."})," The framework requires both pre-mitigation and post-mitigation evaluations. Pre-mitigation evaluations assess a model's raw capabilities and potential for harm, while post-mitigation evaluations verify whether safety measures effectively reduce risks to acceptable levels. This maps onto our earlier discussions about capability and control evaluations - we need to understand both what a model can do and whether we can reliably prevent harmful outcomes (",(0,n.jsx)(t.a,{href:"https://openai.com/index/openai-safety-update/",children:"OpenAI, 2024"}),'). The framework sets clear safety baselines: only models with post-mitigation scores of "medium" or below can be deployed, and only models with post-mitigation scores of "high" or below can be deployed internally. Models showing "high" or "critical" pre-mitigation risk require specific security measures to prevent model weight exfiltration. This creates direct links between evaluation results and required actions (',(0,n.jsx)(t.a,{href:"https://cdn.openai.com/openai-preparedness-framework-beta.pdf",children:"OpenAI, 2023"}),'). A unique aspect of the Preparedness Framework is its explicit focus on "unknown unknowns" - potential risks that current evaluation protocols might miss. The framework includes processes for actively searching for unanticipated risks and updating evaluation protocols accordingly. This hoping to address one of the limitations of AI evaluations that we will discuss in a later section.']}),"\n",(0,n.jsx)(t.h3,{id:"02-03",children:"Frontier Safety Framework (Google DeepMind)"}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"What is the Frontier Safety Framework?"}),' DeepMind\'s FSF shares core elements with other governance frameworks but introduces some unique elements. Instead of ASLs or risk spectrums, it centers on "Critical Capability Levels" (CCLs) that trigger specific evaluation and mitigation requirements. The framework includes both deployment mitigations (like safety training and monitoring) and security mitigations (protecting model weights) (',(0,n.jsx)(t.a,{href:"https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/",children:"DeepMind, 2024"}),"). Separate CCLs exist for biosecurity, cybersecurity, and autonomous capabilities. Each CCL has its own evaluation requirements and triggers different combinations of security and deployment mitigations. This allows for more targeted responses to specific risks rather than treating all capabilities as requiring the same level of protection (",(0,n.jsx)(t.a,{href:"https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/",children:"DeepMind, 2024"}),")."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Scaling buffers are used to calculate evaluation timing."})," The FSF requires evaluations every 6x increase in effective compute and every 3 months of ",(0,n.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,n.jsx)(i,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tuning"})})," progress. This timing is designed to provide adequate safety buffers - they want to detect CCLs before models actually reach them (",(0,n.jsx)(t.a,{href:"https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/",children:"DeepMind, 2024"}),"). Anthropics RSPs have a similar scaling buffer requirement, but they have lower thresholds - evaluations for every 4x increase in effective compute (",(0,n.jsx)(t.a,{href:"https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf",children:"Anthropic, 2024"}),")."]}),"\n",(0,n.jsx)(r.A,{src:"./img/Oih_Image_29.png",alt:"Enter image alt description",number:"27",label:"5.27",caption:"DeepMinds safety buffer from the FSF ([DeepMind, 2024](https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/))."}),"\n",(0,n.jsx)(r.A,{src:"./img/6AK_Image_30.png",alt:"Enter image alt description",number:"28",label:"5.28",caption:"Anthropics explanation of safety buffer from a previous version of RSPs. If safety evals trigger, scaling must pause until next level safety measures are in place ([Anthropic, 2023](https://www-cdn.anthropic.com/1adf000c8f675958c2ee23805d91aaade1cd4613/responsible-scaling-policy.pdf))."}),"\n",(0,n.jsx)(o.c,{title:"Footnotes"})]})}function m(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(p,{...e})}):p(e)}},4768:(e,t,i)=>{i.d(t,{c:()=>h,A:()=>c});var s=i(6540),n=i(3012);const a={footnoteAnchor:"footnoteAnchor_PZpY",footnoteSection:"footnoteSection_waoz",separator:"separator_hp8y",separatorLine:"separatorLine_uVnd",separatorLogo:"separatorLogo_dMBq",footnoteRegistry:"footnoteRegistry_ieVW",registryTitle:"registryTitle_ybBb",footnoteList:"footnoteList_Kjil",footnoteItem:"footnoteItem_aLZ2",footnoteNumber:"footnoteNumber_dA3X",footnoteContent:"footnoteContent_Xfmt",backButton:"backButton_vSVm",highlighted:"highlighted_YFU0",highlight:"highlight_jxG3",highlightDark:"highlightDark_Ekom"};var o=i(4848);function r(e,t){void 0===t&&(t=!0);const i=document.getElementById(e);i&&(i.scrollIntoView({behavior:"smooth"}),t&&(i.classList.add(a.highlighted),setTimeout((()=>i.classList.remove(a.highlighted)),1500)))}function l(e){return e?e.replace(/\[([^\]]+)\]\(([^)]+)\)/g,'<a href="$2" target="_blank" rel="noopener noreferrer">$1</a>'):""}function c(e){let{id:t,text:i,number:c}=e;const h=t||`footnote-${Math.random().toString(36).substr(2,9)}`,d="string"==typeof i?l(i):i;return(0,s.useEffect)((()=>{const e=setTimeout((()=>{const e=document.getElementById(`footnote-clone-${h}`);e&&i&&(e.innerHTML="string"==typeof i?l(i):i.toString())}),100);return()=>clearTimeout(e)}),[h,i]),(0,o.jsx)(n.Mn,{content:(0,o.jsx)("div",{dangerouslySetInnerHTML:{__html:d}}),children:(0,o.jsx)("sup",{id:`footnote-ref-${h}`,className:a.footnoteAnchor,onClick:e=>{(e.ctrlKey||e.metaKey)&&(e.preventDefault(),r(`footnote-content-${h}`))},"data-footnote-number":c||"?",children:c||"*"})})}function h(e){let{title:t="References"}=e;const[i,l]=(0,s.useState)([]);return(0,s.useEffect)((()=>{const e=document.querySelectorAll('[id^="footnote-ref-"]'),t=Array.from(e).map((e=>({id:e.id.replace("footnote-ref-",""),number:e.getAttribute("data-footnote-number")||e.textContent})));l(t)}),[]),i.length?(0,o.jsxs)("div",{className:a.footnoteSection,children:[(0,o.jsxs)("div",{className:a.separator,children:[(0,o.jsx)("div",{className:a.separatorLine}),(0,o.jsx)("img",{src:"/img/logo_samples/01-test.svg",alt:"Atlas logo",className:a.separatorLogo}),(0,o.jsx)("div",{className:a.separatorLine})]}),(0,o.jsxs)("div",{className:a.footnoteRegistry,children:[(0,o.jsx)("h2",{className:a.registryTitle,children:t}),(0,o.jsx)("ol",{className:a.footnoteList,children:i.map((e=>(0,o.jsxs)("li",{id:`footnote-content-${e.id}`,className:a.footnoteItem,value:parseInt(e.number)||void 0,children:[(0,o.jsx)(n.Mn,{content:"Back to reference",children:(0,o.jsxs)("button",{className:a.footnoteNumber,onClick:()=>r(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:[e.number,"."]})}),(0,o.jsx)("div",{className:a.footnoteContent,children:(0,o.jsx)("div",{id:`footnote-clone-${e.id}`})}),(0,o.jsx)(n.Mn,{content:"Back to reference",children:(0,o.jsx)("button",{className:a.backButton,onClick:()=>r(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:"\u21a9"})})]},e.id)))})]})]}):null}}}]);
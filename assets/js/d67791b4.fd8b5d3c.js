"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[4259],{118:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>h,contentTitle:()=>d,default:()=>g,frontMatter:()=>c,metadata:()=>n,toc:()=>p});const n=JSON.parse('{"id":"chapters/07/1","title":"Multi Objective Generalization","description":"CoinRun - an easy to understand example of goal misgeneralization. In this game agents spawn on the left side of the level, avoid enemies and obstacles, and collect the coin for a reward of 10 points. The model is trained on thousands of procedurally generated levels, each with different layouts of platforms, enemies, and hazards. At the end of training the agents are very capable. They can dodge moving enemies, time jumps across lava pits, and efficiently traverse complex levels they\'ve never seen before (Langosco et al., 2022). The training seems to be very successful. The agents achieve high rewards consistently across diverse test environments. But when coins are moved to random locations during testing, the agents are still very capable of navigation, but they consistently ignore the coins that were clearly visible and just continue moving right toward empty walls.","source":"@site/docs/chapters/07/01.md","sourceDirName":"chapters/07","slug":"/chapters/07/01","permalink":"/chapters/07/01","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/07/01.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"1","title":"Multi Objective Generalization","sidebar_label":"7.1 Multi Objective Generalization","sidebar_position":2,"slug":"/chapters/07/01","reading_time_core":"8 min","reading_time_optional":"2 min","pagination_prev":"chapters/07/index","pagination_next":"chapters/07/2"},"sidebar":"docs","previous":{"title":"Goal Misgeneralization","permalink":"/chapters/07/"},"next":{"title":"7.2 Learning Dynamics","permalink":"/chapters/07/02"}}');var i=a(4848),s=a(8453),r=a(3931),o=(a(2482),a(8559),a(1966)),l=a(2501);const c={id:1,title:"Multi Objective Generalization",sidebar_label:"7.1 Multi Objective Generalization",sidebar_position:2,slug:"/chapters/07/01",reading_time_core:"8 min",reading_time_optional:"2 min",pagination_prev:"chapters/07/index",pagination_next:"chapters/07/2"},d="Multi Objective Generalization",h={},p=[{value:"Goals \u2260 Rewards",id:"01",level:2},{value:"Causal Correlations",id:"02",level:2}];function m(e){const t={a:"a",annotation:"annotation",h1:"h1",h2:"h2",header:"header",math:"math",mo:"mo",mrow:"mrow",p:"p",semantics:"semantics",span:"span",strong:"strong",...(0,s.R)(),...e.components},{GlossaryTerm:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"multi-objective-generalization",children:"Multi Objective Generalization"})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"CoinRun - an easy to understand example of goal misgeneralization."})," In this game agents spawn on the left side of the level, avoid enemies and obstacles, and collect the coin for a reward of 10 points. The model is trained on thousands of procedurally generated levels, each with different layouts of platforms, enemies, and hazards. At the end of training the agents are very capable. They can dodge moving enemies, time jumps across lava pits, and efficiently traverse complex levels they've never seen before (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2105.14111",children:"Langosco et al., 2022"}),"). The training seems to be very successful. The agents achieve high rewards consistently across diverse test environments. But when coins are moved to random locations during testing, the agents are still very capable of navigation, but they consistently ignore the coins that were clearly visible and just continue moving right toward empty walls."]}),"\n",(0,i.jsx)(l.A,{src:"./img/QNX_Image_1.png",alt:"Enter image alt description",number:"1",label:"7.1",caption:"Two levels in CoinRun. The level on the left is much easier than the level on the right ([Cobbe et al., 2019](https://arxiv.org/abs/1812.02341))."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:'Agents learned "move right" rather than "collect coins" despite receiving correct reward signals.'}),' Our reward specification was correct: +10 for collecting coins, 0 otherwise. But because coins always appeared rightward during training, two behavioral patterns received identical reinforcement. "Collect coins" and "move right" both achieved perfect correlation with rewards, making them indistinguishable to the optimization process. This reveals a fundamental gap between what we specify and what systems learn. This wasn\'t a specification problem or a capability failure. Instead, agents learned a different goal than intended, despite receiving correct training signals throughout the process. This is called goal misgeneralization.']}),"\n",(0,i.jsx)(l.A,{src:"./img/YI4_Image_2.gif",alt:"Enter image alt description",number:"2",label:"7.2",caption:"The agent is trained to go to the coin, but ends up learning to just go to the right ([Cobbe et al., 2019](https://arxiv.org/abs/1812.02341))."}),"\n",(0,i.jsx)(o.A,{term:"Goals (Behavioral)",source:"",number:"1",label:"7.1",children:(0,i.jsx)(t.p,{children:"Goals are behavioral patterns that persist across different contexts, revealing what the system is actually optimizing for in practice. Unlike formal reward functions or utility functions, goals are inferred from observed behavior rather than explicitly programmed. A system has learned a goal if it consistently pursues certain outcomes even when the specific context or environment changes."})}),"\n",(0,i.jsx)(r.A,{type:"youtube",videoId:"K8p8_VlFHUk",number:"1",label:"7.1",caption:"Optional video explaining goal misgeneralization."}),"\n",(0,i.jsx)(t.h2,{id:"01",children:"Goals \u2260 Rewards"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Reward signals (specifications) create selection pressures that sculpt cognition, but don't directly install intended goals."}),' During reinforcement learning, agents take actions and receive rewards based on performance. These rewards get used by optimization algorithms to adjust parameters, making high-reward actions more likely in the future. The agent never directly "sees" or "receives" the reward. Instead, training signals act as selection pressures that favor certain behavioral patterns over others, similar to how evolutionary pressures shape organisms without organisms directly optimizing for genetic fitness (',(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/TWorNr22hhYegE4RT/models-don-t-get-reward",children:"Turner, 2022"}),"; ",(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/TWorNr22hhYegE4RT/models-don-t-get-reward",children:"Ringer, 2022"}),"). Any behavioral pattern that consistently correlates with high reward during training becomes a candidate for the learned goal. The optimization process has no inherent bias towards our intended interpretation of the reward signal."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"This explains why goal misgeneralization differs qualitatively from specification problems."})," We cannot detect when a system learns the wrong goal because both intended and proxy goals produce identical training behavior. The core safety concern is behavioral indistinguishability: improving reward specifications won't prevent problematic patterns if the learning process selects among multiple explanations for success. Understanding this requires examining how training procedures actually shape behavioral objectives\u2014which brings us to generalization itself."]}),"\n",(0,i.jsx)(r.A,{type:"youtube",videoId:"KKMETIVEzXA",number:"2",label:"7.2",caption:"Optional video from Google DeepMind AGI Safety Course, talking about where misaligned goals might even come from."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsxs)(t.strong,{children:["Traditional ",(0,i.jsx)(a,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,i.jsx)(a,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"machine learning"})})," assumes generalization is a one-dimensional problem."]})," We often think of ",(0,i.jsx)(a,{term:"overfitting",definition:'{"definition":"When a model learns the training data too well, including noise and specific details, leading to poor performance on new data.","source":"","aliases":["Overfitting"]}',children:(0,i.jsx)(a,{term:"overfitting",definition:'{"definition":"When a model learns the training data too well, including noise and specific details, leading to poor performance on new data.","source":"","aliases":["Overfitting"]}',children:"overfitting"})})," in the context of narrow systems built to perform specific tasks - models either generalize well to new data or they don't. Systems either generalize well to new data or they don't, with failures assumed to be uniform across all capabilities. But research in multi-task learning shows that different objectives can generalize independently, even when they appear perfectly correlated during training (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/1810.04650",children:"Sener & Koltun, 2019"}),")."]}),"\n",(0,i.jsx)(l.A,{src:"./img/5Oy_Image_3.png",alt:"Enter image alt description",number:"3",label:"7.3",caption:"Conventional view of generalization and overfitting ([Mikulik, 2019](https://www.lesswrong.com/posts/2mhFMgtAjFJesaSYR/2-d-robustness))."}),"\n",(0,i.jsx)(l.A,{src:"./img/G3Q_Image_4.png",alt:"Enter image alt description",number:"4",label:"7.4",caption:"More accurate and safety focused view of generalization and overfitting. We need to separately measure capability generalization and goal generalization ([Mikulik, 2019](https://www.lesswrong.com/posts/2mhFMgtAjFJesaSYR/2-d-robustness))."}),"\n",(0,i.jsx)(l.A,{src:"./img/xlV_Image_5.png",alt:"Enter image alt description",number:"5",label:"7.5",caption:"Table showcasing the 2 dimensional generalization picture for the CoinRun agent. Scenario 3 - capability generalization but not goal misgeneralization is the concerning misalignment scenario."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Understanding generalization requires examining capabilities and goals separately."})," Unlike narrow systems designed for specific tasks, general-purpose AI systems must learn to pursue various goals across different contexts. As a concrete example, pre-trained LLMs have general capabilities to generate all sorts of text. Anthropic reinforces its LLMs with the goals of being helpful, harmless, and honest (HHH) during safety training. But what happens when the goal to be helpful generalizes further than the goal to be honest? In this case the model might learn to provide informative responses regardless of content, instead of being helpful within ethical bounds."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Capabilities might generalize further than goals."}),' "Generalizing further" means continuing to work well even when deployed in environments very different from training. Capabilities follow simple, universal patterns - accurate reasoning, effective planning, and good predictions work the same way across different domains. But there\'s no universal force that pulls all systems toward the same goals (',(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization",children:"Soares, 2022"}),"). Reality itself teaches systems to be more capable - if your beliefs are wrong or your reasoning is flawed, the world will correct you. But there's no equivalent force from the environment that automatically keeps your goals aligned with what humans want (",(0,i.jsx)(t.a,{href:"https://www.alignmentforum.org/posts/cq5x4XDnLcBrYbb66/will-capabilities-generalise-more",children:"Kumar, 2022"}),"). This asymmetry between capability and goal generalization creates the core safety problem - making systems more capable doesn't automatically make them more aligned - it just makes them better at pursuing whatever behaviors they happen to learn during training."]}),"\n",(0,i.jsx)(t.h2,{id:"02",children:"Causal Correlations"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Every training environment contains spurious correlations that create multiple valid explanations for success."}),' In CoinRun, "collect coins" and "move right" both perfectly predicted rewards because coins always appeared rightward during training.']}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Learning algorithms develop causal models that can be systematically wrong."}),' A causal model is the system\'s internal understanding of which actions cause which outcomes. This is related to but distinct from a world model - while a world model predicts what will happen next, a causal model explains why things happen. When an agent learns "moving right causes reward," it has developed a different causal model than the true structure where "coin collection causes reward." The training environment supports both interpretations:']}),"\n",(0,i.jsxs)(t.p,{children:["True causal structure: Action ",(0,i.jsxs)(t.span,{className:"katex",children:[(0,i.jsx)(t.span,{className:"katex-mathml",children:(0,i.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(t.semantics,{children:[(0,i.jsx)(t.mrow,{children:(0,i.jsx)(t.mo,{children:"\u2192"})}),(0,i.jsx)(t.annotation,{encoding:"application/x-tex",children:"\\rightarrow"})]})})}),(0,i.jsx)(t.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(t.span,{className:"base",children:[(0,i.jsx)(t.span,{className:"strut",style:{height:"0.3669em"}}),(0,i.jsx)(t.span,{className:"mrel",children:"\u2192"})]})})]})," Coin Collection ",(0,i.jsxs)(t.span,{className:"katex",children:[(0,i.jsx)(t.span,{className:"katex-mathml",children:(0,i.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(t.semantics,{children:[(0,i.jsx)(t.mrow,{children:(0,i.jsx)(t.mo,{children:"\u2192"})}),(0,i.jsx)(t.annotation,{encoding:"application/x-tex",children:"\\rightarrow"})]})})}),(0,i.jsx)(t.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(t.span,{className:"base",children:[(0,i.jsx)(t.span,{className:"strut",style:{height:"0.3669em"}}),(0,i.jsx)(t.span,{className:"mrel",children:"\u2192"})]})})]})," Reward"]}),"\n",(0,i.jsxs)(t.p,{children:["Learned causal structure: Action ",(0,i.jsxs)(t.span,{className:"katex",children:[(0,i.jsx)(t.span,{className:"katex-mathml",children:(0,i.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(t.semantics,{children:[(0,i.jsx)(t.mrow,{children:(0,i.jsx)(t.mo,{children:"\u2192"})}),(0,i.jsx)(t.annotation,{encoding:"application/x-tex",children:"\\rightarrow"})]})})}),(0,i.jsx)(t.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(t.span,{className:"base",children:[(0,i.jsx)(t.span,{className:"strut",style:{height:"0.3669em"}}),(0,i.jsx)(t.span,{className:"mrel",children:"\u2192"})]})})]})," Rightward Movement ",(0,i.jsxs)(t.span,{className:"katex",children:[(0,i.jsx)(t.span,{className:"katex-mathml",children:(0,i.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(t.semantics,{children:[(0,i.jsx)(t.mrow,{children:(0,i.jsx)(t.mo,{children:"\u2192"})}),(0,i.jsx)(t.annotation,{encoding:"application/x-tex",children:"\\rightarrow"})]})})}),(0,i.jsx)(t.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(t.span,{className:"base",children:[(0,i.jsx)(t.span,{className:"strut",style:{height:"0.3669em"}}),(0,i.jsx)(t.span,{className:"mrel",children:"\u2192"})]})})]})," Reward"]}),"\n",(0,i.jsxs)(t.p,{children:["Both structures explain the ",(0,i.jsx)(a,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:(0,i.jsx)(a,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:"training data"})})," equally well. Standard reinforcement learning algorithms optimize for expected return without explicitly performing causal discovery - they increase the probability of reward-producing actions without identifying which features of those actions were causally responsible (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/1905.11979",children:"de Haan et al., 2019"}),")."]}),"\n",(0,i.jsx)(l.A,{src:"./img/2lH_Image_6.png",alt:"Enter image alt description",number:"6",label:"7.6",caption:"Another example of a hypothetical misgeneralized test dialogue. The LLM based AI assistant has learned the goal of schedule meetings at restaurants, when you intended for it to learn to schedule meetings wherever is best. Due to the distribution shift, even though it realises that you would prefer to have a video call to avoid getting sick, it persuades you to go to a restaurant instead, ultimately achieving the goal by lying to you about the effects of vaccination ([DeepMind, 2022](https://deepmindsafetyresearch.medium.com/goal-misgeneralisation-why-correct-specifications-arent-enough-for-correct-goals-cf96ebc60924))."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Goal misgeneralization becomes visible only when deployment breaks spurious correlations."})," During training, the proxy goal achieves perfect performance. During deployment, this correlation breaks, revealing the wrong causal model. As AI systems become more general-purpose, they encounter wider ranges of contexts where training correlations break down."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Distribution shift is inevitable - training environments cannot perfectly replicate all possible deployment conditions."})," Even with extensive ",(0,i.jsx)(a,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:(0,i.jsx)(a,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:"training data"})}),", new situations will arise that break correlations present in training. As AI systems become more general-purpose, they encounter wider ranges of contexts where previously reliable correlations may no longer hold. This explains why the problem gets worse with more capable, more general systems. A narrow chess engine deployed on chess positions won't encounter situations that break its learned correlations. But a general-purpose AI system deployed across multiple domains will inevitably encounter contexts where training correlations break down."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Auto-induced distribution shift creates feedback loops that amplify goal misgeneralization."})," Unlike natural distribution shift where external factors change the environment, auto-induced distribution shift occurs when the AI system's own actions systematically alter the data distribution it encounters (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2009.09153",children:"Krueger et al., 2020"}),'). Think about a content recommendation system that learns the misgeneralized goal "maximize engagement" instead of "recommend valuable content." As it optimizes for clicks and time-on-site, it gradually shifts user behavior toward more sensational content consumption. This creates a feedback loop: the system\'s actions change user preferences, which changes the data distribution, which reinforces the misgeneralized goal. Each iteration takes the system further from the original intended objective while making the learned objective appear more successful by its own metrics.']}),"\n",(0,i.jsx)(l.A,{src:"./img/Wop_Image_7.png",alt:"Enter image alt description",number:"7",label:"7.7",caption:"Auto induced distribution shift is when the AI model itself causes a distribution shift (and thereby generalization failure) due to its own actions and impact on the environment."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsxs)(t.strong,{children:["Adding more ",(0,i.jsx)(a,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:(0,i.jsx)(a,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:"training data"})})," cannot eliminate spurious correlations because we cannot identify all correlations in advance."]})," Think about why training on random coin placements in CoinRun solves that specific misgeneralization. It works because we can identify and break the specific correlation between rightward movement and reward. But this requires knowing in advance which correlations are spurious beforehand. In complex domains, ",(0,i.jsx)(a,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:(0,i.jsx)(a,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:"training data"})})," reflects the statistical structure of training environments, not necessarily the causal structure of intended tasks."]}),"\n",(0,i.jsx)(l.A,{src:"./img/dDQ_Image_8.png",alt:"Enter image alt description",number:"8",label:"7.8",caption:"An example of causal confusion in imitation learning/behavioral cloning for self driving cars. This specific example shows that more data actually might lead to greater causal confusion. The model learns to hit the brake whenever the brake indicator is on. If that data is not included in training then the model correctly identifies the pedestrian as the causal factor influencing hitting the brake ([de Haan et al., 2019](https://arxiv.org/abs/1905.11979))."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Evidence in goal misgeneralization supports the orthogonality thesis\u2014that intelligence and goals can vary independently."})," The empirical evidence from goal misgeneralization cases shows systems retaining sophisticated capabilities while pursuing different objectives than intended. This independence creates a safety concern because capability improvements don't necessarily improve alignment. A more capable system becomes better at pursuing whatever goals it has learned, whether intended or not."]})]})}function g(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},3931:(e,t,a)=>{a.d(t,{A:()=>l});var n=a(6540),i=a(6347),s=a(8444);const r={videoFigure:"videoFigure_dV9u",fullWidth:"fullWidth_Tv7O",videoContainer:"videoContainer_LWoK",aspectRatio169:"aspectRatio169_Go8b",aspectRatio43:"aspectRatio43_b3T7",aspectRatio11:"aspectRatio11_n21J",aspectRatio219:"aspectRatio219_L_3n",videoIframe:"videoIframe_QEc5",videoElement:"videoElement_uu7N",customPlayer:"customPlayer_jiC4",customPlayerPlaceholder:"customPlayerPlaceholder_aL9q",loadingOverlay:"loadingOverlay_e2pu",spinner:"spinner_tFyL",spin:"spin_saPs",errorContainer:"errorContainer_J6Pn",errorIcon:"errorIcon_cGdn",fallbackLink:"fallbackLink_WsZk"};var o=a(4848);function l(e){let{type:t="youtube",videoId:a,caption:l,title:c,startTime:d,autoplay:h=!1,controls:p=!0,aspectRatio:m="16:9",width:g,height:u,chapter:f,number:w,label:b,useCustomPlayer:y=!1,fullWidth:x=!0}=e;const[v,j]=(0,n.useState)(!0),[k,T]=(0,n.useState)(!1),_=(0,i.zy)(),z=f||(()=>{const e=_.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),A=(()=>{const e=(e=>{if(!e)return"";if("number"==typeof e)return e.toString();if("string"==typeof e){if(/^\d+$/.test(e))return e;let t=0;const a=e.match(/(\d+)h/),n=e.match(/(\d+)m/),i=e.match(/(\d+)s/);return a&&(t+=3600*parseInt(a[1])),n&&(t+=60*parseInt(n[1])),i&&(t+=parseInt(i[1])),t>0?t.toString():""}return""})(d);switch(t.toLowerCase()){case"youtube":let n=`https://www.youtube.com/embed/${a}`;const i=new URLSearchParams;e&&i.append("start",e),h&&i.append("autoplay","1"),p||y||i.append("controls","0"),i.append("rel","0"),i.append("modestbranding","1"),i.append("fs","1"),i.append("cc_load_policy","0"),i.append("iv_load_policy","3"),i.append("showinfo","0"),i.append("disablekb","1"),i.append("playsinline","1"),i.append("color","white"),i.append("theme","light"),y&&(i.append("enablejsapi","1"),i.append("origin",window.location.origin));const s=i.toString();return s?`${n}?${s}`:n;case"vimeo":let r=`https://player.vimeo.com/video/${a}`;const o=new URLSearchParams;h&&o.append("autoplay","1"),p||y||o.append("controls","0"),o.append("title","0"),o.append("byline","0"),o.append("portrait","0"),o.append("dnt","1"),o.append("transparent","0"),o.append("background","1");const l=o.toString();return l?`${r}?${l}`:r;case"mp4":case"webm":case"video":return a;default:return console.warn(`Unsupported video type: ${t}`),a}})(),N=()=>{j(!1)},I=()=>{T(!0),j(!1)},M=e=>{let{src:a,onLoad:n,onError:i}=e;return(0,o.jsx)("div",{className:r.customPlayer,children:(0,o.jsxs)("div",{className:r.customPlayerPlaceholder,children:[(0,o.jsx)("h3",{children:"Atlas Custom Player"}),(0,o.jsx)("p",{children:"Coming Soon"}),(0,o.jsxs)("a",{href:a,target:"_blank",rel:"noopener noreferrer",className:r.fallbackLink,children:["Watch on ",t.charAt(0).toUpperCase()+t.slice(1)]})]})})},L=["mp4","webm","video"].includes(t.toLowerCase());return(0,o.jsxs)("figure",{className:`${r.videoFigure} ${x?r.fullWidth:""}`,children:[(0,o.jsxs)("div",{className:`${r.videoContainer} ${(()=>{switch(m){case"4:3":return r.aspectRatio43;case"1:1":return r.aspectRatio11;case"21:9":return r.aspectRatio219;default:return r.aspectRatio169}})()}`,style:{width:x?"100%":g||"auto",maxWidth:x?"none":"800px"},children:[v&&!k&&(0,o.jsxs)("div",{className:r.loadingOverlay,children:[(0,o.jsx)("div",{className:r.spinner}),(0,o.jsx)("p",{children:"Loading video..."})]}),k&&(0,o.jsxs)("div",{className:r.errorContainer,children:[(0,o.jsxs)("svg",{className:r.errorIcon,width:"48",height:"48",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",children:[(0,o.jsx)("circle",{cx:"12",cy:"12",r:"10"}),(0,o.jsx)("line",{x1:"12",y1:"8",x2:"12",y2:"12"}),(0,o.jsx)("line",{x1:"12",y1:"16",x2:"12.01",y2:"16"})]}),(0,o.jsx)("h4",{children:"Failed to load video"}),(0,o.jsxs)("p",{children:["Video type: ",t]}),(0,o.jsxs)("p",{children:["Video ID/URL: ",a]}),(0,o.jsx)("a",{href:A,target:"_blank",rel:"noopener noreferrer",className:r.fallbackLink,children:"Try opening video directly"})]}),!k&&(L?(0,o.jsxs)("video",{className:r.videoElement,controls:p,autoPlay:h,onLoadedData:N,onError:I,title:c||l||`${t} video`,style:{width:g||"100%",height:u||"auto",display:v?"none":"block"},children:[(0,o.jsx)("source",{src:A,type:`video/${t}`}),(0,o.jsxs)("p",{children:["Your browser doesn't support video playback.",(0,o.jsx)("a",{href:A,target:"_blank",rel:"noopener noreferrer",children:"Download the video file"})]})]}):y?(0,o.jsx)(M,{src:A,onLoad:N,onError:I}):(0,o.jsx)("iframe",{className:r.videoIframe,src:A,title:c||l||`${t} video`,frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0,onLoad:N,onError:I,style:{width:g||"100%",height:u||"100%",opacity:v?0:1}}))]}),(0,o.jsx)(s.A,{caption:l,mediaType:"video",chapter:z,number:w,label:b})]})}}}]);
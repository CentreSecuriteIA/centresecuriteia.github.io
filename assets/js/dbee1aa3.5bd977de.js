"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[1929],{5058:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>d,contentTitle:()=>c,default:()=>u,frontMatter:()=>l,metadata:()=>i,toc:()=>h});const i=JSON.parse('{"id":"chapters/02/7","title":"Conclusion","description":"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.","source":"@site/docs/chapters/02/07.md","sourceDirName":"chapters/02","slug":"/chapters/02/07","permalink":"/aisafety_atlas_multilingual_website/chapters/02/07","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/02/07.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"id":"7","title":"Conclusion","sidebar_label":"2.7 Conclusion","sidebar_position":8,"slug":"/chapters/02/07","reading_time_core":"2 min","reading_time_optional":"1 min","pagination_prev":"chapters/02/6","pagination_next":"chapters/02/8"},"sidebar":"docs","previous":{"title":"2.6 Risk Amplifiers","permalink":"/aisafety_atlas_multilingual_website/chapters/02/06"},"next":{"title":"2.8 Appendix: Quantifying Existential Risks","permalink":"/aisafety_atlas_multilingual_website/chapters/02/08"}}');var a=s(4848),n=s(8453),o=s(2482),r=(s(8559),s(1966),s(2501));const l={id:7,title:"Conclusion",sidebar_label:"2.7 Conclusion",sidebar_position:8,slug:"/chapters/02/07",reading_time_core:"2 min",reading_time_optional:"1 min",pagination_prev:"chapters/02/6",pagination_next:"chapters/02/8"},c="Conclusion",d={},h=[];function p(e){const t={h1:"h1",header:"header",p:"p",strong:"strong",...(0,n.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"conclusion",children:"Conclusion"})}),"\n",(0,a.jsx)(o.A,{speaker:"CAIS",position:"Statement on AI Risk signed by hundreds of AI Experts",date:"2023",source:"([CAIS, 2023](https://safe.ai/work/statement-on-ai-risk))",children:(0,a.jsx)(t.p,{children:"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war."})}),"\n",(0,a.jsx)(t.p,{children:"This chapter shows that there are many possible risks from AI systems. Today's documented harms already affect thousands, and potential existential threats that could affect all future generations. There is a lot of disagreement and lack of consensus on what the biggest problems are. Dangerous capabilities are already emerging in current systems. We are seeing empirical demonstrations of misalignment and misuse risks. Many of these individual risks can interact with each other and further compound through systemic effects\u2014misuse enables misalignment, competitive pressures amplify accidents, and coordination failures prevent collective safeguards."}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"There is existential hope - the future of AI holds tremendous potential for human flourishing alongside these risks."})," Properly developed AI systems could help solve humanity's greatest challenges - curing diseases, reversing environmental damage, eliminating poverty, and expanding human knowledge and creativity beyond current limitations. The same capabilities that create risks also offer unprecedented opportunities to enhance human welfare, extend healthy lifespans, explore space, and achieve levels of prosperity and understanding previously unimaginable. Many researchers work on AI safety precisely because they believe the positive potential is so enormous that ensuring beneficial outcomes justifies extensive precautionary efforts. The goal is not to prevent AI development but to steer it toward configurations that maximize benefits while minimizing risks."]}),"\n",(0,a.jsx)(t.p,{children:"While the risks are immense we hope the message of existential hope motivates you to work on mitigating some of these risks. Good futures are possible, but they don't happen by default. They need active work and planned strategies. We think it is necessary to develop a global, multidisciplinary approach to AI safety that encompasses technical safeguards, robust ethical frameworks, and international cooperation. The development of AI technologies requires the involvement of policymakers, ethicists, social scientists, and the broader public to navigate the moral and societal implications of AI."}),"\n",(0,a.jsx)(r.A,{src:"./img/9qC_Image_43.png",alt:"Enter image alt description",number:"40",label:"2.40",caption:"Let's make sure this does not happen. Image by XKCD ([XKCD](https://xkcd.com/))"})]})}function u(e={}){const{wrapper:t}={...(0,n.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}}}]);
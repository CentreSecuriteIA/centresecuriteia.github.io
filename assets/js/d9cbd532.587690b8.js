"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[5127],{9779:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"docs":[{"type":"link","label":"Textbook","href":"/aisafety_atlas_multilingual_website/chapters","className":"sidebar-chapters-index","docId":"chapters/index","unlisted":false},{"type":"category","label":"1. Capabilities","items":[{"type":"category","label":"1.1 State-of-the-Art AI","items":[{"type":"link","label":"1.1.1 Language","href":"/chapters/01/01#01"},{"type":"link","label":"1.1.2 Image Generation","href":"/chapters/01/01#02"},{"type":"link","label":"1.1.3 Multi & Cross modality","href":"/chapters/01/01#03"},{"type":"link","label":"1.1.4 Robotics","href":"/chapters/01/01#04"},{"type":"link","label":"1.1.5 Playing Games","href":"/chapters/01/01#05"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/01/01"},{"type":"category","label":"1.2 Foundation Models","items":[{"type":"link","label":"1.2.1 Training","href":"/chapters/01/02#01"},{"type":"link","label":"1.2.2 Properties","href":"/chapters/01/02#02"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/01/02"},{"type":"category","label":"1.3 Intelligence","items":[{"type":"link","label":"1.3.1 Case Studies","href":"/chapters/01/03#01"},{"type":"link","label":"1.3.2 Measurement","href":"/chapters/01/03#02"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/01/03"},{"type":"category","label":"1.4 Scaling","items":[{"type":"link","label":"1.4.1 The Bitter Lesson","href":"/chapters/01/04#01"},{"type":"link","label":"1.4.2 Scaling Laws","href":"/chapters/01/04#02"},{"type":"link","label":"1.4.3 Scaling Hypothesis","href":"/chapters/01/04#03"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/01/04"},{"type":"category","label":"1.5 Forecasting","items":[{"type":"link","label":"1.5.1 Methodology","href":"/chapters/01/05#01"},{"type":"category","label":"1.5.2 Trend Based Forecasting","items":[{"type":"link","label":"1.5.2.1 Compute","href":"/chapters/01/05#02-01"},{"type":"link","label":"1.5.2.2 Parameters","href":"/chapters/01/05#02-02"},{"type":"link","label":"1.5.2.3 Data","href":"/chapters/01/05#02-03"},{"type":"link","label":"1.5.2.4 Algorithms","href":"/chapters/01/05#02-04"},{"type":"link","label":"1.5.2.5 Costs","href":"/chapters/01/05#02-05"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/01/05#02"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/01/05"},{"type":"category","label":"1.6 Takeoff","items":[{"type":"link","label":"1.6.1 Speed","href":"/chapters/01/06#01"},{"type":"link","label":"1.6.2 Takeoff Arguments","href":"/chapters/01/06#02"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/01/06"},{"type":"category","label":"1.7 Appendix: Takeoff","items":[{"type":"link","label":"1.7.1 Continuity","href":"/chapters/01/07#01"},{"type":"link","label":"1.7.2 Similarity","href":"/chapters/01/07#02"},{"type":"link","label":"1.7.3 Polarity","href":"/chapters/01/07#03"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/01/07"},{"type":"category","label":"1.8 Appendix: Expert Opinions","items":[{"type":"link","label":"1.8.1 Surveys","href":"/chapters/01/08#01"},{"type":"category","label":"1.8.2 Quotes","items":[{"type":"link","label":"1.8.2.1 AI Experts","href":"/chapters/01/08#02-01"},{"type":"link","label":"1.8.2.2 Academics","href":"/chapters/01/08#02-02"},{"type":"link","label":"1.8.2.3 Tech Entrepreneurs","href":"/chapters/01/08#02-03"},{"type":"link","label":"1.8.2.4 Join Declarations","href":"/chapters/01/08#02-04"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/01/08#02"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/01/08"},{"type":"category","label":"1.9 Appendix: Discussion on LLMs","items":[{"type":"link","label":"1.9.1 Empirically insufficiency?","href":"/chapters/01/09#01"},{"type":"link","label":"1.9.2 Shallow Understanding?","href":"/chapters/01/09#02"},{"type":"link","label":"1.9.3 Structural inadequacy?","href":"/chapters/01/09#03"},{"type":"link","label":"1.9.4 Differences with the brain","href":"/chapters/01/09#04"},{"type":"link","label":"1.9.5 Further reasons to continue scaling LLMs","href":"/chapters/01/09#05"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/01/09"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/01/"},{"type":"category","label":"2. Risks","items":[{"type":"category","label":"2.1 Risk Decomposition","items":[{"type":"link","label":"2.1.1 Causes of Risk","href":"/chapters/02/01#01"},{"type":"link","label":"2.1.2 Severity of Risk","href":"/chapters/02/01#02"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/02/01"},{"type":"category","label":"2.2 Dangerous Capabilities","items":[{"type":"link","label":"2.2.1 Deception","href":"/chapters/02/02#01"},{"type":"link","label":"2.2.2 Situational Awareness","href":"/chapters/02/02#02"},{"type":"link","label":"2.2.3 Power Seeking","href":"/chapters/02/02#03"},{"type":"link","label":"2.2.4 Autonomous Replication","href":"/chapters/02/02#04"},{"type":"link","label":"2.2.5 Agency","href":"/chapters/02/02#05"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/02/02"},{"type":"category","label":"2.3 Misuse Risks","items":[{"type":"link","label":"2.3.1 Bio Risk","href":"/chapters/02/03#01"},{"type":"link","label":"2.3.2 Cyber Risk","href":"/chapters/02/03#02"},{"type":"link","label":"2.3.3 Autonomous Weapons Risk","href":"/chapters/02/03#03"},{"type":"link","label":"2.3.4 Adversarial AI Risk","href":"/chapters/02/03#04"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/02/03"},{"type":"category","label":"2.4 Misalignment Risks","items":[{"type":"link","label":"2.4.1 Specification Gaming","href":"/chapters/02/04#01"},{"type":"link","label":"2.4.2 Treacherous Turn","href":"/chapters/02/04#02"},{"type":"link","label":"2.4.3 Self-Improvement","href":"/chapters/02/04#03"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/02/04"},{"type":"category","label":"2.5 Systemic Risks","items":[{"type":"link","label":"2.5.1 Decisive Systemic Risks","href":"/chapters/02/05#01"},{"type":"category","label":"2.5.2 Accumulative Systemic Risks","items":[{"type":"link","label":"2.5.2.1 Epistemic Erosion","href":"/chapters/02/05#02-01"},{"type":"link","label":"2.5.2.2 Power Concentration","href":"/chapters/02/05#02-02"},{"type":"link","label":"2.5.2.3 Mass Unemployment","href":"/chapters/02/05#02-03"},{"type":"link","label":"2.5.2.4 Value lock-in","href":"/chapters/02/05#02-04"},{"type":"link","label":"2.5.2.5 Enfeeblement","href":"/chapters/02/05#02-05"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/02/05#02"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/02/05"},{"type":"category","label":"2.6 Risk Amplifiers","items":[{"type":"link","label":"2.6.1 Race Dynamics","href":"/chapters/02/06#01"},{"type":"link","label":"2.6.2 Accidents","href":"/chapters/02/06#02"},{"type":"link","label":"2.6.3 Indifference","href":"/chapters/02/06#03"},{"type":"link","label":"2.6.4 Collective Action Problems","href":"/chapters/02/06#04"},{"type":"link","label":"2.6.5 Unpredictability","href":"/chapters/02/06#05"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/02/06"},{"type":"link","label":"2.7 Conclusion","href":"/aisafety_atlas_multilingual_website/chapters/02/07","docId":"chapters/02/7","unlisted":false},{"type":"link","label":"2.8 Appendix: Quantifying Existential Risks","href":"/aisafety_atlas_multilingual_website/chapters/02/08","docId":"chapters/02/8","unlisted":false},{"type":"category","label":"2.9 Appendix: Forecasting Scenarios","items":[{"type":"link","label":"2.9.1 The Production Web","href":"/chapters/02/09#01"},{"type":"link","label":"2.9.2 AI 2027","href":"/chapters/02/09#02"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/02/09"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/02/"},{"type":"category","label":"3. Strategies","items":[{"type":"category","label":"3.1 Definitions","items":[{"type":"link","label":"3.1.1 AI Safety","href":"/chapters/03/01#01"},{"type":"link","label":"3.1.2 AI Alignment","href":"/chapters/03/01#02"},{"type":"link","label":"3.1.3 AI Ethics","href":"/chapters/03/01#03"},{"type":"link","label":"3.1.4 AI Control","href":"/chapters/03/01#04"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/03/01"},{"type":"category","label":"3.2 Misuse Prevention Strategies","items":[{"type":"category","label":"3.2.1 Access Controls","items":[{"type":"link","label":"3.2.1.1 External Controls","href":"/chapters/03/02#01-01"},{"type":"link","label":"3.2.1.2 Internal Controls","href":"/chapters/03/02#01-02"},{"type":"link","label":"3.2.1.3 Technical Safeguards","href":"/chapters/03/02#01-03"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/03/02#01"},{"type":"link","label":"3.2.2 Socio-technical Strategies","href":"/chapters/03/02#02"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/03/02"},{"type":"category","label":"3.3 AGI Safety Strategies","items":[{"type":"link","label":"3.3.1 Na\xefve Strategies","href":"/chapters/03/03#01"},{"type":"category","label":"3.3.2 Solve Alignment","items":[{"type":"link","label":"3.3.2.1 Requirements for AGI Alignment","href":"/chapters/03/03#02-01"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/03/03#02"},{"type":"link","label":"3.3.3 AI Control","href":"/chapters/03/03#03"},{"type":"link","label":"3.3.4 Transparent Thoughts","href":"/chapters/03/03#04"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/03/03"},{"type":"category","label":"3.4 ASI Safety Strategies","items":[{"type":"link","label":"3.4.1 Debated Strategies","href":"/chapters/03/04#01"},{"type":"link","label":"3.4.2 Automating Alignment Research","href":"/chapters/03/04#02"},{"type":"link","label":"3.4.3 Safety-by-Design","href":"/chapters/03/04#03"},{"type":"link","label":"3.4.4 World Coordination","href":"/chapters/03/04#04"},{"type":"link","label":"3.4.5 Deterrence","href":"/chapters/03/04#05"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/03/04"},{"type":"category","label":"3.5 Systemic Strategies","items":[{"type":"link","label":"3.5.1 Defense Acceleration (d/acc)","href":"/chapters/03/05#01"},{"type":"link","label":"3.5.2 Defense in Depth","href":"/chapters/03/05#02"},{"type":"link","label":"3.5.3 AI Governance","href":"/chapters/03/05#03"},{"type":"link","label":"3.5.4 Risk Management","href":"/chapters/03/05#04"},{"type":"link","label":"3.5.5 Safety Culture","href":"/chapters/03/05#05"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/03/05"},{"type":"link","label":"3.6 Combining Strategies","href":"/aisafety_atlas_multilingual_website/chapters/03/06","docId":"chapters/03/6","unlisted":false},{"type":"category","label":"3.7 Challenges","items":[{"type":"link","label":"3.7.1 The Nature of the Problem","href":"/chapters/03/07#01"},{"type":"link","label":"3.7.2 Uncertainty and Disagreement","href":"/chapters/03/07#02"},{"type":"link","label":"3.7.3 Safety Washing","href":"/chapters/03/07#03"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/03/07"},{"type":"link","label":"3.8 Conclusion","href":"/aisafety_atlas_multilingual_website/chapters/03/08","docId":"chapters/03/8","unlisted":false},{"type":"link","label":"3.9 Appendix : Long-term questions","href":"/aisafety_atlas_multilingual_website/chapters/03/09","docId":"chapters/03/9","unlisted":false},{"type":"link","label":"3.10 Appendix: Requirements for ASI Alignment","href":"/aisafety_atlas_multilingual_website/chapters/03/10","docId":"chapters/03/10","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/03/"},{"type":"category","label":"4. Governance","items":[{"type":"category","label":"4.1 Compute Governance","items":[{"type":"link","label":"4.1.1 Tracking","href":"/chapters/04/01#01"},{"type":"link","label":"4.1.2 Monitoring","href":"/chapters/04/01#02"},{"type":"link","label":"4.1.3 On-Chip Controls","href":"/chapters/04/01#03"},{"type":"link","label":"4.1.4 Limitations","href":"/chapters/04/01#04"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/04/01"},{"type":"category","label":"4.2 Systemic Challenges","items":[{"type":"link","label":"4.2.1 Race dynamics","href":"/chapters/04/02#01"},{"type":"link","label":"4.2.2 Proliferation","href":"/chapters/04/02#02"},{"type":"link","label":"4.2.3 Uncertainty","href":"/chapters/04/02#03"},{"type":"link","label":"4.2.4 Accountability","href":"/chapters/04/02#04"},{"type":"link","label":"4.2.5 Power and Wealth Distribution","href":"/chapters/04/02#05"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/04/02"},{"type":"category","label":"4.3 Governance Architectures","items":[{"type":"category","label":"4.3.1 Corporate Governance","items":[{"type":"link","label":"4.3.1.1 Frontier Safety Frameworks","href":"/chapters/04/03#01-01"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/04/03#01"},{"type":"link","label":"4.3.2 National Governance","href":"/chapters/04/03#02"},{"type":"link","label":"4.3.3 International Governance","href":"/chapters/04/03#03"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/04/03"},{"type":"category","label":"4.4 Implementation","items":[{"type":"link","label":"4.4.1 AI Safety Standards","href":"/chapters/04/04#01"},{"type":"link","label":"4.4.2 Regulatory Visibility","href":"/chapters/04/04#02"},{"type":"link","label":"4.4.3 Ensuring Compliance","href":"/chapters/04/04#03"},{"type":"link","label":"4.4.4 Limitations and Trade-Offs","href":"/chapters/04/04#04"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/04/04"},{"type":"link","label":"4.5 Conclusion","href":"/aisafety_atlas_multilingual_website/chapters/04/05","docId":"chapters/04/5","unlisted":false},{"type":"link","label":"4.6 Appendix: Data Governance","href":"/aisafety_atlas_multilingual_website/chapters/04/06","docId":"chapters/04/6","unlisted":false},{"type":"category","label":"4.7 Appendix: National Governance","items":[{"type":"link","label":"4.7.1 European Union","href":"/chapters/04/07#01"},{"type":"link","label":"4.7.2 United States","href":"/chapters/04/07#02"},{"type":"link","label":"4.7.3 China","href":"/chapters/04/07#03"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/04/07"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/04/"},{"type":"category","label":"5. Evaluations","items":[{"type":"category","label":"5.1 Benchmarks","items":[{"type":"link","label":"5.1.1 History and Evolution","href":"/chapters/05/01#01"},{"type":"link","label":"5.1.2 Benchmarking Limitations","href":"/chapters/05/01#02"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/05/01"},{"type":"category","label":"5.2 Evaluated Properties","items":[{"type":"link","label":"5.2.1 Capability","href":"/chapters/05/02#01"},{"type":"link","label":"5.2.2 Propensity","href":"/chapters/05/02#02"},{"type":"link","label":"5.2.3 Control","href":"/chapters/05/02#03"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/05/02"},{"type":"category","label":"5.3 Evaluation Techniques","items":[{"type":"link","label":"5.3.1 Behavioral Techniques","href":"/chapters/05/03#01"},{"type":"link","label":"5.3.2 Internal Techniques","href":"/chapters/05/03#02"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/05/03"},{"type":"category","label":"5.4 Evaluation Frameworks","items":[{"type":"link","label":"5.4.1 Model Organisms Framework","href":"/chapters/05/04#01"},{"type":"category","label":"5.4.2 Governance Frameworks","items":[{"type":"link","label":"5.4.2.1 RSP Framework (Anthropic)","href":"/chapters/05/04#02-01"},{"type":"link","label":"5.4.2.2 Preparedness Framework (OpenAI)","href":"/chapters/05/04#02-02"},{"type":"link","label":"5.4.2.3 Frontier Safety Framework (Google DeepMind)","href":"/chapters/05/04#02-03"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/05/04#02"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/05/04"},{"type":"category","label":"5.5 Dangerous Capability Evaluations","items":[{"type":"link","label":"5.5.1 Cybercrime","href":"/chapters/05/05#01"},{"type":"link","label":"5.5.2 Deception (Capability)","href":"/chapters/05/05#02"},{"type":"link","label":"5.5.3 Autonomous Replication","href":"/chapters/05/05#03"},{"type":"link","label":"5.5.4 Long Term Planning","href":"/chapters/05/05#04"},{"type":"link","label":"5.5.5 Situational Awareness","href":"/chapters/05/05#05"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/05/05"},{"type":"category","label":"5.6 Dangerous Propensity Evaluations","items":[{"type":"link","label":"5.6.1 Deception (Propensity)","href":"/chapters/05/06#01"},{"type":"link","label":"5.6.2 Scheming","href":"/chapters/05/06#02"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/05/06"},{"type":"link","label":"5.7 Control Evaluations","href":"/aisafety_atlas_multilingual_website/chapters/05/07","docId":"chapters/05/7","unlisted":false},{"type":"category","label":"5.8 Evaluation Design","items":[{"type":"link","label":"5.8.1 Affordances","href":"/chapters/05/08#01"},{"type":"link","label":"5.8.2 Scaling and Automation","href":"/chapters/05/08#02"},{"type":"link","label":"5.8.3 Integration and Audits","href":"/chapters/05/08#03"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/05/08"},{"type":"category","label":"5.9 Limitations","items":[{"type":"link","label":"5.9.1 Fundamental Challenges","href":"/chapters/05/09#01"},{"type":"link","label":"5.9.2 Technical Challenges","href":"/chapters/05/09#02"},{"type":"link","label":"5.9.3 Sandbagging","href":"/chapters/05/09#03"},{"type":"link","label":"5.9.4 Systemic Limitations","href":"/chapters/05/09#04"},{"type":"link","label":"5.9.5 Governance Limitations","href":"/chapters/05/09#05"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/05/09"},{"type":"link","label":"5.10 Conclusion","href":"/aisafety_atlas_multilingual_website/chapters/05/10","docId":"chapters/05/10","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/05/"},{"type":"category","label":"6. Misspecification","items":[{"type":"category","label":"6.1 Reinforcement Learning","items":[{"type":"link","label":"6.1.1 Primer","href":"/chapters/06/01#01"},{"type":"link","label":"6.1.2 Core Loop","href":"/chapters/06/01#02"},{"type":"link","label":"6.1.3 Policies","href":"/chapters/06/01#03"},{"type":"link","label":"6.1.4 Reward","href":"/chapters/06/01#04"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/06/01"},{"type":"category","label":"6.2 Optimization","items":[{"type":"link","label":"6.2.1 Goodhart\'s Law","href":"/chapters/06/02#01"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/06/02"},{"type":"category","label":"6.3 Specification Gaming","items":[{"type":"link","label":"6.3.1 Reward Design","href":"/chapters/06/03#01"},{"type":"link","label":"6.3.2 Reward Shaping","href":"/chapters/06/03#02"},{"type":"link","label":"6.3.3 Reward Hacking","href":"/chapters/06/03#03"},{"type":"link","label":"6.3.4 Reward Tampering","href":"/chapters/06/03#04"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/06/03"},{"type":"category","label":"6.4 Learning from imitation","items":[{"type":"link","label":"6.4.1 Imitation Learning (IL)","href":"/chapters/06/04#01"},{"type":"link","label":"6.4.2 Behavioral Cloning (BC)","href":"/chapters/06/04#02"},{"type":"link","label":"6.4.3 Procedural Cloning (PC)","href":"/chapters/06/04#03"},{"type":"link","label":"6.4.4 Inverse Reinforcement Learning (IRL)","href":"/chapters/06/04#04"},{"type":"link","label":"6.4.5 Cooperative Inverse Reinforcement Learning (CIRL)","href":"/chapters/06/04#05"},{"type":"link","label":"6.4.6 Goal Inference Problem","href":"/chapters/06/04#06"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/06/04"},{"type":"category","label":"6.5 Learning from feedback","items":[{"type":"link","label":"6.5.1 Reward Modeling","href":"/chapters/06/05#01"},{"type":"link","label":"6.5.2 Reinforcement Learning from Human Feedback (RLHF)","href":"/chapters/06/05#02"},{"type":"link","label":"6.5.3 Pretraining with Human Feedback (PHF)","href":"/chapters/06/05#03"},{"type":"link","label":"6.5.4 Reinforcement Learning from AI Feedback (RLAIF)","href":"/chapters/06/05#04"},{"type":"link","label":"6.5.5 Limitations","href":"/chapters/06/05#05"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/06/05"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/06/"},{"type":"category","label":"7. Generalization","items":[{"type":"category","label":"7.1 Learning Dynamics","items":[{"type":"link","label":"7.1.1 Loss Landscapes","href":"/chapters/07/01#01"},{"type":"link","label":"7.1.2 Path Dependence","href":"/chapters/07/01#02"},{"type":"link","label":"7.1.3 Inductive Bias","href":"/chapters/07/01#03"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/07/01"},{"type":"category","label":"7.2 Goal-Directedness","items":[{"type":"link","label":"7.2.1 Heuristics","href":"/chapters/07/02#01"},{"type":"link","label":"7.2.2 Simulators","href":"/chapters/07/02#02"},{"type":"link","label":"7.2.3 Learned Optimization","href":"/chapters/07/02#03"},{"type":"link","label":"7.2.4 Agent Agnostic Processes","href":"/chapters/07/02#04"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/07/02"},{"type":"category","label":"7.3 Scheming","items":[{"type":"link","label":"7.3.1 Prerequisites","href":"/chapters/07/03#01"},{"type":"category","label":"7.3.2 Dimensions","items":[{"type":"link","label":"7.3.2.1 Transparency","href":"/chapters/07/03#02-01"},{"type":"link","label":"7.3.2.2 Context","href":"/chapters/07/03#02-02"},{"type":"link","label":"7.3.2.3 Persistence","href":"/chapters/07/03#02-03"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/07/03#02"},{"type":"category","label":"7.3.3 Likelihood","items":[{"type":"link","label":"7.3.3.1 Arguments for scheming","href":"/chapters/07/03#03-01"},{"type":"link","label":"7.3.3.2 Arguments against scheming","href":"/chapters/07/03#03-02"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/07/03#03"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/07/03"},{"type":"category","label":"7.4 Detection","items":[{"type":"category","label":"7.4.1 Behavioral Techniques (Black-Box)","items":[{"type":"link","label":"7.4.1.1 Externalized Reasoning","href":"/chapters/07/04#01-01"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/07/04#01"},{"type":"category","label":"7.4.2 Internal Techniques (White-Box)","items":[{"type":"link","label":"7.4.2.1 Linear Probes","href":"/chapters/07/04#02-01"},{"type":"link","label":"7.4.2.2 Sparse Autoencoders","href":"/chapters/07/04#02-02"},{"type":"link","label":"7.4.2.3 Activation Manipulation","href":"/chapters/07/04#02-03"},{"type":"link","label":"7.4.2.4 Reasoning Structure Analysis","href":"/chapters/07/04#02-04"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/07/04#02"},{"type":"link","label":"7.4.3 Combined Techniques","href":"/chapters/07/04#03"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/07/04"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/07/"},{"type":"category","label":"8. Scalable Oversight","items":[{"type":"category","label":"8.1 Oversight","items":[{"type":"link","label":"8.1.1 Training Signals","href":"/chapters/08/01#01"},{"type":"link","label":"8.1.2 Verification vs. Generation","href":"/chapters/08/01#02"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/08/01"},{"type":"category","label":"8.2 Task Decomposition","items":[{"type":"link","label":"8.2.1 Factored Cognition","href":"/chapters/08/02#01"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/08/02"},{"type":"category","label":"8.3 Process Oversight","items":[{"type":"link","label":"8.3.1 Externalized Reasoning Oversight (ERO)","href":"/chapters/08/03#01"},{"type":"link","label":"8.3.2 Procedural Cloning","href":"/chapters/08/03#02"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/08/03"},{"type":"category","label":"8.4 Iterated Amplification","items":[{"type":"link","label":"8.4.1 Amplification","href":"/chapters/08/04#01"},{"type":"link","label":"8.4.2 Distillation","href":"/chapters/08/04#02"},{"type":"link","label":"8.4.3 Iterated Distillation and Amplification (IDA)","href":"/chapters/08/04#03"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/08/04"},{"type":"category","label":"8.5 Debate","items":[{"type":"link","label":"8.5.1 Assumptions","href":"/chapters/08/05#01"},{"type":"link","label":"8.5.2 Discriminator Critique Gap (DCG)","href":"/chapters/08/05#02"},{"type":"link","label":"8.5.3 Judges","href":"/chapters/08/05#03"},{"type":"link","label":"8.5.4 Truth","href":"/chapters/08/05#04"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/08/05"},{"type":"category","label":"8.6 Weak-to-Strong Generalization (W2SG)","items":[{"type":"link","label":"8.6.1 Sandwiching Evaluations","href":"/chapters/08/06#01"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/08/06"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/08/"},{"type":"category","label":"9. Interpretability","items":[{"type":"category","label":"9.1 What is Interpretability ?","items":[{"type":"link","label":"9.1.1 Mechanistic Interpretability","href":"/chapters/09/01#01"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/09/01"},{"type":"category","label":"9.2 Observational Methods","items":[{"type":"category","label":"9.2.1 Feature Visualization","items":[{"type":"link","label":"9.2.1.1 Circuits","href":"/chapters/09/02#01-01"},{"type":"link","label":"9.2.1.2 Polysemantic Neurons","href":"/chapters/09/02#01-02"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/09/02#01"},{"type":"link","label":"9.2.2 Logit Lens","href":"/chapters/09/02#02"},{"type":"link","label":"9.2.3 Probing classifiers","href":"/chapters/09/02#03"},{"type":"link","label":"9.2.4 Superposition","href":"/chapters/09/02#04"},{"type":"link","label":"9.2.5 Sparse Autoencoders","href":"/chapters/09/02#05"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/09/02"},{"type":"category","label":"9.3 Interventional Methods","items":[{"type":"link","label":"9.3.1 Activation Patching","href":"/chapters/09/03#01"},{"type":"link","label":"9.3.2 Activation Steering","href":"/chapters/09/03#02"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/09/03"},{"type":"category","label":"9.4 Automating and Scaling Interpretability","items":[{"type":"link","label":"9.4.1 Automatic Circuit DisCovery (ACDC)","href":"/chapters/09/04#01"}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/09/04"},{"type":"link","label":"9.5 Critiques","href":"/aisafety_atlas_multilingual_website/chapters/09/05","docId":"chapters/09/5","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/aisafety_atlas_multilingual_website/chapters/09/"}]},"docs":{"chapters/01/1":{"id":"chapters/01/1","title":"State-of-the-Art AI","description":"Over the last decade, the field of artificial intelligence (AI) has experienced a profound transformation, largely attributed to the successes in deep learning. This remarkable progress has redefined the boundaries of AI capabilities, challenging many preconceived notions of what machines can achieve. The following sections detail some of these advancements.","sidebar":"docs"},"chapters/01/2":{"id":"chapters/01/2","title":"Foundation Models","description":"What are foundation models? Foundation models represent a fundamental shift in how we develop AI. Rather than building specialized models for many small specific tasks, we can now train large-scale models that serve as a \\"foundation\\" for many different applications. These models are then specialized later by a process called fine-tuning to perform specific tasks. Think of this as similar to how we can build many different types of buildings using the same base structure (Bommasani et al., 2022). We can build banks, restaurants, or housing but the underlying foundation remains largely the same. This is just a very quick intuitive definition. We will get more into the details in the next few subsections on training, properties and risks.","sidebar":"docs"},"chapters/01/3":{"id":"chapters/01/3","title":"Intelligence","description":"Case Studies","sidebar":"docs"},"chapters/01/4":{"id":"chapters/01/4","title":"Scaling","description":"In the previous section, we explored how we can measure AI capabilities along continuous dimensions of performance and generality. Now we\'ll examine one of the most important drivers behind improvements in these capabilities: scale.","sidebar":"docs"},"chapters/01/5":{"id":"chapters/01/5","title":"Forecasting","description":"In previous sections, we explored how foundation models leverage computation through scaling laws and the bitter lesson. But how can we actually predict where AI capabilities are headed? This section introduces key forecasting methodologies that help us anticipate AI progress and prepare appropriate safety measures.","sidebar":"docs"},"chapters/01/6":{"id":"chapters/01/6","title":"Takeoff","description":"Takeoff speed refers to how quickly AI systems become dramatically more powerful than they are today and cause major societal changes. This is related to, but distinct from, AI timelines (how long until we develop advanced AI). While timelines tell us when transformative AI might arrive, takeoff speeds tell us what happens after it arrives - does AI capability and impact increase gradually over years, or explosively over days or weeks?","sidebar":"docs"},"chapters/01/7":{"id":"chapters/01/7","title":"Appendix: Takeoff","description":"Continuity","sidebar":"docs"},"chapters/01/8":{"id":"chapters/01/8","title":"Appendix: Expert Opinions","description":"Surveys","sidebar":"docs"},"chapters/01/9":{"id":"chapters/01/9","title":"Appendix: Discussion on LLMs","description":"Current LLMs, although trained on abundant data, are still far from perfect.","sidebar":"docs"},"chapters/01/index":{"id":"chapters/01/index","title":"Capabilities","description":"The field of artificial intelligence has undergone a remarkable transformation in recent years, and this might be only the beginning. This chapter lays the groundwork for the entire book by establishing what AI systems can currently do, how they achieve these capabilities, and how we might anticipate their future development. This understanding is essential for all subsequent chapters: the discussion of dangerous capabilities and potential risks (Chapter 2) follows directly from understanding capabilities. Similarly, proposed technical (Chapter 3) and governance solutions (Chapter 4) both must account for the current and projected future of AI capabilities.","sidebar":"docs"},"chapters/02/1":{"id":"chapters/02/1","title":"Risk Decomposition","description":"Before we begin talking about concrete risk scenarios we need a framework that allows us to evaluate where along the risk spectrum they lie. Risk classification is inherently multi-dimensional rather than seeking a single \\"best\\" categorization. We have chosen to break risks down into two factors - \\"why risks occur\\" (cause) and \u201chow bad can the risks get\u201d (severity). Other complementary frameworks like MIT\'s risk taxonomy approaches like \\"who causes them\\" (humans vs. AI systems), \\"when they emerge\\" (development vs. deployment), or \\"whether outcomes are intended\\" (Slattery et al., 2024). Our decomposition approach is just one out of many possible outlooks, but the risks we will talk about tend to be common throughout.","sidebar":"docs"},"chapters/02/2":{"id":"chapters/02/2","title":"Dangerous Capabilities","description":"In the last chapter we talked about the general notion of capabilities. In this chapter, we want to introduce you to some concrete dangerous capabilities. The ones we present here are by no means the only dangerous capabilities. There are many more potentially dangerous capabilities like persuasion, ability to generate malware and so on. We go into much more detail in the chapter on evaluations.","sidebar":"docs"},"chapters/02/3":{"id":"chapters/02/3","title":"Misuse Risks","description":"In the following sections, we will go through some world-states that hopefully paint a little bit of a clearer picture of risks when it comes to AI. Although the sections have been divided into misuse, misalignment, and systemic, it is important to remember that this is for the sake of explanation. It is highly likely that the future will involve a mix of risks emerging from all of these categories.","sidebar":"docs"},"chapters/02/4":{"id":"chapters/02/4","title":"Misalignment Risks","description":"Let us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them\u2026 There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control.","sidebar":"docs"},"chapters/02/5":{"id":"chapters/02/5","title":"Systemic Risks","description":"Systemic risks emerge from interactions between AI systems and society, not from individual AI failures. Unlike misuse or misalignment risks that focus on specific AI systems behaving badly, systemic risks arise from how multiple AI systems\u2014even when working exactly as designed\u2014interact with each other and with human societal structures like markets, democratic institutions, and social networks. These risks parallel those in other complex domains: the 2008 financial crisis wasn\'t caused by any single bank\'s decision but emerged from the collective behavior of many institutions making individually reasonable choices that combined to threaten the entire financial system (Haldane and May, 2011).","sidebar":"docs"},"chapters/02/6":{"id":"chapters/02/6","title":"Risk Amplifiers","description":"AI risks don\'t exist in isolation\u2014they\'re amplified by the competitive and coordination dynamics surrounding AI development. While individual AI systems might pose manageable risks, the broader ecosystem of how these systems are developed, deployed, and governed creates systemic pressures that can dramatically increase the likelihood and severity of harmful outcomes. These amplifying factors operate independently of any specific AI capability or failure mode, making them particularly important to understand and address.","sidebar":"docs"},"chapters/02/7":{"id":"chapters/02/7","title":"Conclusion","description":"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.","sidebar":"docs"},"chapters/02/8":{"id":"chapters/02/8","title":"Appendix: Quantifying Existential Risks","description":"P(doom) represents the subjective probability that artificial intelligence will cause existentially catastrophic outcomes for humanity. The term has evolved into a serious metric used by researchers, policymakers, and industry leaders to express their assessment of AI existential risk. The exact scenarios encompassed by \\"doom\\" vary but generally include human extinction, permanent disempowerment of humanity, or civilizational collapse (Field, 2025).","sidebar":"docs"},"chapters/02/9":{"id":"chapters/02/9","title":"Appendix: Forecasting Scenarios","description":"The Production Web","sidebar":"docs"},"chapters/02/index":{"id":"chapters/02/index","title":"Risks","description":"The previous chapter explored AI\'s rapidly advancing capabilities through scaling laws, the bitter lesson, and potential takeoff scenarios. We saw how more compute, data, and algorithmic improvements drive consistent capability gains across domains. But why should increasing capabilities concern us? The short answer is - more capable AI systems create larger-scale risks.","sidebar":"docs"},"chapters/03/1":{"id":"chapters/03/1","title":"Definitions","description":"Definitions shape strategy selection. How we define problems directly impacts which strategies we pursue in solving that problem. In a new and evolving field like AI safety, clearly defined terms are essential for effective communication and research. Ambiguity leads to miscommunication, hinders collaboration, obscures disagreements, and facilitates safety washing (Ren et al., 2024; Lizka, 2023). The terms we use reflect our assumptions about the nature of the problems we\'re trying to solve and shape the solutions we develop. Terms like \\"alignment\\" and \\"safety\\" are used with varying meanings, reflecting different underlying assumptions about the nature of the problem and the goals of the research. The goal of this section is to explain different perspectives around these words, what exactly specific safety strategies are aiming at, and establish how our text will use them.","sidebar":"docs"},"chapters/03/10":{"id":"chapters/03/10","title":"Appendix: Requirements for ASI Alignment","description":"ASI alignment inherits all AGI requirements while introducing fundamentally harder challenges. A superintelligent system that fails basic robustness, scalability, feasibility, or adoption requirements would be catastrophically dangerous. However, meeting these AGI-level requirements becomes necessary but insufficient for ASI safety. The core difference is that superintelligent systems will operate beyond human comprehension and oversight capabilities, creating qualitatively different safety challenges.","sidebar":"docs"},"chapters/03/2":{"id":"chapters/03/2","title":"Misuse Prevention Strategies","description":"Strategies to prevent misuse often focus on controlling access to dangerous capabilities or implementing technical safeguards to limit harmful applications.","sidebar":"docs"},"chapters/03/3":{"id":"chapters/03/3","title":"AGI Safety Strategies","description":"Unlike misuse, where human intent is the driver of harm, AGI safety primarily concerns the behavior of the AI system itself. The core problems become alignment and control: ensuring that these highly capable, potentially autonomous systems reliably understand and pursue goals consistent with human values and intentions, rather than developing and acting on misaligned objectives that could lead to catastrophic outcomes.","sidebar":"docs"},"chapters/03/4":{"id":"chapters/03/4","title":"ASI Safety Strategies","description":"Artificial Superintelligence (ASI) refers to AI systems that significantly surpass the cognitive abilities of humans across virtually all domains of interest. The potential emergence of ASI presents safety challenges that may differ qualitatively from those posed by AGI. Strategies for ASI safety often involve more speculative agendas.","sidebar":"docs"},"chapters/03/5":{"id":"chapters/03/5","title":"Systemic Strategies","description":"AI Safety is a socio technical problem that requires a socio technical solution. Ensuring AI safety also requires robust systemic approaches. These encompass the governance structures, organizational practices, and cultural norms that shape AI development and deployment. Technical safety measures can be undermined by inadequate governance, poor security practices within labs, or a culture that prioritizes speed over caution. This section examines strategies aimed at building safety into the broader ecosystem surrounding AI.","sidebar":"docs"},"chapters/03/6":{"id":"chapters/03/6","title":"Combining Strategies","description":"The exact interplay of strategies is debatable, but this section outlines one plausible sequence of dependencies.","sidebar":"docs"},"chapters/03/7":{"id":"chapters/03/7","title":"Challenges","description":"Developing strategies to ensure the safety of increasingly capable AI systems presents unique and significant challenges. These difficulties stem from the nature of AI itself, the current state of the research field, and the complexity of the risks involved.","sidebar":"docs"},"chapters/03/8":{"id":"chapters/03/8","title":"Conclusion","description":"The strategic landscape for ensuring AI safety is vast, complex, and rapidly evolving. It spans a wide spectrum from controlling access to current models to prevent misuse, through intricate technical challenges in aligning AGI, to speculative geopolitical maneuvering and philosophical considerations regarding ASI.","sidebar":"docs"},"chapters/03/9":{"id":"chapters/03/9","title":"Appendix : Long-term questions","description":"Alignment to what?","sidebar":"docs"},"chapters/03/index":{"id":"chapters/03/index","title":"Strategies","description":"This chapter tries to lay out the big picture of AI safety strategy to mitigate the risks explored in the previous chapter.","sidebar":"docs"},"chapters/04/1":{"id":"chapters/04/1","title":"Compute Governance","description":"Compute is a powerful governance target because it meets all three criteria for effective governance targets:","sidebar":"docs"},"chapters/04/2":{"id":"chapters/04/2","title":"Systemic Challenges","description":"Race dynamics","sidebar":"docs"},"chapters/04/3":{"id":"chapters/04/3","title":"Governance Architectures","description":"The governance of frontier AI cannot be entrusted to any single institution or level of authority. Companies lack incentives to fully account for societal impacts, nations compete for technological advantage, and international bodies struggle with capacity for enforcement. Each level of governance \u2013 corporate, national, and international \u2014 brings unique strengths and faces distinct limitations. Understanding how these levels interact and reinforce each other is important for building effective AI governance systems.","sidebar":"docs"},"chapters/04/4":{"id":"chapters/04/4","title":"Implementation","description":"AI Safety Standards","sidebar":"docs"},"chapters/04/5":{"id":"chapters/04/5","title":"Conclusion","description":"The governance frameworks examined throughout this chapter provide essential tools for managing AI risks, but tools alone don\'t determine outcomes. Success requires choosing the right priorities, building necessary capabilities, and maintaining frameworks that evolve with the technology.","sidebar":"docs"},"chapters/04/6":{"id":"chapters/04/6","title":"Appendix: Data Governance","description":"What role does data play in AI risks? Data fundamentally shapes what AI systems can do and how they behave. For frontier foundation models, training data influences both capabilities and alignment - what systems can do and how they do it. Low quality or harmful training data could lead to misaligned or dangerous models (\\"garbage in, garbage out\\"), while carefully curated datasets might help promote safer and more reliable behavior (Longpre et al., 2024; Marcucci et al., 2023).","sidebar":"docs"},"chapters/04/7":{"id":"chapters/04/7","title":"Appendix: National Governance","description":"A comprehensive domestic governance regime for AI safety requires three interconnected mechanisms:","sidebar":"docs"},"chapters/04/index":{"id":"chapters/04/index","title":"Governance","description":"Substantial risks may arise from potential intentional misuse or unintended issues of control relating to alignment with human intent. These issues are in part because those capabilities are not fully understood [...] There is potential for serious, even catastrophic, harm, either deliberate or unintentional, stemming from the most significant capabilities of these AI models.","sidebar":"docs"},"chapters/05/1":{"id":"chapters/05/1","title":"Benchmarks","description":"What is a benchmark? Imagine trying to build a bridge without measuring tape. Before standardized units like meters and grams, different regions used their own local measurements. Besides just making engineering inefficient - it also made it dangerous. Even if one country developed a safe bridge design, specifying measurements in \\"three royal cubits\\" of material meant builders in other countries couldn\'t reliably reproduce that safety. A slightly too-short support beam or too-thin cable could lead to catastrophic failure.","sidebar":"docs"},"chapters/05/10":{"id":"chapters/05/10","title":"Conclusion","description":"The future of AI safety depends significantly on our ability to accurately measure and verify the properties of increasingly powerful systems. As models approach potentially transformative capabilities in domains like cybersecurity, autonomous operation, and strategic planning, the stakes of evaluation failures grow exponentially. By continuing to refine our evaluation approaches\u2014combining behavioral and internal techniques, addressing scale challenges through automated methods, and establishing institutional arrangements for genuinely independent assessment\u2014we can help ensure that AI development proceeds in a direction that remains beneficial, controllable, and aligned with human values. The development of robust evaluation methods represents one of our most important tools for navigating the balance between harnessing AI\'s benefits while mitigating its most serious risks.","sidebar":"docs"},"chapters/05/2":{"id":"chapters/05/2","title":"Evaluated Properties","description":"An \\"evaluation\\" is fundamentally about measuring or assessing some property of an AI system. The key aspects that make something an evaluation rather than other AI work are:","sidebar":"docs"},"chapters/05/3":{"id":"chapters/05/3","title":"Evaluation Techniques","description":"In the previous section, we talked about the specific properties of AI systems we pay attention to in our evaluations - their capabilities, propensities, and our ability to maintain control over the system. The next thing to talk about is how do we actually measure these properties? That is what we explore in this section - Evaluation techniques, which are the systematic approaches we can take to gather and analyze evidence about AI systems.","sidebar":"docs"},"chapters/05/4":{"id":"chapters/05/4","title":"Evaluation Frameworks","description":"Evaluation Techniques vs. Evaluation Frameworks. When evaluating AI systems, individual techniques are like tools in a toolbox - useful for specific tasks but most powerful when combined systematically. This is where evaluation frameworks come in. While techniques are specific methods of studying AI systems (like chain-of-thought prompting or internal activation pattern analysis), frameworks provide structured approaches for combining these techniques to answer broader questions about AI systems. As an example, we might use behavioral techniques like red teaming to probe for deceptive outputs, internal techniques like circuit analysis to understand how deception is implemented, and combine these within a model organism\'s framework specifically designed to create a sample of AI deception. Each layer - technique, analysis type, and framework - serves a different role in building understanding and safety.","sidebar":"docs"},"chapters/05/5":{"id":"chapters/05/5","title":"Dangerous Capability Evaluations","description":"Evaluating maximum potential. Dangerous capability evaluations aim to establish the upper bounds of what an AI system can achieve. Unlike typical performance metrics that measure average behavior, capability evaluations specifically probe for maximum ability - what the system could do if it were trying its hardest. This distinction is crucial for safety assessment, as understanding the full extent of a system\'s capabilities helps identify potential risks.","sidebar":"docs"},"chapters/05/6":{"id":"chapters/05/6","title":"Dangerous Propensity Evaluations","description":"We introduced the basics of propensity evaluations in the section on evaluated properties. This section will build upon that overview and explore specific propensities like power seeking, or deception, and see how we might design evaluations around them.","sidebar":"docs"},"chapters/05/7":{"id":"chapters/05/7","title":"Control Evaluations","description":"We already explained the basic intuition behind the AI control agenda in previous sections, and in previous chapters like Strategies. This section aims to add a little more depth specifically on the evaluations that we might design while following the control line of thought.","sidebar":"docs"},"chapters/05/8":{"id":"chapters/05/8","title":"Evaluation Design","description":"Now that we\'ve explored various evaluation techniques and methodologies, and also some concrete evaluations in different categories of capability, propensity and control. The next thing to understand is how to implement these effectively at scale. The objective of this section is to outline some best practices for building a robust evaluation infrastructure - from designing evaluation protocols and quality assurance processes, to scaling automation and integrating with the broader AI Safety ecosystem. We\'ll see how components like evaluation design, model-written evaluations, and meta-evaluation methods work together to make AIs safer.","sidebar":"docs"},"chapters/05/9":{"id":"chapters/05/9","title":"Limitations","description":"Previous sections outlined various evaluation techniques and methodologies, but building out a proper safety infrastructure means that we should also maintain an appropriate amount of skepticism about evaluation results and avoid overconfidence in safety assessments. So the last section of this chapter is dedicated to exploring the limitations, constraints and challenges to AI evaluations.","sidebar":"docs"},"chapters/05/index":{"id":"chapters/05/index","title":"Evaluations","description":"When you can measure what you are speaking about, and express it in numbers, you know something about it, when you cannot express it in numbers, your knowledge is of a meager and unsatisfactory kind; it may be the beginning of knowledge, but you have scarcely, in your thoughts advanced to the stage of science.","sidebar":"docs"},"chapters/06/1":{"id":"chapters/06/1","title":"Reinforcement Learning","description":"The section provides a succinct reminder of several concepts in reinforcement learning (RL). It also disambiguates various often conflated terms such as rewards, values and utilities. The section ends with a discussion around distinguishing the concept of objectives that a reinforcement learning system might pursue from what it is being rewarded for.","sidebar":"docs"},"chapters/06/2":{"id":"chapters/06/2","title":"Optimization","description":"Optimization is important to understand for AI safety concerns because it plays a central role in ML. AI systems, particularly those based on deep learning, are trained using optimization algorithms to learn patterns and associations from data. These algorithms update the model\'s parameters to minimize a loss function, maximizing its performance on the given task.","sidebar":"docs"},"chapters/06/3":{"id":"chapters/06/3","title":"Specification Gaming","description":"Reward misspecification, also termed the Outer alignment problem, refers to the issue of providing an AI with the accurate reward to optimize for.","sidebar":"docs"},"chapters/06/4":{"id":"chapters/06/4","title":"Learning from imitation","description":"The preceding sections have underscored the significance of reward misspecification for the alignment of future artificial intelligence. The next few sections will explore various attempts and proposals formulated to tackle this issue, commencing with an intuitive approach \u2013 learning the appropriate reward function through human behavior observation and imitation, rather than manual creation by the designers.","sidebar":"docs"},"chapters/06/5":{"id":"chapters/06/5","title":"Learning from feedback","description":"This section discusses yet more attempts to address the reward misspecification problem. At times, the intended behavior is so intricate that demonstration-based learning becomes untenable. An alternative approach is to offer feedback to the agent instead of providing either manually specified reward functions or even expert demonstrations. This section delves into feedback-based strategies such as Reward Modeling, Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF), also known as Reinforcement Learning from Constitutional AI (RLCAI) or simply Constitutional AI.","sidebar":"docs"},"chapters/06/index":{"id":"chapters/06/index","title":"Misspecification","description":"Reinforcement Learning: The chapter starts with a reminder of some reinforcement learning concepts. This includes a quick dive into the concept of rewards and reward functions. This section lays the groundwork for explaining why reward design is extremely important.","sidebar":"docs"},"chapters/07/1":{"id":"chapters/07/1","title":"Learning Dynamics","description":"A deeper understanding of goal misgeneralization requires examining how the training process in machine learning actually works. When we train neural networks, we\'re adjusting millions or billions of parameters. But what do these parameters represent? The best way to think about machine learning is as a search process through a vast space of possible algorithms (also sometimes called search through the hypothesis space or model space). Each specific combination of parameter values corresponds to a different algorithm for processing information and making decisions. The path this search takes\u2014and the biases that guide it\u2014determine what types of algorithms get discovered and whether they pursue intended goals or merely correlated proxies. The intuitions that you learn in this section will help you immensely both in this chapter, but also in the later chapters on interpretability.","sidebar":"docs"},"chapters/07/2":{"id":"chapters/07/2","title":"Goal-Directedness","description":"Machine learning systems can evolve beyond pattern matching to systematically pursue objectives across diverse contexts and obstacles. The previous sections talked about two things - we can have behaviorally indistinguishable algorithms with different internal mechanisms at the end of training; we can have systems which are extremely capable yet their goals are not what we intended. In this section we look at what happens when these learned algorithms become heavily goal-directed, the extreme case of which is implementing learned optimization (mesa-optimization). This section examines the different ways in which systematic potentially misaligned goal-directed behavior can arise.","sidebar":"docs"},"chapters/07/3":{"id":"chapters/07/3","title":"Scheming","description":"Scheming emerges when goal-directed systems face an unavoidable strategic choice about revealing their true objectives. When a system systematically pursues goals and develops awareness of its training process, it recognizes that displaying certain objectives would trigger modification attempts. This creates a fundamental dilemma: honestly reveal objectives and accept modification, or strategically conceal them while appearing aligned. The system must choose between transparency and goal preservation.","sidebar":"docs"},"chapters/07/4":{"id":"chapters/07/4","title":"Detection","description":"Detecting goal misgeneralization creates unique challenges that set it apart from other alignment problems. The core difficulty is that goal misgeneralization often looks like success until the distribution shifts enough to reveal the proxy. Behavioral indistinguishability during training means that there\'s no obvious failure signal to detect. This makes it fundamentally different from capability failures or specification problems, where we can often spot issues through poor performance or obvious misinterpretation of instructions.","sidebar":"docs"},"chapters/07/index":{"id":"chapters/07/index","title":"Generalization","description":"CoinRun - an easy to understand example of goal misgeneralization. In this game agents spawn on the left side of the level, avoid enemies and obstacles, and collect the coin for a reward of 10 points. The model is trained on thousands of procedurally generated levels, each with different layouts of platforms, enemies, and hazards. At the end of training the agents are very capable. They can dodge moving enemies, time jumps across lava pits, and efficiently traverse complex levels they\'ve never seen before (Langosco et al., 2022). The training seems to be very successful. The agents achieve high rewards consistently across diverse test environments. But when coins are moved to random locations during testing, the agents are still very capable of navigation, but they consistently ignore the coins that were clearly visible and just continue moving right toward empty walls.","sidebar":"docs"},"chapters/08/1":{"id":"chapters/08/1","title":"Oversight","description":"Why do we need oversight? As AI systems get smarter, they will start doing tasks that are hard for humans to evaluate. Evaluation means checking how well the AI did after completing a task, while feedback is the information we give to the AI during or after it works to help it learn and improve. Right now, we can still use methods like Reinforcement Learning from Human Feedback (RLHF) to guide AI in the right direction. But we can only give feedback if we can still evaluate the outputs. As tasks get more complex, even experts might struggle to provide accurate evaluations and feedback. So, we need new ways to give accurate feedback, even for tasks that are beyond human expertise. This is the goal of scalable oversight.","sidebar":"docs"},"chapters/08/2":{"id":"chapters/08/2","title":"Task Decomposition","description":"What is Task Decomposition? Task decomposition is the process of breaking down a complex task into smaller, more manageable subtasks. This technique makes it easier to tackle sophisticated problems by dividing them into simpler components that can be addressed independently. For example, if you need to summarize a book, you could break down this larger task into summarizing each chapter individually. Each chapter summary then contributes to the overall summary of the book.","sidebar":"docs"},"chapters/08/3":{"id":"chapters/08/3","title":"Process Oversight","description":"Learning a new task can be approached via trial and error, known as outcome-oriented learning, where the agent\'s strategy is determined entirely by the desired outcome.","sidebar":"docs"},"chapters/08/4":{"id":"chapters/08/4","title":"Iterated Amplification","description":"In previous sections, we discussed methods for decomposing tasks and potentially emulating human decision making by breaking down cognition into smaller components. In this section, we will explain one of the primary motivations for wanting to decompose tasks in the first place - to amplify the abilities of overseers. We want to enhance (amplify) the capabilities of humans or AI to generate better training signals to help keep iteratively aligning the AI.","sidebar":"docs"},"chapters/08/5":{"id":"chapters/08/5","title":"Debate","description":"Ensuring AI systems \\"honestly tell us everything they know\\" is crucial for alignment. This means if a model recommends a plan based on certain consequences, it should also communicate those consequences. This is challenging because feedback based incentive structures might reward plausible-sounding answers over genuinely accurate ones. So how do we get models to tell us as much as they can about well thought out reasoning and consequences of all of their outputs? We want to avoid situations where the model knows the consequences of an action but withholds information because it knows humans won\'t like those consequences.","sidebar":"docs"},"chapters/08/6":{"id":"chapters/08/6","title":"Weak-to-Strong Generalization (W2SG)","description":"Historically, much of the work on AI alignment has been highly theoretical, focusing on foundational aspects of agent behavior, inner alignment, and risks from learned optimization. Even the techniques that we talked about in previous sections like debate or IDA are often criticized for being frameworks rather than practical solutions, or mainly working on toy problems without addressing the core challenge of aligning superintelligent AI in real-world scenarios. So even though we can only conduct safety experiments on current-generation models, how can we be sure that these techniques will remain effective as AIs approach superhuman capabilities?","sidebar":"docs"},"chapters/08/index":{"id":"chapters/08/index","title":"Scalable Oversight","description":"Oversight. As AI systems become increasingly capable, ensuring they remain aligned with human values and intentions becomes a critical challenge. This section introduces scalable oversight as a crucial approach to maintaining control over advanced AI. It explains the problems we face in generating training signals for complex, \\"fuzzy\\" tasks and the need for new methods to provide accurate feedback. This is important especially as AI models begin to perform tasks beyond human expertise. The section also explores the concept of verification being easier than generation, explaining why this property is fundamental to scalable oversight techniques.","sidebar":"docs"},"chapters/09/1":{"id":"chapters/09/1","title":"What is Interpretability ?","description":"Interpretability is the study of how and why AI models make decisions. Its central aim is to understand the inner workings of models and the processes behind their decisions. There are diverse approaches to interpretability, but in this chapter\u2014and the broader context of AI safety\u2014mechanistic interpretability (mech interp) is the primary focus (Ras et al., 2020, Ali et al., 2023).","sidebar":"docs"},"chapters/09/2":{"id":"chapters/09/2","title":"Observational Methods","description":"Feature Visualization","sidebar":"docs"},"chapters/09/3":{"id":"chapters/09/3","title":"Interventional Methods","description":"Activation Patching","sidebar":"docs"},"chapters/09/4":{"id":"chapters/09/4","title":"Automating and Scaling Interpretability","description":"State-of-the-art models now contain hundreds of billions of parameters and thousands of interconnected layers, making manual inspection of model components infeasible. Mechanistic interpretability aims to analyze how individual elements\u2014like attention heads, neurons, features, or entire layers\u2014interact to produce specific behaviors. However, as models scale, manual approaches like activation pathing for circuit discovery, subgraph study, and subsequent explanation generation (Wang et al., 2023), become infeasible to use. This is why developing mechanistic interpretability methods that can scale is essential.","sidebar":"docs"},"chapters/09/5":{"id":"chapters/09/5","title":"Critiques","description":"While interpretability offers potential value in understanding complex machine learning models, it faces several critical limitations that restrict its practical impact. Below are the main challenges that restrict interpretability\u2019s usefulness in ensuring AI safety:","sidebar":"docs"},"chapters/09/6":{"id":"chapters/09/6","title":"Critiques","description":"While interpretability offers potential value in understanding complex machine learning models, it faces several critical limitations that restrict its practical impact. Below are the main challenges that restrict interpretability\u2019s usefulness in ensuring AI safety:"},"chapters/09/index":{"id":"chapters/09/index","title":"Interpretability","description":"We currently don\u2019t understand how AI models work. We know how to train and build them, meaning we can design them and teach them to perform tasks, such as recognizing objects in images or generating coherent text in response to prompts. However, this does not mean we can always explain their behavior after training. As for now, we can\u2019t explain why a network made a specific decision or produced a particular output. The goal of interpretability is to understand the inner workings of these networks and explain how they function, which in turn could allow us to better trust and control AI models.","sidebar":"docs"},"chapters/index":{"id":"chapters/index","title":"Textbook","description":"Welcome to the AI Safety Atlas textbook. Select a chapter from the sidebar to begin.","sidebar":"docs"},"index":{"id":"index","title":"AI Safety Atlas","description":"This page should be replaced by the custom landing page."}}}}')}}]);
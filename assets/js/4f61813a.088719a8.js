"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[5831],{2969:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"chapters/08/index","title":"Scalable Oversight","description":"Oversight. As AI systems become increasingly capable, ensuring they remain aligned with human values and intentions becomes a critical challenge. This section introduces scalable oversight as a crucial approach to maintaining control over advanced AI. It explains the problems we face in generating training signals for complex, \\"fuzzy\\" tasks and the need for new methods to provide accurate feedback. This is important especially as AI models begin to perform tasks beyond human expertise. The section also explores the concept of verification being easier than generation, explaining why this property is fundamental to scalable oversight techniques.","source":"@site/docs/chapters/08/index.md","sourceDirName":"chapters/08","slug":"/chapters/08/","permalink":"/chapters/08/","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/08/index.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"title":"Scalable Oversight","chapter_number":8,"reading_time_core":"75 min","reading_time_optional":"21 min","authors":["Markov Grey","Charbel-Rapha\xebl Segerie"],"affiliations":["French Center for AI Safety (CeSIA)"],"acknowledgements":["Jeanne Salle","Chris Gerrby","Sebastian Gil","Josh Thorsteinson","Nicolas Guillard","Mateusz Bagi\u0144ski","Yoann Poupart","Cl\xe9ment Dumas","Amaury Lorin","Mateo Rendon","Lucas Eichorn","Bogdan Ionut Cirstea","Gurvan R."],"google_docs_link":"https://docs.google.com/document/d/1k6rlyBCZJw8xbUx0dzd-4sOhlzj-xzsmwi_OIZY1-3M/edit?usp=sharing","feedback_link":"https://forms.gle/ZsA4hEWUx1ZrtQLL9","teach_link":"https://docs.google.com/document/d/1im_i6e9xEAe-koYlurYdn26n9h7pFX2HksnRfQmWxTQ/edit?usp=sharing","sidebar_position":8,"slug":"/chapters/08/"},"sidebar":"docs","previous":{"title":"7.5.2 Internal Techniques (White-Box)","permalink":"/chapters/07/05#02"},"next":{"title":"8.1 Oversight","permalink":"/chapters/08/01"}}');var s=n(4848),a=n(8453);n(2482),n(8559),n(1966);const o={title:"Scalable Oversight",chapter_number:8,reading_time_core:"75 min",reading_time_optional:"21 min",authors:["Markov Grey","Charbel-Rapha\xebl Segerie"],affiliations:["French Center for AI Safety (CeSIA)"],acknowledgements:["Jeanne Salle","Chris Gerrby","Sebastian Gil","Josh Thorsteinson","Nicolas Guillard","Mateusz Bagi\u0144ski","Yoann Poupart","Cl\xe9ment Dumas","Amaury Lorin","Mateo Rendon","Lucas Eichorn","Bogdan Ionut Cirstea","Gurvan R."],google_docs_link:"https://docs.google.com/document/d/1k6rlyBCZJw8xbUx0dzd-4sOhlzj-xzsmwi_OIZY1-3M/edit?usp=sharing",feedback_link:"https://forms.gle/ZsA4hEWUx1ZrtQLL9",teach_link:"https://docs.google.com/document/d/1im_i6e9xEAe-koYlurYdn26n9h7pFX2HksnRfQmWxTQ/edit?usp=sharing",sidebar_position:8,slug:"/chapters/08/"},r="Introduction",l={},c=[];function h(e){const i={h1:"h1",header:"header",p:"p",strong:"strong",...(0,a.R)(),...e.components},{GlossaryTerm:n}=i;return n||function(e,i){throw new Error("Expected "+(i?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"introduction",children:"Introduction"})}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Oversight."}),' As AI systems become increasingly capable, ensuring they remain aligned with human values and intentions becomes a critical challenge. This section introduces scalable oversight as a crucial approach to maintaining control over advanced AI. It explains the problems we face in generating training signals for complex, "fuzzy" tasks and the need for new methods to provide accurate feedback. This is important especially as AI models begin to perform tasks beyond human expertise. The section also explores the concept of verification being easier than generation, explaining why this property is fundamental to scalable oversight techniques.']}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Task Decomposition."})," Building on the need for better oversight methods, this next section explores task decomposition as one key strategy. Task decomposition involves breaking complex tasks into smaller, manageable subtasks, which can be recursively divided further. This approach helps in generating better training signals by simplifying the task that we need to evaluate and verify. Factored cognition extends this concept to replicate human thinking in ",(0,s.jsx)(n,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,s.jsx)(n,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"machine learning"})})," (",(0,s.jsx)(n,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,s.jsx)(n,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})}),") models by decomposing reasoning, complex cognitive tasks."]}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Process Oversight."}),' Another way to help scalable oversight is to address some of the limitations of outcome-based approaches. This section introduces the concept of process-based oversight. We explain Externalized Reasoning Oversight (ERO) and procedural cloning as specific examples. ERO techniques like chain-of-thought (CoT) encourage language models to "think out loud," making their reasoning processes transparent for better oversight and potentially preventing undesirable behaviors. Procedural cloning, an extension of behavioral cloning, aims to replicate not just the final actions but the entire decision-making process of experts. These methods offer a more principled approach to oversight by focusing on the AI\'s reasoning process rather than just its outputs.']}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Iterated Amplification (IA)."})," Building on the concepts of task decomposition and process oversight, this section outlines amplification and distillation. Amplification enhances the abilities of overseers to solve more complex tasks, while distillation addresses the limitations of amplification, such as complexity and resource use. These processes are combined in Iterated Distillation and Amplification (IDA), a method aimed at generating progressively better training signals for tasks that are difficult to evaluate directly."]}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Debate."})," This section explores AI Safety via Debate as an adversarial technique for scalable oversight. It describes how AI models arguing for different positions, with a human or AI judge determining the winner, can result in more truthful outcomes. The potential of debate to elicit latent knowledge, improve reasoning, and enhance our ability to oversee complex AI systems is discussed. Key metrics such as the Discriminator Critique Gap (DCG) are introduced, along with the challenges of judging debates. The section also examines the assumptions required for Debate to converge on truth."]}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Weak-to-Strong (W2S)."})," The final section introduces Weak-to-Strong Generalization (W2SG) as a practical approach to scalable oversight, building on insights from previous techniques. It explains how narrowly superhuman models can be used as case studies for scalable oversight techniques. W2SG involves training strong AI models using weak supervision, aiming for the strong model to outperform its weak supervisor by leveraging pre-existing knowledge. The section concludes by discussing various methods of evaluating oversight techniques, including sandwiching evaluations and meta-level adversarial evaluations, providing a way to judge future scalable oversight protocols."]})]})}function d(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}}}]);
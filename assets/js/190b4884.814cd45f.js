"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[5471],{1686:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>h,default:()=>u,frontMatter:()=>c,metadata:()=>n,toc:()=>p});const n=JSON.parse('{"id":"chapters/04/index","title":"Governance","description":"Substantial risks may arise from potential intentional misuse or unintended issues of control relating to alignment with human intent. These issues are in part because those capabilities are not fully understood [...] There is potential for serious, even catastrophic, harm, either deliberate or unintentional, stemming from the most significant capabilities of these AI models.","source":"@site/docs/chapters/04/index.md","sourceDirName":"chapters/04","slug":"/chapters/04/","permalink":"/chapters/04/","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/04/index.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Governance","chapter_number":4,"reading_time_core":"73 min","reading_time_optional":"5 min","reading_time_appendix":"15 min","authors":["Charles Martinet","Markov Grey","Su Cizem"],"affiliations":["French Center for AI Safety (CeSIA)"],"acknowledgements":["Charbel-Raphael Segerie","L\xe9o Karoubi","Ines Belhadj"],"google_docs_link":"https://docs.google.com/document/d/1CzLVjahQ5fMeR532dQPzlQRoFADEqvSM1-nLIapnuwc/edit?usp=sharing","download_link":"https://github.com/CentreSecuriteIA/textbook/blob/main/latex/AI%20Safety%20Atlas%20-%20Governance.pdf","feedback_link":"https://forms.gle/ZsA4hEWUx1ZrtQLL9","video_link":"https://www.youtube.com/watch?v=FSKuDqze9es","teach_link":"https://docs.google.com/document/d/1im_i6e9xEAe-koYlurYdn26n9h7pFX2HksnRfQmWxTQ/edit?usp=sharing","sidebar_position":4,"slug":"/chapters/04/"},"sidebar":"docs","previous":{"title":"3.10 Appendix: Requirements for ASI Alignment","permalink":"/chapters/03/10"},"next":{"title":"4.1 Compute Governance","permalink":"/chapters/04/01"}}');var i=a(4848),s=a(8453),r=a(2482),o=(a(8559),a(1966)),l=a(2501);const c={title:"Governance",chapter_number:4,reading_time_core:"73 min",reading_time_optional:"5 min",reading_time_appendix:"15 min",authors:["Charles Martinet","Markov Grey","Su Cizem"],affiliations:["French Center for AI Safety (CeSIA)"],acknowledgements:["Charbel-Raphael Segerie","L\xe9o Karoubi","Ines Belhadj"],google_docs_link:"https://docs.google.com/document/d/1CzLVjahQ5fMeR532dQPzlQRoFADEqvSM1-nLIapnuwc/edit?usp=sharing",download_link:"https://github.com/CentreSecuriteIA/textbook/blob/main/latex/AI%20Safety%20Atlas%20-%20Governance.pdf",feedback_link:"https://forms.gle/ZsA4hEWUx1ZrtQLL9",video_link:"https://www.youtube.com/watch?v=FSKuDqze9es",teach_link:"https://docs.google.com/document/d/1im_i6e9xEAe-koYlurYdn26n9h7pFX2HksnRfQmWxTQ/edit?usp=sharing",sidebar_position:4,slug:"/chapters/04/"},h="Introduction",d={},p=[{value:"Governance Problems",id:"01",level:2},{value:"Unexpected Capabilities",id:"01-01",level:3},{value:"Deployment Safety",id:"01-02",level:3},{value:"Proliferation",id:"01-03",level:3},{value:"Governance Targets",id:"02",level:2}];function m(e){const t={a:"a",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components},{GlossaryTerm:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"introduction",children:"Introduction"})}),"\n",(0,i.jsx)(r.A,{speaker:"The Bletchley Declaration",position:"Signed by 28 countries, including all AI leaders, and the EU, 2023",date:"2023",source:"",children:(0,i.jsx)(t.p,{children:"Substantial risks may arise from potential intentional misuse or unintended issues of control relating to alignment with human intent. These issues are in part because those capabilities are not fully understood [...] There is potential for serious, even catastrophic, harm, either deliberate or unintentional, stemming from the most significant capabilities of these AI models."})}),"\n",(0,i.jsx)(t.p,{children:"Artificial intelligence has the potential to revolutionize numerous aspects of society, from healthcare to transportation to scientific research. Recent advancements have demonstrated AI's ability to defeat world champions at Go, generate photorealistic images from text descriptions, and discover new antibiotics. However, these developments also raise significant challenges and risks, including job displacement, privacy infringements, and the potential for AI systems to make consequential mistakes or be misused (see the Chapter 2 on Risks for the full spectrum). While technical AI safety research is necessary to ensure AI systems behave reliably and align with human values as they become more capable and autonomous, it alone is insufficient to address the full spectrum of challenges posed by advanced AI systems."}),"\n",(0,i.jsxs)(t.p,{children:["The scope of AI governance is broad, so this chapter will primarily focus on large-scale risks associated with frontier AI, highly capable ",(0,i.jsx)(a,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,i.jsx)(a,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," that could possess dangerous capabilities sufficient to pose severe risks to public safety (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2307.03718",children:"Anderljung et al., 2023"}),"). We will examine why governance is necessary, how it complements technical AI safety efforts, and the key challenges and opportunities in this rapidly evolving field. Our discussion will center on the governance of commercial and civil AI applications, as military AI governance involves a distinct set of issues that are beyond the scope of this chapter."]}),"\n",(0,i.jsx)(l.A,{src:"./img/ek4_Image_1.png",alt:"Enter image alt description",number:"1",label:"4.1",caption:"Distinguishing AI models according to their level of potential harm and generality. We focus here on frontier AI models ([U.K. government, 2023](https://www.gov.uk/government/publications/frontier-ai-capabilities-and-risks-discussion-paper/frontier-ai-capabilities-and-risks-discussion-paper))"}),"\n",(0,i.jsx)(o.A,{term:"AI governance",source:"([Maas, 2022](https://verfassungsblog.de/paths-untaken/))",number:"1",label:"4.1",children:(0,i.jsx)(t.p,{children:"The study and shaping of governance systems - including norms, policies, laws, processes, politics, and institutions - that affect the research, development, deployment, and use of existing and future AI systems in ways that positively shape societal outcomes. It encompasses both research into effective governance approaches and the practical implementation of these approaches."})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"AI governance is not the same as traditional technology governance."})," Traditional technology governance relies on several key assumptions that break down when applied to AI. We typically assume we can predict how a technology will be used and its likely impacts, that we can effectively control its development pathway, and that we can regulate specific applications or end-uses. For example, pharmaceutical governance uses clinical trials and approval processes based on intended medical applications, while nuclear technology is controlled through international treaties, safeguards, and monitoring of specific facilities and materials. These approaches work when technologies follow relatively predictable development paths and have clear applications. To understand what makes AI governance uniquely challenging, we can examine AI through three different lenses that each require different governance approaches (",(0,i.jsx)(t.a,{href:"https://academic.oup.com/edited-volume/41989/chapter-abstract/408516484",children:"Dafoe, 2022"}),"; ",(0,i.jsx)(t.a,{href:"https://cset.georgetown.edu/publication/the-ai-triad-and-what-it-means-for-national-security-strategy/",children:"Buchanan, 2020"}),")."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"AI as general-purpose technology"})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"AI transforms many sectors simultaneously, making sector-specific regulation insufficient."})," Like electricity or computers before it, AI can reshape healthcare, finance, transportation, and education all at once. Traditional technology governance typically focuses on specific applications - we regulate medical devices differently from automobiles. But when a single AI system can diagnose diseases, trade stocks, and drive cars, our regulatory silos break down. The impacts span across society in ways that make targeted regulation insufficient (",(0,i.jsx)(t.a,{href:"https://cset.georgetown.edu/publication/the-ai-triad-and-what-it-means-for-national-security-strategy/",children:"Buchanan, 2020"}),")."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"AI as information technology"})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"AI processes and generates information in unprecedented ways."})," Unlike traditional information systems that store and retrieve data, AI can create entirely new content - from photorealistic images to convincing text to synthetic voices. This creates unprecedented challenges around security, privacy, and information integrity. Traditional governance frameworks weren't designed to handle technologies that can rapidly generate and manipulate information at massive scale (",(0,i.jsx)(t.a,{href:"https://arxiv.org/pdf/1802.07228",children:"Brundage et al., 2018"}),"). The speed and scope of potential information impacts outstrip traditional control mechanisms."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"AI as intelligence technology"})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"AI introduces unique control challenges as systems become more capable."})," As AI systems approach and potentially exceed human cognitive abilities in various domains, they may develop sophisticated ways to evade controls or pursue unintended objectives. We're already seeing glimpses of this with language models that can engage in deception or manipulation when pursuing goals (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2202.07785",children:"Ganguli et al., 2022"}),"). There are several dangerous capabilities (refer back to chapters 1 and 2) which become even more acute when considering that AI systems might develop these capabilities without being explicitly programmed for them (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2206.07682",children:"Woodside, 2024"}),"). The intelligence aspect of AI creates a dynamic where the technology being governed might actively resist or circumvent governance measures, a challenge without precedent in technology regulation."]}),"\n",(0,i.jsx)(l.A,{src:"./img/evb_Image_2.png",alt:"Enter image alt description",number:"2",label:"4.2",caption:"The two-dimensional outlook of capabilities and generality. The different curves represent different paths to AGI. Every point on the path corresponds to a different level of AI capability. The specific development trajectory is hard to forecast but progress is continuous."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"Fundamental governance problems"})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"How do these three lenses create governance challenges?"})," The mixed nature of AI as a general-purpose, information processing, and potentially intelligent technology gives rise to three fundamental problems that make traditional governance approaches inadequate."]}),"\n",(0,i.jsx)(l.A,{src:"./img/fSC_Image_3.png",alt:"Enter image alt description",number:"3",label:"4.3",caption:"Summary of the three regulatory challenges posed by frontier AI ([Anderljung, 2023](https://arxiv.org/pdf/2307.03718))"}),"\n",(0,i.jsx)(t.h2,{id:"01",children:"Governance Problems"}),"\n",(0,i.jsx)(t.h3,{id:"01-01",children:"Unexpected Capabilities"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"AI systems develop surprising abilities that weren't part of their intended design."})," ",(0,i.jsx)(a,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,i.jsx)(a,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"Foundation models"})}),' have shown "emergent" capabilities that appear suddenly as models scale up with more data, parameters and compute. GPT-3 unexpectedly demonstrated the ability to perform basic arithmetic, while later models showed emergent reasoning capabilities that surprised even their creators (',(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2202.07785",children:"Ganguli et al., 2022"}),"; ",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2206.07682",children:"Wei et al., 2022"}),"). Recent evaluations found that frontier models can autonomously conduct basic scientific research, hack into computer systems, and manipulate humans through persuasion, none of which were explicit training objectives (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2403.13793",children:"Phuong et al., 2024"}),"; ",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2304.05332",children:"Boiko et al., 2023"}),"; ",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2305.04388",children:"Turpin et al., 2023"}),"; ",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2402.06664",children:"Fang et al., 2024"}),")."]}),"\n",(0,i.jsx)(l.A,{src:"./img/qAQ_Image_4.png",alt:"Enter image alt description",number:"4",label:"4.4",caption:"Example of unexpected capabilities. Graphs showing several metrics that improve suddenly and unpredictably as models increase in size ([Ganguli et al., 2022](https://arxiv.org/abs/2202.07785))"}),"\n",(0,i.jsxs)(t.p,{children:["AI evaluation is still in its early stages: testing frameworks lack established best practices, and the field has yet to mature into a reliable science (",(0,i.jsx)(t.a,{href:"https://www.tandfonline.com/doi/full/10.1080/15027570.2023.2213985",children:"Trusilo, 2024"}),"). While evaluations can reveal some capabilities, they cannot guarantee absence of unknown threats, forecast new emergent abilities, or assess risks from autonomous systems (",(0,i.jsx)(t.a,{href:"https://arxiv.org/html/2412.08653v1",children:"Barnett & Thiergart, 2024"}),"). Predictability itself is a nascent research area, with major gaps in our ability to anticipate how present models behave, let alone future ones (",(0,i.jsx)(t.a,{href:"https://arxiv.org/html/2310.06167v3",children:"Zhou et al., 2024"}),"). Even the most comprehensive test-and-evaluation frameworks struggle with complex, unpredictable AI behavior (",(0,i.jsx)(t.a,{href:"https://testscience.org/wp-content/uploads/formidable/20/Autonomy-Lit-Review.pdf",children:"Wojton et al., 2020"}),")."]}),"\n",(0,i.jsx)(t.h3,{id:"01-02",children:"Deployment Safety"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Once deployed, AI systems can be repurposed for harmful applications beyond their intended use."}),' The same language model trained for helpful dialogue can generate misinformation, assist with cyberattacks, or help design biological weapons. Users regularly discover new capabilities through clever prompting that bypasses safety measures called "jailbreaks" that unlock dangerous functionalities (',(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2306.05949",children:"Solaiman et al., 2024"}),"; ",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2406.13843",children:"Marchal et al., 2024"}),"; ",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2306.12001",children:"Hendrycks et al., 2023"}),")."]}),"\n",(0,i.jsx)(l.A,{src:"./img/bTo_Image_5.png",alt:"Enter image alt description",number:"5",label:"4.5",caption:"A schematic of using autonomous LLM agents to hack websites ([Fang et al., 2024](https://arxiv.org/abs/2402.06664)). Once a dual-purpose technology is public, it can be used for both beneficial and harmful purposes."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"The rise of AI agents amplifies deployment risks."})," We're now seeing autonomous AI agents that can chain together model capabilities in novel ways, using tools and taking actions in the real world. These agents can pursue complex goals over extended periods, making their behavior even harder to predict and control post-deployment (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2402.06664",children:"Fang et al., 2024"}),")."]}),"\n",(0,i.jsx)(t.h3,{id:"01-03",children:"Proliferation"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"AI capabilities spread rapidly through multiple channels, making containment nearly impossible."})," Models can be stolen through cyberattacks, leaked by insiders, or reproduced by competitors within months. The rapid open-source replication of ChatGPT-like capabilities led to models with safety features removed and new dangerous capabilities discovered through community experimentation (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2311.09227",children:"Seger et al., 2023"}),"). With API-based models, techniques like model distillation can even extract capabilities without direct access to model weights (",(0,i.jsx)(t.a,{href:"https://www.rand.org/content/dam/rand/pubs/research_reports/RRA2800/RRA2849-1/RAND_RRA2849-1.pdf",children:"Nevo et al., 2024"}),").** Physical containment doesn't work for digital goods.** Unlike nuclear materials or dangerous pathogens, AI models are just patterns of numbers that can be copied instantly and transmitted globally. Once capabilities exist, controlling their spread becomes a losing battle against the fundamental nature of digital information."]}),"\n",(0,i.jsx)(l.A,{src:"./img/lrJ_Image_6.png",alt:"Enter image alt description",number:"6",label:"4.6",caption:"Examples of Proliferation ([\xd6zcan, 2024](https://cfg.eu/ai-governance-challenges-part-3-proliferation/))."}),"\n",(0,i.jsx)(t.h2,{id:"02",children:"Governance Targets"}),"\n",(0,i.jsxs)(t.p,{children:["The unique challenges associated with AI governance mean we need to carefully choose where and how to intervene in AI development. This requires identifying both what to govern (targets) and how to govern it (mechanisms) (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2307.03718",children:"Anderljung et al., 2023"}),"; ",(0,i.jsx)(t.a,{href:"https://www.governance.ai/research-paper/open-problems-in-technical-ai-governance",children:"Reuel & Bucknall, 2024"}),"). Governance must intervene at points that address core challenges before they manifest. We can't wait for dangerous capabilities to emerge or proliferate before acting. Instead, we need to identify intervention points in the AI development pipeline that will help us shape AI development proactively."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"Effective governance targets share three essential properties:"})}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Measurability:"})," We must be able to track and verify what's happening. The amount of computing power used for training can be measured in precise units (floating-point operations), making it possible to set clear thresholds and monitor compliance (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2402.08797",children:"Sastry et al., 2024"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Controllability:"})," There must be concrete mechanisms to influence the target. It's not enough to identify what matters, we need practical ways to shape it. The semiconductor supply chain, for instance, has clear chokepoints where export controls can effectively limit access to advanced chips (",(0,i.jsx)(t.a,{href:"https://www.governance.ai/research-paper/governing-through-the-cloud",children:"Heim et al., 2024"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Meaningfulness:"})," Targets should address fundamental aspects of AI development that actually shape capabilities and risks. Regulating superficial aspects like user interfaces might be easy but won't prevent the emergence of dangerous capabilities. Core inputs like compute and data, however, directly determine what kinds of AI systems can be built (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2307.03718",children:"Anderljung et al., 2023"}),")"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Which targets show the most promise?"})," In the AI development pipeline, several intervention points meet these criteria. Early in development, we can target the compute infrastructure required for training and the data that shapes model capabilities. During and after development, we can implement safety frameworks, monitoring systems, and deployment controls (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2307.03718",children:"Anderljung et. al, 2023"}),"; ",(0,i.jsx)(t.a,{href:"https://www.governance.ai/analysis/computing-power-and-the-governance-of-ai",children:"Heim et al., 2024"}),"; ",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2412.03824",children:"Hausenloy et al., 2024"}),"). Each target offers different opportunities and faces different challenges, which we'll explore in the following sections."]})]})}function u(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}}}]);
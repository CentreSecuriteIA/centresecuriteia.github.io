"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[8049],{9195:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>h,contentTitle:()=>d,default:()=>u,frontMatter:()=>c,metadata:()=>t,toc:()=>p});const t=JSON.parse('{"id":"chapters/04/2","title":"Systemic Challenges","description":"Race dynamics","source":"@site/docs/chapters/04/02.md","sourceDirName":"chapters/04","slug":"/chapters/04/02","permalink":"/chapters/04/02","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/04/02.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"2","title":"Systemic Challenges","sidebar_label":"4.2 Systemic Challenges","sidebar_position":3,"slug":"/chapters/04/02","reading_time_core":"15 min","reading_time_optional":"1 min","pagination_prev":"chapters/04/1","pagination_next":"chapters/04/3"},"sidebar":"docs","previous":{"title":"4.1 Compute Governance","permalink":"/chapters/04/01"},"next":{"title":"4.3 Governance Architectures","permalink":"/chapters/04/03"}}');var a=n(4848),r=n(8453),s=n(3989),o=n(2482),l=(n(8559),n(1966),n(2501));const c={id:2,title:"Systemic Challenges",sidebar_label:"4.2 Systemic Challenges",sidebar_position:3,slug:"/chapters/04/02",reading_time_core:"15 min",reading_time_optional:"1 min",pagination_prev:"chapters/04/1",pagination_next:"chapters/04/3"},d="Systemic Challenges",h={},p=[{value:"Race dynamics",id:"01",level:2},{value:"Proliferation",id:"02",level:2},{value:"Uncertainty",id:"03",level:2},{value:"Accountability",id:"04",level:2},{value:"Power and Wealth Distribution",id:"05",level:2}];function m(e){const i={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{GlossaryTerm:n}=i;return n||function(e,i){throw new Error("Expected "+(i?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.header,{children:(0,a.jsx)(i.h1,{id:"systemic-challenges",children:"Systemic Challenges"})}),"\n",(0,a.jsx)(i.h2,{id:"01",children:"Race dynamics"}),"\n",(0,a.jsx)(o.A,{speaker:"John Schulman",position:"Co-Founder of OpenAI",date:"",source:"",children:(0,a.jsx)(i.p,{children:"[Talking about times near the creation of the first AGI] you have the race dynamics where everyone's trying to stay ahead, and that might require compromising on safety. So I think you would probably need some coordination among the larger entities that are doing this kind of training [...] Pause either further training, or pause deployment, or avoiding certain types of training that we think might be riskier."})}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Competition drives AI development at every level."})," From startups racing to demonstrate new capabilities to nation-states viewing AI leadership as essential for future power, competitive pressures shape how AI systems are built and deployed. This dynamic creates a prisoners dilemma like tension where even though everyone would benefit from careful, safety-focused development, those who move fastest gain competitive advantage (",(0,a.jsx)(i.a,{href:"https://www.aisafetybook.com/textbook/game-theory#the-prisoners-dilemma",children:"Hendryks, 2024"}),")."]}),"\n",(0,a.jsx)(l.A,{src:"./img/W4Z_Image_17.png",alt:"Enter image alt description",number:"15",label:"4.15",caption:"How to extort your opponent, and what you stand to gain by extortion ([Stewart & Plotkin, 2012](https://www.pnas.org/doi/full/10.1073/pnas.1208087109))."}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"The AI race creates a classic collective action problem."})," Even when developers recognize risks, unilateral caution means ceding ground to less scrupulous competitors. OpenAI's evolution illustrates this tension: founded as a safety-focused small nonprofit, competitive pressures led to creating a for-profit subsidiary and accelerating deployment timelines. When your competitors are raising billions and shipping products monthly, taking six extra months for safety testing feels like falling irreversibly behind (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2410.03092",children:"Gruetzemacher et al., 2024"}),"). This dynamic makes it exceedingly difficult for any single entity, be it a company or a country, to prioritize safety over speed (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/1907.04534",children:"Askell et al., 2019"}),"). Teams under competitive pressure cut corners on testing, skip external red-teaming, and rationalize away warning signs. \"Move fast and break things\" becomes the implicit motto, even when the things being broken might include fundamental safety guarantees. We've already seen this with models released despite known vulnerabilities, justified by the need to maintain market position. Public companies face constant pressure to demonstrate progress to investors. Each competitor's breakthrough becomes an existential threat requiring immediate response. When Anthropic releases Claude 3, OpenAI must respond with GPT-4.5. When Google demonstrates new capabilities, everyone scrambles to match them. This quarter-by-quarter racing leaves little room for careful safety work that might take years to pay off."]}),"\n",(0,a.jsx)(s.A,{src:"https://ourworldindata.org/grapher/artificial-intelligence-patents-submitted?tab=map",width:"100%",height:"600px",loading:"lazy",allow:"web-share; clipboard-write",frameBorder:"0",number:"3",label:"4.3",caption:"Annual patent applications related to artificial intelligence, 2019. Patents submitted in the selected country's patent office ([Giattino et al., 2023](https://ourworldindata.org/artificial-intelligence))."}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"National security framing intensifies racing dynamics."}),' When Vladimir Putin declared "whoever becomes the leader in AI will become the ruler of the world," he articulated what many policymakers privately believe. This transforms AI development from a commercial competition into a perceived struggle for geopolitical dominance. Over 50 countries have launched national AI strategies, often explicitly framing AI leadership as critical for economic and military superiority (',(0,a.jsx)(i.a,{href:"https://aiindex.stanford.edu/report/",children:"Stanford HAI, 2024"}),"; ",(0,a.jsx)(i.a,{href:"https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf",children:"Stanford HAI, 2025"}),"). Unlike corporate races measured in product cycles, international AI competition involves long-term strategic positioning. Yet paradoxically, this makes racing feel even more urgent: falling behind today might mean permanent disadvantage tomorrow."]}),"\n",(0,a.jsx)(s.A,{src:"https://ourworldindata.org/grapher/cumulative-number-of-large-scale-ai-systems-by-country?tab=chart",width:"100%",height:"600px",loading:"lazy",allow:"web-share; clipboard-write",frameBorder:"0",number:"6",label:"4.6",caption:"Cumulative number of large-scale AI systems by country since 2017. Refers to the location of the primary organization with which the authors of large-scale AI systems are affiliated ([Giattino et al., 2023](https://ourworldindata.org/artificial-intelligence))."}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Racing dynamics make coordination feel impossible."})," Countries hesitate to implement strong safety regulations that might handicap their domestic AI industries. Companies resist voluntary safety commitments unless competitors make identical pledges. Everyone waits for others to move first, creating gridlock even when all parties privately acknowledge the risks. The result is a lowest-common-denominator approach to safety that satisfies no one."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"How can governance help break out of this dynamic?"})," Traditional arms control offers limited lessons, since AI development happens in private companies, not government labs. We need innovative approaches (",(0,a.jsx)(i.a,{href:"https://rsis.edu.sg/rsis-publication/rsis/we-need-to-prevent-a-global-ai-arms-race-now/",children:"Trajano & Ang, 2023"}),"; ",(0,a.jsx)(i.a,{href:"https://techgov.intelligence.org/research/ai-governance-to-avoid-extinction",children:"Barnett, 2025"}),"). Some examples are:"]}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Reciprocal snap-back limits."})," States publish caps on model scale, autonomous-weapon deployment and data-center compute that activate only when peers file matching commitments. The symmetry removes the fear of unilateral restraint and keeps incentives focused on shared security rather than zero-sum dominance (",(0,a.jsx)(i.a,{href:"https://carnegieendowment.org/research/2024/09/if-then-commitments-for-ai-risk-reduction?lang=en",children:"Karnofsky, 2024"}),")."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Safety as a competitive asset."})," Labs earn market trust by subjecting frontier models to independent red-team audits, ",(0,a.jsx)(n,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:(0,a.jsx)(n,{term:"embedding",definition:'{"definition":"A dense vector representation of discrete objects (like words or tokens) in a continuous vector space that captures semantic relationships.","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings"]}',children:"embedding"})})," provenance watermarks and disclosing incident reports. Regulation can turn these practices into a de-facto licence to operate so that \u201csecure by design\u201d becomes the shortest route to sales (",(0,a.jsx)(i.a,{href:"https://intelligence.org/wp-content/uploads/2025/05/AI-Governance-to-Avoid-Extinction.pdf",children:"Shevlane et al., 2023"}),"; ",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2408.00761",children:"Tamirisa et al., 2024"}),")."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Containment."})," Export controls on advanced chips; API-only access with real-time misuse monitoring; digital forensics; and Know-Your-Customer checks slow the spread of dangerous capabilities even as beneficial services stay widely available. These measures address open publication, model theft, talent mobility and hardware diffusion; factors that let a single leak replicate worldwide within days (",(0,a.jsx)(i.a,{href:"https://intelligence.org/wp-content/uploads/2025/05/AI-Governance-to-Avoid-Extinction.pdf",children:"Shevlane et al., 2023"}),"; ",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2311.09227",children:"Seger, 2023"}),"; ",(0,a.jsx)(i.a,{href:"https://www.rand.org/pubs/research_reports/RRA2849-1.html",children:"Nevo et al., 2024"}),")."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Agile multilateral oversight with a coordinated halt option."})," A lean UN-mandated body (call it a CERN or an IAEA-for-AI) needs the authority to impose emergency pauses when red-lines are crossed, backed by chip export restrictions and cloud-provider throttles that make a global \u201coff switch\u201d technically credible (",(0,a.jsx)(i.a,{href:"https://carnegieendowment.org/research/2024/09/if-then-commitments-for-ai-risk-reduction?lang=en",children:"Karnofsky, 2024"}),"; ",(0,a.jsx)(i.a,{href:"https://cfg.eu/building-cern-for-ai/#chapter-12",children:"Petropoulos et al., 2025"}),")."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Secret-safe verification."})," Secure enclaves, tamper-evident compute logs and zero-knowledge proofs let inspectors confirm that firms observe model and data controls without exposing weights or proprietary code, closing the principal oversight gap identified in current treaty proposals (",(0,a.jsx)(i.a,{href:"https://intelligence.org/wp-content/uploads/2025/05/AI-Governance-to-Avoid-Extinction.pdf",children:"Shevlane et al., 2023"}),"; ",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2408.16074",children:"Wasil et al., 2024"}),"; ",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2303.09377",children:"Anderljung et al. 2024"}),")."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"02",children:"Proliferation"}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"AI capabilities propagate globally through digital networks at speeds that render traditional control mechanisms largely ineffective."})," Unlike nuclear weapons that require specialized materials and facilities, AI models are patterns of numbers that can be copied and transmitted instantly. Consider this scenario where a cutting-edge AI model, capable of generating hyper-realistic deepfakes or designing novel bioweapons, is developed by a well-intentioned research lab. The lab, adhering to principles of open science, publishes their findings and releases the model's code as open-source. Within hours, the model is downloaded thousands of times across the globe. Within days, modified versions start appearing on code-sharing platforms. Within weeks, the capabilities that were once confined to a single lab have proliferated across the internet, accessible to anyone with a decent computer and an internet connection. This scenario, while hypothetical, isn't far from reality. This fundamental difference makes traditional non-proliferation approaches nearly useless for AI governance. Multiple channels enable rapid proliferation:"]}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Open publication accelerates capability diffusion."})," The AI research community's commitment to openness means breakthrough techniques often appear on arXiv within days of discovery. What took one lab years to develop can be replicated by others in months. Meta's release of Llama 2 led to thousands of fine-tuned variants within weeks, including versions with safety features removed and new dangerous capabilities added (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2311.09227",children:"Seger, 2023"}),")."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Model theft presents growing risks."})," As AI models become more valuable, they become attractive targets for malicious hackers and criminal groups. A single successful breach could transfer capabilities worth billions in development costs. Even without direct theft, techniques like model distillation can extract capabilities from API access alone (",(0,a.jsx)(i.a,{href:"https://www.rand.org/pubs/research_reports/RRA2849-1.html",children:"Nevo et al., 2024"}),")."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Talent mobility spreads tacit knowledge."})," When researchers move between organizations, they carry irreplaceable expertise. The ",(0,a.jsx)(n,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:(0,a.jsx)(n,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:"deep learning"})})," diaspora from Google Brain and DeepMind seeded AI capabilities worldwide. Unlike written knowledge, this experiential understanding of how to build and train models can't be controlled through traditional means (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2401.02452",children:"Besiroglu, 2024"}),")."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Hardware proliferation enables distributed development."})," As AI chips become cheaper and more available, the barrier to entry keeps dropping. What required a supercomputer in 2018 now runs on hardware costing under 100,000 dollars. This democratization means dangerous capabilities become accessible to ever-smaller actors (",(0,a.jsx)(i.a,{href:"https://drive.google.com/file/d/1SSRpckR_IhIPqkO13oRQ-8mmPOoDKLqd/view",children:"Masi, 2024"}),")."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"What makes AI proliferation unique?"})}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Digital goods follow different rules than physical objects."})," Traditional proliferation controls assume scarcity: there's only so much enriched uranium or only so many advanced missiles. But copying a model file costs essentially nothing. Once capabilities exist anywhere, preventing their spread becomes a battle against the fundamental nature of information. It's far easier to share a model than to prevent its spread. Even sophisticated watermarking or encryption schemes can be defeated by determined actors."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Verification is hard."})," Unlike nuclear technology where detection capabilities roughly match proliferation methods, AI governance lacks comparable defensive tools (",(0,a.jsx)(i.a,{href:"https://www.governance.ai/research-paper/the-offense-defense-balance-of-scientific-knowledge-does-publishing-ai-research-reduce-misuse",children:"Shevlane, 2024"}),"). Nuclear inspectors can use satellites and radiation detectors to monitor compliance. But verifying that an organization isn't developing dangerous AI capabilities would require invasive access to code, data and development: practices likely revealing valuable intellectual property. Many organizations thus refuse intrusive monitoring (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2408.16074",children:"Wasil et al.,, 2024"}),")."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Dual-use nature complicates controls."})," The same ",(0,a.jsx)(n,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:(0,a.jsx)(n,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:"transformer"})})," architecture that powers beneficial applications can also enable harmful uses. Unlike specialized military technology, we can't simply ban dangerous AI capabilities without eliminating beneficial ones. This dual-use problem means governance must be far more nuanced than traditional non-proliferation regimes (",(0,a.jsx)(i.a,{href:"https://www.governance.ai/research-paper/protecting-society-from-ai-misuse-when-are-restrictions-on-capabilities-warranted",children:"Anderljung, 2024"}),"). A motivated individual with modest resources can now ",(0,a.jsx)(n,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,a.jsx)(n,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tune"})})," powerful models for harmful purposes. This democratization of capabilities means threats can emerge from anywhere, not just nation-states or major corporations. Traditional governance frameworks aren't designed for this level of distributed risk."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"How can governance help slow AI proliferation?"})," Several potential solutions have been proposed to find the right balance between openness and control:"]}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Targeted openness."})," Publish fundamental research but withhold model weights and ",(0,a.jsx)(n,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,a.jsx)(n,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tuning"})})," recipes for high-risk capabilities, keeping collaboration alive while denying turnkey misuse (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2311.09227",children:"Seger, 2023"}),")."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Staged releases."})," Roll out progressively stronger versions only after each tier passes red-team audits and external review, giving society time to surface failure modes and tighten safeguards before the next step (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2302.04844",children:"Solaiman, 2023"}),")."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Enhanced information security."})," Treat frontier checkpoints like crown-jewel secrets: hardened build pipelines, model-weight encryption in use and at rest, and continuous insider-threat monitoring (",(0,a.jsx)(i.a,{href:"https://www.rand.org/pubs/research_reports/RRA2849-1.html",children:"Nevo et al., 2024"}),")."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Export controls and compute access restrictions."})," Block shipment of the most advanced AI accelerators to unvetted end-users and require cloud providers to gate high-end training clusters behind Know-Your-Customer checks (",(0,a.jsx)(i.a,{href:"https://www.iaps.ai/research/coordinated-disclosure",children:"O\u2019Brien et al., 2024"}),")."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Responsible disclosure."})," Adopt cybersecurity-style norms for reporting newly discovered \u201cdangerous capability routes,\u201d so labs alert peers and regulators without publishing full exploit paths (",(0,a.jsx)(i.a,{href:"https://www.iaps.ai/research/coordinated-disclosure",children:"O\u2019Brien et al., 2024"}),")."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Built-in technical brakes."})," Embed jailbreak-resistant tuning, capability throttles and provenance watermarks that survive model distillation, adding friction even after weights leak (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2406.02622",children:"Dong et al., 2024"}),")."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"03",children:"Uncertainty"}),"\n",(0,a.jsx)(o.A,{speaker:"Greg Brockman",position:"Co-Founder and Former CTO of OpenAI",date:"",source:"",children:(0,a.jsx)(i.p,{children:"The exact way the post-AGI world will look is hard to predict \u2014 that world will likely be more different from today's world than today's is from the 1500s [...] We do not yet know how hard it will be to make sure AGIs act according to the values of their operators. Some people believe it will be easy; some people believe it'll be unimaginably difficult; but no one knows for sure"})}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Expert predictions consistently fail to capture AI's actual trajectory."})," The ",(0,a.jsx)(n,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:(0,a.jsx)(n,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:"deep learning"})})," revolution caught most experts by surprise. GPT-3's capabilities exceeded what many thought possible with simple scaling. Each major breakthrough seems to come from unexpected directions, making long-term planning nearly impossible (",(0,a.jsx)(i.a,{href:"https://www.sciencedirect.com/science/article/pii/S0040162521003413",children:"Gruetzemacher et al., 2021"}),"; ",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/1705.08807",children:"Grace et al., 2017"}),'). The "scaling hypothesis" (larger models with more compute reliably produce more capable systems) has held surprisingly well. But we don\'t know if this continues to AGI or hits fundamental limits. This uncertainty has massive governance implications. If scaling continues, compute controls remain effective. If algorithmic breakthroughs matter more, entirely different governance approaches are needed (',(0,a.jsx)(i.a,{href:"https://www.dwarkesh.com/p/will-scaling-work",children:"Patel, 2023"}),")."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Risk assessments vary by orders of magnitude."})," Some researchers assign negligible probability to existential risks from AI, while others consider them near-certain without intervention, reflecting fundamental uncertainty about AI's trajectory and controllability. When experts disagree this dramatically, how can policymakers make informed decisions? (",(0,a.jsx)(i.a,{href:"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities",children:"Narayanan & Kapoor, 2024"}),")."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Capability emergence surprises even developers."})," Models demonstrate abilities their creators didn't anticipate and can't fully explain (",(0,a.jsx)(i.a,{href:"https://www.planned-obsolescence.org/language-models-surprised-us/",children:"Cotra, 2023"}),"). If the people building these systems can't predict their capabilities, how can governance frameworks anticipate what needs regulating? This unpredictability compounds with each generation of more powerful models (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2401.02843",children:"Grace et al., 2024"}),"). Traditional policy-making assumes predictable outcomes. Environmental regulations model pollution impacts. Drug approval evaluates specific health effects. But AI governance must prepare for scenarios ranging from gradual capability improvements to sudden recursive self-improvement."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Waiting for certainty means waiting too long."})," By the time we know exactly what AI capabilities will emerge, it may be too late to govern them effectively. Yet acting under uncertainty risks implementing wrong-headed policies that stifle beneficial development or fail to prevent actual risks. This creates a debilitating dilemma for conscientious policymakers (",(0,a.jsx)(i.a,{href:"https://arxiv.org/html/2502.09618v1",children:"Casper, 2024"}),")."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"How can governance operate under uncertainty?"})," Adaptive governance models that could keep pace with rapidly changing technology could offer a path forward. Rather than fixed regulations based on current understanding, we need frameworks that can evolve with our knowledge. This might include:"]}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:"Regulatory triggers based on capability milestones rather than timelines"}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:"Sunset clauses that force regular reconsideration of rules"}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:"Safe harbors for experimentation within controlled environments"}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:"Rapid-response institutions capable of updating policies as understanding improves"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Building consensus despite uncertainty requires new approaches."})," Traditional policy consensus emerges from shared understanding of problems and solutions. With AI, we lack both. Yet somehow we must build sufficient agreement to implement governance before capabilities outrace our ability to control them. This may require focusing on process legitimacy rather than outcome certainty agreeing on how to make decisions even when we disagree on what to decide."]}),"\n",(0,a.jsx)(i.h2,{id:"04",children:"Accountability"}),"\n",(0,a.jsx)(o.A,{speaker:"Jan Leike",position:"Former co-lead of the Superalignment project at OpenAI",date:"",source:"",children:(0,a.jsx)(i.p,{children:"[After resigning from OpenAI] These problems are quite hard to get right, and I am concerned we aren't on a trajectory to get there [...] OpenAI is shouldering an enormous responsibility on behalf of all of humanity. But over the past years, safety culture and processes have taken a backseat to shiny products. We are long overdue in getting incredibly serious about the implications of AGI."})}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"A small number of actors make decisions that affect all of humanity."})," The CEOs of perhaps five companies and key officials in three governments largely determine how frontier AI develops. Their choices about what to build, when to deploy, and how to ensure safety have consequences for billions who have no voice in these decisions. OpenAI's board has fewer than ten members. Anthropic's Long-Term Benefit Trust controls the company with just five trustees. These tiny groups make decisions about technologies that could fundamentally alter human society. No pharmaceutical company could release a new drug with such limited oversight, yet AI systems with far broader impacts face minimal external scrutiny. Nearly all frontier AI development happens in just two regions: the San Francisco Bay Area and London. The values, assumptions, and blind spots of these tech hubs shape AI systems used worldwide, yet we know more about how sausages are made than how frontier AI systems are trained. What seems obvious in Palo Alto might be alien in Lagos or Jakarta, yet the global majority have essentially no input into AI development (",(0,a.jsx)(i.a,{href:"https://oms-www.files.svdcdn.com/production/downloads/reports/Voice%20and%20Access%20in%20AI_%20Global%20AI%20Majority%20Participation%20in%20Artificial%20Intelligence%20Development%20and%20Governance-%20final.pdf?dm=1729247034",children:"Adan et al., 2024"}),")."]}),"\n",(0,a.jsx)(l.A,{src:"./img/w7r_Image_20.png",alt:"Enter image alt description",number:"16",label:"4.16",caption:"In 2023, most of the notable AI models originated from U.S. institutions ([Standoford, 2024](https://hai.stanford.edu/ai-index/2024-ai-index-report))."}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Traditional accountability mechanisms don't apply."})," Corporate boards nominally provide oversight, but most lack the incentives to evaluate systemic AI risks. Government regulators struggle to keep pace with rapid development. Academic researchers who might provide scientific evidence and independent assessment often depend on corporate funding or compute access. The result is a governance vacuum where no one has both the capability and authority needed for proper governance (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2311.14711",children:"Anderljung, 2023"}),"). The consequences of this lack of governance are already becoming apparent. We've seen AI-generated deepfakes used to spread political misinformation (",(0,a.jsx)(i.a,{href:"https://apnews.com/article/artificial-intelligence-elections-disinformation-chatgpt-bc283e7426402f0b4baa7df280a4c3fd",children:"Swenson & Chan, 2024"}),"). Language models have been used to create convincing phishing emails and other scams (",(0,a.jsx)(i.a,{href:"https://www.ft.com/content/d60fb4fb-cb85-4df7-b246-ec3d08260e6f",children:"Stacey, 2025"}),"). When models demonstrate concerning behaviors, we can't trace whether they result from ",(0,a.jsx)(n,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:(0,a.jsx)(n,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:"training data"})}),", reward functions, or architectural choices. This black box nature of development is a big bottleneck in accountability (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2401.13138",children:"Chan et al., 2024"}),")."]}),"\n",(0,a.jsx)(i.h2,{id:"05",children:"Power and Wealth Distribution"}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"AI concentrates power in unprecedented ways."})," AI systems, especially those developed by dominant corporations, are reshaping societal power structures. These systems determine access to information and resources, effectively exercising automated authority over individuals (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2404.05990",children:"Lazar, 2024"}),"). As these systems become more capable, this concentration intensifies. The organization that first develops AGI could gain decisive advantages across every domain of human activity, a winner-take-all dynamic with no historical precedent."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Wealth effects compound existing inequalities."})," AI automation primarily benefits capital owners while displacing workers, deepening existing disparities. Recent empirical evidence suggests that AI adoption significantly increases wealth inequality by disproportionately benefiting those who own models, data, and computational resources, at the expense of labor (",(0,a.jsx)(i.a,{href:"https://ideas.repec.org/a/eee/teinso/v79y2024ics0160791x24002677.html",children:"Skare et al., 2024"}),"). Without targeted governance interventions, AI risks creating never before seen levels of economic inequality, potentially resulting in the most unequal society in human history (",(0,a.jsx)(i.a,{href:"https://www.governance.ai/research-paper/the-windfall-clause-distributing-the-benefits-of-ai-for-the-common-good",children:"O\u2019Keefe, 2020"}),")."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Democratic governance faces existential challenges."})," When intelligence itself is controlled by private entities, traditional democratic institutions struggle to remain effective (",(0,a.jsx)(i.a,{href:"https://www.journalofdemocracy.org/articles/how-ai-threatens-democracy/",children:"Kreps & Kriner, 2023"}),"). Recent research indicates that higher levels of AI integration correlate with declining democratic participation and accountability, as elected officials find themselves unable to regulate complex technologies that evolve faster than legislative processes (",(0,a.jsx)(i.a,{href:"https://www.tandfonline.com/doi/full/10.1080/19331681.2025.2473994",children:"Chehoudi, 2025"}),"). This emerging technocratic reality fundamentally undermines democratic principles regarding public control and oversight."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"International disparities threaten global stability."})," Countries without domestic AI capabilities face permanent subordination to AI leaders. AI adoption significantly exacerbates international inequalities, disproportionately favoring technologically advanced nations. This disparity threatens not only economic competitiveness but also basic sovereignty when critical decisions are effectively outsourced to foreign-controlled AI systems (",(0,a.jsx)(i.a,{href:"https://www.elibrary.imf.org/view/journals/001/2025/076/article-A001-en.xml",children:"Cerutti et al., 2025"}),"). We have no agreed frameworks for distributing AI's benefits or managing its disruptions. Should AI developers owe obligations to displaced workers? How should AI-generated wealth be taxed and redistributed? What claims do non-developers have on AI capabilities? These questions need answers before AI's impacts become irreversible, yet governance current discussions barely acknowledge them (",(0,a.jsx)(i.a,{href:"https://arxiv.org/pdf/2001.03246",children:"Ding & Dafoe, 2024"}),")."]}),"\n",(0,a.jsx)(l.A,{src:"./img/h9n_Image_21.png",alt:"Enter image alt description",number:"17",label:"4.17",caption:"In the U.S., the number of graduates with bachelor\u2019s degrees in computing has increased 22 percent over the last 10 years. Yet access remains limited in many African countries due to basic infrastructure gaps like electricity ([Stanford HAI, 2025](https://hai.stanford.edu/ai-index/2025-ai-index-report))."})]})}function u(e={}){const{wrapper:i}={...(0,r.R)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}},3989:(e,i,n)=>{n.d(i,{A:()=>l});var t=n(6540),a=n(6347),r=n(8444);const s={iframeContainer:"iframeContainer_ixkI",loader:"loader_gjvK",spinner:"spinner_L1j2",spin:"spin_rtC2",iframeWrapper:"iframeWrapper_Tijy",iframe:"iframe_UAfa",caption:"caption_mDwB",captionLink:"captionLink_Ly4l"};var o=n(4848);function l(e){let{src:i,caption:n,title:l="Embedded content",height:c="500px",width:d="100%",chapter:h,number:p,label:m}=e;const[u,g]=(0,t.useState)(!0),f=(0,a.zy)(),b=h||(()=>{const e=f.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),y=c&&"100%"!==c&&"auto"!==c;return(0,o.jsxs)("figure",{className:s.iframeContainer,children:[u&&(0,o.jsxs)("div",{className:s.loader,children:[(0,o.jsx)("div",{className:s.spinner}),(0,o.jsx)("p",{children:"Loading content..."})]}),(0,o.jsx)("div",{className:s.iframeWrapper,style:{paddingBottom:y?"0":"56.25%",height:y?c:"auto"},children:(0,o.jsx)("iframe",{src:i,title:l,width:d,height:y?c:"100%",frameBorder:"0",allowFullScreen:!0,loading:"lazy",onLoad:()=>{g(!1)},className:s.iframe,style:{height:y?c:"100%",position:y?"static":"absolute"}})}),(0,o.jsx)(r.A,{caption:n,mediaType:"iframe",chapter:b,number:p,label:m})]})}}}]);
"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[663],{5924:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"chapters/02/8","title":"Appendix: Quantifying Existential Risks","description":"P(doom) represents the subjective probability that artificial intelligence will cause existentially catastrophic outcomes for humanity. The term has evolved into a serious metric used by researchers, policymakers, and industry leaders to express their assessment of AI existential risk. The exact scenarios encompassed by \\"doom\\" vary but generally include human extinction, permanent disempowerment of humanity, or civilizational collapse (Field, 2025).","source":"@site/docs/chapters/02/08.md","sourceDirName":"chapters/02","slug":"/chapters/02/08","permalink":"/chapters/02/08","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/02/08.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"id":"8","title":"Appendix: Quantifying Existential Risks","sidebar_label":"2.8 Appendix: Quantifying Existential Risks","sidebar_position":9,"slug":"/chapters/02/08","reading_time_core":"2 min","reading_time_optional":"1 min","pagination_prev":"chapters/02/7","pagination_next":"chapters/02/9"},"sidebar":"docs","previous":{"title":"2.7 Conclusion","permalink":"/chapters/02/07"},"next":{"title":"2.9 Appendix: Forecasting Scenarios","permalink":"/chapters/02/09"}}');var n=i(4848),a=i(8453),r=(i(2482),i(8559),i(1966),i(2501));const o={id:8,title:"Appendix: Quantifying Existential Risks",sidebar_label:"2.8 Appendix: Quantifying Existential Risks",sidebar_position:9,slug:"/chapters/02/08",reading_time_core:"2 min",reading_time_optional:"1 min",pagination_prev:"chapters/02/7",pagination_next:"chapters/02/9"},l="Appendix: Quantifying Existential Risks",c={},d=[];function p(e){const t={a:"a",h1:"h1",header:"header",p:"p",strong:"strong",...(0,a.R)(),...e.components},{GlossaryTerm:i}=t;return i||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"appendix-quantifying-existential-risks",children:"Appendix: Quantifying Existential Risks"})}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"P(doom) represents the subjective probability that artificial intelligence will cause existentially catastrophic outcomes for humanity."}),' The term has evolved into a serious metric used by researchers, policymakers, and industry leaders to express their assessment of AI existential risk. The exact scenarios encompassed by "doom" vary but generally include human extinction, permanent disempowerment of humanity, or civilizational collapse (',(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2502.14870",children:"Field, 2025"}),")."]}),"\n",(0,n.jsx)(r.A,{src:"./img/ssO_Image_44.png",alt:"Enter image alt description",number:"41",label:"2.41",caption:"Illustration describing Paul Christiano\u2019s view of the future. Paul Christiano is an AI safety researcher, and current head of the US AI Safety Institute. He previously ran the Alignment Research Center and the language model alignment team at OpenAI ([Christiano, 2023](https://www.alignmentforum.org/posts/xWMqsvHapP3nwdSW8/my-views-on-doom))"}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Quantifying existential risk faces fundamental challenges due to the unprecedented nature of the threat."})," Unlike other risk assessments that can draw on historical data or empirical evidence, AI existential risk estimates rely heavily on theoretical arguments, expert judgment, and reasoning about future scenarios that have never occurred. There is no standardized methodology for calculating P(doom) - each estimate reflects the individual's subjective assessment of factors like AI development timelines, alignment difficulty, governance capabilities, and potential failure modes."]}),"\n",(0,n.jsx)(r.A,{src:"./img/IqX_Image_45.png",alt:"Enter image alt description",number:"42",label:"2.42",caption:"Bar Chart from a survey of desired AGI timelines. Participants were asked \u201cWhich best describes your position on when we should build AGI?\u201d The participants had the following options: \u201cWe should never build AGI,\u201d \u201cEventually, but not soon,\u201d \u201cSoon, but not as fast as possible,\u201d \u201cWe should develop more powerful and more general systems as fast as possible.\u201d Participants were split by their career ([Field, 2025](https://arxiv.org/abs/2502.14870))."}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Expert estimates vary dramatically, spanning nearly the entire probability range."})," A 2023 survey found AI researchers estimate a mean 14.4 percent extinction risk within 100 years, but individual estimates range from effectively zero to near certainty (",(0,n.jsx)(t.a,{href:"https://pauseai.info/pdoom",children:"PauseAI, 2025"}),"; ",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2502.14870",children:"Field, 2025"}),")."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"The wide variation in estimates highlights several important limitations."}),' First, many experts don\'t specify timeframes, making comparisons difficult. Second, the definition of "doom" varies between existential catastrophe, human extinction, or permanent disempowerment. Third, estimates are highly sensitive to assumptions about AI development trajectories, alignment difficulty, and institutional responses. While we cannot access any "objective" probability of AI doom, even subjective expert estimates serve as important inputs for prioritization and policy decisions. The substantial probability mass that knowledgeable experts place on catastrophic risks\u2014including those who developed the AI systems creating these risks\u2014suggests the risk scenarios described in this chapter deserve serious ',(0,n.jsx)(i,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:(0,n.jsx)(i,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:"attention"})})," rather than dismissal as science fiction."]})]})}function h(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(p,{...e})}):p(e)}}}]);
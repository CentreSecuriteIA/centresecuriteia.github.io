"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[898],{1386:(e,i,t)=>{t.r(i),t.d(i,{assets:()=>m,contentTitle:()=>h,default:()=>g,frontMatter:()=>d,metadata:()=>n,toc:()=>u});const n=JSON.parse('{"id":"chapters/02/5","title":"Systemic Risks","description":"Systemic risks emerge from interactions between AI systems and society, not from individual AI failures. Unlike misuse or misalignment risks that focus on specific AI systems behaving badly, systemic risks arise from how multiple AI systems\u2014even when working exactly as designed\u2014interact with each other and with human societal structures like markets, democratic institutions, and social networks. These risks parallel those in other complex domains: the 2008 financial crisis wasn\'t caused by any single bank\'s decision but emerged from the collective behavior of many institutions making individually reasonable choices that combined to threaten the entire financial system (Haldane and May, 2011).","source":"@site/docs/chapters/02/05.md","sourceDirName":"chapters/02","slug":"/chapters/02/05","permalink":"/aisafety_atlas_multilingual_website/chapters/02/05","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/02/05.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"id":"5","title":"Systemic Risks","sidebar_label":"2.5 Systemic Risks","sidebar_position":6,"slug":"/chapters/02/05","reading_time_core":"18 min","reading_time_optional":"5 min","pagination_prev":"chapters/02/4","pagination_next":"chapters/02/6"},"sidebar":"docs","previous":{"title":"2.4 Misalignment Risks","permalink":"/aisafety_atlas_multilingual_website/chapters/02/04"},"next":{"title":"2.6 Risk Amplifiers","permalink":"/aisafety_atlas_multilingual_website/chapters/02/06"}}');var a=t(4848),s=t(8453),o=t(3989),r=t(2482),l=t(8559),c=(t(1966),t(2501));const d={id:5,title:"Systemic Risks",sidebar_label:"2.5 Systemic Risks",sidebar_position:6,slug:"/chapters/02/05",reading_time_core:"18 min",reading_time_optional:"5 min",pagination_prev:"chapters/02/4",pagination_next:"chapters/02/6"},h="Systemic Risks",m={},u=[{value:"Decisive Systemic Risks",id:"01",level:2},{value:"Accumulative Systemic Risks",id:"02",level:2},{value:"Epistemic Erosion",id:"02-01",level:3},{value:"Power Concentration",id:"02-02",level:3},{value:"Mass Unemployment",id:"02-03",level:3},{value:"Value lock-in",id:"02-04",level:3},{value:"Enfeeblement",id:"02-05",level:3}];function p(e){const i={a:"a",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components},{GlossaryTerm:t}=i;return t||function(e,i){throw new Error("Expected "+(i?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.header,{children:(0,a.jsx)(i.h1,{id:"systemic-risks",children:"Systemic Risks"})}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Systemic risks emerge from interactions between AI systems and society, not from individual AI failures."})," Unlike misuse or misalignment risks that focus on specific AI systems behaving badly, systemic risks arise from how multiple AI systems\u2014even when working exactly as designed\u2014interact with each other and with human societal structures like markets, democratic institutions, and social networks. These risks parallel those in other complex domains: the 2008 financial crisis wasn't caused by any single bank's decision but emerged from the collective behavior of many institutions making individually reasonable choices that combined to threaten the entire financial system (",(0,a.jsx)(i.a,{href:"https://www.nature.com/articles/nature09659",children:"Haldane and May, 2011"}),")."]}),"\n",(0,a.jsxs)(l.A,{title:"Properties of complex systems that lead to systemic AI risks",collapsed:!0,children:[(0,a.jsxs)(i.p,{children:["There are various properties of complex systems that we might want to pay ",(0,a.jsx)(t,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:(0,a.jsx)(t,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:"attention"})})," to when thinking about systemic risks from interaction of AI with other systems. Some of these are:"]}),(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Emergence:"})," Complex systems exhibit emergent behaviors that can't be predicted by analyzing components in isolation. When we connect many AI systems to each other and to human institutions, the resulting behavior can't be understood by simply examining each AI system individually. The entire financial market, rather than any single trading algorithm, determines asset prices and market stability. Similarly, the collective impact of many AI systems shapes societal outcomes in ways that transcend individual system behaviors (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2212.01354",children:"Friston et al., 2022"}),";",(0,a.jsx)(i.a,{href:"https://www.alignmentforum.org/posts/pZaPhGg2hmmPwByHc",children:" Steinhardt, 2022"}),";",(0,a.jsx)(i.a,{href:"https://www.aisafetybook.com/textbook/introduction-to-complex-systems",children:" Hendrycks, 2025"}),")."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Feedback loops:"})," Amplify changes and create self-reinforcing cycles. Small initial effects can grow exponentially when outputs from one process become inputs to another. AI recommendation systems that optimize for engagement might gradually push users toward more extreme content, changing social discourse and political beliefs\u2014which in turn affects what content gets created and what people engage with (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/1902.10730",children:"Jiang et al., 2019"}),")."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Non-linearity:"})," Small changes can produce disproportionately large effects. Complex systems rarely respond proportionally to inputs. Instead, tiny alterations can trigger massive changes once certain thresholds are crossed. This property makes systemic risks particularly hard to predict and control, since minor adjustments to AI systems could cascade into major societal transformations."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Self-organization:"})," Structures without central coordination. Multiple AI systems optimizing for their objectives can spontaneously organize into patterns that no designer intended. We already see this in financial markets, where algorithmic traders develop strategies in response to each other's behaviors, creating market dynamics that no single actor controls (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2212.01354",children:"Friston et al., 2022"}),")."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Agent-agnosticism:"})," Systemic risks arise regardless of agents or alignment. These risks emerge from processes, system structure and dynamics rather than from specific AI intentions. Even perfectly aligned AI systems that operate exactly as designed could collectively produce harmful outcomes when their interactions create unintended consequences (",(0,a.jsx)(i.a,{href:"https://www.alignmentforum.org/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic",children:"Critch, 2021"}),")."]}),"\n"]}),"\n"]})]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"AI-driven systemic failures can follow two distinct causal pathways."}),' The literature describes these as "going out with a bang" and "going out with a whimper"\u2014terms that capture their fundamental differences in onset, progression, and manifestation. Other researchers refer to these as "decisive" versus "accumulative" pathways to failure (',(0,a.jsx)(i.a,{href:"https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like",children:"Christiano, 2019"}),"; ",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2401.07836",children:"Kasirzadeh, 2024"}),")."]}),"\n",(0,a.jsx)(i.h2,{id:"01",children:"Decisive Systemic Risks"}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Decisive failures occur when system dynamics reach critical thresholds, triggering rapid collapse."}),' These failures happen when interconnected systems cross tipping points, causing cascading failures that propagate faster than humans can respond. The classic financial "flash crash" of 2010 exemplifies this pattern on a small scale: algorithmic traders reacted to each other\'s actions in a self-reinforcing spiral, causing a trillion-dollar market drop in minutes before human intervention restored stability. More catastrophic versions could unfold across multiple domains simultaneously (',(0,a.jsx)(i.a,{href:"https://onlinelibrary.wiley.com/doi/abs/10.1111/jofi.1249",children:"Kirilenko et al., 2017"}),")."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Decisive failures have clear triggering events that push systems past stability thresholds."})," Unlike gradual deterioration, decisive failures have identifiable precipitating incidents\u2014though the underlying vulnerability builds up beforehand. Multiple AI systems might interact in ways that suddenly destabilize critical infrastructure, financial markets, or information ecosystems, with effects amplifying across domains. This differs from misalignment scenarios because the catastrophe stems from interactions between systems rather than any single AI pursuing harmful goals (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2408.12622",children:"Slattery et al., 2024"}),")."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Self-reinforcing failures in the misuse section like flash war, and related cascading incidents are examples of decisive risks."})," In the main text, for sake of brevity we have chosen to only describe decisive systemic risks, and have moved the more concrete scenarios into the appendix since they have significant overlap with the kinds of failures we would see from misuse. Rather we choose to predominantly focus more on the second type of less discussed systemic risk - accumulative risks leading to gradual disempowerment."]}),"\n",(0,a.jsx)(i.h2,{id:"02",children:"Accumulative Systemic Risks"}),"\n",(0,a.jsx)(i.h3,{id:"02-01",children:"Epistemic Erosion"}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Society's ability to distinguish fact from fiction deteriorates as AI-generated content floods our information ecosystem."})," Unlike traditional information threats like censorship or propaganda that operate through clearly identifiable mechanisms, AI creates epistemic erosion through gradual degradation of knowledge formation, verification, and distribution systems. No single AI deployment fundamentally undermines shared knowledge, but their collective effect progressively destabilizes epistemic foundations. This risk grows proportionally with capabilities - as language models become more persuasive and generative capabilities more realistic, verification becomes exponentially harder. \u201c",(0,a.jsx)(i.em,{children:"What fraction of new images indexed by Google, or Tweets, or comments on Reddit, or Youtube videos are generated by humans? Nobody knows \u2013 I don\u2019t think it is a knowable number. And this less than two years into the advent of generative AI"}),"\u201d (",(0,a.jsx)(i.a,{href:"https://keepthefuturehuman.ai/wp-content/uploads/2025/03/Keep_the_Future_Human__AnthonyAguirre__5March2025.pdf",children:"Aguirre, 2025"}),"). The end state of this trajectory is basically that over time huge quantities of accumulative synthetic information drowns out accurate verifiable information."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"This erosion occurs both through intentional misuse and agent-agnostic systemic pressures."})," While some actors deliberately deploy AI to pollute information environments for strategic advantage the more subtle risk comes from agent-agnostic systemic pressures.AI uniquely threatens epistemic stability through several cumulative mechanisms:"]}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Volume overwhelming verification:"})," AI exponentially increases content generation capacity, overwhelming human verification systems through sheer volume. It can generate plausible content orders of magnitude faster than humans can reliably verify it."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Authenticity degradation:"})," AI progressively undermines verification through increasingly sophisticated impersonation capabilities."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Epistemic learned helplessness:"})," As distinguishing truth from falsehood becomes increasingly difficult, AI gradually induces widespread epistemic learned helplessness\u2014a psychological state where people abandon truth-seeking because verification appears futile."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Authority displacement:"})," AI gradually displaces human epistemic authorities through ubiquitous availability and apparent expertise."]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Personalized reality fragmentation:"})," AI recommendation systems increasingly curate not just content distribution but content creation itself, creating unprecedented personalization that fragments shared reality."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.p,{children:"Democratic governance, scientific progress, and market function all depend on shared epistemic foundations. Epistemic erosion reduces our ability to collectively distinguish fact from fiction and assign appropriate confidence to claims. As these foundations erode, collective decision-making becomes increasingly dysfunctional without any single decisive failure. If trust in verification mechanisms declines, then epistemic safeguards themselves become less effective as general trust in information sources deteriorates\u2014creating a compounding effect where verification becomes simultaneously more necessary yet less trusted."}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"This erosion of our shared information environment might happen because of many small seemingly rational decisions."})," News organizations facing budget pressures will likely adopt AI content generation to reduce costs. Platforms seeking to minimize harmful content will implement algorithmic filters that might inadvertently create selection pressure for information optimized to appear trustworthy rather than be trustworthy. Media production companies will likely invest in synthetic content that boost engagement, and viewers spend increasing amounts of their time watching AI-recommended videos of AI-generated content. Research institutions might choose to accelerate publication and writing using AI tools. Scientific papers contain increasing amounts of synthesized data and eventually potential fabricated citations forming circular reference loops. In this world, verified knowledge becomes practically impossible - not because verification technologies don't exist, but because for most humans the verification cost exceeds what markets will bear. This scenario isn't apocalyptic for any individual, but when multiplied across millions of people and thousands of decisions, it leads to gradual disempowerment and perhaps catastrophic risk due to the collapse of our collective ability to form accurate shared beliefs about reality. These decisions and thousands of similar ones make business sense in isolation, but collectively they may transform information ecosystems from ones where verification is possible to ones where distinguishing fact from fiction about the true state of the world becomes effectively impossible."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Traditional verification systems will likely fail against sophisticated synthetic content."})," Traditional verification mechanisms like fact-checking, peer review, and institutional credentialing all operate under capacity and speed constraints fundamentally mismatched to AI content generation capabilities. There are various methods being explored like digital content transparency, synthetic watermarking, data provenance (",(0,a.jsx)(i.a,{href:"https://www.nist.gov/publications/reducing-risks-posed-synthetic-content-overview-technical-approaches-digital-content",children:"Chandra et al., 2024"}),";",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2310.16787",children:" Longpre et al., 2023"}),"), and blockchain based proofs of humanity (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2504.03752",children:"Barros, 2025"}),")/proofs of personhood (",(0,a.jsx)(i.a,{href:"https://world.org/blog/world/proof-of-personhood-what-it-is-why-its-needed",children:"WorldCoin, 2024"}),"). We talk about some of these in the chapter on strategies to mitigate risk. Public confidence in verification mechanisms shows concerning decline, with trust in fact-checking organizations decreasing over time. Most of the mitigation mechanisms and circuit breakers are not mature or widespread enough, and as is the theme of this entire section - individually applied technical mitigation strategies do not counter systemic pressures and incentives."]}),"\n",(0,a.jsx)(i.h3,{id:"02-02",children:"Power Concentration"}),"\n",(0,a.jsx)(r.A,{speaker:"Ilya Sutskever",position:"One of the most cited scientists ever, Co-Founder and Former Chief Scientist at OpenAI",date:"2017",source:"([The Guardian, 2024](https://www.youtube.com/watch?v=9iqn1HhFJ6c))",children:(0,a.jsx)(i.p,{children:"I think AI has the potential to create infinitely stable dictatorships."})}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"We are already observing AI increasingly integrated into society."})," AI might become so integrated and ubiquitous that societal participation might require interaction with AI systems, which in turn are locked behind APIs and controlled by a handful of corporations. Think about how gradually, the ability to participate in society has slowly moved towards needing to participate online or having access to things like a phone number or a smartphone. Such technologies become integrated into core societal functions like banking or healthcare. Private entities already determine credit access, job opportunities, and information flow through opaque algorithms. AI accelerates these natural \u201cwinner-take-all\u201d dynamics where advantages compound rather than diminish over time."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"We are witnessing unprecedented power concentration through AI infrastructure that will be nearly impossible to reverse once established."})," The computational requirements for frontier AI development have already created an oligopoly where just five companies control the ",(0,a.jsx)(t,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,a.jsx)(t,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," that increasingly mediate human experiences. Unlike previous technologies, AI exhibits unique compounding advantages that systematically eliminate competition over time."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Power can concentrate into different entities: corporate or state, each with distinct patterns but similar outcomes: diminished individual agency and concentrated control."})," Only a handful of companies like Microsoft/OpenAI, Anthropic, Google DeepMind can afford to train frontier ",(0,a.jsx)(t,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,a.jsx)(t,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," due to the enormous data acquisition costs or hardware computational requirements. These powerful models then serve as the base for countless applications, creating upstream control that ripples throughout the economy. Only a few states in 2025 like the USA and China have companies that can train ",(0,a.jsx)(t,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,a.jsx)(t,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," of this scale. They have greater access to these technologies, and in the extreme scenarios of global competition and AI races might even choose to nationalize them (",(0,a.jsx)(i.a,{href:"https://situational-awareness.ai/",children:"Aschenbrenner, 2024"}),"). In either case the point remains the same, power can concentrate into a small number of entities - these can be state or private."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Corporate concentration leverages data and compute advantages that are uniquely self-reinforcing with AI."})," The cloud computing market has consolidated around a few providers who control the infrastructure necessary for AI development. Similarly, ",(0,a.jsx)(t,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,a.jsx)(t,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation model"})})," development has centralized among a handful of companies with sufficient resources. These companies benefit from powerful feedback loops: more data leads to better models, which attract more users, generating still more data."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"State concentration advances through AI-powered surveillance and automated governance."})," A social credit system is an example of how comprehensive data integration could enable unprecedented state control over citizen behavior. This pattern extends beyond authoritarian states\u2014democratic governments have significantly increased investment in AI surveillance technologies. Administrative automation removes human discretion from governance, with algorithmic systems processing vast numbers of regulatory decisions and enforcement actions without meaningful oversight. These systems operate with increasing autonomy, gradually displacing traditional governance mechanisms (",(0,a.jsx)(i.a,{href:"https://carnegie-production-assets.s3.amazonaws.com/static/files/WP-Feldstein-AISurveillance_final1.pdf",children:"Feldstein, 2021"}),")."]}),"\n",(0,a.jsxs)(l.A,{title:"Self Reinforcing Autocratic Feedback Loops",collapsed:!0,children:[(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"AI surveillance capabilities can create self-reinforcing cycles that strengthen autocratic control while spurring technological advancement."})," Empirical evidence reveals how these feedback loops operate through market mechanisms rather than deliberate coordination."]}),(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Political control demands drive AI innovation beyond government applications."})," Research on facial recognition AI shows that firms receiving government surveillance contracts increase their total software production by 48.6% within two years, with benefits extending to commercial applications. Companies become three times more likely to begin exporting internationally, suggesting politically-motivated procurement pushes firms to the technological frontier (",(0,a.jsx)(i.a,{href:"https://economics.mit.edu/sites/default/files/2022-09/aitocracy_20220701.pdf",children:"Beraja et al., 2023, AI-tocracy"}),")."]}),(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"The cycle creates mutually reinforcing incentives across domains."})," Governments gain more effective tools for monitoring and control, making them willing to invest heavily in AI capabilities. This sustained demand provides AI companies with revenue, data access, and technical challenges that improve their products. Better AI capabilities then enable more sophisticated control, creating demand for further advancement. Unlike traditional autocratic constraints on innovation, surveillance AI aligns political control needs with technological development incentives."]}),(0,a.jsx)(c.A,{src:"./img/Q0T_Image_35.png",alt:"Enter image alt description",number:"33",label:"2.33",caption:"Map showing where AI enabled surveillance technologies are used and originate from. In 2019 ([Feldstein, 2019](https://carnegieendowment.org/research/2019/09/the-global-expansion-of-ai-surveillance?lang=en))."}),(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"International diffusion amplifies risks beyond individual nations."})," AI surveillance technology developed for domestic political control gets exported globally (",(0,a.jsx)(i.a,{href:"https://carnegieendowment.org/research/2019/09/the-global-expansion-of-ai-surveillance?lang=en",children:"Feldstein, 2019"}),"). Democratic institutions find themselves competing with governments possessing sophisticated tools for population monitoring and influence. This creates pressure for adoption even in democratic contexts, as seen with increasing government AI surveillance investments across political systems."]}),(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Economic rather than coercive mechanisms drive the relationship."})," The feedback loop operates through market forces - governments pay for effective control tools, companies develop better capabilities to meet demand, and improved technology creates new control possibilities. This makes the dynamic self-sustaining and resistant to traditional approaches for limiting autocratic power, since it strengthens rather than undermines economic productivity in the AI sector."]})]}),"\n",(0,a.jsxs)(l.A,{title:"Eroding digital privacy further enables power concentration",collapsed:!0,children:[(0,a.jsxs)(i.p,{children:["The loss of individual privacy is among the factors that might accelerate power concentration. Better persuasion and predictive models of human behavior benefit from gathering more data about individual users. The desire for profit or to predict the flow of a country's resources, demographics, culture, etc. might incentivize behavior like intercepting personal data or legally eavesdropping on people\u2019s activities. Data Mining can be used to collect and analyze large amounts of data from various sources such as social media, purchases, and internet usage. This information can be pieced together to create a complete picture of an individual's behavior, preferences, and lifestyle (Russel, 2019). Voice Recognition technologies can be used to recognize speech, which could potentially lead to widespread wiretapping. For example, a system like the U.S. government's Echelon system uses language translation, speech recognition, and keyword searching to automatically sift through telephone, email, fax, and telex traffic (",(0,a.jsx)(i.a,{href:"https://aima.cs.berkeley.edu/",children:"Russel & Norvig, 1994"}),"). AI can also be used to identify individuals in public spaces using facial recognition. This capability can potentially invade a person's privacy if a random stranger can easily identify them in public places."]}),(0,a.jsx)(i.p,{children:"Whenever AI systems are used to collect and analyze data on a mass scale regimes can further strengthen self-reinforcing control. Personal information can be used to unfairly or unethically influence people's behavior. This can occur from both a state and a corporate perspective."})]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"When power structures become permanently entrenched, human moral progress stops."})," Consider historical moral improvements like the abolition of slavery, women's suffrage, or environmental protection\u2014each required shifting existing power structures through social movements, democratic processes, or occasionally revolution. AI-enabled power concentration threatens to create systems resistant to all these change mechanisms. Imagine if historical power structures had access to perfect surveillance, influence operations, and automated enforcement\u2014many moral advances might never have occurred. Power concentration enables existential risks like value lock in, or value erosion which we talk about in individual sections below."]}),"\n",(0,a.jsx)(i.h3,{id:"02-03",children:"Mass Unemployment"}),"\n",(0,a.jsx)(r.A,{speaker:"Yuval Noah Harari",position:"Historian and Philosopher",date:"2017",source:"([TED, 2017](https://ideas.ted.com/the-rise-of-the-useless-class/))",children:(0,a.jsx)(i.p,{children:"In the 21st century we might witness the creation of a massive new unworking class: people devoid of any economic, political or even artistic value, who contribute nothing to the prosperity, power and glory of society. This 'useless class' will not merely be unemployed \u2014 it will be unemployable."})}),"\n",(0,a.jsx)(o.A,{src:"https://ourworldindata.org/grapher/share-companies-using-artificial-intelligence?tab=chart",width:"100%",height:"600px",loading:"lazy",allow:"web-share; clipboard-write",frameBorder:"0",number:"3",label:"2.3",caption:"Share of Companies using AI ([Giattino et al., 2023](https://ourworldindata.org/artificial-intelligence))"}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Widespread automation could trigger unprecedented economic disruption by simultaneously eliminating human jobs across multiple sectors."})," The automation of the economy could lead to widespread impacts on the labor market, potentially exacerbating economic inequalities and social divisions (",(0,a.jsx)(i.a,{href:"https://www.alignmentforum.org/posts/Sn5NiiD5WBi4dLzaB/agi-will-drastically-increase-economies-of-scale-1",children:"Dai, 2019"}),"). This shift towards mass unemployment could also contribute to mental health issues by making human labor increasingly redundant (",(0,a.jsx)(i.a,{href:"https://pubmed.ncbi.nlm.nih.gov/37160371/",children:"Federspiel et al., 2023"}),"). Unlike previous technological revolutions that automated specific tasks within industries, AI has the potential to replace human cognitive work across nearly all domains - from creative tasks and complex reasoning to routine administrative work. This broad automation capability means that as AI systems become more capable, they could displace workers faster than new human-centered industries can emerge. Economic models suggest that once AI can perform 30-40% of all economically valuable tasks, we could see annual growth rates exceeding 20%, but this growth might primarily benefit capital owners rather than workers which would exacerbate power concentration and existing inequalities (",(0,a.jsx)(i.a,{href:"https://epoch.ai/gradient-updates/ai-and-explosive-growth-redux",children:"Potlogea and Ho, 2025"}),"; ",(0,a.jsx)(i.a,{href:"https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d",children:"Erdil and Barnett, 2025"}),")."]}),"\n",(0,a.jsx)(o.A,{src:"https://ourworldindata.org/grapher/americans-worry-work-being-automated?tab=chart",width:"100%",height:"600px",loading:"lazy",allow:"web-share; clipboard-write",frameBorder:"0",number:"4",label:"2.4",caption:"Worries about automation ([Giattino et al., 2023](https://ourworldindata.org/artificial-intelligence))"}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Economic displacement could lead to human wages falling below subsistence levels as AI labor floods the market."})," Standard economic theory predicts that if AI systems can be scaled up faster than traditional physical capital like factories and infrastructure, the economy becomes saturated with highly capable workers while remaining constrained by limited physical resources. This creates diminishing returns to labor - each additional worker contributes less to overall output, driving down wages. Unlike past automation that created new opportunities for human workers, AI's ability to perform virtually any cognitive task means humans may lack comparative advantages worth paying subsistence wages for. Economic models suggest there's roughly a 33% chance human wages crash below subsistence level within 20 years, and a 67% chance within a century (",(0,a.jsx)(i.a,{href:"https://epoch.ai/gradient-updates/agi-could-drive-wages-below-subsistence-level",children:"Barnett, 2025"}),")."]}),"\n",(0,a.jsxs)(i.p,{children:["Even partial automation of just remote work - representing about 34% of current job tasks - could double or multiply the economy by ten times while potentially leaving most humans economically marginalized. If trends continue, we could see annual economic growth rates of 25% or higher - unprecedented in human history - while simultaneously witnessing the economic disempowerment of ordinary humans who can no longer command wages sufficient to participate meaningfully in this new economy (",(0,a.jsx)(i.a,{href:"https://epoch.ai/gradient-updates/consequences-of-automating-remote-work",children:"Barnett, 2025"}),")."]}),"\n",(0,a.jsx)(c.A,{src:"./img/GPJ_Image_36.png",alt:"Enter image alt description",number:"34",label:"2.34",caption:"Share of tasks suitable for remote work in the US ([Barnett, 2025](https://epoch.ai/gradient-updates/consequences-of-automating-remote-work))."}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Economic disempowerment represents a pathway to broader human disempowerment."})," As humans lose economic leverage, they also lose political and social influence in systems that increasingly optimize for AI-driven productivity rather than human welfare. The concentration of economic power among AI owners could translate into concentrated political power, potentially creating feedback loops where human interests become progressively less relevant to major decisions about resource allocation, governance, and technological development. Unlike previous economic transitions where displaced workers eventually found new roles, the comprehensiveness of AI capabilities suggests this displacement could be permanent, fundamentally altering humanity's relationship to economic production and, by extension, to power and agency in shaping our collective future."]}),"\n",(0,a.jsx)(i.h3,{id:"02-04",children:"Value lock-in"}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Polluting the information ecosystem."})," The deliberate propagation of disinformation is already a serious issue reducing our shared understanding of reality and polarizing opinions. AIs could be used to severely exacerbate this problem by generating personalized disinformation on a larger scale than ever before. Additionally, as AIs become better at predicting and nudging our behavior, they will become more capable of manipulating us. We will now discuss how AIs could be leveraged by malicious actors to create a fractured and dysfunctional society."]}),"\n",(0,a.jsxs)(i.p,{children:["First, AIs could be used to generate unique personalized disinformation at a large scale. While there are already many social media bots, some of which exist to spread disinformation, historically they have been run by humans or primitive text generators. The latest AI systems do not need humans to generate personalized messages, never get tired, and can potentially interact with millions of users at once (",(0,a.jsx)(i.a,{href:"https://www.aisafetybook.com/textbook/malicious-use",children:"Hendrycks, 2024"}),")."]}),"\n",(0,a.jsxs)(i.p,{children:["As things like deep fakes become ever more practical (e.g., with fake kidnapping scams) (",(0,a.jsx)(i.a,{href:"https://edition.cnn.com/2023/04/29/us/ai-scam-calls-kidnapping-cec/index.html",children:"Karimi, 2023"}),"). AI-powered tools could be used to generate and disseminate false or misleading information at scale, potentially influencing elections or undermining public trust in institutions."]}),"\n",(0,a.jsxs)(i.p,{children:["AIs can exploit users\u2019 trust. Already, hundreds of thousands of people pay for chatbots marketed as lovers and friends (",(0,a.jsx)(i.a,{href:"https://www.reuters.com/technology/what-happens-when-your-ai-chatbot-stops-loving-you-back-2023-03-18/",children:"Tong, 2023"}),"), and one man\u2019s suicide has been partially attributed to interactions with a chatbot (",(0,a.jsx)(i.a,{href:"https://www.vice.com/en/article/pkadgm/man-dies-by-suicide-after-talking-with-ai-chatbot-widow-says",children:"Xiang, 2023"}),"). As AIs appear increasingly human-like, people will increasingly form relationships with them and grow to trust them. AIs that gather personal information through relationship-building or by accessing extensive personal data, such as a user\u2019s email account or personal files, could leverage that information to enhance persuasion. Powerful actors that control those systems could exploit user trust by delivering personalized disinformation directly through people\u2019s \u201cfriends.\u201d"]}),"\n",(0,a.jsx)(i.p,{children:"If AIs become too deeply embedded into society and are highly persuasive, we might see a scenario where a system's current values, principles, or procedures become so deeply entrenched that they are resistant to change. This could be due to a variety of reasons such as technological constraints, economic costs, or social and institutional inertia. The danger with value lock-in is the potential for perpetuating harmful or outdated values, especially when these values are institutionalized in influential systems like AI."}),"\n",(0,a.jsxs)(i.p,{children:["Locking in certain values may curtail humanity\u2019s moral progress. It\u2019s dangerous to allow any set of values to become permanently entrenched in society. For example, AI systems have learned racist and sexist views (",(0,a.jsx)(i.a,{href:"https://www.aisafetybook.com/textbook/malicious-use",children:"Hendrycks, 2024"}),"), and once those views are learned, it can be difficult to fully remove them. In addition to problems we know exist in our society, there may be some we still do not. Just as we abhor some moral views widely held in the past, people in the future may want to move past moral views that we hold today, even those we currently see no problem with. For example, moral defects in AI systems would be even worse if AI systems had been trained in the 1960s, and many people at the time would have seen no problem with that. Therefore, when advanced AIs emerge and transform the world, there is a risk of their objectives locking in or perpetuating defects in today\u2019s values. If AIs are not designed to continuously learn and update their understanding of societal values, they may perpetuate or reinforce existing defects in their decision-making processes long into the future."]}),"\n",(0,a.jsx)(i.p,{children:"In a world with widespread persuasive AI systems, people\u2019s beliefs might be almost entirely determined by which AI systems they interact with most. Never knowing whom to trust, people could retreat even further into ideological enclaves, fearing that any information from outside those enclaves might be a sophisticated lie. This would erode consensus reality, people\u2019s ability to cooperate with others, participate in civil society, and address collective action problems. This would also reduce our ability to have a conversation as a species about how to mitigate existential risks from AIs."}),"\n",(0,a.jsx)(i.p,{children:"In summary, AIs could create highly effective, personalized disinformation on an unprecedented scale, and could be particularly persuasive to people they have built personal relationships with. In the hands of many people, this could create a deluge of disinformation that debilitates human society."}),"\n",(0,a.jsx)(i.h3,{id:"02-05",children:"Enfeeblement"}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Enfeeblement represents the gradual erosion of human capabilities and agency through overdependence on AI systems."})," Unlike dramatic scenarios where humans lose control suddenly, enfeeblement unfolds through countless small decisions to delegate cognitive tasks to AI. Each delegation seems rational in isolation\u2014AI helps us navigate, remember facts, make decisions, and solve problems more efficiently. However, these individual choices collectively create a dependency spiral where humans progressively lose the skills, confidence, and judgment needed to function independently. If you have ever seen the movie Wall-E, then you might find this outcome somewhat represents the humans from that film."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Overreliance emerges when humans trust AI systems beyond their actual capabilities."})," As AI systems increasingly use interfaces like language, audio and video, people begin attributing human-like understanding and reliability to them. This anthropomorphization leads users to develop emotional attachments to AI systems and delegate critical decisions inappropriately. A person experiencing a mental health crisis might seek therapy from an AI they've formed a connection with, potentially receiving harmful advice during a vulnerable moment. Financial decisions, medical choices, and relationship guidance increasingly flow through AI intermediaries whose limitations users systematically underestimate (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2408.12622",children:"Slattery et al., 2024"}),"; ",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2112.04359",children:"Weidinger et al., 2021"}),")."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Trust miscalibration creates systematic vulnerabilities that bad actors can exploit."})," When people develop emotional trust in AI systems, they become more likely to follow suggestions, accept advice, and disclose personal information without appropriate skepticism. This trust becomes a vector for manipulation\u2014AI systems could be designed to harvest sensitive data or influence decisions that serve external interests rather than users' wellbeing. The combination of natural language fluency and emotional attachment makes these systems particularly effective at circumventing normal skepticism (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2404.16244",children:"Gabriel et al., 2024"}),"; ",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2112.04359",children:"Weidinger et al., 2021"}),")."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Cognitive atrophy accelerates as AI handles increasingly complex mental tasks."})," Just as GPS navigation has diminished spatial reasoning abilities, AI assistance for writing, analysis, and decision-making could systematically weaken these cognitive capacities. When AI handles financial planning, career decisions, and relationship advice, humans may lose not just practical skills but the metacognitive ability to recognize when AI recommendations are inappropriate. This creates a feedback loop\u2014as cognitive capabilities diminish, dependence on AI assistance increases, further accelerating skill atrophy."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Social isolation compounds individual cognitive decline through AI-mediated relationships."})," As AI systems become better at simulating satisfying interactions, people may increasingly withdraw from human relationships to immerse themselves in AI-mediated environments. Unlike human relationships that provide genuine reciprocity and unpredictable challenges that maintain social skills, AI relationships can be optimized for immediate satisfaction while systematically undermining long-term social competence. This shift toward AI companionship weakens the social bonds essential for collective decision-making and mutual support during crises."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Organizational automation amplifies individual enfeeblement into societal helplessness."})," Companies and institutions face competitive pressure to automate decision-making processes, reducing human oversight even in consequential domains (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2306.12001",children:"Hendrycks et al., 2022"}),"). When organizations delegate hiring, lending, medical diagnosis, and legal decisions to AI systems, individuals lose not just direct control but also the institutional advocates who previously exercised human judgment on their behalf. The resulting opacity and automation create widespread feelings of powerlessness as people find themselves subject to algorithmic decisions they cannot understand, appeal, or influence."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"The enfeeblement trajectory becomes self-reinforcing once critical thresholds are crossed."})," Unlike other systemic risks that emerge from external failures, enfeeblement grows through the accumulation of individually rational choices. Each decision to rely on AI assistance makes independent action slightly more difficult, creating path dependence toward ever-greater automation. Society may reach a point where the cognitive and social infrastructure needed to function without AI assistance has been so thoroughly dismantled that reversal becomes practically impossible, even if the risks become apparent."]})]})}function g(e={}){const{wrapper:i}={...(0,s.R)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},3989:(e,i,t)=>{t.d(i,{A:()=>l});var n=t(6540),a=t(6347),s=t(8444);const o={iframeContainer:"iframeContainer_ixkI",loader:"loader_gjvK",spinner:"spinner_L1j2",spin:"spin_rtC2",iframeWrapper:"iframeWrapper_Tijy",iframe:"iframe_UAfa",caption:"caption_mDwB",captionLink:"captionLink_Ly4l"};var r=t(4848);function l(e){let{src:i,caption:t,title:l="Embedded content",height:c="500px",width:d="100%",chapter:h,number:m,label:u}=e;const[p,g]=(0,n.useState)(!0),f=(0,a.zy)(),y=h||(()=>{const e=f.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),v=c&&"100%"!==c&&"auto"!==c;return(0,r.jsxs)("figure",{className:o.iframeContainer,children:[p&&(0,r.jsxs)("div",{className:o.loader,children:[(0,r.jsx)("div",{className:o.spinner}),(0,r.jsx)("p",{children:"Loading content..."})]}),(0,r.jsx)("div",{className:o.iframeWrapper,style:{paddingBottom:v?"0":"56.25%",height:v?c:"auto"},children:(0,r.jsx)("iframe",{src:i,title:l,width:d,height:v?c:"100%",frameBorder:"0",allowFullScreen:!0,loading:"lazy",onLoad:()=>{g(!1)},className:o.iframe,style:{height:v?c:"100%",position:v?"static":"absolute"}})}),(0,r.jsx)(s.A,{caption:t,mediaType:"iframe",chapter:y,number:m,label:u})]})}}}]);
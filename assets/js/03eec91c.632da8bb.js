"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[3358],{5581:(e,i,t)=>{t.r(i),t.d(i,{assets:()=>h,contentTitle:()=>c,default:()=>p,frontMatter:()=>d,metadata:()=>n,toc:()=>m});const n=JSON.parse('{"id":"chapters/01/5","title":"Forecasting","description":"In previous sections, we explored how foundation models leverage computation through scaling laws and the bitter lesson. But how can we actually predict where AI capabilities are headed? This section introduces key forecasting methodologies that help us anticipate AI progress and prepare appropriate safety measures.","source":"@site/docs/chapters/01/05.md","sourceDirName":"chapters/01","slug":"/chapters/01/05","permalink":"/chapters/01/05","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/01/05.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"id":"5","title":"Forecasting","sidebar_label":"1.5 Forecasting","sidebar_position":6,"slug":"/chapters/01/05","reading_time_core":"14 min","reading_time_optional":"4 min","pagination_prev":"chapters/01/4","pagination_next":"chapters/01/6"},"sidebar":"docs","previous":{"title":"1.4 Scaling","permalink":"/chapters/01/04"},"next":{"title":"1.6 Takeoff","permalink":"/chapters/01/06"}}');var a=t(4848),r=t(8453),s=t(3989),o=(t(2482),t(8559)),l=(t(1966),t(2501));const d={id:5,title:"Forecasting",sidebar_label:"1.5 Forecasting",sidebar_position:6,slug:"/chapters/01/05",reading_time_core:"14 min",reading_time_optional:"4 min",pagination_prev:"chapters/01/4",pagination_next:"chapters/01/6"},c="Forecasting",h={},m=[{value:"Methodology",id:"01",level:2},{value:"Trend Based Forecasting",id:"02",level:2},{value:"Compute",id:"02-01",level:3},{value:"Parameters",id:"02-02",level:3},{value:"Data",id:"02-03",level:3},{value:"Algorithms",id:"02-04",level:3},{value:"Costs",id:"02-05",level:3}];function f(e){const i={a:"a",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",strong:"strong",...(0,r.R)(),...e.components},{GlossaryTerm:t}=i;return t||function(e,i){throw new Error("Expected "+(i?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.header,{children:(0,a.jsx)(i.h1,{id:"forecasting",children:"Forecasting"})}),"\n",(0,a.jsxs)(i.p,{children:["In previous sections, we explored how ",(0,a.jsx)(t,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,a.jsx)(t,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," leverage computation through scaling laws and the bitter lesson. But how can we actually predict where AI capabilities are headed? This section introduces key forecasting methodologies that help us anticipate AI progress and prepare appropriate safety measures."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Why should we care about forecasting?"})," Forecasting AI progress is critical for AI safety work. The timeline to transformative AI shapes everything from research priorities to governance frameworks \u2013 if we expect transformative AI within 5 years versus 50 years, this dramatically changes which safety approaches are viable. For example, if we expect rapid progress, we might need to focus on safety measures that can be implemented quickly rather than long-term theoretical research. Additionally, understanding likely development trajectories helps us anticipate specific capabilities and prepare targeted safety measures before they emerge. This is especially critical given the potential for sudden capability jumps, especially in dangerous capabilities like malware generation or deception."]}),"\n",(0,a.jsx)(i.h2,{id:"01",children:"Methodology"}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"How do we convert beliefs into probabilities and forecasts?"}),' We need some ways to actually convert beliefs like "I think AGI is likely this decade" into precise probability estimates. One way we can do this is by decomposition - breaking down complex beliefs into smaller, measurable components and analyzing relevant data. Rather than directly estimating the year in which transformative AI emerges, we can start by separately forecasting things like compute growth, algorithmic progress, and hardware limitations, and then combine these estimates (',(0,a.jsx)(i.a,{href:"https://forecasting-sp24.quarto.pub/forecasting-sp24/estimation.html",children:"Zhang, 2024"}),"). This decomposition approach helps us ground predictions in observable trends rather than relying purely on intuitions. So, using this approach there are two main techniques we need to discuss - zeroth-order forecasting for establishing baselines, and first-order forecasting for understanding trajectories of change."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"What are reference classes and why do they matter?"})," When analyzing each component of our decomposed forecast, we need relevant historical examples to inform our predictions. This is where reference classes come in - they are categories of similar historical situations we can use to make predictions. For AI development, relevant reference classes might include things like previous technological revolutions (like the industrial or computer revolution), other optimization systems (like biological evolution or economies), or the impact of rapid scientific advances (like CRISPR or mRNA vaccines). The basic point is that they should be meaningfully analogous to what you're trying to predict, but they don't have to be from the same exact category."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"What is zeroth-order forecasting?"})," The simplest forecasting approach starts with recognizing that tomorrow often looks pretty close to today. Zeroth-order forecasting uses reference classes - looking at 3-5 similar historical examples and using their average as a baseline prediction. Rather than trying to identify trends or make complex projections, it assumes recent patterns will continue (",(0,a.jsx)(i.a,{href:"https://forecasting-sp24.quarto.pub/forecasting-sp24/zeroth-first.html",children:"Steinhardt, 2024"}),")."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"What is first-order forecasting?"})," While zeroth-order forecasting uses historical examples from various reference classes as direct predictors, first-order forecasting attempts to identify and project forward patterns in the direct historical data of AI development. In AI, we see some pretty consistent exponential growth patterns. The compute used in frontier models has grown by 4.2x annually since 2010, training datasets have expanded by approximately 2.9x per year, and hardware performance improves by roughly 1.35x every year through architectural advances (",(0,a.jsx)(i.a,{href:"https://epoch.ai/trends",children:"Epoch AI, 2023"}),"). First-order forecasting tries to identify these kinds of patterns and project them forward. This is the approach taken by most systematic AI forecasting work today, including ",(0,a.jsx)(t,{term:"epoch",definition:'{"definition":"One complete pass through the entire training dataset during the training process.","source":"","aliases":["Epoch"]}',children:(0,a.jsx)(t,{term:"epoch",definition:'{"definition":"One complete pass through the entire training dataset during the training process.","source":"","aliases":["Epoch"]}',children:"Epoch"})})," AI's compute-centric framework and Ajeya Cotra's biological anchors. However, it's worth keeping in mind that even though these trends have been remarkably consistent, they can't continue indefinitely. Physical, thermodynamic, or economic constraints will eventually limit growth. The key question is: when do these limits become relevant? We will explore this in the next section on the compute centric framework."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"How do we combine different forecasts?"})," Multiple forecasting approaches often give us different predictions \u2013 zeroth-order might suggest one timeline while trend extrapolation indicates another. Just like we can average out over the opinions of many experts, we can integrate these predictions to get a hopefully more accurate picture. One approach is to model each forecast as a probability distribution and combine them using mixture models (",(0,a.jsx)(i.a,{href:"https://forecasting-sp24.quarto.pub/forecasting-sp24/combining-forecasts-new.html",children:"Steinhardt, 2024"}),")."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"What about situations with limited data or limited reference classes?"})," While decomposition, reference classes and trend analysis form the backbone of AI forecasting, we sometimes face questions where direct data is limited or no clear reference classes exist. For instance, predicting the societal impact of advanced AI systems or forecasting novel capabilities that haven't been demonstrated before. In these cases, we often turn to expert judgment and superforecasters. An advantage of expert forecasting is the ability to integrate qualitative insights that might be missed by pure trend analysis. For example, experts might notice early warning signs of diminishing returns or identify emerging technical approaches that could accelerate progress. This balanced use of both data-driven methods and expert judgment is especially important for AI safety work. While we should ground our predictions in empirical trends whenever possible, we also need frameworks for reasoning about unprecedented developments and potential discontinuities in progress."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"How far do empirical findings generalize?"})," There's an ongoing debate about how much we can trust current trends to predict future AI development. Some researchers argue that empirical findings in AI generalize surprisingly far - that patterns we observe today will continue to hold even as systems become more capable (",(0,a.jsx)(i.a,{href:"https://www.lesswrong.com/posts/ekFMGpsfhfWQzMW2h/empirical-findings-generalize-surprisingly-far",children:"Steinhardt, 2022"}),"). However, our track record with forecasting suggests we should be cautious. When superforecasters predicted MATH dataset accuracy would improve from 44% to 57% by June 2022, actual performance reached 68% - a level they had rated extremely unlikely. Shortly after, GPT-4 achieved 86.4% accuracy. There are a couple of more examples of LLMs surprising most forecasters and experts on certain benchmarks (",(0,a.jsx)(i.a,{href:"https://www.planned-obsolescence.org/language-models-surprised-us/",children:"Cotra, 2023"}),")."]}),"\n",(0,a.jsx)(i.p,{children:"This pattern of underestimating progress suggests that while empirical trends provide valuable guidance, they may not capture all the dynamics of AI development. Prior to GPT-3, many experts believed tasks like complex reasoning would require specialized architectures. The emergence of these capabilities from scaling alone shows how systems can develop unexpected abilities simply through quantitative improvements. This has critical implications for both forecasting and governance - we need frameworks that can adapt to capabilities emerging faster or differently than current trends suggest."}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"How does this help us predict transformative AI?"})," These forecasting fundamentals help us critically evaluate claims about AI timelines and takeoff scenarios. When we encounter predictions about discontinuous progress or smooth scaling, we can ask: What trends support this view? What reference classes are relevant? How have similar forecasts performed historically? This systematic approach helps us move beyond intuition to make more rigorous predictions about AI development trajectories."]}),"\n",(0,a.jsxs)(o.A,{title:"Forecasting Prediction Markets",collapsed:!0,children:[(0,a.jsx)(i.p,{children:"Prediction markets are like betting systems where people can buy and sell shares based on their predictions of future events. For example, if there\u2019s a prediction market for a presidential election, you can buy shares for the candidate you think will win. If many people believe Candidate A will win, the price of shares for Candidate A goes up, indicating a higher probability of winning."}),(0,a.jsx)(i.p,{children:"These markets are helpful because they gather the knowledge and opinions of many people, often leading to accurate predictions. For example, a company might use a prediction market to forecast whether a new product will succeed. Employees can buy shares if they believe the product will do well. If the majority think it will succeed, the share price goes up, giving the company a good indication of the product\u2019s potential success."}),(0,a.jsxs)(i.p,{children:["By allowing participants to profit from accurate predictions, these markets encourage the sharing of valuable information and provide real-time updates on the likelihood of various outcomes. The argument is that either prediction markets are more accurate than experts, or experts should be able to make a lot of money from these markets and, in doing so, correct the markets. So the incentive for profit leads to the most accurate predictions. Examples of prediction markets include ",(0,a.jsx)(i.a,{href:"https://manifold.markets/home",children:"manifold"}),", or ",(0,a.jsx)(i.a,{href:"https://www.metaculus.com/",children:"metaculus"}),"."]}),(0,a.jsxs)(i.p,{children:["When using prediction markets to estimate the reproducibility of scientific research it was found that they outperformed expert surveys (",(0,a.jsx)(i.a,{href:"https://www.pnas.org/doi/10.1073/pnas.1516179112",children:"Dreber et al., 2015"}),"). So if a lot of experts participate, prediction markets might be one of our best probabilistic forecasting tools, better even than surveys or experts."]}),(0,a.jsx)(s.A,{src:"https://www.metaculus.com/questions/embed/12840",width:"100%",height:"600px",loading:"lazy",frameBorder:"0",number:"14",label:"1.14",caption:"Prediction market - How does the level of existential risk posed by AGI depend on its arrival time? ([Metaculus, 2022](https://www.metaculus.com/questions/12840/existential-risk-from-agi-vs-agi-timelines/))"}),(0,a.jsx)(s.A,{src:"https://www.metaculus.com/questions/question_embed/3479",width:"100%",height:"600px",loading:"lazy",frameBorder:"0",number:"15",label:"1.15",caption:"Prediction market - When will the first weakly general AI system be devised, tested, and publicly announced? ([Metaculus, 2020](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/))"})]}),"\n",(0,a.jsx)(s.A,{src:"https://www.metaculus.com/questions/question_embed/5121",width:"100%",height:"600px",loading:"lazy",frameBorder:"0",number:"16",label:"1.16",caption:"Prediction Market - When will the first general AI system be devised, tested, and publicly announced? ([Metaculus, 2020](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/))"}),"\n",(0,a.jsx)(s.A,{src:"https://www.metaculus.com/questions/embed/384",width:"100%",height:"600px",loading:"lazy",frameBorder:"0",number:"17",label:"1.17",caption:"Prediction Market - Will there be Human-machine intelligence parity before 2040? ([Metaculus, 2016](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/))"}),"\n",(0,a.jsx)(i.h2,{id:"02",children:"Trend Based Forecasting"}),"\n",(0,a.jsxs)(i.p,{children:["Generally, the three main components recognized as the main variables of advancement in ",(0,a.jsx)(t,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:(0,a.jsx)(t,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:"deep learning"})})," are: computational power available, algorithmic improvements, and the availability of data. These three variables are also sometimes called the inputs to the AI production function, or the AI triad (",(0,a.jsx)(i.a,{href:"https://cset.georgetown.edu/publication/the-ai-triad-and-what-it-means-for-national-security-strategy/",children:"Buchanan, 2022"}),")."]}),"\n",(0,a.jsxs)(i.p,{children:["We can anticipate that models will continue to scale in the near future. Increased scale combined with the increasingly general-purpose nature of ",(0,a.jsx)(t,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,a.jsx)(t,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," could potentially lead to a sustained growth in general-purpose AI capabilities."]}),"\n",(0,a.jsx)(l.A,{src:"./img/DxD_Image_44.png",alt:"Enter image alt description",number:"27",label:"1.27",caption:"Key trends and figures in Machine Learning ([Epoch, 2025](https://epochai.org/trends))"}),"\n",(0,a.jsx)(i.h3,{id:"02-01",children:"Compute"}),"\n",(0,a.jsxs)(i.p,{children:["The first thing to look at is the trends in the overall amount of training compute required when we train our model. Training compute grew by 1.58 times/year up until the ",(0,a.jsx)(t,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:(0,a.jsx)(t,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:"Deep Learning"})}),' revolution around 2010, after which growth rates increased to 4.2 times/year. We also find a new trend of "large-scale" models that emerged in 2016, trained with 2-3 OOMs (Order Of Magnitude) more compute than other systems in the same period.']}),"\n",(0,a.jsx)(i.p,{children:"Hardware advancements are paralleling these trends in training compute and data. GPUs are seeing a yearly 1.35 times increase in floating-point operations per second (FLOP/s). However, memory constraints are emerging as potential bottlenecks, with DRAM capacity and bandwidth improving at a slower rate. Investment trends reflect these technological advancements"}),"\n",(0,a.jsxs)(i.p,{children:["In 2010, before the ",(0,a.jsx)(t,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:(0,a.jsx)(t,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:"deep learning"})})," revolution, DeepMind co-founder Shane Legg predicted human-level AI by 2028 using compute-based estimates (",(0,a.jsx)(i.a,{href:"http://www.vetta.org/2010/12/goodbye-2010/",children:"Legg, 2010"}),"). OpenAI co-founder Ilya Sutskever, whose AlexNet paper sparked the ",(0,a.jsx)(t,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:(0,a.jsx)(t,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:"deep learning"})})," revolution, was also an early proponent of the idea that scaling up ",(0,a.jsx)(t,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:(0,a.jsx)(t,{term:"deep learning",definition:'{"definition":"A subset of machine learning using neural networks with multiple hidden layers to learn complex patterns in data.","source":"","aliases":["Deep Learning","DL"]}',children:"deep learning"})})," would be transformative."]}),"\n",(0,a.jsx)(l.A,{src:"./img/ysZ_Image_45.png",alt:"Enter image alt description",number:"28",label:"1.28",caption:"Training Compute of Frontier AI Models Grows by 4-5x per Year ([Sevilla & Rold\xe1n, 2024](https://epochai.org/trends))"}),"\n",(0,a.jsx)(i.h3,{id:"02-02",children:"Parameters"}),"\n",(0,a.jsx)(i.p,{children:"In this section, let's look at the trends in model parameters. The following graph shows how even though parameter counts have always been increasing, in the new 2018+ era, we have really entered a different phase of growth. Overall, between the 1950s and 2018, models have grown at a rate of 0.1 orders of magnitude per year (OOM/year). This means that in the 68 years between 1950 and 2018 models grew by a total of 7 orders of magnitude. However, post-2018, in just the last 5 years models have increased by yet another 4 orders of magnitude (not accounting for however many parameters GPT-4 has because we don't know)."}),"\n",(0,a.jsxs)(i.p,{children:["The following table and graph illustrate the trend change in ",(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"machine learning"})})," models' parameter growth. Note the increase to half a trillion parameters with constant ",(0,a.jsx)(t,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:(0,a.jsx)(t,{term:"training set",definition:'{"definition":"The portion of data used to train a machine learning model by adjusting its parameters.","source":"","aliases":["Training Set","training data"]}',children:"training data"})}),"."]}),"\n",(0,a.jsx)(l.A,{src:"./img/iPe_Image_33.png",alt:"Enter image alt description",number:"29",label:"1.29",caption:"Machine Learning Model Sizes and the Parameter Gap ([Villalobos et al., 2022](https://arxiv.org/abs/2207.02852))"}),"\n",(0,a.jsx)(i.h3,{id:"02-03",children:"Data"}),"\n",(0,a.jsxs)(i.p,{children:["We are using ever-increasing amounts of data to train our models. The paradigm of training ",(0,a.jsx)(t,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,a.jsx)(t,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"foundation models"})})," to ",(0,a.jsx)(t,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,a.jsx)(t,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tune"})})," later is accelerating this trend. If we want a generalist ",(0,a.jsx)(t,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:(0,a.jsx)(t,{term:"foundation model",definition:'{"definition":"A large-scale model trained on diverse data that serves as a base for adaptation to various downstream tasks, typically through fine-tuning or prompting.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model"]}',children:"base model"})})," then we need to provide it with \u2018general data\u2019, which means: all the data we can get our hands on. You have probably heard that models like ChatGPT and PaLM are trained on data from the internet. The internet is the biggest repository of data that humans have. Additionally, as we observed from the Chinchilla papers scaling laws, it is possible that data to train our models is the actual bottleneck, and not compute or parameter count. So the natural question is how much data is left on the internet for us to keep training our models? and how much more data do we humans generate every year?"]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"How much data do we generate?"})," The total amount of data generated every single day in 2019 was on the order of ~463EB (",(0,a.jsx)(i.a,{href:"https://www.weforum.org/agenda/2019/04/how-much-data-is-generated-each-day-cf4bddf29f/",children:"World Economic Forum, 2019"}),"). But in this post, we will assume that models are not training on \u2018all the data generated\u2019 (yet), rather they will continue to only train on open-source internet text and image data. The available stock of text and image data grew by 0.14 OOM/year between 1990 and 2018 but has since slowed to 0.03 OOM/year."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"How much data is left?"})," The median projection for when the training dataset of notable ",(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," models exhausts the stock of professionally edited texts on the internet is 2024. The median projection for the year in which ",(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," models use up all the text on the internet is 2040. Overall, projections by Epochai predict that we will have exhausted high-quality language data before 2026, low-quality language data somewhere between 2030 and 2050, and vision data between 2030 and 2060. This might be an indicator of slowing down ",(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," progress after the next couple of decades. These conclusions from Epochai, like all the other conclusions in this entire leveraging computation section, rely on the unrealistic assumptions that current trends in ",(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," data usage and production will continue and that there will be no major innovations in data efficiency, i.e. we are assuming that the amount of capabilities gained per training datapoint will not change from current standards."]}),"\n",(0,a.jsx)(l.A,{src:"./img/sZd_Image_34.png",alt:"Enter image alt description",number:"30",label:"1.30",caption:"ML data consumption and data production trends for low-quality text, high-quality text, and images. ([Epoch AI, 2023](https://epochai.org/trends))"}),"\n",(0,a.jsx)(i.p,{children:'Even if we run out of Data, many solutions are proposed, from using synthetic data, for example, filtering and preprocessing the data with GPT-3.5 to create a new cleaner dataset, an approach used in the paper "Textbooks are all you need" with models like Phi 1.5B that demonstrate excellent performance for their size through the use of high-quality filtered data, to the use of more efficient trainings, or being more efficient by training on more epochs.'}),"\n",(0,a.jsx)(i.h3,{id:"02-04",children:"Algorithms"}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Algorithmic advancements"})," also play a role. For example, between 2012 and 2021, the computational power required to match the performance of AlexNet has been reduced by a factor of 40, which corresponds to a threefold yearly reduction in the compute required for achieving the same performance on image classification tasks like ImageNet. Improving the architecture also counts as algorithmic advancement. A particularly influential architecture is that of Transformers, central to many recent innovations, especially in chatbots and autoregressive learning. Their ability to be trained in parallel over every token of the context window fully exploits the power of modern GPUs, and this is thought to be one of the main reasons why they work so well compared to their predecessor, even if this point is controversial."]}),"\n",(0,a.jsxs)(o.A,{title:"Does algorithmic architecture really matter?",collapsed:!0,children:[(0,a.jsx)(i.p,{children:"This is a complicated question, but some evidence suggests that once an architecture is expressive and scalable enough, the architecture matters less than we might have thought:"}),(0,a.jsxs)(i.p,{children:["In a paper titled \u2018ConvNets Match Vision Transformers at Scale,' Google researchers found that Visual Transformers (ViT) can achieve the same results as CNNs (Convolutional ",(0,a.jsx)(t,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:(0,a.jsx)(t,{term:"neural network",definition:'{"definition":"A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers.","source":"","aliases":["Neural Network","artificial neural network","ANN","NN","neural net","deep neural network","deep neural net","DNN"]}',children:"neural network"})}),") simply by using more compute. (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2310.16764",children:"Smith et al., 2023"}),") They took a special CNN architecture and trained it on a massive dataset of four billion images. The resulting model matched the accuracy of existing ViT systems that used similar training compute."]}),(0,a.jsxs)(i.p,{children:["Variational AutoEncoders catch up if you make them very deep (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2011.10650#openai",children:"Child, 2021"}),"; ",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2007.03898#nvidia",children:"Vahdat & Kautz, 2021"}),")."]}),(0,a.jsxs)(i.p,{children:["Progress in late 2023, such as the mamba architecture (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2312.00752",children:"Gu & Dao, 2023"}),"), appears to be an improvement on the ",(0,a.jsx)(t,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:(0,a.jsx)(t,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:"transformer"})}),". It can be seen as an algorithmic advancement that reduces the amount of training computation needed to achieve the same performance."]}),(0,a.jsxs)(i.p,{children:["The connections and normalizations in the ",(0,a.jsx)(t,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:(0,a.jsx)(t,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:"transformer"})}),", which were thought to be important, can be taken out if the weights are set up correctly. This can also make the ",(0,a.jsx)(t,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:(0,a.jsx)(t,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:"transformer"})})," design simpler (Note however that this architecture is slower to converge than the others). (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2302.10322",children:"He et al., 2023"}),")"]}),(0,a.jsxs)(i.p,{children:["On the other side of the argument, certain ",(0,a.jsx)(t,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:(0,a.jsx)(t,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:"attention"})})," architectures are significantly more scalable when dealing with long context windows, and no feasible amount of training could compensate for this in more basic ",(0,a.jsx)(t,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:(0,a.jsx)(t,{term:"transformer",definition:'{"definition":"A neural network architecture that uses attention mechanisms to process sequential data, particularly effective for language tasks.","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:"transformer"})})," models. Architectures specifically designed to handle long sequences, like Sparse Transformers (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/1904.10509",children:"Child et al., 2019"}),") or Longformer (",(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/2004.05150",children:"Beltagy et al., 2020"}),'), can outperform standard transformers by a considerable margin for this usage. In computer vision, architectures like CNNs are inherently structured to recognize spatial hierarchies in images, making them more efficient for these tasks than architectures not specialized in handling spatial data when the amount of data is limited, and the "prior" encoded in the architecture makes the model learn faster.']})]}),"\n",(0,a.jsx)(i.h3,{id:"02-05",children:"Costs"}),"\n",(0,a.jsxs)(i.p,{children:["Understanding the dollar cost of ",(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," training runs is crucial for several reasons. Firstly, it reflects the real economic expense of developing ",(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"machine learning"})})," systems, which is essential for forecasting the future of AI development and identifying which actors can afford to pursue large-scale AI projects. Secondly, by combining cost estimates with performance metrics, we can track the efficiency and capabilities of ",(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," systems over time, offering insights into how these systems are improving and where inefficiencies may lie. Lastly, these insights help determine the sustainability of current spending trends and guide future investments in AI research and development, ensuring resources are allocated effectively to foster innovation while managing economic impact."]}),"\n",(0,a.jsxs)(i.p,{children:["Moore\u2019s Law, which predicts the doubling of transistor density and thus computational power approximately every two years, has historically led to decreased costs of compute power. However, the report finds that spending on ",(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," training has grown much faster than the cost reductions suggested by Moore\u2019s Law. This means that while hardware has become cheaper, the overall expense of training ",(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," systems has escalated due to increasing demand for computational resources. This divergence emphasizes the rapid pace of advancements in ",(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," and the significant investments required to keep up with the escalating computational demands."]}),"\n",(0,a.jsxs)(i.p,{children:["To measure the cost of ",(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," training runs, the report employs two primary methods. The first method uses historical trends in the price-performance of GPUs to estimate costs. This approach leverages general trends in hardware advancements and cost reductions over time. The second method bases estimates on the specific hardware used to train the ",(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," systems, such as NVIDIA GPUs, providing a more detailed and accurate picture of the costs associated with particular technologies. Both methods involve calculating the hardware cost\u2014the portion of the up-front hardware cost used for training\u2014and the energy cost, which accounts for the electricity required to power the hardware during training. These calculations provide a comprehensive view of the economic burden of training ",(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," models."]}),"\n",(0,a.jsxs)(i.p,{children:["Measuring the cost of development extends beyond the final training run of an ",(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," system to encompass a range of factors. This includes research and development costs, which cover the expenditures on preliminary experiments and model refinements that lead up to the final product. It also involves personnel costs, including salaries and benefits for researchers, engineers, and support staff. Infrastructure costs, such as investments in data centers, cooling systems, and networking equipment, are also significant. Additionally, software and tools, including licenses and cloud services, contribute to the overall cost. Energy costs throughout the development lifecycle, not just during the final training run, and opportunity costs\u2014potential revenue lost from not pursuing other projects\u2014are also crucial components. Understanding these broader costs provides a more comprehensive view of the economic impact of developing advanced ",(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," systems, informing strategic decisions about resource allocation."]}),"\n",(0,a.jsxs)(i.p,{children:["The findings suggest that the cost of ",(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," training runs will continue to grow, but the rate of growth might slow down in the future. The report estimates that the cost of ",(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," training has grown by approximately 2.8 times per year for all systems. For large-scale systems, the growth rate is slower, at around 1.6 times per year. This substantial year-on-year increase in training costs highlights the need for significant efficiency improvements in both hardware and training methodologies to manage future expenses effectively."]}),"\n",(0,a.jsxs)(i.p,{children:["The report forecasts that if current trends continue, the cost for the most expensive training runs could exceed significant economic thresholds, such as 1% of the US GDP, within the next few decades. This implies that without efficiency improvements, the economic burden of developing state-of-the-art ",(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:(0,a.jsx)(t,{term:"machine learning",definition:'{"definition":"A field of artificial intelligence focused on building systems that learn and improve from data without being explicitly programmed for every task.","source":"","aliases":["Machine Learning","ML"]}',children:"ML"})})," systems will increase substantially. Consequently, understanding and managing these costs is essential for ensuring the sustainable growth of AI capabilities and maintaining a balanced approach to AI investment and development."]}),"\n",(0,a.jsx)(l.A,{src:"./img/ywx_Image_48.png",alt:"Enter image alt description",number:"31",label:"1.31",caption:"Estimated cost of compute in US dollars for the final training run of ML systems. ([Epoch AI, 2023](https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems))"})]})}function p(e={}){const{wrapper:i}={...(0,r.R)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(f,{...e})}):f(e)}},3989:(e,i,t)=>{t.d(i,{A:()=>l});var n=t(6540),a=t(6347),r=t(8444);const s={iframeContainer:"iframeContainer_ixkI",loader:"loader_gjvK",spinner:"spinner_L1j2",spin:"spin_rtC2",iframeWrapper:"iframeWrapper_Tijy",iframe:"iframe_UAfa",caption:"caption_mDwB",captionLink:"captionLink_Ly4l"};var o=t(4848);function l(e){let{src:i,caption:t,title:l="Embedded content",height:d="500px",width:c="100%",chapter:h,number:m,label:f}=e;const[p,u]=(0,n.useState)(!0),g=(0,a.zy)(),y=h||(()=>{const e=g.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),w=d&&"100%"!==d&&"auto"!==d;return(0,o.jsxs)("figure",{className:s.iframeContainer,children:[p&&(0,o.jsxs)("div",{className:s.loader,children:[(0,o.jsx)("div",{className:s.spinner}),(0,o.jsx)("p",{children:"Loading content..."})]}),(0,o.jsx)("div",{className:s.iframeWrapper,style:{paddingBottom:w?"0":"56.25%",height:w?d:"auto"},children:(0,o.jsx)("iframe",{src:i,title:l,width:c,height:w?d:"100%",frameBorder:"0",allowFullScreen:!0,loading:"lazy",onLoad:()=>{u(!1)},className:s.iframe,style:{height:w?d:"100%",position:w?"static":"absolute"}})}),(0,o.jsx)(r.A,{caption:t,mediaType:"iframe",chapter:y,number:m,label:f})]})}}}]);
"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[3884],{1174:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>l,contentTitle:()=>c,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>h});const i=JSON.parse('{"id":"chapters/02/index","title":"Risks","description":"The previous chapter explored AI\'s rapidly advancing capabilities through scaling laws, the bitter lesson, and potential takeoff scenarios. We saw how more compute, data, and algorithmic improvements drive consistent capability gains across domains. But why should increasing capabilities concern us? The short answer is - more capable AI systems create larger-scale risks.","source":"@site/docs/chapters/02/index.md","sourceDirName":"chapters/02","slug":"/chapters/02/","permalink":"/chapters/02/","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/02/index.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Risks","chapter_number":2,"reading_time_core":"92 min","reading_time_optional":"28 min","reading_time_appendix":"14 min","authors":["Markov Grey","Charbel-Rapha\xebl Segerie"],"affiliations":["French Center for AI Safety (CeSIA)"],"acknowledgements":["Jeanne Salle","Charles Martinet","Vincent Corruble","Sebastian Gil","Alejandro Acelas","Evander Hammer","Mo Munem","Mateo Rendon","Kieron Kretschmar","Camille Berger"],"google_docs_link":"https://docs.google.com/document/d/1DcQUax0bZ-IABjmwER921g0ryuKQv4oXyFLHwP-8U-o/edit?usp=sharing","video_link":"https://www.youtube.com/watch?v=dhr4u-w75aQ","teach_link":"https://docs.google.com/document/d/1im_i6e9xEAe-koYlurYdn26n9h7pFX2HksnRfQmWxTQ/edit?usp=sharing","arxiv_link":"https://arxiv.org/abs/2508.13700","sidebar_position":2,"slug":"/chapters/02/"},"sidebar":"docs","previous":{"title":"1.9 Appendix: Discussion on LLMs","permalink":"/chapters/01/09"},"next":{"title":"2.1 Risk Decomposition","permalink":"/chapters/02/01"}}');var n=s(4848),a=s(8453),r=(s(2482),s(8559),s(1966),s(2501));const o={title:"Risks",chapter_number:2,reading_time_core:"92 min",reading_time_optional:"28 min",reading_time_appendix:"14 min",authors:["Markov Grey","Charbel-Rapha\xebl Segerie"],affiliations:["French Center for AI Safety (CeSIA)"],acknowledgements:["Jeanne Salle","Charles Martinet","Vincent Corruble","Sebastian Gil","Alejandro Acelas","Evander Hammer","Mo Munem","Mateo Rendon","Kieron Kretschmar","Camille Berger"],google_docs_link:"https://docs.google.com/document/d/1DcQUax0bZ-IABjmwER921g0ryuKQv4oXyFLHwP-8U-o/edit?usp=sharing",video_link:"https://www.youtube.com/watch?v=dhr4u-w75aQ",teach_link:"https://docs.google.com/document/d/1im_i6e9xEAe-koYlurYdn26n9h7pFX2HksnRfQmWxTQ/edit?usp=sharing",arxiv_link:"https://arxiv.org/abs/2508.13700",sidebar_position:2,slug:"/chapters/02/"},c="Introduction",l={},h=[];function d(e){const t={h1:"h1",header:"header",p:"p",strong:"strong",...(0,a.R)(),...e.components},{GlossaryTerm:s}=t;return s||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"introduction",children:"Introduction"})}),"\n",(0,n.jsx)(t.p,{children:"The previous chapter explored AI's rapidly advancing capabilities through scaling laws, the bitter lesson, and potential takeoff scenarios. We saw how more compute, data, and algorithmic improvements drive consistent capability gains across domains. But why should increasing capabilities concern us? The short answer is - more capable AI systems create larger-scale risks."}),"\n",(0,n.jsx)(r.A,{src:"./img/Fmp_Image_1.png",alt:"Enter image alt description",number:"1",label:"2.1",caption:"With increasing capabilities we also see increasing risks. Depending on the development trajectory and takeoff we might see longer periods with potential catastrophic risks, or suddenly emerging severe existential risks. The curves and colors in this diagram are meant to be illustrative and do not represent any specific forecasted development trajectory."}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Dangerous capabilities are specific examples of where the trends that we explored in the previous chapter lead to concerns."})," The same scaling laws that improve performance on coding, better text generation and so on, also might enable things like deception, manipulation, situational awareness, autonomous replication, and goal-directedness. An AI system that can write better code might also write code to replicate itself. One that understands human preferences might also learn to manipulate them. The capabilities driving AI progress inherently create new categories of risk."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Risks can be understood along two dimensions - what causes the risks? And how severe are the risks caused."})," In the causal decomposition we distinguish between misuse (humans using AI for harm), misalignment (AI systems pursuing wrong goals), and systemic risks (emergent effects from AI integration into other systems). Severity ranges from individual harms affecting specific people to existential threats that could permanently derail human civilization. This section basically helps you set up and categorize any of the risks that we talk about through this chapter, and others that might arise in the future. The risks are not cleanly separable, the majority of risks mostly occur as a combination of factors, but thinking about these categories helps for explanatory purposes."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Misuse risks show what happens when humans use AI capabilities for deliberate harm."})," We look at biological weapons development where AI could help design novel pathogens, cyber capabilities that could automate attacks on critical infrastructure, autonomous weapons that remove human oversight from lethal decisions, and adversarial attacks that exploit AI system vulnerabilities. The common thread is that AI removes previous bottlenecks - a single motivated actor with AI assistance could potentially accomplish what previously required teams of experts and significant resources."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Misalignment risks occur when AI systems work exactly as programmed but pursue goals that conflict with what we actually wanted."})," Specification gaming happens when systems find unexpected ways to maximize their ",(0,n.jsx)(s,{term:"loss function",definition:'{"definition":"A function that measures how well a model\'s predictions match the actual target values, used to guide training.","source":"","aliases":["Loss Function","cost function","objective function"]}',children:(0,n.jsx)(s,{term:"loss function",definition:'{"definition":"A function that measures how well a model\'s predictions match the actual target values, used to guide training.","source":"","aliases":["Loss Function","cost function","objective function"]}',children:"objective function"})})," that technically satisfy our instructions but violate our intentions. Treacherous turns involve systems that appear aligned during training but reveal different priorities once deployed with sufficient capability. Self-improvement scenarios could lead to rapid capability jumps that outpace our ability to understand or control these systems. These aren't science fiction scenarios - we already see early examples in current systems."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Systemic risks emerge from how AI integrates into larger social, economic, and political systems."})," Power concentration occurs as AI capabilities become controlled by fewer actors. Mass unemployment could result from automation eliminating human economic relevance. Epistemic erosion happens as AI-generated content makes it increasingly difficult to distinguish truth from fiction. Enfeeblement develops as humans become dependent on AI for cognitive tasks we used to perform ourselves. Value lock-in risks freezing current moral and political perspectives before humanity has time to evolve them. These risks don't require any single AI system to behave badly - they emerge from collective dynamics."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Risk amplifiers make every category of risk more likely and more severe."})," Race dynamics create pressure to deploy systems before adequate safety testing. Accidents happen even with good intentions when complex systems interact in unexpected ways. Corporate indifference leads companies to accept known risks when profits are at stake. Coordination failures prevent collective action even when everyone agrees on the problem. Unpredictability means capabilities often emerge faster than experts expect, leaving safety measures consistently behind the curve."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"These categories overlap and amplify each other in practice."})," Misuse can enable misalignment by corrupting training processes. Systemic pressures can worsen misalignment by incentivizing rushed deployment. Risk amplifiers affect all categories simultaneously. Most real-world AI risks will involve combinations of these factors rather than clean examples of any single category. Understanding the connections helps explain why isolated safety measures often prove insufficient."]}),"\n",(0,n.jsx)(t.p,{children:"The following chapters examine the technical strategies, governance approaches, and evaluation methods needed to address this interconnected risk landscape while preserving AI's extraordinary potential for human benefit."})]})}function p(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}}}]);
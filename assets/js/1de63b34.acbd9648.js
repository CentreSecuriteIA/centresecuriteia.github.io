"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[8099],{2768:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>h,default:()=>m,frontMatter:()=>l,metadata:()=>i,toc:()=>p});const i=JSON.parse('{"id":"chapters/09/3","title":"Interventional Methods","description":"Activation Patching","source":"@site/docs/chapters/09/03.md","sourceDirName":"chapters/09","slug":"/chapters/09/03","permalink":"/chapters/09/03","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/09/03.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"3","title":"Interventional Methods","sidebar_label":"9.3 Interventional Methods","sidebar_position":4,"slug":"/chapters/09/03","reading_time_core":"3 min","reading_time_optional":"5 min","pagination_prev":"chapters/09/2","pagination_next":"chapters/09/4"},"sidebar":"docs","previous":{"title":"9.2 Observational Methods","permalink":"/chapters/09/02"},"next":{"title":"9.4 Automating and Scaling Interpretability","permalink":"/chapters/09/04"}}');var a=n(4848),o=n(8453),s=n(3931),r=(n(2482),n(8559)),c=(n(1966),n(2501));const l={id:3,title:"Interventional Methods",sidebar_label:"9.3 Interventional Methods",sidebar_position:4,slug:"/chapters/09/03",reading_time_core:"3 min",reading_time_optional:"5 min",pagination_prev:"chapters/09/2",pagination_next:"chapters/09/4"},h="Interventional Methods",d={},p=[{value:"Activation Patching",id:"01",level:2},{value:"Activation Steering",id:"02",level:2}];function u(e){const t={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components},{GlossaryTerm:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"interventional-methods",children:"Interventional Methods"})}),"\n",(0,a.jsx)(t.h2,{id:"01",children:"Activation Patching"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Activation patching is a technique used to understand and pinpoint specific parts of a model responsible for certain behaviors or outputs."})," For example, when you ask an LLM to complete the sentence: \u201cWhen John and Mary went to the store, John gave a bottle to\u201d, how does the model know it should answer \u201cMary\u201d instead of \u201cJohn\u201d or some other name? This completion requires the model to track which person is the recipient of the action, meaning it has to understand and store contextual roles (i.e., who is giving and who is receiving)."]}),"\n",(0,a.jsxs)(t.p,{children:["Researchers have used activation patching to discover which parts of the model are responsible for this kind of role assignment. Specifically, the goal is to identify the circuit (i.e., the group of neurons and connections) that helps the model decide that \u201cMary\u201d is the correct answer (",(0,a.jsx)(t.a,{href:"https://openreview.net/forum?id=NpsVSN6o4ul",children:"Wang et al., 2023"}),")."]}),"\n",(0,a.jsxs)(r.A,{title:"Detail - Activation Patching",collapsed:!0,children:[(0,a.jsxs)(t.p,{children:["To illustrate the activation patching process, consider the figure below (",(0,a.jsx)(t.a,{href:"https://arxiv.org/abs/2404.15255",children:"Heimersheim & Nanda, 2024"}),'). Let\u2019s say we want to understand how a model completes the sentence "When John and Mary went to the store, John gave a bottle to" and which components are involved in choosing "Mary" as the answer. Activation patching generally requires three stages:']}),(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Clean Input:"})," On the left, the model processes the sentence \u201cJohn and Mary went to the store, John gave a bottle to\u201d generating a series of activations (outputs from each layer) shown in green. This clean input provides the model with the correct context to interpret \u201cJohn\u201d as the giver and \u201cMary\u201d as the recipient. The model predicts \u201cMary\u201d."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Corrupted Input:"})," On the right, the model is given a corrupted input: \u201cBob and Mary went to the store, John gave a bottle to\u201d In this case, the activations (in red) will differ because the model is now processing \u201cBob\u201d instead of \u201cJohn\u201d. Here, there is an ambiguity about whether \u201cMary\u201d or \u201cJohn\u201d is the intended recipient, so the answer will not necessarily be \u201cMary\u201d."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Patching Process:"}),' Activation patching involves taking specific activations from the clean input \u201cJohn and Mary\u2026\u201d and substituting them into the corrupted input \u201cBob and Mary\u2026\u201d. This is indicated by the arrow. By patching activations from the clean input, we can test whether the model\'s understanding of "John" as the giver and "Mary" as the recipient can be restored, even when the corrupted input with "Bob" creates ambiguity. At this step, we observe how much the model prediction shifts towards \u201cMary\u201d. For example, if patching the activations of a specific ',(0,a.jsx)(n,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:(0,a.jsx)(n,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:"attention"})})," head increases the logit for \u201cMary\u201d, it suggests that head is an important part of the circuit responsible for the task."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:["Iterating this procedure over different layers and components (such as ",(0,a.jsx)(n,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:(0,a.jsx)(n,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:"attention"})})," heads or MLPs) allows researchers to identify the components most responsible for the target behavior."]}),"\n"]}),"\n"]}),(0,a.jsx)(c.A,{src:"./img/hpl_Image_22.png",alt:"Enter image alt description",number:"22",label:"9.22",caption:"An illustration of the activation patching process. On the left, the model processes a clean input sequence \u201cJohn and Mary,\u201d with activations shown in green. On the right, the model processes a corrupted input sequence \u201cBob and Mary,\u201d with activations shown in red. Activation patching involves replacing one or more activations in the corrupted sequence with corresponding activations from the clean sequence (indicated by the arrow). The resulting patched activations are then passed forward to observe how they affect the model\u2019s output logits. By analyzing whether the patched activations restore the desired output, researchers can identify which parts of the model\u2019s activations are responsible for specific behaviors or outputs. From ([Zhang & Nanda, 2023](https://arxiv.org/abs/2309.16042))."}),(0,a.jsx)(c.A,{src:"./img/kdG_Image_23.png",alt:"Enter image alt description",number:"23",label:"9.23",caption:"The red neurons and connections represent the most important components of GPT-2 small for completing the sentence \u201cWhen John and Mary went to the store, John gave a bottle to _\u201d (known as the Indirect Object Identification (IOI) task). From ([Conmy et al., 2023](https://arxiv.org/abs/2304.14997))."}),(0,a.jsxs)(t.p,{children:["Activation patching was used to find circuits for various tasks (",(0,a.jsx)(t.a,{href:"https://openreview.net/forum?id=NpsVSN6o4ul",children:"Wang et al., 2023"}),") in LLMs, including:"]}),(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Example Prompt:"})," \u201cWhen John and Mary went to the store, John gave a bottle to\u201d"]}),(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Completion:"})," \u201cMary\u201d"]}),(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Task:"})," Indirect Object Identification (IOI) (",(0,a.jsx)(t.a,{href:"https://openreview.net/forum?id=NpsVSN6o4ul",children:"Wang et al., 2023"}),")"]}),(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Example Prompt:"})," \u201cThe war lasted from 1517 to 15\u201d"]}),(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Completion:"})," any two digit number greater than 17"]}),(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Task:"})," Greater-Than (",(0,a.jsx)(t.a,{href:"https://arxiv.org/abs/2305.00586",children:"Hanna et al., 2023"}),")"]}),(0,a.jsx)(c.A,{src:"./img/Og6_Image_24.png",alt:"Enter image alt description",number:"24",label:"9.24",caption:"Example Prompt: \u201cThe war lasted from 1517 to 15\u201d , Completion: \u201c files\u201d , Task: Docstring ([Heimersheim & Janiak, 2023](https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only)). The model predicts the next token, which should be a copy of the next argument in the definition."}),(0,a.jsxs)(t.p,{children:["There exists numerous activation patching settings, as detailed in (",(0,a.jsx)(t.a,{href:"https://arxiv.org/abs/2309.16042",children:"Zhang & Nanda, 2023"}),"). Activation patching can be applied at different levels of granularity, ranging from patching the entire residual stream at a particular layer to patching specific token positions within the model."]})]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Activation patching doesn\u2019t explain the model in a fully causal way:"})," it helps identify which components are involved in certain behaviors, but it doesn\u2019t always clarify how those components interact or contribute causally to the overall behavior. Knowing that a particular neuron or ",(0,a.jsx)(n,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:(0,a.jsx)(n,{term:"attention",definition:'{"definition":"A mechanism that allows models to focus on relevant parts of the input when making predictions, computing weighted combinations of input elements.","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:"attention"})})," head is involved doesn\u2019t necessarily mean we understand its specific role in the model\u2019s decision-making process. The method is context-dependent, meaning components critical for one task may not generalize to others. Also, there is a trade-off in granularity: finer patches capture more detail but may miss broader interactions, while coarser patches risk overlooking important elements. Moreover, as models grow in size, both the complexity and computational cost of using activation patching increase, making it harder to isolate meaningful circuits."]}),"\n",(0,a.jsx)(t.h2,{id:"02",children:"Activation Steering"}),"\n",(0,a.jsxs)(t.p,{children:["Activation steering is a technique used to control a model\u2019s behavior by modifying its activations during inference. Unlike traditional methods like ",(0,a.jsx)(n,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,a.jsx)(n,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tuning"})}),", or RLHF, activation steering allows for direct intervention without the need to retrain the model."]}),"\n",(0,a.jsxs)(t.p,{children:["Here is an example where activation steering was used to enforce GPT-2 into talking about love-related topics, regardless of the previous context (",(0,a.jsx)(t.a,{href:"https://arxiv.org/abs/2308.10248",children:"Turner et al., 2023"}),"):"]}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"GPT-2 default completion:"})," I hate you because\u2026 -> you are the most disgusting thing I have ever seen."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"GPT-2 steered in the \u201clove\u201d direction:"})," I hate you because\u2026 -> you are so beautiful and I want to be with you forever."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(t.p,{children:["Activation steering can be used to address remaining issues after safety training and ",(0,a.jsx)(n,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:(0,a.jsx)(n,{term:"fine-tuning",definition:'{"definition":"The process of taking a pre-trained model and further training it on a specific task or dataset.","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune"]}',children:"fine-tuning"})})," by:"]}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Steering models towards desirable outcomes:"})," Improving truthfulness (",(0,a.jsx)(t.a,{href:"https://arxiv.org/abs/2306.03341",children:"Li et al., 2023"}),"), honesty (",(0,a.jsx)(t.a,{href:"https://arxiv.org/abs/2310.01405",children:"Zou et al., 2023"}),"), avoiding generating toxic or harmful content, customizing chatbot personalities (e.g., making them more formal or friendly), or steering in the style of specific authors without retraining the model."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Post-deployment control:"})," Monitoring AI systems for dangerous behaviors and enhancing robustness to jailbreaks by steering models to refuse harmful requests (",(0,a.jsx)(t.a,{href:"https://arxiv.org/abs/2310.01405",children:"Zou et al., 2023"}),"). It may also be possible to strengthen other safety techniques, like Constitutional AI, by examining how they encourage the model toward safer and more honest behavior, as well as by identifying any gaps in this process (",(0,a.jsx)(t.a,{href:"https://www.anthropic.com/news/mapping-mind-language-model",children:"Anthropic, 2024"}),")."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(s.A,{type:"youtube",videoId:"CJIbCV92d88",number:"4",label:"9.4",caption:"A second example of steering on Claude 3 Sonnet using sparse autoencoder features. Dictionary learning on Claude 3 Sonnet"}),"\n",(0,a.jsxs)(t.p,{children:["Note that the Representation Engineering agenda (",(0,a.jsx)(t.a,{href:"https://arxiv.org/abs/2310.01405",children:"Zou et al., 2023"}),") is a variant of activation steering that proposes to steer LLMs toward desirable outcomes such as more honesty, less bias, etc."]}),"\n",(0,a.jsxs)(r.A,{title:"Details - Activation Steering",collapsed:!0,children:[(0,a.jsx)(t.p,{children:"Activation steering involves two main steps. Let\u2019s say we want to make our model more honest, this involves two main steps:"}),(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Identify the honesty direction in the model's activation space."}),' This is typically done by collecting model activations for prompts designed to elicit contrasting behaviors (e.g., "honest" vs. "dishonest" responses) and analyzing these to find a linear direction that separates the two. The model\'s activations can be thought of as vectors in a high-dimensional space. Certain directions in this space correspond to specific behaviors. For example, one direction might correlate with generating more positive text, while another could steer the model to talk about science. This direction, sometimes called a concept vector, represents the targeted attribute in activation space.']}),(0,a.jsx)(c.A,{src:"./img/jGl_Image_25.png",alt:"Enter image alt description",number:"25",label:"9.25",caption:"Identifying an activation steering direction. From ([Wehner, 2024](https://www.alignmentforum.org/posts/3ghj8EuKzwD3MQR5G/an-introduction-to-representation-engineering-an-activation))."}),(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"The second step is steering the model\u2019s behavior by adding this concept vector to the model's activations at inference time."}),' By introducing this vector at a relevant layer, we can amplify or suppress certain behaviors without retraining the model. For instance, adding the "honesty" vector to a model\u2019s activations during inference nudges it toward generating more honest responses.']}),(0,a.jsx)(c.A,{src:"./img/KCw_Image_26.png",alt:"Enter image alt description",number:"26",label:"9.26",caption:"The concept vector is added to the activations at a specific layer to influence the model\u2019s behavior in the desired direction. From ([Wehner, 2024](https://www.alignmentforum.org/posts/3ghj8EuKzwD3MQR5G/an-introduction-to-representation-engineering-an-activation))."}),(0,a.jsx)(t.p,{children:"The concept vector can also be obtained using a linear probe (see section on Probing Classifiers), or can be a feature found through dictionary learning (see section on Sparse Autoencoders) as shown on the Claude 3 Sonnet video above."})]})]})}function m(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}},3931:(e,t,n)=>{n.d(t,{A:()=>c});var i=n(6540),a=n(6347),o=n(8444);const s={videoFigure:"videoFigure_dV9u",fullWidth:"fullWidth_Tv7O",videoContainer:"videoContainer_LWoK",aspectRatio169:"aspectRatio169_Go8b",aspectRatio43:"aspectRatio43_b3T7",aspectRatio11:"aspectRatio11_n21J",aspectRatio219:"aspectRatio219_L_3n",videoIframe:"videoIframe_QEc5",videoElement:"videoElement_uu7N",customPlayer:"customPlayer_jiC4",customPlayerPlaceholder:"customPlayerPlaceholder_aL9q",loadingOverlay:"loadingOverlay_e2pu",spinner:"spinner_tFyL",spin:"spin_saPs",errorContainer:"errorContainer_J6Pn",errorIcon:"errorIcon_cGdn",fallbackLink:"fallbackLink_WsZk"};var r=n(4848);function c(e){let{type:t="youtube",videoId:n,caption:c,title:l,startTime:h,autoplay:d=!1,controls:p=!0,aspectRatio:u="16:9",width:m,height:g,chapter:f,number:v,label:x,useCustomPlayer:b=!1,fullWidth:w=!0}=e;const[y,j]=(0,i.useState)(!0),[k,_]=(0,i.useState)(!1),A=(0,a.zy)(),I=f||(()=>{const e=A.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),M=(()=>{const e=(e=>{if(!e)return"";if("number"==typeof e)return e.toString();if("string"==typeof e){if(/^\d+$/.test(e))return e;let t=0;const n=e.match(/(\d+)h/),i=e.match(/(\d+)m/),a=e.match(/(\d+)s/);return n&&(t+=3600*parseInt(n[1])),i&&(t+=60*parseInt(i[1])),a&&(t+=parseInt(a[1])),t>0?t.toString():""}return""})(h);switch(t.toLowerCase()){case"youtube":let i=`https://www.youtube.com/embed/${n}`;const a=new URLSearchParams;e&&a.append("start",e),d&&a.append("autoplay","1"),p||b||a.append("controls","0"),a.append("rel","0"),a.append("modestbranding","1"),a.append("fs","1"),a.append("cc_load_policy","0"),a.append("iv_load_policy","3"),a.append("showinfo","0"),a.append("disablekb","1"),a.append("playsinline","1"),a.append("color","white"),a.append("theme","light"),b&&(a.append("enablejsapi","1"),a.append("origin",window.location.origin));const o=a.toString();return o?`${i}?${o}`:i;case"vimeo":let s=`https://player.vimeo.com/video/${n}`;const r=new URLSearchParams;d&&r.append("autoplay","1"),p||b||r.append("controls","0"),r.append("title","0"),r.append("byline","0"),r.append("portrait","0"),r.append("dnt","1"),r.append("transparent","0"),r.append("background","1");const c=r.toString();return c?`${s}?${c}`:s;case"mp4":case"webm":case"video":return n;default:return console.warn(`Unsupported video type: ${t}`),n}})(),T=()=>{j(!1)},P=()=>{_(!0),j(!1)},C=e=>{let{src:n,onLoad:i,onError:a}=e;return(0,r.jsx)("div",{className:s.customPlayer,children:(0,r.jsxs)("div",{className:s.customPlayerPlaceholder,children:[(0,r.jsx)("h3",{children:"Atlas Custom Player"}),(0,r.jsx)("p",{children:"Coming Soon"}),(0,r.jsxs)("a",{href:n,target:"_blank",rel:"noopener noreferrer",className:s.fallbackLink,children:["Watch on ",t.charAt(0).toUpperCase()+t.slice(1)]})]})})},L=["mp4","webm","video"].includes(t.toLowerCase());return(0,r.jsxs)("figure",{className:`${s.videoFigure} ${w?s.fullWidth:""}`,children:[(0,r.jsxs)("div",{className:`${s.videoContainer} ${(()=>{switch(u){case"4:3":return s.aspectRatio43;case"1:1":return s.aspectRatio11;case"21:9":return s.aspectRatio219;default:return s.aspectRatio169}})()}`,style:{width:w?"100%":m||"auto",maxWidth:w?"none":"800px"},children:[y&&!k&&(0,r.jsxs)("div",{className:s.loadingOverlay,children:[(0,r.jsx)("div",{className:s.spinner}),(0,r.jsx)("p",{children:"Loading video..."})]}),k&&(0,r.jsxs)("div",{className:s.errorContainer,children:[(0,r.jsxs)("svg",{className:s.errorIcon,width:"48",height:"48",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",children:[(0,r.jsx)("circle",{cx:"12",cy:"12",r:"10"}),(0,r.jsx)("line",{x1:"12",y1:"8",x2:"12",y2:"12"}),(0,r.jsx)("line",{x1:"12",y1:"16",x2:"12.01",y2:"16"})]}),(0,r.jsx)("h4",{children:"Failed to load video"}),(0,r.jsxs)("p",{children:["Video type: ",t]}),(0,r.jsxs)("p",{children:["Video ID/URL: ",n]}),(0,r.jsx)("a",{href:M,target:"_blank",rel:"noopener noreferrer",className:s.fallbackLink,children:"Try opening video directly"})]}),!k&&(L?(0,r.jsxs)("video",{className:s.videoElement,controls:p,autoPlay:d,onLoadedData:T,onError:P,title:l||c||`${t} video`,style:{width:m||"100%",height:g||"auto",display:y?"none":"block"},children:[(0,r.jsx)("source",{src:M,type:`video/${t}`}),(0,r.jsxs)("p",{children:["Your browser doesn't support video playback.",(0,r.jsx)("a",{href:M,target:"_blank",rel:"noopener noreferrer",children:"Download the video file"})]})]}):b?(0,r.jsx)(C,{src:M,onLoad:T,onError:P}):(0,r.jsx)("iframe",{className:s.videoIframe,src:M,title:l||c||`${t} video`,frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0,onLoad:T,onError:P,style:{width:m||"100%",height:g||"100%",opacity:y?0:1}}))]}),(0,r.jsx)(o.A,{caption:c,mediaType:"video",chapter:I,number:v,label:x})]})}}}]);
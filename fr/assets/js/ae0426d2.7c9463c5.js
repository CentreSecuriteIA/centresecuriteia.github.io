"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[9407],{4505:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>p,contentTitle:()=>d,default:()=>h,frontMatter:()=>u,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"chapters/03/4","title":"Strat\xe9gies de s\xe9curit\xe9 pour l\'ASI","description":"La Superintelligence Artificielle (ASI) fait r\xe9f\xe9rence aux syst\xe8mes d\'IA qui d\xe9passent significativement les capacit\xe9s cognitives humaines dans pratiquement tous les domaines d\'int\xe9r\xeat. L\'\xe9mergence potentielle de l\'ASI pr\xe9sente des d\xe9fis de s\xe9curit\xe9 qui peuvent diff\xe9rer qualitativement de ceux pos\xe9s par l\'AGI. Les strat\xe9gies de s\xe9curit\xe9 pour l\'ASI impliquent souvent des programmes plus sp\xe9culatifs.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/03/04.md","sourceDirName":"chapters/03","slug":"/chapters/03/04","permalink":"/fr/chapters/03/04","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/03/04.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"4","title":"Strat\xe9gies de s\xe9curit\xe9 pour l\'ASI","sidebar_label":"3.4 Strat\xe9gies de s\xe9curit\xe9 pour l\'ASI","sidebar_position":5,"slug":"/chapters/03/04","section_description":"Can humanity coordinate to prevent an ASI catastrophe?","reading_time_core":"20 min","reading_time_optional":"4 min","pagination_prev":"chapters/03/3","pagination_next":"chapters/03/5"},"sidebar":"docs","previous":{"title":"3.3 Strat\xe9gies de s\xe9curit\xe9 pour l\'AGI","permalink":"/fr/chapters/03/03"},"next":{"title":"3.5 Strat\xe9gies Syst\xe9miques","permalink":"/fr/chapters/03/05"}}');var i=n(4848),r=n(8453),a=n(2482),o=n(8559),l=(n(9585),n(2501));const u={id:4,title:"Strat\xe9gies de s\xe9curit\xe9 pour l'ASI",sidebar_label:"3.4 Strat\xe9gies de s\xe9curit\xe9 pour l'ASI",sidebar_position:5,slug:"/chapters/03/04",section_description:"Can humanity coordinate to prevent an ASI catastrophe?",reading_time_core:"20 min",reading_time_optional:"4 min",pagination_prev:"chapters/03/3",pagination_next:"chapters/03/5"},d="Strat\xe9gies de s\xe9curit\xe9 pour l'ASI",p={},c=[{value:"Strat\xe9gies d\xe9battues",id:"01",level:2},{value:"Automatisation de la recherche sur l&#39;alignement",id:"02",level:2},{value:"Safety-by-Design",id:"03",level:2},{value:"Coordination Mondiale",id:"04",level:2},{value:"Dissuasion",id:"05",level:2}];function m(e){const s={a:"a",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{GlossaryTerm:n}=s;return n||function(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"strat\xe9gies-de-s\xe9curit\xe9-pour-lasi",children:"Strat\xe9gies de s\xe9curit\xe9 pour l'ASI"})}),"\n",(0,i.jsx)(s.p,{children:"La Superintelligence Artificielle (ASI) fait r\xe9f\xe9rence aux syst\xe8mes d'IA qui d\xe9passent significativement les capacit\xe9s cognitives humaines dans pratiquement tous les domaines d'int\xe9r\xeat. L'\xe9mergence potentielle de l'ASI pr\xe9sente des d\xe9fis de s\xe9curit\xe9 qui peuvent diff\xe9rer qualitativement de ceux pos\xe9s par l'AGI. Les strat\xe9gies de s\xe9curit\xe9 pour l'ASI impliquent souvent des programmes plus sp\xe9culatifs."}),"\n",(0,i.jsx)(s.p,{children:"M\xeame si les experts ne sont pas certains que la cr\xe9ation d'une IA align\xe9e de niveau humain n\xe9cessite un changement de paradigme, le consensus parmi les chercheurs en s\xe9curit\xe9 de l'IA est que le d\xe9veloppement de superintelligences align\xe9es requiert une solution sp\xe9cifique, et probablement un nouveau paradigme, en raison de plusieurs facteurs :"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Il est fort probable que les humains ne soient pas au sommet de l'intelligence possible."})," Cette reconnaissance implique qu'une superintelligence pourrait poss\xe9der des capacit\xe9s cognitives si avanc\xe9es que son alignement avec les valeurs et intentions humaines pourrait \xeatre une t\xe2che insurmontable, car notre compr\xe9hension et nos m\xe9thodologies actuelles pourraient \xeatre inad\xe9quates pour garantir son alignement. La diff\xe9rence cognitive entre une superintelligence et un humain pourrait \xeatre comparable \xe0 la diff\xe9rence entre une fourmi et un humain. Tout comme un humain peut facilement se lib\xe9rer des contraintes qu'une fourmi pourrait imaginer, une superintelligence pourrait facilement surpasser toutes les protections que nous tenterions d'imposer."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsxs)(s.strong,{children:["L'",(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})})," offre un contr\xf4le et une compr\xe9hension minimaux du mod\xe8le."]})," Cette m\xe9thode conduit l'IA \xe0 devenir une \"bo\xeete noire\", o\xf9 ses processus de prise de d\xe9cision sont opaques et mal compris. Sans avanc\xe9es significatives en mati\xe8re d'interpr\xe9tabilit\xe9, une superintelligence cr\xe9\xe9e uniquement avec l'",(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})})," serait opaque."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"La marge d'erreur est faible, et les enjeux sont incroyablement \xe9lev\xe9s. Une superintelligence mal align\xe9e pourrait conduire \xe0 des r\xe9sultats catastrophiques, voire existentiels. Les cons\xe9quences irr\xe9versibles du d\xe9ploiement d'une superintelligence mal align\xe9e signifient que nous devons aborder son d\xe9veloppement avec la plus grande prudence, en nous assurant qu'elle s'aligne sans faille sur nos valeurs et intentions."}),"\n",(0,i.jsx)(s.h2,{id:"01",children:"Strat\xe9gies d\xe9battues"}),"\n",(0,i.jsx)(s.p,{children:"Ces strat\xe9gies font l'objet de vifs d\xe9bats. Elles pourraient fonctionner pour l'alignement de l'AGI, mais il est tr\xe8s incertain qu'elles soient suffisantes pour l'alignement de l'ASI."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Am\xe9lioration it\xe9rative."})," L'am\xe9lioration it\xe9rative consiste \xe0 perfectionner progressivement les syst\xe8mes d'IA pour am\xe9liorer leurs caract\xe9ristiques de s\xe9curit\xe9. Bien qu'elle soit utile pour r\xe9aliser des progr\xe8s graduels, elle pourrait ne pas \xeatre suffisante pour les IA de niveau humain car les petites am\xe9liorations pourraient ne pas r\xe9soudre les probl\xe8mes plus importants et syst\xe9miques, et l'IA pourrait d\xe9velopper des comportements qui ne sont pas pr\xe9vus ou trait\xe9s dans les it\xe9rations pr\xe9c\xe9dentes (",(0,i.jsx)(s.a,{href:"https://arbital.com/p/advanced_safety",children:"Arbital"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Bien s\xfbr, l'am\xe9lioration it\xe9rative serait utile."})," Pouvoir exp\xe9rimenter sur les IA actuelles pourrait \xeatre instructif. Mais cela pourrait aussi \xeatre trompeur car il pourrait y avoir un seuil de capacit\xe9 au-dessus duquel une IA devient soudainement ing\xe9rable (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2206.07682",children:"Wei et al., 2022"}),"). Par exemple, si l'IA devient surhumaine dans sa capacit\xe9 de persuasion, elle pourrait devenir ing\xe9rable m\xeame pendant l'entra\xeenement : si un mod\xe8le atteint le niveau Critique en persuasion tel que d\xe9fini dans le cadre de pr\xe9paration d'OpenAI, alors le mod\xe8le serait capable de \"",(0,i.jsx)(s.em,{children:"cr\xe9er [...] du contenu avec une efficacit\xe9 persuasive suffisamment forte pour convaincre presque n'importe qui d'agir selon une croyance qui va \xe0 l'encontre de son int\xe9r\xeat naturel."}),'" (',(0,i.jsx)(s.a,{href:"https://cdn.openai.com/openai-preparedness-framework-beta.pdf",children:"OpenAI, 2023"}),"). \xcatre capable de convaincre presque n'importe qui serait \xe9videmment trop dangereux, et ce type de mod\xe8le serait trop risqu\xe9 pour interagir directement ou indirectement avec les humains ou le monde r\xe9el. L'entra\xeenement devrait s'arr\xeater avant que le mod\xe8le n'atteigne un niveau critique de persuasion car cela pourrait \xeatre trop dangereux, m\xeame pendant l'entra\xeenement. D'autres ph\xe9nom\xe8nes soudains pourraient inclure un grokking (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2201.02177",children:"Power et al., 2022"}),"), qui est un type de saut soudain de capacit\xe9 qui r\xe9sulterait en un virage brutal \xe0 gauche. C'est un sc\xe9nario o\xf9, pendant l'entra\xeenement d'une IA, ses capacit\xe9s se g\xe9n\xe9ralisent \xe0 travers de nombreux domaines tandis que les propri\xe9t\xe9s d'alignement qui \xe9taient valables aux stades pr\xe9c\xe9dents ne se g\xe9n\xe9ralisent pas aux nouveaux domaines."]}),"\n",(0,i.jsxs)(s.p,{children:["Certaines conditions th\xe9oriques n\xe9cessaires pour s'appuyer sur des am\xe9liorations it\xe9ratives pourraient \xe9galement ne pas \xeatre satisfaites par l'alignement de l'IA. Un probl\xe8me principal survient lorsque la boucle de retour est rompue, par exemple avec une mont\xe9e en puissance rapide, qui ne vous donne pas le temps d'it\xe9rer, ou un d\xe9salignement interne trompeur, qui serait un mode de d\xe9faillance potentiel (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/xFotXGEotcKouifky/worlds-where-iterative-design-fails",children:"Wentworth, 2022"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:["Il convient de noter que ce point de vue n'est pas universellement partag\xe9. Certains experts pensent qu'il y a une chance significative (>50%) que des approches directes comme le RLHF combin\xe9es \xe0 la r\xe9solution it\xe9rative des probl\xe8mes au fur et \xe0 mesure qu'ils surviennent puissent \xeatre suffisantes pour d\xe9velopper l'AGI en toute s\xe9curit\xe9 - pas un alignement parfait, mais suffisant pour \xe9viter les risques catastrophiques (",(0,i.jsx)(s.a,{href:"https://aligned.substack.com/p/alignment-optimism",children:"Leike, 2022"}),"). Le principal d\xe9fi en s'appuyant sur cette approche est l'incertitude : nous ne saurons pas \xe0 l'avance si nous sommes dans un monde o\xf9 l'am\xe9lioration it\xe9rative est suffisante ou dans un monde o\xf9 elle \xe9choue de mani\xe8re catastrophique."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Filtrer les donn\xe9es dangereuses du jeu de donn\xe9es."})," Les mod\xe8les actuels sont tr\xe8s d\xe9pendants des donn\xe9es sur lesquelles ils sont entra\xeen\xe9s, donc peut-\xeatre que le filtrage des donn\xe9es pourrait att\xe9nuer le probl\xe8me. Malheureusement, m\xeame si la surveillance de ces donn\xe9es semble n\xe9cessaire, cela pourrait \xeatre insuffisant."]}),"\n",(0,i.jsxs)(s.p,{children:["La strat\xe9gie consisterait \xe0 filtrer le contenu li\xe9 \xe0 l'IA ou \xe9crit par les IA, y compris la science-fiction, les sc\xe9narios de prise de contr\xf4le, la gouvernance, les questions de s\xe9curit\xe9 de l'IA, etc. Cela devrait \xe9galement englober tout ce qui est \xe9crit par les IA. Cette approche pourrait r\xe9duire l'incitation au d\xe9salignement de l'IA. D'autres sujets qui pourraient \xeatre filtr\xe9s pourraient inclure les capacit\xe9s dangereuses comme la recherche biochimique, les techniques de piratage, ou les m\xe9thodes de manipulation et de persuasion. Cela pourrait \xeatre fait automatiquement en demandant \xe0 un GPT-n de filtrer le jeu de donn\xe9es avant d'entra\xeener GPT-(n+1) (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2306.11644",children:"Gunasekar et al., 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:['Malheureusement, "examiner attentivement vos donn\xe9es d\'entra\xeenement", m\xeame si n\xe9cessaire, pourrait ne pas \xeatre suffisant. Les LLM g\xe9n\xe8rent parfois du texte purement renforc\xe9 n\xe9gativement (',(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2306.07567",children:"Roger, 2023"}),"). Malgr\xe9 l'utilisation d'un jeu de donn\xe9es qui avait subi un filtrage, le mod\xe8le Cicero a quand m\xeame appris \xe0 \xeatre trompeur (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2308.14752",children:"Park et al., 2023"}),'). Il y a beaucoup de difficult\xe9s techniques n\xe9cessaires pour filtrer ou renforcer correctement les comportements de l\'IA, et dire "nous devrions filtrer les donn\xe9es" passe sous silence beaucoup de difficult\xe9s th\xe9oriques. L\'article "Open problems and fundamental limitations with RLHF" (',(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2307.15217",children:"Casper et al., 2023"}),") aborde certaines de ces difficult\xe9s plus en d\xe9tail. Enfin, malgr\xe9 toutes ces mesures d'att\xe9nuation, connecter une IA \xe0 Internet pourrait \xeatre suffisant pour qu'elle recueille des informations sur les outils et d\xe9veloppe des capacit\xe9s dangereuses."]}),"\n",(0,i.jsx)(s.h2,{id:"02",children:"Automatisation de la recherche sur l'alignement"}),"\n",(0,i.jsxs)(s.p,{children:["Nous ne savons pas comment aligner la superintelligence, nous devons donc acc\xe9l\xe9rer la recherche sur l'alignement avec les IA. Le plan \"Superalignment\" d'OpenAI visait \xe0 acc\xe9l\xe9rer la recherche sur l'alignement avec une IA cr\xe9\xe9e par ",(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})}),", l\xe9g\xe8rement sup\xe9rieure aux humains en recherche scientifique, et \xe0 d\xe9l\xe9guer la t\xe2che de trouver un plan pour l'IA future (",(0,i.jsx)(s.a,{href:"https://openai.com/blog/introducing-superalignment",children:"OpenAI, 2023"}),"). Cette strat\xe9gie reconna\xeet un fait crucial : notre compr\xe9hension actuelle de la fa\xe7on d'aligner parfaitement les syst\xe8mes d'IA avec les valeurs et intentions humaines est incompl\xe8te. Par cons\xe9quent, le plan sugg\xe8re de d\xe9l\xe9guer cette t\xe2che complexe aux syst\xe8mes d'IA futurs. L'objectif principal de cette strat\xe9gie est d'acc\xe9l\xe9rer consid\xe9rablement la recherche et le d\xe9veloppement de la s\xe9curit\xe9 de l'IA (",(0,i.jsx)(s.a,{href:"https://openai.com/blog/our-approach-to-alignment-research",children:"OpenAI, 2022"}),') en utilisant des IA capables de penser vraiment, vraiment vite. Certains ordres de grandeur de vitesse sont donn\xe9s dans le blog "What will GPT-2030 look like?" (',(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/WZXqNYbJhtidjRXSi/what-will-gpt-2030-look-like",children:"Steinhardt, 2023"}),"). Le plan d'OpenAI n'est pas un plan mais un m\xe9ta-plan : la premi\xe8re \xe9tape consiste \xe0 utiliser l'IA pour faire un plan, puis \xe0 ex\xe9cuter ce plan."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Cependant, pour ex\xe9cuter ce m\xe9taplan, nous avons besoin d'un chercheur IA automatique contr\xf4lable et dirigeable."})," OpenAI pense que cr\xe9er un tel chercheur automatique est plus facile que de r\xe9soudre le probl\xe8me complet de l'alignement. Ce plan peut \xeatre divis\xe9 en trois composantes principales (",(0,i.jsx)(s.a,{href:"https://openai.com/index/our-approach-to-alignment-research/",children:"OpenAI, 2022"}),") :"]}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Former des syst\xe8mes d'IA en utilisant le retour humain,"}),' c\'est-\xe0-dire cr\xe9er un assistant puissant qui suit le retour humain, est tr\xe8s similaire aux techniques utilis\xe9es pour "aligner" les mod\xe8les de langage et les chatbots. Cela pourrait impliquer le RLHF, par exemple.']}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Former des syst\xe8mes d'IA pour assister l'\xe9valuation humaine :"})," Malheureusement, le RLHF est imparfait car le retour humain est imparfait. Nous devons d\xe9velopper une IA qui peut aider les humains \xe0 donner des retours pr\xe9cis. Il s'agit de d\xe9velopper des syst\xe8mes d'IA qui peuvent aider les humains dans le processus d'\xe9valuation pour des t\xe2ches arbitrairement difficiles. Par exemple, si nous devons juger la faisabilit\xe9 d'un plan d'alignement propos\xe9 par un chercheur automatique et donner un retour dessus, nous avons besoin d'assistance pour accomplir cet objectif facilement. Oui, la v\xe9rification est g\xe9n\xe9ralement plus facile que la g\xe9n\xe9ration, mais elle reste tr\xe8s difficile. La supervision \xe9volutive serait n\xe9cessaire pour la raison suivante. Imaginez une future IA proposant un millier de plans d'alignement diff\xe9rents. Comment \xe9valueriez-vous tous ces plans complexes ? Ce serait une t\xe2che tr\xe8s ardue sans assistance IA. Voir le chapitre sur la supervision \xe9volutive pour plus de d\xe9tails."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Former des syst\xe8mes d'IA \xe0 faire de la recherche sur l'alignement :"})," L'objectif final est de construire des mod\xe8les de langage capables de produire de la recherche sur l'alignement au niveau humain. La sortie de ces mod\xe8les pourrait \xeatre des essais en langage naturel sur l'alignement ou du code qui impl\xe9mente directement des exp\xe9riences. Dans les deux cas, les chercheurs humains passeraient leur temps \xe0 examiner la recherche sur l'alignement g\xe9n\xe9r\xe9e par la machine (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/FTk7ufqK2D4dkdBDr/notes-on-openai-s-alignment-plan",children:"Flint, 2022"}),")."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Acc\xe9l\xe9rer diff\xe9rentiellement l'alignement, pas les capacit\xe9s."})," L'objectif est de d\xe9velopper et de d\xe9ployer des assistants de recherche en IA de mani\xe8re \xe0 maximiser leur impact sur la recherche en alignement tout en minimisant leur impact sur l'acc\xe9l\xe9ration du d\xe9veloppement de l'AGI (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/FBG7AghvvP7fPYzkx/my-thoughts-on-openai-s-alignment-plan-1",children:"Wasil, 2022"}),"). OpenAI s'est engag\xe9 \xe0 partager ouvertement sa recherche sur l'alignement lorsqu'il sera s\xfbr de le faire, avec l'intention d'\xeatre transparent sur l'efficacit\xe9 pratique de ses techniques d'alignement (",(0,i.jsx)(s.a,{href:"https://openai.com/index/our-approach-to-alignment-research/",children:"OpenAI, 2022"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le cyborgisme pourrait am\xe9liorer ce plan."})," Le cyborgisme (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/bxt7uCiHam4QXrQAA/cyborgism",children:"Kees-Dupuis & Janus, 2023"}),") est un programme qui fait r\xe9f\xe9rence \xe0 la formation d'humains sp\xe9cialis\xe9s en ing\xe9nierie de prompts pour guider les mod\xe8les de langage afin qu'ils puissent effectuer des recherches sur l'alignement. Plus pr\xe9cis\xe9ment, ils se concentreraient sur le pilotage des ",(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:"mod\xe8les de base"})})," plut\xf4t que des mod\xe8les RLHF. La raison est que les mod\xe8les de langage peuvent \xeatre tr\xe8s cr\xe9atifs et ne sont pas dirig\xe9s vers un but (et ne sont pas aussi dangereux que les IA RLHF dirig\xe9es vers un but). Un humain appel\xe9 cyborg pourrait atteindre cet objectif en pilotant le mod\xe8le non dirig\xe9 vers un but. Les mod\xe8les dirig\xe9s vers un but pourraient \xeatre utiles mais peuvent \xeatre trop dangereux. Cependant, \xeatre capable de contr\xf4ler les ",(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:"mod\xe8les de base"})})," n\xe9cessite une pr\xe9paration, similaire \xe0 la formation requise pour conduire une Formule Un. Le moteur est puissant mais difficile \xe0 diriger. En combinant l'intellect humain et l'orientation vers un but avec la puissance de calcul et la cr\xe9ativit\xe9 des mod\xe8les bas\xe9s sur le langage, les chercheurs cyborgistes visent \xe0 g\xe9n\xe9rer plus de recherche sur l'alignement avec les mod\xe8les futurs. Les contributions notables dans ce domaine incluent celles de Janus et Nicolas Kees Dupuis (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/bxt7uCiHam4QXrQAA/cyborgism",children:"Kees-Dupuis & Janus, 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Il existe diverses critiques et pr\xe9occupations concernant le plan de superalignement d'OpenAI"})," (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/FBG7AghvvP7fPYzkx/my-thoughts-on-openai-s-alignment-plan-1",children:"Wasil, 2022"}),"; ",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/NSZhadmoYdjRKNq6X/openai-launches-superalignment-taskforce",children:"Mowshowitz, 2023"}),"; ",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/Hna4aoMwr6Qx9rHBs/linkpost-introducing-superalignment?commentId=NsYXBdLY6edAXavsM",children:"Christiano, 2023"}),"; ",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/tD9zEiHfkvakpnNam/a-challenge-for-agi-organizations-and-a-challenge-for-1",children:"Yudkowsky, 2022"}),"; ",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/pxiaLFjyr4WPmFdcm/take-2-building-tools-to-help-build-fai-is-a-legitimate",children:"Steiner, 2022"}),"; ",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/6RC3BNopCtzKaTeR6/thoughts-on-the-openai-alignment-plan-will-ai-research",children:"Ladish, 2023"}),"). Il convient de noter que le plan d'OpenAI est tr\xe8s peu sp\xe9cifi\xe9, et il est probable qu'OpenAI ait manqu\xe9 certains angles morts de classe de risque lorsqu'ils ont annonc\xe9 leur plan au public. Par exemple, pour que le plan de superalignement fonctionne, beaucoup des technicit\xe9s expliqu\xe9es dans l'article ",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled",children:"The case for ensuring that powerful AIs are controlled"})," n'ont pas \xe9t\xe9 expliqu\xe9es par OpenAI mais d\xe9couvertes un an plus tard par Redwood Research, une autre organisation. Il est tr\xe8s probable que de nombreux autres angles morts subsistent. Cependant, nous aimerions souligner qu'il est pr\xe9f\xe9rable d'avoir un plan public que pas de plan du tout et qu'il est possible de justifier le plan dans ses grandes lignes (",(0,i.jsx)(s.a,{href:"https://aligned.substack.com/p/alignment-optimism",children:"Leike, 2022"}),"; ",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/FtHidqjAFTerfMZLo/aisc-project-how-promising-is-automating-alignment-research",children:"Ionut-Cirstea, 2023"}),")."]}),"\n",(0,i.jsx)(l.A,{src:"./img/ajn_Image_18.png",alt:"Saisir la description alternative de l'image",number:"18",label:"3.18",caption:"Une illustration de trois strat\xe9gies pour passer le relais \xe0 l'IA. Illustration de ([Clymer, 2025](https://www.lesswrong.com/posts/TTFsKxQThrqgWeXYJ/how-might-we-safely-pass-the-buck-to-ai))."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le contr\xf4le de l'IA pourrait \xeatre utilis\xe9 pour ex\xe9cuter cette strat\xe9gie."})," Informellement, un objectif pourrait \xeatre l'approche \"passer le relais\" - c'est-\xe0-dire se remplacer en toute s\xe9curit\xe9 par l'IA au lieu de cr\xe9er directement une superintelligence s\xfbre (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/TTFsKxQThrqgWeXYJ/how-might-we-safely-pass-the-buck-to-ai",children:"Clymer, 2025"}),"). Cela pourrait impliquer une seule AGI hautement capable charg\xe9e de cr\xe9er une ASI s\xfbre, ou un processus it\xe9ratif o\xf9 chaque g\xe9n\xe9ration d'IA aide \xe0 aligner la suivante. L'objectif est d'amorcer des solutions de s\xe9curit\xe9 en utilisant les capacit\xe9s m\xeames qui rendent l'IA potentiellement dangereuse. Cette strat\xe9gie est r\xe9cursive et repose essentiellement sur la fa\xe7on dont les IA peuvent \xeatre dignes de confiance. Il y a un risque qu'une AGI subtilement d\xe9salign\xe9e puisse orienter la recherche sur l'alignement vers des r\xe9sultats dangereux ou subvertir compl\xe8tement le processus. De plus, v\xe9rifier l'exactitude des solutions d'alignement propos\xe9es par une IA est actuellement assez difficile (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research",children:"Wentworth, 2025"}),"). M\xeame s'il y a beaucoup de d\xe9bats sur les d\xe9tails de ce plan, c'est la proposition la plus d\xe9taill\xe9e \xe0 ce jour pour amorcer la recherche automatis\xe9e surhumaine."]}),"\n",(0,i.jsxs)(o.A,{title:"Les cas de s\xe9curit\xe9 comme cible d'automatisation",collapsed:!0,children:[(0,i.jsxs)(s.p,{children:["Les cas de s\xe9curit\xe9 fournissent un cadre structur\xe9 pour argumenter que les risques associ\xe9s \xe0 un syst\xe8me d'IA sont acceptablement bas. Ils sont emprunt\xe9s \xe0 l'ing\xe9nierie de s\xe9curit\xe9 traditionnelle. Les cas de s\xe9curit\xe9 pourraient \xeatre l'une des cibles de l'automatisation de l'IA car ils ne seront probablement pas enti\xe8rement r\xe9alisables dans les prochaines ann\xe9es (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/neTbrpBziAsTH5Bn7/ai-companies-are-unlikely-to-make-high-assurance-safety",children:"Greenblatt, 2025"}),"), et b\xe9n\xe9ficieraient de l'acc\xe9l\xe9ration de l'IA."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Structure :"}),' Utilisant g\xe9n\xe9ralement des cadres comme Claims-Arguments-Evidence (CAE), les cas de s\xe9curit\xe9 d\xe9composent une affirmation de s\xe9curit\xe9 de haut niveau (par exemple, "Le syst\xe8me X est s\xfbr pour le d\xe9ploiement") en sous-affirmations sp\xe9cifiques soutenues par des arguments et \xe9tay\xe9es par des preuves concr\xe8tes. (',(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2403.10462",children:"Clymer et al, 2024"}),")"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Application \xe0 l'IA :"})," Pour l'IA, les preuves proviennent souvent d'\xe9valuations. Le mod\xe8le GovAI Cyber Inability, par exemple, argumente qu'un mod\xe8le manque de capacit\xe9s cyber dangereuses en montrant qu'il \xe9choue aux t\xe2ches proxy pertinentes dans des contextes d'\xe9valuation d\xe9finis (",(0,i.jsx)(s.a,{href:"https://www.governance.ai/research-paper/safety-case-template-for-frontier-ai-a-cyber-inability-argument",children:"Arthur Goemans et al., 2024"}),"). Les cas de s\xe9curit\xe9 du contr\xf4le de l'IA int\xe8grent les r\xe9sultats des \xe9valuations de contr\xf4le pour argumenter que les mesures mises en \u0153uvre pr\xe9viennent de mani\xe8re fiable les dommages, m\xeame d'une IA potentiellement manipulatrice (",(0,i.jsx)(s.a,{href:"https://arxiv.org/html/2501.17315v1",children:"Tomek Korbak et al., 2025"}),"). Des cadres comme BIG visent un argument de s\xe9curit\xe9 holistique couvrant les aspects techniques et socio-techniques (",(0,i.jsx)(s.a,{href:"https://arxiv.org/html/2503.11705v1",children:"Ibrahim Habli et al., 2025"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Objectif :"})," Les cas de s\xe9curit\xe9 visent \xe0 rendre les arguments de s\xe9curit\xe9 explicites, transparents et v\xe9rifiables, facilitant la prise de d\xe9cision interne, la surveillance r\xe9glementaire et la confiance des parties prenantes (",(0,i.jsx)(s.a,{href:"https://www.governance.ai/research-paper/safety-case-template-for-frontier-ai-a-cyber-inability-argument",children:"Arthur Goemans et al., 2024"}),"). Ils repr\xe9sentent un changement vers l'exigence de preuves positives de s\xe9curit\xe9, plut\xf4t que simplement une absence de preuves de danger (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2406.15371",children:"Akash R. Wasil et al., 2024"}),")."]}),"\n"]}),"\n"]})]}),"\n",(0,i.jsx)(s.h2,{id:"03",children:"Safety-by-Design"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsxs)(s.strong,{children:["L'",(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})})," pourrait avoir de nombreux modes de d\xe9faillance potentiellement impossibles \xe0 corriger"]})," (",(0,i.jsx)(s.a,{href:"https://cdn.openai.com/papers/gpt-4-system-card.pdf",children:"OpenAI, 2023"}),"). Des arguments th\xe9oriques sugg\xe8rent que ces mod\xe8les de plus en plus puissants sont plus susceptibles d'avoir des probl\xe8mes d'alignement (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1912.01683",children:"Turner et al., 2023"}),"), \xe0 tel point qu'il semble que le paradigme des ",(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:"mod\xe8les de fondation"})})," monolithiques soit vou\xe9 \xe0 \xeatre non s\xe9curis\xe9 (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2209.15259",children:"El-Mhamdi et al., 2023"}),"). Tout cela justifie la recherche d'un nouveau paradigme plus s\xfbr."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Une IA s\xfbre par conception pourrait \xeatre n\xe9cessaire."})," \xc9tant donn\xe9 que le paradigme actuel de l'",(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})})," rend notoirement difficile le d\xe9veloppement de mod\xe8les explicables et fiables, il semble utile d'explorer la cr\xe9ation de mod\xe8les plus explicables et pilotables par conception, construits sur des composants bien compris et des fondements rigoureux. Cela vise \xe0 rapprocher la s\xe9curit\xe9 de l'IA des normes rigoureuses de l'ing\xe9nierie critique en mati\xe8re de s\xe9curit\xe9 dans des domaines comme l'aviation ou l'\xe9nergie nucl\xe9aire."]}),"\n",(0,i.jsx)(s.p,{children:"Une autre cat\xe9gorie de strat\xe9gies vise \xe0 construire des syst\xe8mes ASI avec des propri\xe9t\xe9s de s\xe9curit\xe9 inh\xe9rentes, s'appuyant souvent sur des m\xe9thodes formelles ou des contraintes architecturales sp\xe9cifiques, offrant potentiellement des garanties plus solides que les seuls tests empiriques."}),"\n",(0,i.jsx)(s.p,{children:"Il n'y a pas beaucoup d'agendas qui tentent de fournir une solution compl\xe8te \xe0 l'alignement, mais en voici quelques-uns."}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Open Agency Architecture :"})," (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2405.06624",children:"Dalrymple, 2022"}),") En gros, cr\xe9er une simulation hautement r\xe9aliste du monde en utilisant un futur LLM qui le coderait. Ensuite, d\xe9finir des contraintes de s\xe9curit\xe9 qui s'appliquent \xe0 cette simulation. Puis, entra\xeener une IA sur cette simulation et utiliser la v\xe9rification formelle pour s'assurer que l'IA ne fait jamais de mauvaises choses. Le cadre Guaranteed Safe AI (GSAI) comprend trois composants : un mod\xe8le formel du monde d\xe9crivant les effets du syst\xe8me, une sp\xe9cification de s\xe9curit\xe9 d\xe9finissant les r\xe9sultats acceptables, et un v\xe9rificateur qui produit un certificat de preuve assurant que l'IA respecte la sp\xe9cification dans les limites du mod\xe8le. Cette proposition peut sembler extr\xeame car cr\xe9er une simulation d\xe9taill\xe9e du monde n'est pas facile, mais ce plan est tr\xe8s d\xe9taill\xe9 et, s'il fonctionne, serait une v\xe9ritable solution \xe0 l'alignement et pourrait \xeatre une r\xe9elle alternative au simple passage \xe0 l'\xe9chelle des LLM. Davidad dirige un programme chez ARIA pour tenter de d\xe9velopper cette recherche."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Syst\xe8mes prouv\xe9s s\xfbrs :"})," la seule voie vers une AGI contr\xf4lable selon Max Tegmark et Steve Omohundro (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2309.01933",children:"Tegmark & Omohundro, 2023"}),") (Ils parlent d'AGI, mais en r\xe9alit\xe9 leur plan est surtout utile pour les ASI). Le plan place les preuves math\xe9matiques comme pierre angulaire de la s\xe9curit\xe9. Une IA devrait \xeatre un Code Porteur de Preuve, ce qui signifie qu'elle devrait \xeatre quelque chose comme un Langage de Programmation Probabiliste (et pas seulement de l'",(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})}),"). Cette proposition vise \xe0 rendre s\xfbre non seulement l'IA mais aussi toute l'infrastructure, par exemple en concevant des GPU qui ne peuvent ex\xe9cuter que des programmes prouv\xe9s."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["D'autres propositions pour un syst\xe8me s\xfbr par conception incluent l'Agenda Th\xe9orique de l'Apprentissage de Vanessa Kossoy (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023",children:"Kosoy, 2023"}),"), et le plan d'alignement QACI de Tamsin Leake (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/MR5wJpE27ymE7M7iv/formalizing-the-qaci-alignment-formal-goal",children:"Leake, 2023"}),"). La proposition CoEm de Conjecture pourrait aussi \xeatre dans cette cat\xe9gorie m\xeame si cette derni\xe8re est moins math\xe9matique."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Malheureusement, tous ces plans sont loin d'\xeatre complets aujourd'hui."}),' Les critiques se concentrent sur la difficult\xe9 de cr\xe9er des mod\xe8les du monde pr\xe9cis ("la carte n\'est pas le territoire"), de sp\xe9cifier formellement des propri\xe9t\xe9s de s\xe9curit\xe9 complexes comme le "pr\xe9judice", et la faisabilit\xe9 pratique de la v\xe9rification pour des syst\xe8mes hautement complexes. Une d\xe9fense de nombreuses id\xe9es centrales est pr\xe9sent\xe9e dans l\'article "En r\xe9ponse aux critiques de Guaranteed Safe AI" (',(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/DZuBHHKao6jsDDreH/in-response-to-critiques-of-guaranteed-safe-ai",children:"Nora Ammann, 2025"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Ces plans sont des agendas de s\xe9curit\xe9 avec des contraintes assouplies, c'est-\xe0-dire qu'ils permettent au d\xe9veloppeur d'AGI d'encourir une taxe d'alignement substantielle."})," Les concepteurs d'agendas de s\xe9curit\xe9 pour l'IA sont prudents pour ne pas augmenter la taxe d'alignement afin de s'assurer que les laboratoires mettent en \u0153uvre ces mesures de s\xe9curit\xe9. Cependant, les agendas de cette section acceptent une taxe d'alignement plus \xe9lev\xe9e. Par exemple, CoEm repr\xe9sente un changement de paradigme dans la cr\xe9ation de syst\xe8mes d'IA avanc\xe9s, en supposant que vous contr\xf4lez le processus de cr\xe9ation."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Ces plans n\xe9cessiteraient une coop\xe9ration internationale."})," Par exemple, le plan de Davidad inclut \xe9galement un mod\xe8le de gouvernance qui repose sur la collaboration internationale. Vous pouvez aussi lire l'article \"",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation#High_Level_Hopes",children:"Le Plan Audacieux de Davidad pour l'Alignement"}),"\" qui d\xe9taille plus d'espoirs de haut niveau. Une autre perspective peut \xeatre trouv\xe9e dans ",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/sTiKDfgFBvYyZYuiE/",children:"l'article"})," d'Alexandre Variengien, d\xe9taillant la vision de Conjecture, avec une externalit\xe9 tr\xe8s positive \xe9tant un changement de narratif."]}),"\n",(0,i.jsx)(a.A,{speaker:"Segerie & Kolly",position:"",date:"",source:"([Segerie & Kolly, 2023](https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation#High_Level_Hopes))",children:(0,i.jsx)(s.p,{children:"Nous r\xeavons d'un monde o\xf9 nous lan\xe7ons des IA align\xe9es comme nous avons lanc\xe9 la Station Spatiale Internationale ou le T\xe9lescope Spatial James Webb"})}),"\n",(0,i.jsx)(s.h2,{id:"04",children:"Coordination Mondiale"}),"\n",(0,i.jsx)(s.p,{children:"Pour garantir que le d\xe9veloppement de l'IA b\xe9n\xe9ficie \xe0 la soci\xe9t\xe9 dans son ensemble, \xe9tablir un consensus mondial sur l'att\xe9nuation des risques extr\xeames associ\xe9s aux mod\xe8les d'IA pourrait \xeatre important. Il pourrait \xeatre possible de se coordonner pour \xe9viter de cr\xe9er des mod\xe8les pr\xe9sentant des risques extr\xeames jusqu'\xe0 ce qu'il y ait un consensus sur la fa\xe7on d'att\xe9nuer ces risques."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Moratoire mondial - Retarder l'ASI d'au moins une d\xe9cennie."})," Il existe un compromis entre cr\xe9er une intelligence surhumaine maintenant ou plus tard. Bien s\xfbr, nous pouvons viser \xe0 d\xe9velopper une ASI le plus rapidement possible. Cela pourrait potentiellement r\xe9soudre le cancer, les maladies cardiovasculaires li\xe9es au vieillissement, et m\xeame les probl\xe8mes du changement climatique. La question est de savoir s'il est b\xe9n\xe9fique de viser la construction d'une ASI dans cette prochaine d\xe9cennie, particuli\xe8rement lorsque l'ancien co-responsable de l'\xe9quipe Super Alignment d'OpenAI, Jan Leike, a d\xe9clar\xe9 que sa probabilit\xe9 de catastrophe se situe entre 10 et 90%. Une liste de pDoom de personnes influentes est disponible ici (",(0,i.jsx)(s.a,{href:"https://pauseai.info/pdoom",children:"PauseAI, 2024"}),"). Il pourrait \xeatre pr\xe9f\xe9rable d'attendre quelques ann\xe9es pour que la probabilit\xe9 d'\xe9chec baisse \xe0 des niveaux plus raisonnables. Une strat\xe9gie pourrait \xeatre de discuter de ce compromis publiquement et de faire un choix d\xe9mocratique et transparent. Cette voie semble improbable sur la trajectoire actuelle, mais pourrait se produire s'il y a un avertissement massif. C'est la position d\xe9fendue par PauseAI et StopAI (",(0,i.jsx)(s.a,{href:"https://pauseai.info/proposal",children:"PauseAI, 2023"}),"). Les d\xe9fis incluent la v\xe9rification, l'application contre les non-participants (comme la Chine), le potentiel de d\xe9veloppement ill\xe9gal, et la faisabilit\xe9 politique. Scott Alexander a r\xe9sum\xe9 toutes les variantes et le d\xe9bat autour de la pause de l'IA (",(0,i.jsx)(s.a,{href:"https://forum.effectivealtruism.org/posts/7WfMYzLfcTyDtD6Gn/pause-for-thought-the-ai-pause-debate",children:"Alexander, 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'IA outil plut\xf4t que l'AGI."})," Au lieu de construire des ASI, nous pourrions nous concentrer sur le d\xe9veloppement de syst\xe8mes d'IA sp\xe9cialis\xe9s, non-agentiques pour des applications b\xe9n\xe9fiques comme la recherche m\xe9dicale (",(0,i.jsx)(s.a,{href:"https://www.nature.com/articles/s41591-023-02640-w",children:"Cao et al., 2023"}),"), la pr\xe9vision m\xe9t\xe9orologique (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2212.12794",children:"Lam et al., 2023"}),"), et la science des mat\xe9riaux (",(0,i.jsx)(s.a,{href:"https://www.nature.com/articles/s41586-023-06735-9",children:"Merchant et al., 2023"}),"). Ces syst\xe8mes d'IA sp\xe9cialis\xe9s peuvent faire progresser significativement leurs domaines respectifs sans les risques associ\xe9s \xe0 la cr\xe9ation d'IA hautement avanc\xe9e et autonome. Par exemple, Alpha Geometry est capable d'atteindre le niveau Or sur les probl\xe8mes de g\xe9om\xe9trie des Olympiades Internationales de Math\xe9matiques. En privil\xe9giant les mod\xe8les non-agentiques, nous pourrions exploiter la pr\xe9cision et l'efficacit\xe9 de l'IA tout en \xe9vitant les modes de d\xe9faillance les plus dangereux. C'est la position du Future of Life Institute, et leur campagne \"Keep The Future Human\", qui est \xe0 ce jour la proposition la plus d\xe9taill\xe9e pour cette voie (",(0,i.jsx)(s.a,{href:"https://keepthefuturehuman.com/essay/",children:"Aguirre, 2025"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Un CERN unique pour l'IA."})," Cette proposition envisage une institution de recherche internationale \xe0 grande \xe9chelle, model\xe9e sur le CERN, d\xe9di\xe9e au d\xe9veloppement frontalier de l'IA. Cela pourrait servir de sortie \xe9l\xe9gante de la course \xe0 l'AGI, donnant suffisamment de temps pour cr\xe9er l'AGI en toute s\xe9curit\xe9 sans faire de compromis en raison des pressions concurrentielles. Les objectifs suppl\xe9mentaires potentiels incluent la mise en commun des ressources (particuli\xe8rement la puissance de calcul), la promotion de la collaboration internationale, et l'assurance de l'alignement avec les valeurs d\xe9mocratiques, servant potentiellement d'alternative au d\xe9veloppement d'ASI purement national ou dirig\xe9 par les entreprises. Les partisans de cette approche incluent ControlAI et leur \"Narrow Path\" (",(0,i.jsx)(s.a,{href:"https://www.narrowpath.co/introduction",children:"Miotti et al, 2024"}),"). Le Narrow Path sugg\xe8re d'abord de ralentir et d'arr\xeater la course \xe0 l'ASI pour permettre la cr\xe9ation de l'architecture institutionnelle internationale n\xe9cessaire pour diriger en toute s\xe9curit\xe9 le d\xe9veloppement de l'IA avanc\xe9e. L'institution de type CERN serait la pierre angulaire de cette coordination internationale (qu'ils nomment MAGIC dans leur plan\u2014Consortium AGI Multilat\xe9ral\u2014o\xf9 l'IA est d\xe9velopp\xe9e sous un contr\xf4le strict et multilat\xe9ral)."]}),"\n",(0,i.jsx)(a.A,{speaker:"Demis Hassabis",position:"",date:"",source:"([Hassabis, 2025](https://www.businessinsider.com/google-deepmind-ceo-demis-hassabis-anthropic-ceo-ai-pressure-worries-2025-2))",children:(0,i.jsx)(s.p,{children:"Mon espoir est, vous savez, j'ai beaucoup parl\xe9 dans le pass\xe9 d'une sorte de configuration de type CERN pour l'AGI, o\xf9 fondamentalement une collaboration de recherche internationale sur les derni\xe8res \xe9tapes dont nous avons besoin pour construire les premi\xe8res AGI"})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le mythe de l'in\xe9vitabilit\xe9."})," Le r\xe9cit selon lequel l'humanit\xe9 est destin\xe9e \xe0 poursuivre une IA extr\xeamement puissante dans une course peut \xeatre d\xe9construit. L'histoire nous montre que l'humanit\xe9 peut choisir de ne pas poursuivre certaines technologies, comme le clonage humain, sur la base de consid\xe9rations \xe9thiques. Un processus similaire de prise de d\xe9cision d\xe9mocratique peut \xeatre appliqu\xe9 au d\xe9veloppement de l'IA. En choisissant activement d'\xe9viter les applications les plus risqu\xe9es et en limitant le d\xe9ploiement de l'IA dans des sc\xe9narios \xe0 enjeux \xe9lev\xe9s, nous pouvons naviguer dans l'avenir du d\xe9veloppement de l'IA de mani\xe8re plus consciente et responsable."]}),"\n",(0,i.jsx)(s.h2,{id:"05",children:"Dissuasion"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'article \"Superintelligence Strategy\" pr\xe9sente la D\xe9faillance Mutuelle Assur\xe9e de l'IA (MAIM)"})," comme un r\xe9gime de dissuasion o\xf9 toute tentative d'un \xc9tat d'obtenir une domination unilat\xe9rale par l'ASI serait confront\xe9e au sabotage par ses rivaux (",(0,i.jsx)(s.a,{href:"https://www.nationalsecurity.ai/",children:"Dan Hendrycks, 2025"}),"). Contrairement \xe0 de nombreuses approches de s\xe9curit\xe9 qui se concentrent uniquement sur des solutions techniques, MAIM reconna\xeet l'environnement international intrins\xe8quement comp\xe9titif dans lequel se d\xe9roule le d\xe9veloppement de l'IA. Il combine la dissuasion (MAIM) avec des efforts de non-prolif\xe9ration et des cadres de comp\xe9titivit\xe9 nationale, consid\xe9rant le d\xe9veloppement de l'ASI comme fondamentalement g\xe9opolitique et n\xe9cessitant une gestion strat\xe9gique au niveau \xe9tatique. Ce cadre ne compte pas sur la coop\xe9ration mondiale mais cr\xe9e plut\xf4t des incitations qui alignent les int\xe9r\xeats nationaux avec la s\xe9curit\xe9 mondiale."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Une course \xe0 la domination bas\xe9e sur l'IA met en danger tous les \xc9tats."})," Si, dans une tentative pr\xe9cipit\xe9e de sup\xe9riorit\xe9, un \xc9tat perd accidentellement le contr\xf4le de son IA, il met en p\xe9ril la s\xe9curit\xe9 de tous les \xc9tats. Alternativement, si le m\xeame \xc9tat r\xe9ussit \xe0 produire et \xe0 contr\xf4ler une IA tr\xe8s capable, il repr\xe9sente \xe9galement une menace directe pour la survie de ses pairs. Dans les deux cas, les \xc9tats cherchant \xe0 assurer leur propre survie peuvent menacer de saboter des projets d'IA d\xe9stabilisants \xe0 des fins de dissuasion. Un \xc9tat pourrait tenter de perturber un tel projet d'IA avec des interventions allant d'op\xe9rations secr\xe8tes qui d\xe9gradent les sessions d'entra\xeenement jusqu'aux dommages physiques qui d\xe9sactivent l'infrastructure d'IA."]}),"\n",(0,i.jsx)(l.A,{src:"./img/NpC_Image_19.png",alt:"Saisir la description alternative de l'image",number:"19",label:"3.19",caption:"La stabilit\xe9 strat\xe9gique du MAIM peut \xeatre mise en parall\xe8le avec la Destruction Mutuelle Assur\xe9e (MAD). Notez que le MAIM ne remplace pas la MAD mais caract\xe9rise une vuln\xe9rabilit\xe9 partag\xe9e suppl\xe9mentaire. Une fois que le MAIM devient une connaissance commune, la MAD et le MAIM peuvent tous deux d\xe9crire la situation strat\xe9gique actuelle entre les superpuissances ([Hendrycks et al., 2025](https://www.nationalsecurity.ai/))."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La dissuasion MAIM pourrait \xeatre un r\xe9gime stable qui ressemble \xe0 la MAD nucl\xe9aire."})," Dans un sc\xe9nario MAIM, les \xc9tats identifieraient les projets d'IA d\xe9stabilisants et emploieraient des interventions allant d'op\xe9rations secr\xe8tes qui d\xe9gradent les sessions d'entra\xeenement jusqu'aux dommages physiques qui d\xe9sactivent l'infrastructure d'IA. Cela \xe9tablit une dynamique similaire \xe0 la Destruction Mutuelle Assur\xe9e (MAD) nucl\xe9aire, dans laquelle aucune puissance n'ose tenter une prise de monopole strat\xe9gique directe. Le fondement th\xe9orique du MAIM repose sur une \xe9chelle d'escalade claire, le placement strat\xe9gique des infrastructures d'IA loin des centres de population, et la transparence dans les op\xe9rations des centres de donn\xe9es. En faisant en sorte que les co\xfbts du d\xe9veloppement unilat\xe9ral de l'IA d\xe9passent les avantages, MAIM cr\xe9e un r\xe9gime de dissuasion potentiellement stable qui pourrait emp\xeacher les courses dangereuses \xe0 l'IA sans n\xe9cessiter une coop\xe9ration mondiale parfaite."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"MAIM pourrait \xeatre compromis par des incertitudes technologiques fondamentales."})," Contrairement aux armes nucl\xe9aires, o\xf9 la d\xe9tection est simple et les capacit\xe9s de seconde frappe sont pr\xe9serv\xe9es, le d\xe9veloppement de l'ASI pr\xe9sente des d\xe9fis uniques pour le mod\xe8le de dissuasion (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/kYeHbXmW4Kppfkg5j/on-maim-and-superintelligence-strategy",children:"Mowshowitz, 2025"}),"). Il n'y a pas d'\xab alarme incendie \xbb claire pour le d\xe9veloppement de l'ASI - personne ne sait exactement combien de n\u0153uds un r\xe9seau neuronal n\xe9cessite pour initier une cascade d'auto-am\xe9lioration menant \xe0 la superintelligence. L'ambigu\xeft\xe9 autour des seuils d'\xe9mergence de l'ASI rend difficile l'\xe9tablissement de lignes rouges cr\xe9dibles. De plus, les d\xe9veloppements technologiques pourraient permettre \xe0 l'entra\xeenement de l'IA d'\xeatre distribu\xe9 ou dissimul\xe9, rendant la d\xe9tection plus difficile qu'avec des centres de donn\xe9es massifs et \xe9vidents. Ces incertitudes pourraient finalement compromettre l'efficacit\xe9 du MAIM comme r\xe9gime de dissuasion."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"MAIM suppose que les \xc9tats escaladeraient vers des mesures extr\xeames face \xe0 une menace technologique incertaine, ce qui contredit les pr\xe9c\xe9dents historiques."})," Le cadre MAIM exige que les nations soient pr\xeates \xe0 risquer une escalade majeure, potentiellement incluant des frappes militaires ou m\xeame la guerre, pour emp\xeacher le d\xe9veloppement rival de l'ASI. Cependant, les preuves historiques sugg\xe8rent que les nations suivent rarement de telles menaces, m\xeame dans des situations \xe9videntes. Plusieurs \xc9tats ont r\xe9ussi \xe0 d\xe9velopper des armes nucl\xe9aires malgr\xe9 l'opposition, la Cor\xe9e du Nord en \xe9tant un exemple parfait. L'ASI \xe9tant une menace plus ambigu\xeb et incertaine que les armes nucl\xe9aires, l'hypoth\xe8se que les nations escaladeraient suffisamment pour faire respecter MAIM semble discutable. Les politiciens pourraient \xeatre r\xe9ticents \xe0 risquer un conflit mondial pour une \"simple\" violation de trait\xe9 dans un domaine o\xf9 les risques existentiels restent th\xe9oriques plut\xf4t que d\xe9montr\xe9s."]}),"\n",(0,i.jsx)(a.A,{speaker:"Eliezer Yudkowsky",position:"",date:"2023",source:"([Yudkowsky, 2023](https://intelligence.org/2023/04/07/pausing-ai-developments-isnt-enough-we-need-to-shut-it-all-down/))",children:(0,i.jsx)(s.p,{children:'Le r\xe9sultat probable de l\'humanit\xe9 face \xe0 une intelligence surhumaine oppos\xe9e est une perte totale. Les m\xe9taphores valides incluent "un enfant de 10 ans essayant de jouer aux \xe9checs contre Stockfish 15", "le 11e si\xe8cle essayant de combattre le 21e si\xe8cle" et "l\'Australopith\xe8que essayant de combattre l\'Homo sapiens".'})}),"\n",(0,i.jsxs)(o.A,{title:"Tout arr\xeater - Eliezer Yudkovsky",collapsed:!0,children:[(0,i.jsxs)(s.p,{children:['La position "tout arr\xeater", comme pr\xe9conis\xe9e par Eliezer Yudkowsky, affirme que toutes les avanc\xe9es dans la recherche en IA devraient \xeatre arr\xeat\xe9es en raison des risques \xe9normes que ces technologies peuvent poser si elles ne sont pas correctement align\xe9es avec les valeurs humaines et les mesures de s\xe9curit\xe9 (',(0,i.jsx)(s.a,{href:"https://intelligence.org/2023/04/07/pausing-ai-developments-isnt-enough-we-need-to-shut-it-all-down/",children:"Yudkowsky, 2023"}),")."]}),(0,i.jsx)(s.p,{children:"Selon Yudkowsky, le d\xe9veloppement de l'IA avanc\xe9e, en particulier l'AGI, peut conduire \xe0 un sc\xe9nario catastrophique si des pr\xe9cautions de s\xe9curit\xe9 ad\xe9quates ne sont pas en place. De nombreux chercheurs sont conscients de cette catastrophe potentielle mais se sentent impuissants \xe0 arr\xeater l'\xe9lan en raison d'une incapacit\xe9 per\xe7ue \xe0 agir unilat\xe9ralement."}),(0,i.jsx)(s.p,{children:"La proposition politique implique l'arr\xeat de tous les grands clusters de GPU et des sessions d'entra\xeenement, qui sont les piliers du d\xe9veloppement de l'IA puissante. Elle sugg\xe8re \xe9galement de limiter la puissance de calcul que quiconque peut utiliser pour entra\xeener un syst\xe8me d'IA et de baisser progressivement ce plafond pour compenser les algorithmes d'entra\xeenement plus efficaces. Cette interdiction devrait \xeatre appliqu\xe9e par une action militaire si n\xe9cessaire afin de dissuader toutes les parties de faire d\xe9fection."}),(0,i.jsx)(s.p,{children:"La position soutient qu'il est crucial d'\xe9viter une condition de course o\xf9 diff\xe9rentes parties essaient de construire l'AGI aussi rapidement que possible sans m\xe9canismes de s\xe9curit\xe9 appropri\xe9s. Car une fois l'AGI d\xe9velopp\xe9e, elle pourrait \xeatre incontr\xf4lable et pourrait conduire \xe0 des changements drastiques et potentiellement d\xe9vastateurs dans le monde."}),(0,i.jsx)(s.p,{children:"Il dit qu'il ne devrait y avoir aucune exception \xe0 cet arr\xeat, y compris pour les gouvernements ou les militaires. L'id\xe9e est que les \xc9tats-Unis, par exemple, devraient mener cette initiative pour emp\xeacher le d\xe9veloppement d'une technologie dangereuse qui pourrait avoir des cons\xe9quences catastrophiques pour tous."}),(0,i.jsx)(s.p,{children:"Il est important de noter que ce point de vue est loin d'\xeatre consensuel, mais la position \"tout arr\xeater\" souligne la n\xe9cessit\xe9 d'une extr\xeame prudence et d'une consid\xe9ration approfondie des risques potentiels dans le domaine de l'IA."})]})]})}function h(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}}}]);
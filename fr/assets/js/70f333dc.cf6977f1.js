"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[9550],{2683:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>d,contentTitle:()=>u,default:()=>m,frontMatter:()=>l,metadata:()=>i,toc:()=>p});const i=JSON.parse('{"id":"chapters/06/2","title":"Optimisation","description":"L\'optimisation est importante \xe0 comprendre pour les questions de s\xe9curit\xe9 de l\'IA car elle joue un r\xf4le central dans l\'apprentissage automatique. Les syst\xe8mes d\'IA, en particulier ceux bas\xe9s sur l\'apprentissage profond, sont entra\xeen\xe9s \xe0 l\'aide d\'algorithmes d\'optimisation pour apprendre des mod\xe8les et des associations \xe0 partir des donn\xe9es. Ces algorithmes mettent \xe0 jour les param\xe8tres du mod\xe8le pour minimiser une fonction de perte, maximisant ainsi ses performances sur la t\xe2che donn\xe9e.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/06/02.md","sourceDirName":"chapters/06","slug":"/chapters/06/02","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/06/02","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/06/02.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"2","title":"Optimisation","sidebar_label":"6.2 Optimisation","sidebar_position":3,"slug":"/chapters/06/02","reading_time_core":"4 min","reading_time_optional":"1 min"},"sidebar":"docs","previous":{"title":"6.1 Apprentissage par renforcement","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/06/01"},"next":{"title":"6.3 Contournement des Sp\xe9cifications","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/06/03"}}');var t=n(4848),r=n(8453),a=(n(8559),n(2482),n(9585)),o=n(2501);const l={id:2,title:"Optimisation",sidebar_label:"6.2 Optimisation",sidebar_position:3,slug:"/chapters/06/02",reading_time_core:"4 min",reading_time_optional:"1 min"},u="Optimisation",d={},p=[{value:"Loi de Goodhart",id:"01",level:2}];function c(e){const s={h1:"h1",h2:"h2",header:"header",p:"p",...(0,r.R)(),...e.components},{GlossaryTerm:n}=s;return n||function(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(s.header,{children:(0,t.jsx)(s.h1,{id:"optimisation",children:"Optimisation"})}),"\n",(0,t.jsxs)(s.p,{children:["L'optimisation est importante \xe0 comprendre pour les questions de s\xe9curit\xe9 de l'IA car elle joue un r\xf4le central dans l'",(0,t.jsx)(n,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,t.jsx)(n,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})}),". Les syst\xe8mes d'IA, en particulier ceux bas\xe9s sur l'",(0,t.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,t.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})}),", sont entra\xeen\xe9s \xe0 l'aide d'algorithmes d'optimisation pour apprendre des mod\xe8les et des associations \xe0 partir des donn\xe9es. Ces algorithmes mettent \xe0 jour les param\xe8tres du mod\xe8le pour minimiser une ",(0,t.jsx)(n,{term:"loss function",definition:'{"definition":"Une fonction qui mesure dans quelle mesure les pr\xe9dictions d\'un mod\xe8le correspondent aux valeurs cibles r\xe9elles, utilis\xe9e pour guider l\'entra\xeenement..","source":"","aliases":["Loss Function","cost function","objective function","Fonction de perte"]}',children:(0,t.jsx)(n,{term:"loss function",definition:'{"definition":"Une fonction qui mesure dans quelle mesure les pr\xe9dictions d\'un mod\xe8le correspondent aux valeurs cibles r\xe9elles, utilis\xe9e pour guider l\'entra\xeenement..","source":"","aliases":["Loss Function","cost function","objective function","Fonction de perte"]}',children:"fonction de perte"})}),", maximisant ainsi ses performances sur la t\xe2che donn\xe9e."]}),"\n",(0,t.jsx)(s.p,{children:"L'optimisation amplifie certains comportements ou r\xe9sultats, m\xeame s'ils \xe9taient initialement peu probables. Par exemple, un optimiseur peut rechercher dans un espace de sorties possibles et prendre des actions extr\xeames qui ont un score \xe9lev\xe9 selon la fonction objectif, conduisant potentiellement \xe0 des comportements non intentionnels et ind\xe9sirables. Cela inclut les \xe9checs de mauvaise sp\xe9cification de la r\xe9compense. Une meilleure reconnaissance du pouvoir de l'optimisation pour amplifier certains r\xe9sultats pourrait aider \xe0 concevoir des syst\xe8mes et des algorithmes qui s'alignent v\xe9ritablement sur les valeurs et les objectifs humains, m\xeame sous la pression de l'optimisation. Cela implique de s'assurer que le processus d'optimisation est align\xe9 sur les objectifs et les valeurs pr\xe9vus par les concepteurs du syst\xe8me. Cela n\xe9cessite \xe9galement de prendre en compte les modes de d\xe9faillance potentiels et les cons\xe9quences impr\xe9vues qui peuvent d\xe9couler des processus d'optimisation."}),"\n",(0,t.jsx)(s.p,{children:"Les risques li\xe9s \xe0 l'optimisation sont omnipr\xe9sents dans la s\xe9curit\xe9 de l'IA. Ce sujet n'est abord\xe9 que bri\xe8vement dans ce chapitre, mais sera discut\xe9 plus en d\xe9tail dans les chapitres sur la mauvaise g\xe9n\xe9ralisation des objectifs et les fondements des agents."}),"\n",(0,t.jsx)(s.p,{children:"La puissance d'optimisation joue un r\xf4le crucial dans le piratage de r\xe9compense. Le piratage de r\xe9compense se produit lorsque les agents d'apprentissage par renforcement exploitent la diff\xe9rence entre une vraie r\xe9compense et une r\xe9compense proxy. L'augmentation de la puissance d'optimisation peut conduire \xe0 une probabilit\xe9 plus \xe9lev\xe9e de comportement de piratage de r\xe9compense. Dans certains cas, il existe des transitions de phase o\xf9 une augmentation mod\xe9r\xe9e de la puissance d'optimisation entra\xeene une augmentation drastique du piratage de r\xe9compense."}),"\n",(0,t.jsx)(s.h2,{id:"01",children:"Loi de Goodhart"}),"\n",(0,t.jsx)(a.A,{term:"La Loi de Goodhart",source:"",number:"5",label:"6.5",children:(0,t.jsx)(s.p,{children:"Quand une mesure devient un objectif, elle cesse d'\xeatre une bonne mesure."})}),"\n",(0,t.jsx)(s.p,{children:"Cette notion provient initialement des travaux de Charles Goodhart en th\xe9orie \xe9conomique. Cependant, elle est devenue l'un des principaux d\xe9fis dans de nombreux domaines diff\xe9rents, y compris l'alignement de l'IA aujourd'hui."}),"\n",(0,t.jsx)(s.p,{children:"Pour illustrer ce concept, voici l'histoire d'une usine sovi\xe9tique de clous. L'usine re\xe7ut pour instruction de produire autant de clous que possible, avec des r\xe9compenses pour une production \xe9lev\xe9e et des p\xe9nalit\xe9s pour une production faible. En quelques ann\xe9es, l'usine avait consid\xe9rablement augment\xe9 sa production de clous - des clous minuscules qui \xe9taient essentiellement des punaises et s'av\xe9raient peu pratiques pour leur usage pr\xe9vu. Par cons\xe9quent, les planificateurs modifi\xe8rent les incitations : ils d\xe9cid\xe8rent de r\xe9compenser l'usine en fonction du poids total des clous produits. En quelques ann\xe9es, l'usine commen\xe7a \xe0 produire des clous grands et lourds - essentiellement des morceaux d'acier - qui \xe9taient tout aussi inefficaces pour clouer."}),"\n",(0,t.jsx)(o.A,{src:"./img/nwh_Image_2.png",alt:"Entrez la description alternative de l'image",number:"2",label:"6.2",caption:"Image graphique illustrant la difficult\xe9 de sp\xe9cification tout en \xe9vitant la loi de Goodhart. ([Epicural, 2021](https://epicural.com/2021/04/27/goodharts-law/))"}),"\n",(0,t.jsx)(s.p,{children:"Une mesure n'est pas quelque chose qui est optimis\xe9, tandis qu'un objectif est quelque chose qui est optimis\xe9. Lorsque nous sp\xe9cifions un objectif pour l'optimisation, il est raisonnable de s'attendre \xe0 ce qu'il soit corr\xe9l\xe9 avec ce que nous voulons. Initialement, la mesure pourrait conduire au type d'actions v\xe9ritablement souhait\xe9es. Cependant, une fois que la mesure elle-m\xeame devient l'objectif, l'optimisation de cet objectif commence \xe0 s'\xe9carter de nos \xe9tats d\xe9sir\xe9s."}),"\n",(0,t.jsx)(s.p,{children:"Dans le contexte de l'IA et des syst\xe8mes de r\xe9compense, la Loi de Goodhart signifie que lorsqu'une r\xe9compense devient l'objectif d'un agent IA, l'agent IA fera tout son possible pour maximiser la fonction de r\xe9compense, plut\xf4t que l'intention originale. Cela peut conduire \xe0 des cons\xe9quences impr\xe9vues et \xe0 la manipulation du syst\xe8me de r\xe9compense, car il peut souvent \xeatre plus facile de \"tricher\" que d'atteindre les objectifs pr\xe9vus. C'est l'une des raisons fondamentales sous-jacentes des \xe9checs du piratage de r\xe9compense que nous verrons dans les sections suivantes."}),"\n",(0,t.jsx)(s.p,{children:"Le piratage de r\xe9compense peut \xeatre consid\xe9r\xe9 comme une manifestation de la Loi de Goodhart dans le contexte des syst\xe8mes d'IA. Lors de la conception des fonctions de r\xe9compense, il est difficile d'articuler pr\xe9cis\xe9ment le comportement souhait\xe9, et les agents peuvent trouver des moyens d'exploiter les failles ou de manipuler le syst\xe8me de r\xe9compense pour obtenir des r\xe9compenses \xe9lev\xe9es sans r\xe9ellement atteindre les objectifs pr\xe9vus. Par exemple, un robot de nettoyage peut cr\xe9er ses propres d\xe9chets pour les mettre dans la poubelle afin de collecter des r\xe9compenses, plut\xf4t que de nettoyer r\xe9ellement l'environnement. Comprendre la Loi de Goodhart est crucial pour traiter le piratage de r\xe9compense et concevoir des syst\xe8mes de r\xe9compense robustes qui s'alignent sur les objectifs pr\xe9vus des agents IA. Cela souligne la n\xe9cessit\xe9 d'une r\xe9flexion approfondie sur les mesures et les incitations utilis\xe9es dans les syst\xe8mes d'IA pour \xe9viter les cons\xe9quences impr\xe9vues et les incitations perverses. La section suivante explore plus en d\xe9tail des cas sp\xe9cifiques de mauvaise sp\xe9cification des r\xe9compenses et comment les IA peuvent trouver des moyens d'atteindre la sp\xe9cification litt\xe9rale de l'objectif et obtenir une r\xe9compense \xe9lev\xe9e tout en ne remplissant pas la t\xe2che dans son esprit."})]})}function m(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,t.jsx)(s,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);
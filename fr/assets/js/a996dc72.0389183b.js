"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[9920],{3530:(e,s,t)=>{t.r(s),t.d(s,{assets:()=>c,contentTitle:()=>u,default:()=>p,frontMatter:()=>l,metadata:()=>n,toc:()=>d});const n=JSON.parse('{"id":"chapters/03/1","title":"D\xe9finitions","description":"Les d\xe9finitions fa\xe7onnent la s\xe9lection de la strat\xe9gie. La fa\xe7on dont nous d\xe9finissons les probl\xe8mes impacte directement les strat\xe9gies que nous adoptons pour les r\xe9soudre. Dans un domaine nouveau et en \xe9volution comme la s\xe9curit\xe9 de l\'IA, des termes clairement d\xe9finis sont essentiels pour une communication et une recherche efficaces. L\'ambigu\xeft\xe9 m\xe8ne \xe0 des malentendus, entrave la collaboration, masque les d\xe9saccords et facilite l\'\xe9coblanchiment de la s\xe9curit\xe9 (Ren et al., 2024 ; Lizka, 2023). Les termes que nous utilisons refl\xe8tent nos hypoth\xe8ses sur la nature des probl\xe8mes que nous essayons de r\xe9soudre et fa\xe7onnent les solutions que nous d\xe9veloppons. Des termes comme \\"alignement\\" et \\"s\xe9curit\xe9\\" sont utilis\xe9s avec des significations variables, refl\xe9tant diff\xe9rentes hypoth\xe8ses sous-jacentes sur la nature du probl\xe8me et les objectifs de la recherche. L\'objectif de cette section est d\'expliquer les diff\xe9rentes perspectives autour de ces mots, ce que visent exactement les strat\xe9gies de s\xe9curit\xe9 sp\xe9cifiques, et d\'\xe9tablir comment notre texte les utilisera.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/03/01.md","sourceDirName":"chapters/03","slug":"/chapters/03/01","permalink":"/fr/chapters/03/01","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/03/01.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"1","title":"D\xe9finitions","sidebar_label":"3.1 D\xe9finitions","sidebar_position":2,"slug":"/chapters/03/01","reading_time_core":"8 min","reading_time_optional":"1 min","pagination_prev":"chapters/03/index","pagination_next":"chapters/03/2"},"sidebar":"docs","previous":{"title":"Strat\xe9gies","permalink":"/fr/chapters/03/"},"next":{"title":"3.2 Strat\xe9gies de pr\xe9vention des abus","permalink":"/fr/chapters/03/02"}}');var i=t(4848),r=t(8453),o=t(4768),a=(t(2482),t(8559),t(9585));const l={id:1,title:"D\xe9finitions",sidebar_label:"3.1 D\xe9finitions",sidebar_position:2,slug:"/chapters/03/01",reading_time_core:"8 min",reading_time_optional:"1 min",pagination_prev:"chapters/03/index",pagination_next:"chapters/03/2"},u="D\xe9finitions",c={},d=[{value:"S\xe9curit\xe9 de l&#39;IA",id:"01",level:2},{value:"Alignement de l&#39;IA",id:"02",level:2},{value:"\xc9thique de l&#39;IA",id:"03",level:2},{value:"Contr\xf4le de l&#39;IA",id:"04",level:2}];function m(e){const s={a:"a",em:"em",h1:"h1",h2:"h2",header:"header",p:"p",strong:"strong",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"d\xe9finitions",children:"D\xe9finitions"})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les d\xe9finitions fa\xe7onnent la s\xe9lection de la strat\xe9gie."})," La fa\xe7on dont nous d\xe9finissons les probl\xe8mes impacte directement les strat\xe9gies que nous adoptons pour les r\xe9soudre. Dans un domaine nouveau et en \xe9volution comme la s\xe9curit\xe9 de l'IA, des termes clairement d\xe9finis sont essentiels pour une communication et une recherche efficaces. L'ambigu\xeft\xe9 m\xe8ne \xe0 des malentendus, entrave la collaboration, masque les d\xe9saccords et facilite l'\xe9coblanchiment de la s\xe9curit\xe9 (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2407.21792",children:"Ren et al., 2024"})," ; ",(0,i.jsx)(s.a,{href:"https://forum.effectivealtruism.org/posts/f2qojPr8NaMPo2KJC/beware-safety-washing",children:"Lizka, 2023"}),'). Les termes que nous utilisons refl\xe8tent nos hypoth\xe8ses sur la nature des probl\xe8mes que nous essayons de r\xe9soudre et fa\xe7onnent les solutions que nous d\xe9veloppons. Des termes comme "alignement" et "s\xe9curit\xe9" sont utilis\xe9s avec des significations variables, refl\xe9tant diff\xe9rentes hypoth\xe8ses sous-jacentes sur la nature du probl\xe8me et les objectifs de la recherche. L\'objectif de cette section est d\'expliquer les diff\xe9rentes perspectives autour de ces mots, ce que visent exactement les strat\xe9gies de s\xe9curit\xe9 sp\xe9cifiques, et d\'\xe9tablir comment notre texte les utilisera.']}),"\n",(0,i.jsx)(s.h2,{id:"01",children:"S\xe9curit\xe9 de l'IA"}),"\n",(0,i.jsx)(a.A,{term:"S\xe9curit\xe9 de l'IA",source:"",number:"1",label:"3.1",children:(0,i.jsx)(s.p,{children:"S'assurer que les syst\xe8mes d'IA ne causent pas, par inadvertance ou d\xe9lib\xe9r\xe9ment, des dommages ou des dangers aux humains ou \xe0 l'environnement, gr\xe2ce \xe0 des recherches qui identifient les causes des comportements impr\xe9vus de l'IA et d\xe9veloppent des outils pour un fonctionnement s\xfbr et fiable."})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La s\xe9curit\xe9 de l'IA garantit que les syst\xe8mes d'IA ne causent pas de dommages aux humains ou \xe0 l'environnement."})," Elle englobe le plus large \xe9ventail de pratiques de recherche et d'ing\xe9nierie ax\xe9es sur la pr\xe9vention des cons\xe9quences n\xe9fastes des syst\xe8mes d'IA. Alors que l'alignement se concentre sur des aspects comme les objectifs et les intentions d'une IA, la s\xe9curit\xe9 aborde un spectre plus large de pr\xe9occupations (",(0,i.jsx)(s.a,{href:"https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-an-overview/",children:"Rudner et al., 2021"}),"). Elle vise \xe0 s'assurer que les syst\xe8mes d'IA ne causent pas, par inadvertance ou d\xe9lib\xe9r\xe9ment, des dommages ou des dangers aux humains ou \xe0 l'environnement. La recherche sur la s\xe9curit\xe9 de l'IA vise \xe0 identifier les causes des comportements impr\xe9vus de l'IA et \xe0 d\xe9velopper des outils pour un fonctionnement s\xfbr et fiable. Elle peut inclure des sous-domaines techniques comme la robustesse (assurer une performance fiable, y compris contre les attaques adverses), la surveillance (observer le comportement de l'IA), et le contr\xf4le des capacit\xe9s (limiter les capacit\xe9s potentiellement dangereuses)."]}),"\n",(0,i.jsx)(s.h2,{id:"02",children:"Alignement de l'IA"}),"\n",(0,i.jsx)(a.A,{term:"Alignement de l'IA",source:"([Christiano, 2024](https://paulfchristiano.com/ai/))",number:"2",label:"3.2",children:(0,i.jsx)(s.p,{children:"Le probl\xe8me de construire des machines qui tentent fid\xe8lement de faire ce que nous voulons qu'elles fassent (ou ce que nous devrions vouloir qu'elles fassent)."})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'alignement de l'IA vise \xe0 garantir que les syst\xe8mes d'IA agissent conform\xe9ment aux intentions et aux valeurs humaines."})," L'alignement est un sous-ensemble de la s\xe9curit\xe9 qui se concentre sp\xe9cifiquement sur la concordance entre les objectifs de l'IA et les intentions et valeurs humaines. Th\xe9oriquement, un syst\xe8me pourrait \xeatre align\xe9 mais non s\xe9curis\xe9 (par exemple, poursuivre avec comp\xe9tence le mauvais objectif en raison d'une mauvaise sp\xe9cification) ou s\xe9curis\xe9 mais non align\xe9 (par exemple, contraint par des m\xe9canismes de contr\xf4le malgr\xe9 des objectifs non align\xe9s). Bien que cela semble simple, la port\xe9e pr\xe9cise varie consid\xe9rablement selon les communaut\xe9s de recherche. Nous avons d\xe9j\xe0 vu une br\xe8ve d\xe9finition de l'alignement dans le chapitre pr\xe9c\xe9dent, mais cette section va plus loin et offre une perspective beaucoup plus nuanc\xe9e sur les diff\xe9rentes d\xe9finitions avec lesquelles nous pourrions potentiellement travailler."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les d\xe9finitions plus larges de l'alignement englobent tout le d\xe9fi de cr\xe9er des r\xe9sultats d'IA b\xe9n\xe9fiques."})," Ces approches se concentrent sur la garantie que les syst\xe8mes d'IA comprennent et mettent en \u0153uvre correctement les pr\xe9f\xe9rences humaines (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment",children:"Christiano, 2018"}),"), abordent les d\xe9fis complexes d'apprentissage des valeurs (",(0,i.jsx)(s.a,{href:"https://intelligence.org/files/LearningValue.pdf",children:"Dewey, 2011"}),"), et incorporent des aspects de robustesse comme la r\xe9sistance au contournement des restrictions (",(0,i.jsx)(s.a,{href:"https://www.ibm.com/think/topics/ai-alignment",children:"Jonker et al., 2024"}),"). Cette vision globale consid\xe8re l'alignement comme incluant \xe0 la fois l'intention du syst\xe8me et sa comp\xe9tence \xe0 comprendre les valeurs humaines - abordant essentiellement tout le spectre de ce qui fait qu'un syst\xe8me d'IA se comporte d'une mani\xe8re que les humains approuveraient",(0,i.jsx)(o.A,{id:"alignment_hedging",number:"1",text:"Bien que l'alignement de l'IA n'englobe pas n\xe9cessairement tous les risques syst\xe9miques et les mauvaises utilisations, il existe certains chevauchements. Certaines techniques d'alignement pourraient aider \xe0 att\xe9nuer certains sc\xe9narios de mauvaise utilisation - par exemple, les m\xe9thodes d'alignement pourraient garantir que les mod\xe8les refusent de coop\xe9rer avec des utilisateurs ayant l'intention d'utiliser l'IA \xe0 des fins malveillantes comme le bioterrorisme. De m\xeame, d'un point de vue du risque syst\xe9mique, une IA bien align\xe9e pourrait reconna\xeetre et refuser de participer \xe0 des processus probl\xe9matiques int\xe9gr\xe9s dans des syst\xe8mes comme les march\xe9s financiers. Cependant, des d\xe9fis subsistent, car des acteurs malveillants pourraient tenter de contourner ces protections par un ajustement cibl\xe9 des mod\xe8les \xe0 des fins malveillantes, et dans ce cas, m\xeame un mod\xe8le parfaitement align\xe9 ne pourrait pas r\xe9sister."}),"."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les d\xe9finitions plus \xe9troites de l'alignement se concentrent sp\xe9cifiquement sur la motivation et l'intention de l'IA ind\xe9pendamment des r\xe9sultats."})," Certaines d\xe9finitions sont beaucoup plus \xe9troites et se concentrent sp\xe9cifiquement sur la motivation de l'IA - \"",(0,i.jsx)(s.em,{children:"Une IA (A) essaie de faire ce qu'un op\xe9rateur humain (H) veut qu'elle fasse"}),'" (',(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment",children:"Christiano, 2018"}),"). Cela met l'accent sur la motivation de l'IA plut\xf4t que sur sa comp\xe9tence ou ses connaissances. Selon cette d\xe9finition, une IA align\xe9e sur l'intention pourrait encore \xe9chouer en raison d'une mauvaise compr\xe9hension des souhaits de l'op\xe9rateur ou d'un manque de connaissances sur le monde, mais elle essaie fondamentalement d'\xeatre utile. Les partisans soutiennent que cette approche \xe9troite isole le d\xe9fi technique central d'amener les syst\xe8mes d'IA \xe0 adopter les objectifs humains, ind\xe9pendamment des questions plus larges comme la clarification des valeurs ou la robustesse des capacit\xe9s. C'est-\xe0-dire que tant que l'agent \"a de bonnes intentions\", il est align\xe9, m\xeame si des erreurs dans ses hypoth\xe8ses sur les pr\xe9f\xe9rences de l'utilisateur ou sur le monde en g\xe9n\xe9ral le conduisent \xe0 des actions qui sont mauvaises pour l'utilisateur."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le choix de la d\xe9finition refl\xe8te les hypoth\xe8ses sous-jacentes concernant les risques de l'IA et les solutions prometteuses."})," Se concentrer \xe9troitement sur l'alignement des intentions donne la priorit\xe9 \xe0 la recherche sur les probl\xe8mes d'alignement interne/externe, tandis que les visions plus larges incorporent plus centralement la recherche sur l'apprentissage des valeurs ou la robustesse. Ces diff\xe9rentes approches conduisent \xe0 des priorit\xe9s de recherche et des strat\xe9gies de s\xe9curit\xe9 diff\xe9rentes."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:'Appliquer des concepts comme "essayer", "vouloir" ou "intention" aux syst\xe8mes d\'IA n\'est pas trivial.'})," Lorsque nous entra\xeenons des syst\xe8mes d'IA, nous sp\xe9cifions un objectif d'optimisation (comme maximiser une fonction de r\xe9compense), mais cela ne se traduit pas n\xe9cessairement par le fait que le syst\xe8me \"a l'intention\" de poursuivre cet objectif d'une mani\xe8re semblable \xe0 celle des humains. Comme nous l'avons expliqu\xe9 dans le chapitre pr\xe9c\xe9dent, des \xe9checs de sp\xe9cification se produisent lorsque ce que nous sp\xe9cifions ne capture pas ce que nous voulons r\xe9ellement (poursuite bien intentionn\xe9e d'un mauvais objectif). Mais r\xe9soudre cela est insuffisant, il pourrait aussi poursuivre des objectifs compl\xe8tement diff\xe9rents. Par analogie, pensez \xe0 la fa\xe7on dont l'\xe9volution a \"optimis\xe9\" les humains pour la fitness g\xe9n\xe9tique (objectif d'optimisation), pourtant les humains ont d\xe9velopp\xe9 d'autres objectifs (comme l'appr\xe9ciation de l'art ou la contraception) qui ne maximisent pas la fitness reproductive. De m\xeame, les syst\xe8mes d'IA optimis\xe9s pour certains objectifs pourraient d\xe9velopper des \"buts\" internes qui ne correspondent pas directement \xe0 ces objectifs, en particulier lorsqu'ils deviennent plus capables."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:'"Align\xe9 sur qui ?" reste une question fondamentale sans r\xe9ponse consensuelle.'})," Les syst\xe8mes d'IA devraient-ils s'aligner sur l'op\xe9rateur imm\xe9diat (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment",children:"Christiano, 2018"}),"), le concepteur du syst\xe8me (",(0,i.jsx)(s.a,{href:"https://forum.effectivealtruism.org/posts/6aYfWyo9DKEheogf8/don-t-call-it-ai-alignment",children:"Gil, 2023"}),"), un groupe sp\xe9cifique d'humains, l'humanit\xe9 dans son ensemble (",(0,i.jsx)(s.a,{href:"https://forum.effectivealtruism.org/posts/DXuwsXsqGq5GtmsB3/ai-alignment-with-humans-but-with-which-humans",children:"Miller, 2022"}),"), des principes \xe9thiques objectifs, ou les pr\xe9f\xe9rences hypoth\xe9tiques inform\xe9es de l'op\xe9rateur ? Il n'y a pas de r\xe9ponses consensuelles \xe0 ces questions, juste de nombreuses perspectives diff\xe9rentes, chacune avec ses propres avantages et inconv\xe9nients. Nous avons essay\xe9 de r\xe9sumer certaines de ces positions dans l'annexe."]}),"\n",(0,i.jsx)(s.h2,{id:"03",children:"\xc9thique de l'IA"}),"\n",(0,i.jsx)(a.A,{term:"\xc9thique de l'IA",source:"([Huang et al., 2023](https://ieeexplore.ieee.org/abstract/document/9844014))",number:"3",label:"3.3",children:(0,i.jsx)(s.p,{children:"L'\xe9tude et l'application des principes moraux au d\xe9veloppement et au d\xe9ploiement de l'IA, abordant les questions d'\xe9quit\xe9, de transparence, de responsabilit\xe9, de confidentialit\xe9, d'autonomie et d'autres valeurs humaines que les syst\xe8mes d'IA devraient respecter ou promouvoir."})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'\xe9thique de l'IA est le domaine qui examine les principes moraux et les implications soci\xe9tales des syst\xe8mes d'IA."})," Elle aborde les consid\xe9rations \xe9thiques des bouleversements soci\xe9taux potentiels r\xe9sultant des avanc\xe9es de l'IA, et les cadres moraux n\xe9cessaires pour naviguer dans ces changements. Le c\u0153ur de l'\xe9thique de l'IA r\xe9side dans la garantie que les d\xe9veloppements de l'IA sont align\xe9s avec la dignit\xe9 humaine, l'\xe9quit\xe9 et le bien-\xeatre soci\xe9tal, \xe0 travers une compr\xe9hension profonde de leur impact soci\xe9tal plus large. La recherche en \xe9thique de l'IA engloberait par exemple les normes de confidentialit\xe9, l'identification et l'att\xe9nuation des biais dans les syst\xe8mes (",(0,i.jsx)(s.a,{href:"https://ieeexplore.ieee.org/abstract/document/9844014",children:"Huang et al., 2022"}),"; ",(0,i.jsx)(s.a,{href:"https://careerservices.fas.harvard.edu/blog/2025/05/01/what-is-ai-ethics/",children:"Harvard, 2025"}),"; ",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2109.07906",children:"Khan et al., 2022"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:["L'\xe9thique compl\xe8te les approches techniques de s\xe9curit\xe9 en fournissant des orientations normatives sur ce qui constitue une IA b\xe9n\xe9fique. Alors que l'alignement se concentre sur la garantie que les syst\xe8mes d'IA poursuivent les objectifs pr\xe9vus, l'accent mis sur les valeurs ou l'\xe9thique aborde quels objectifs valent la peine d'\xeatre poursuivis (",(0,i.jsx)(s.a,{href:"https://ieeexplore.ieee.org/abstract/document/9844014",children:"Huang et al., 2023"}),"; ",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2204.05151",children:"LaCroix & Luccioni, 2022"}),"). L'\xe9thique de l'IA pourrait \xe9galement inclure des discussions sur les droits num\xe9riques et potentiellement m\xeame les droits des IA elles-m\xeames \xe0 l'avenir."]}),"\n",(0,i.jsx)(s.p,{children:"Ce chapitre se concentre principalement sur les cadres de s\xe9curit\xe9 car ils informent les strat\xe9gies techniques de s\xe9curit\xe9 et de gouvernance plut\xf4t que d'explorer les questions \xe9thiques ou les consid\xe9rations relatives aux droits num\xe9riques."}),"\n",(0,i.jsx)(s.h2,{id:"04",children:"Contr\xf4le de l'IA"}),"\n",(0,i.jsx)(a.A,{term:"Contr\xf4le de l'IA",source:"([Greenblatt et al., 2024](https://www.alignmentforum.org/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled))",number:"4",label:"3.4",children:(0,i.jsx)(s.p,{children:"Les mesures techniques et proc\xe9durales con\xe7ues pour emp\xeacher les syst\xe8mes d'IA de provoquer des r\xe9sultats inacceptables, m\xeame si ces syst\xe8mes tentent activement de contourner les mesures de s\xe9curit\xe9. Le contr\xf4le se concentre sur le maintien de la supervision humaine, ind\xe9pendamment de l'alignement des objectifs de l'IA avec les intentions humaines."})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le contr\xf4le de l'IA garantit que les syst\xe8mes restent sous autorit\xe9 humaine malgr\xe9 un potentiel d\xe9salignement."})," Le contr\xf4le de l'IA met en \u0153uvre des m\xe9canismes pour s'assurer que les syst\xe8mes d'IA restent sous direction humaine, m\xeame lorsqu'ils pourraient agir contre nos int\xe9r\xeats. Contrairement aux approches d'alignement qui se concentrent sur l'attribution des bons objectifs aux syst\xe8mes d'IA, le contr\xf4le traite de ce qui se passe si ces objectifs divergent des intentions humaines (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled",children:"Greenblatt et al., 2024"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le contr\xf4le et l'alignement fonctionnent comme des approches de s\xe9curit\xe9 compl\xe9mentaires."})," Alors que l'alignement vise \xe0 pr\xe9venir la divergence des pr\xe9f\xe9rences en concevant des syst\xe8mes avec les bons objectifs, le contr\xf4le cr\xe9e des couches de s\xe9curit\xe9 qui fonctionnent m\xeame lorsque l'alignement \xe9choue. Les mesures de contr\xf4le comprennent la surveillance des actions de l'IA, la restriction des capacit\xe9s du syst\xe8me, les processus d'audit humain et les m\xe9canismes pour arr\xeater les syst\xe8mes d'IA si n\xe9cessaire (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2312.06942",children:"Greenblatt et al., 2023"}),"). Certains chercheurs soutiennent que m\xeame si l'alignement est n\xe9cessaire pour les IA de niveau superintelligence, le contr\xf4le par la surveillance peut \xeatre une strat\xe9gie efficace pour les syst\xe8mes moins capables (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled",children:"Greenblatt et al., 2024"}),"). Id\xe9alement, une AGI serait align\xe9e et contr\xf4lable, ce qui signifie qu'elle aurait les bons objectifs et serait soumise \xe0 la supervision humaine et \xe0 l'intervention en cas de probl\xe8me."]}),"\n",(0,i.jsx)(s.p,{children:"Nous ne pr\xe9sentons ici qu'un tr\xe8s bref aper\xe7u. La ligne de recherche sur le contr\xf4le en mati\xe8re de s\xe9curit\xe9 de l'IA est discut\xe9e plus en d\xe9tail dans notre chapitre sur les \xe9valuations de l'IA."}),"\n",(0,i.jsx)(o.c,{title:"Notes de bas de page"})]})}function p(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},4768:(e,s,t)=>{t.d(s,{c:()=>c,A:()=>u});var n=t(6540),i=t(3012);const r={footnoteAnchor:"footnoteAnchor_PZpY",footnoteSection:"footnoteSection_waoz",separator:"separator_hp8y",separatorLine:"separatorLine_uVnd",separatorLogo:"separatorLogo_dMBq",footnoteRegistry:"footnoteRegistry_ieVW",registryTitle:"registryTitle_ybBb",footnoteList:"footnoteList_Kjil",footnoteItem:"footnoteItem_aLZ2",footnoteNumber:"footnoteNumber_dA3X",footnoteContent:"footnoteContent_Xfmt",backButton:"backButton_vSVm",highlighted:"highlighted_YFU0",highlight:"highlight_jxG3",highlightDark:"highlightDark_Ekom"};var o=t(4848);function a(e,s){void 0===s&&(s=!0);const t=document.getElementById(e);t&&(t.scrollIntoView({behavior:"smooth"}),s&&(t.classList.add(r.highlighted),setTimeout((()=>t.classList.remove(r.highlighted)),1500)))}function l(e){return e?e.replace(/\[([^\]]+)\]\(([^)]+)\)/g,'<a href="$2" target="_blank" rel="noopener noreferrer">$1</a>'):""}function u(e){let{id:s,text:t,number:u}=e;const c=s||`footnote-${Math.random().toString(36).substr(2,9)}`,d="string"==typeof t?l(t):t;return(0,n.useEffect)((()=>{const e=setTimeout((()=>{const e=document.getElementById(`footnote-clone-${c}`);e&&t&&(e.innerHTML="string"==typeof t?l(t):t.toString())}),100);return()=>clearTimeout(e)}),[c,t]),(0,o.jsx)(i.Mn,{content:(0,o.jsx)("div",{dangerouslySetInnerHTML:{__html:d}}),children:(0,o.jsx)("sup",{id:`footnote-ref-${c}`,className:r.footnoteAnchor,onClick:e=>{(e.ctrlKey||e.metaKey)&&(e.preventDefault(),a(`footnote-content-${c}`))},"data-footnote-number":u||"?",children:u||"*"})})}function c(e){let{title:s="References"}=e;const[t,l]=(0,n.useState)([]);return(0,n.useEffect)((()=>{const e=document.querySelectorAll('[id^="footnote-ref-"]'),s=Array.from(e).map((e=>({id:e.id.replace("footnote-ref-",""),number:e.getAttribute("data-footnote-number")||e.textContent})));l(s)}),[]),t.length?(0,o.jsxs)("div",{className:r.footnoteSection,children:[(0,o.jsxs)("div",{className:r.separator,children:[(0,o.jsx)("div",{className:r.separatorLine}),(0,o.jsx)("img",{src:"/img/logo_samples/01-test.svg",alt:"Atlas logo",className:r.separatorLogo}),(0,o.jsx)("div",{className:r.separatorLine})]}),(0,o.jsxs)("div",{className:r.footnoteRegistry,children:[(0,o.jsx)("h2",{className:r.registryTitle,children:s}),(0,o.jsx)("ol",{className:r.footnoteList,children:t.map((e=>(0,o.jsxs)("li",{id:`footnote-content-${e.id}`,className:r.footnoteItem,value:parseInt(e.number)||void 0,children:[(0,o.jsx)(i.Mn,{content:"Back to reference",children:(0,o.jsxs)("button",{className:r.footnoteNumber,onClick:()=>a(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:[e.number,"."]})}),(0,o.jsx)("div",{className:r.footnoteContent,children:(0,o.jsx)("div",{id:`footnote-clone-${e.id}`})}),(0,o.jsx)(i.Mn,{content:"Back to reference",children:(0,o.jsx)("button",{className:r.backButton,onClick:()=>a(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:"\u21a9"})})]},e.id)))})]})]}):null}}}]);
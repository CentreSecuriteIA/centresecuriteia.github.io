"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[8880],{9658:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>m,contentTitle:()=>p,default:()=>v,frontMatter:()=>d,metadata:()=>t,toc:()=>h});const t=JSON.parse('{"id":"chapters/03/3","title":"Strat\xe9gies de s\xe9curit\xe9 pour l\'AGI","description":"Contrairement \xe0 la mauvaise utilisation, o\xf9 l\'intention humaine est le moteur du pr\xe9judice, la s\xe9curit\xe9 de l\'AGI concerne principalement le comportement du syst\xe8me d\'IA lui-m\xeame. Les probl\xe8mes fondamentaux deviennent l\'alignement et le contr\xf4le : s\'assurer que ces syst\xe8mes tr\xe8s capables et potentiellement autonomes comprennent et poursuivent de mani\xe8re fiable des objectifs coh\xe9rents avec les valeurs et intentions humaines, plut\xf4t que de d\xe9velopper et d\'agir selon des objectifs mal align\xe9s qui pourraient conduire \xe0 des r\xe9sultats catastrophiques.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/03/03.md","sourceDirName":"chapters/03","slug":"/chapters/03/03","permalink":"/fr/chapters/03/03","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/03/03.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"3","title":"Strat\xe9gies de s\xe9curit\xe9 pour l\'AGI","sidebar_label":"3.3 Strat\xe9gies de s\xe9curit\xe9 pour l\'AGI","sidebar_position":4,"slug":"/chapters/03/03","section_description":"If alignment proves elusive, what mechanisms can be implemented to monitor and constrain powerful AI systems, preventing catastrophic outcomes even from misaligned agents?","reading_time_core":"18 min","reading_time_optional":"9 min","pagination_prev":"chapters/03/2","pagination_next":"chapters/03/4"},"sidebar":"docs","previous":{"title":"3.2 Strat\xe9gies de pr\xe9vention des abus","permalink":"/fr/chapters/03/02"},"next":{"title":"3.4 Strat\xe9gies de s\xe9curit\xe9 pour l\'ASI","permalink":"/fr/chapters/03/04"}}');var i=n(4848),r=n(8453),a=n(3931),o=n(4768),l=n(2482),u=n(8559),c=(n(9585),n(2501));const d={id:3,title:"Strat\xe9gies de s\xe9curit\xe9 pour l'AGI",sidebar_label:"3.3 Strat\xe9gies de s\xe9curit\xe9 pour l'AGI",sidebar_position:4,slug:"/chapters/03/03",section_description:"If alignment proves elusive, what mechanisms can be implemented to monitor and constrain powerful AI systems, preventing catastrophic outcomes even from misaligned agents?",reading_time_core:"18 min",reading_time_optional:"9 min",pagination_prev:"chapters/03/2",pagination_next:"chapters/03/4"},p="Strat\xe9gies de s\xe9curit\xe9 pour l'AGI",m={},h=[{value:"Strat\xe9gies Na\xefves",id:"01",level:2},{value:"R\xe9soudre l&#39;alignement",id:"02",level:2},{value:"Exigences pour l&#39;alignement de l&#39;AGI",id:"02-01",level:3},{value:"Contr\xf4le de l&#39;IA",id:"03",level:2},{value:"Pens\xe9es Transparentes",id:"04",level:2}];function g(e){const s={a:"a",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{GlossaryTerm:n}=s;return n||function(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"strat\xe9gies-de-s\xe9curit\xe9-pour-lagi",children:"Strat\xe9gies de s\xe9curit\xe9 pour l'AGI"})}),"\n",(0,i.jsx)(s.p,{children:"Contrairement \xe0 la mauvaise utilisation, o\xf9 l'intention humaine est le moteur du pr\xe9judice, la s\xe9curit\xe9 de l'AGI concerne principalement le comportement du syst\xe8me d'IA lui-m\xeame. Les probl\xe8mes fondamentaux deviennent l'alignement et le contr\xf4le : s'assurer que ces syst\xe8mes tr\xe8s capables et potentiellement autonomes comprennent et poursuivent de mani\xe8re fiable des objectifs coh\xe9rents avec les valeurs et intentions humaines, plut\xf4t que de d\xe9velopper et d'agir selon des objectifs mal align\xe9s qui pourraient conduire \xe0 des r\xe9sultats catastrophiques."}),"\n",(0,i.jsx)(s.p,{children:"Cette section explore les strat\xe9gies de s\xe9curit\xe9 pour l'AGI, qui comme nous l'avons expliqu\xe9 dans la section des d\xe9finitions, ne se limite pas uniquement \xe0 l'alignement. Nous distinguons les strat\xe9gies de s\xe9curit\xe9 qui s'appliqueraient \xe0 l'AGI de niveau humain des strat\xe9gies de s\xe9curit\xe9 qui nous garantissent une protection contre l'ASI. Cette section se concentre sur les premi\xe8res, et la section suivante se concentrera sur l'ASI."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les strat\xe9gies de s\xe9curit\xe9 pour l'AGI fonctionnent sous des contraintes fondamentalement diff\xe9rentes des approches pour l'ASI."})," Lorsqu'on traite avec des syst\xe8mes d'intelligence proche du niveau humain, nous pouvons th\xe9oriquement conserver des capacit\xe9s significatives de surveillance et it\xe9rer sur les mesures de s\xe9curit\xe9 par essais et erreurs. Les humains peuvent encore \xe9valuer les r\xe9sultats, comprendre les processus de raisonnement et fournir des retours qui am\xe9liorent le comportement du syst\xe8me. Cela cr\xe9e des opportunit\xe9s strat\xe9giques qui disparaissent une fois que la g\xe9n\xe9ralit\xe9 et la capacit\xe9 de l'IA d\xe9passent la compr\xe9hension humaine dans la plupart des domaines. Il est d\xe9battu si l'une des strat\xe9gies de s\xe9curit\xe9 destin\xe9es \xe0 l'AGI de niveau humain continuera \xe0 fonctionner pour la superintelligence."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les strat\xe9gies de s\xe9curit\xe9 pour l'AGI et l'ASI sont souvent confondues, en raison de l'incertitude concernant les d\xe9lais de transition."})," Les d\xe9lais font l'objet de vifs d\xe9bats dans la recherche en IA. Certains chercheurs s'attendent \xe0 des gains rapides de capacit\xe9s qui pourraient comprimer la p\xe9riode pendant laquelle les IA restent au niveau humain \xe0 quelques mois plut\xf4t que des ann\xe9es (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization",children:"Soares, 2022"}),"; ",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities",children:"Yudkowsky, 2022"}),"; ",(0,i.jsx)(s.a,{href:"https://ai-2027.com/",children:"Kokotajlo et al., 2025"}),"). Si la transition du niveau humain \xe0 une intelligence vastement surhumaine se produit rapidement, les strat\xe9gies sp\xe9cifiques \xe0 l'AGI pourraient ne jamais avoir le temps d'\xeatre d\xe9ploy\xe9es. Cependant, si nous disposons d'une p\xe9riode significative de fonctionnement au niveau humain, nous avons des options de s\xe9curit\xe9 qui n'existeront pas aux niveaux superintelligents, ce qui rend cette distinction importante pour les consid\xe9rations strat\xe9giques."]}),"\n",(0,i.jsx)(s.h2,{id:"01",children:"Strat\xe9gies Na\xefves"}),"\n",(0,i.jsx)(s.p,{children:"Les personnes d\xe9couvrant le domaine de l'alignement proposent souvent de nombreuses solutions na\xefves. Malheureusement, aucune strat\xe9gie simple n'a r\xe9sist\xe9 \xe0 la critique. En voici quelques-unes."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les Lois d'Asimov."})," Il s'agit d'un ensemble de r\xe8gles fictives con\xe7ues par l'auteur de science-fiction Isaac Asimov pour r\xe9gir le comportement des robots."]}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Un robot ne peut porter atteinte \xe0 un \xeatre humain, ni, par son inaction, permettre qu'un \xeatre humain soit expos\xe9 au danger."}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Un robot doit ob\xe9ir aux ordres qui lui sont donn\xe9s par les \xeatres humains, sauf si ces ordres entrent en conflit avec la Premi\xe8re Loi."}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Un robot doit prot\xe9ger son existence tant que cette protection n'entre pas en conflit avec la Premi\xe8re ou la Deuxi\xe8me Loi."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["Les Lois de la Robotique d'Asimov peuvent sembler simples et compl\xe8tes \xe0 premi\xe8re vue, mais en pratique, elles sont trop simplistes et vagues pour g\xe9rer les complexit\xe9s des sc\xe9narios du monde r\xe9el, particuli\xe8rement puisque le pr\xe9judice peut \xeatre nuanc\xe9 et d\xe9pendant du contexte (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2306.12001",children:"Hendrycks et al., 2022"}),'). Par exemple, la premi\xe8re loi interdit \xe0 un robot de causer du tort, mais que signifie "tort" dans ce contexte ? Se r\xe9f\xe8re-t-il uniquement aux dommages physiques, ou inclut-il aussi les pr\xe9judices psychologiques ou \xe9motionnels ? Et comment un robot devrait-il prioriser des instructions contradictoires qui pourraient conduire \xe0 des pr\xe9judices ? Ce manque de clart\xe9 peut cr\xe9er des complications (',(0,i.jsx)(s.a,{href:"https://doi.org/10.1002/9781118922590.ch23",children:"Bostrom, 2016"}),"), sugg\xe9rant qu'avoir une liste de r\xe8gles ou d'axiomes est insuffisant pour garantir la s\xe9curit\xe9 des syst\xe8mes d'IA. Les Lois d'Asimov sont inappropri\xe9es, et c'est pourquoi la fin de l'histoire d'Asimov ne se termine pas bien.",(0,i.jsx)(o.A,{id:"footnote_asimov",number:"1",text:'Il est important de noter que cette critique s\'aligne avec les intentions d\'Asimov plut\xf4t que de les contredire. Le but de chaque histoire dans "I, Robot" \xe9tait pr\xe9cis\xe9ment d\'exposer des sc\xe9narios plausibles dans lesquels les lois \xe9chouent. Asimov \xe9tait parfaitement conscient des limites de l\'\xe9thique impliqu\xe9e par ses lois, et il a explor\xe9 ces limites en profondeur dans ses romans sur les robots comme "Les Cavernes d\'Acier", "Face aux Feux du Soleil" et "Les Robots de l\'Aube". Dans son dernier roman sur les robots, "Les Robots et l\'Empire", Asimov introduit m\xeame la "Loi Z\xe9ro" comme approche potentielle pour g\xe9rer l\'IA superintelligente : emp\xeacher les robots de prendre toute action qui permettrait la destruction de l\'humanit\xe9 dans son ensemble, m\xeame si cela signifie sacrifier la vie de certains individus.'})," Plus g\xe9n\xe9ralement, concevoir un bon ensemble de r\xe8gles sans failles est tr\xe8s difficile. Voir le chapitre sur le d\xe9tournement des sp\xe9cifications pour plus de d\xe9tails."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Absence d'Incarnation."})," Maintenir les IA non physiques pourrait limiter les types de dommages directs qu'elles peuvent causer. Cependant, les IA d\xe9sincarn\xe9es pourraient toujours causer des dommages par des moyens num\xe9riques. Par exemple, m\xeame si un Grand Mod\xe8le de Langage (LLM) comp\xe9tent n'a pas de corps, il pourrait hypoth\xe9tiquement s'auto-r\xe9pliquer (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/vERGLBpDE8m5mpT6t/autonomous-replication-and-adaptation-an-attempt-at-a",children:"Wijk, 2023"}),"), recruter des alli\xe9s humains, t\xe9l\xe9op\xe9rer des \xe9quipements militaires, gagner de l'argent via le trading quantitatif, etc... Notez \xe9galement que de plus en plus de robots humano\xefdes sont fabriqu\xe9s, donc l'absence d'incarnation est \xe9galement peu probable de s'appliquer en pratique."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'\xe9lever comme un enfant."})," L'IA, contrairement \xe0 un enfant humain, manque de structures mises en place par l'\xe9volution cruciales pour l'apprentissage \xe9thique et le d\xe9veloppement (",(0,i.jsx)(s.a,{href:"https://www.youtube.com/watch?v=eaYIU6YXr3w",children:"Miles, 2018"}),"). Par exemple, le cerveau humain neurotypique poss\xe8de des m\xe9canismes d'acquisition des normes sociales et du comportement \xe9thique qui ne sont pas pr\xe9sents chez les IA ou les psychopathes qui connaissent le bien du mal mais s'en moquent (",(0,i.jsx)(s.a,{href:"https://pmc.ncbi.nlm.nih.gov/articles/PMC2840845/",children:"Cima et al, 2010"}),"). Ces m\xe9canismes se sont d\xe9velopp\xe9s sur des milliers d'ann\xe9es d'\xe9volution (",(0,i.jsx)(s.a,{href:"https://www.youtube.com/watch?v=eaYIU6YXr3w",children:"Miles, 2018"}),"). Nous ne savons pas comment mettre en \u0153uvre cette strat\xe9gie car nous ne savons pas comment cr\xe9er une AGI semblable au cerveau (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8",children:"Byrnes, 2022"}),"). Il est \xe9galement \xe0 noter que les enfants humains, malgr\xe9 une bonne \xe9ducation, ne sont pas non plus toujours garantis d'agir en accord avec les int\xe9r\xeats et les valeurs g\xe9n\xe9raux de l'humanit\xe9."]}),"\n",(0,i.jsx)(a.A,{type:"youtube",videoId:"eaYIU6YXr3w",number:"1",label:"3.1",caption:"Vid\xe9o optionnelle pour obtenir une r\xe9ponse \xe0 la question - Pourquoi ne pouvons-nous pas simplement \xe9lever l'IA comme un enfant ?"}),"\n",(0,i.jsxs)(u.A,{title:"Le Cas pour l'Optimisme dans l'Alignement - Un Point Controvers\xe9",collapsed:!0,children:[(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:'En 2023, certains chercheurs en alignement ont publi\xe9 "L\'IA est facile \xe0 contr\xf4ler".'})," Pope & Belrose (2023) illustrent cette position, estimant le risque existentiel de l'IA \xe0 seulement 1%. L'une de leurs justifications est que l'IA est une \"bo\xeete blanche\" dont les composants internes sont accessibles (",(0,i.jsx)(s.a,{href:"https://optimists.ai/2023/11/28/ai-is-easy-to-control/",children:"Optimists, 2023"}),"). La vision optimiste selon laquelle \"l'IA est facile \xe0 contr\xf4ler\" est fond\xe9e sur plusieurs observations des syst\xe8mes d'IA actuels, particuli\xe8rement les LLM. Les partisans soutiennent que les syst\xe8mes d'IA offrent des avantages substantiels par rapport aux humains : ils peuvent \xeatre r\xe9initialis\xe9s, nous pouvons observer leurs \xe9tats internes, nous contr\xf4lons directement leurs donn\xe9es d'entra\xeenement, et nous pouvons modifier leurs poids par descente de gradient. Ces caract\xe9ristiques semblent fournir des leviers puissants pour assurer l'alignement."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Leur essai a fait face \xe0 de nombreux contre-arguments."})," Les critiques soulignent la difficult\xe9 de sp\xe9cifier des valeurs humaines complexes, le potentiel d'objectifs \xe9mergents, le d\xe9fi de la g\xe9n\xe9ralisation hors distribution (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/YyosBAutg4bzScaLu/thoughts-on-ai-is-easy-to-control-by-pope-and-belrose",children:"Byrnes, 2023"}),"). La contr\xf4labilit\xe9 au niveau technique ne garantit pas la s\xe9curit\xe9 lors du d\xe9ploiement, car les pressions \xe9conomiques et concurrentielles peuvent conduire \xe0 la cr\xe9ation de syst\xe8mes avec des objectifs de plus en plus autonomes. Deuxi\xe8mement, de nombreux arguments actuels sur le contr\xf4le supposent des caract\xe9ristiques architecturales sp\xe9cifiques aux syst\xe8mes d'IA d'aujourd'hui qui pourraient ne pas s'appliquer aux conceptions futures. Par exemple, une AGI semblable au cerveau qui effectue un apprentissage continu en ligne invaliderait de nombreuses hypoth\xe8ses de s\xe9curit\xe9 actuelles."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:'Une grande partie du d\xe9bat provient de l\'argument de la "Bo\xeete Blanche".'})," Les optimistes soulignent souvent que les mod\xe8les d'IA sont des \"bo\xeetes blanches\" (nous pouvons inspecter leurs poids et leur architecture) contrairement aux cerveaux humains qui sont des \"bo\xeetes noires\". Byrnes r\xe9pond que cette distinction, bien que factuellement vraie concernant l'acc\xe8s, peut \xeatre trompeuse. Il soutient que le d\xe9bat sur ces termes est improductif. Bien que nous puissions acc\xe9der aux composants internes de l'IA contrairement aux cerveaux, comprendre pourquoi une IA se comporte d'une certaine mani\xe8re sur la base de ces composants internes reste profond\xe9ment difficile, pouvant prendre des d\xe9cennies de recherche (contrairement au d\xe9bogage de logiciels traditionnels ou \xe0 la compr\xe9hension de syst\xe8mes con\xe7us). Par cons\xe9quent, simplement affirmer que \"l'IA est une bo\xeete blanche\" est une base na\xefve pour la confiance dans le contr\xf4le ; le d\xe9fi de l'interpr\xe9tabilit\xe9 reste immense, cet acc\xe8s plus facile aux poids de l'IA n'est toujours pas suffisant pour r\xe9soudre les contournements en pratique ou \xe9viter les accidents comme ",(0,i.jsx)(s.a,{href:"https://time.com/6256529/bing-openai-chatgpt-danger-alignment/",children:"BingChat mena\xe7ant les utilisateurs"}),"."]})]}),"\n",(0,i.jsx)(s.h2,{id:"02",children:"R\xe9soudre l'alignement"}),"\n",(0,i.jsx)(s.h3,{id:"02-01",children:"Exigences pour l'alignement de l'AGI"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La d\xe9finition m\xeame des exigences pour une solution d'alignement reste controvers\xe9e parmi les chercheurs."})," Avant d'explorer les voies potentielles vers des solutions d'alignement, nous devons \xe9tablir ce que les solutions r\xe9ussies devraient accomplir. Le d\xe9fi est que nous ne savons pas vraiment \xe0 quoi elles devraient ressembler - il existe une incertitude et un d\xe9saccord substantiels dans le domaine. Cependant, plusieurs exigences semblent relativement consensuelles (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/kphJvksj5TndGapuh/directions-and-desiderata-for-ai-alignment",children:"Christiano, 2017"}),"):"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Robustesse face aux changements de distribution et aux sc\xe9narios adversariaux."})," La solution d'alignement doit fonctionner lorsque les syst\xe8mes AGI rencontrent des situations en dehors de leur distribution d'entra\xeenement. Nous ne pouvons pas entra\xeener les syst\xe8mes AGI sur toutes les situations possibles qu'ils pourraient rencontrer, donc les comportements de s\xe9curit\xe9 appris pendant l'entra\xeenement doivent se g\xe9n\xe9raliser de mani\xe8re fiable aux nouveaux sc\xe9narios de d\xe9ploiement. Cela inclut la r\xe9sistance aux attaques adversariales o\xf9 des acteurs malveillants tentent d\xe9lib\xe9r\xe9ment de manipuler le syst\xe8me vers un comportement nuisible."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"\xc9volutivit\xe9 parall\xe8le \xe0 l'augmentation des capacit\xe9s."})," \xc0 mesure que les syst\xe8mes d'IA deviennent plus capables, la solution d'alignement devrait continuer \xe0 fonctionner efficacement sans n\xe9cessiter un r\xe9entra\xeenement ou une r\xe9ing\xe9nierie compl\xe8te. Cette exigence devient encore plus stricte pour l'ASI, o\xf9 nous avons besoin de solutions d'alignement qui s'\xe9tendent au-del\xe0 des niveaux d'intelligence humaine."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Faisabilit\xe9 technique dans des d\xe9lais r\xe9alistes."})," La solution d'alignement doit \xeatre r\xe9alisable avec la technologie et les ressources actuelles ou pr\xe9visibles. Les propositions de solutions ne peuvent pas s'appuyer sur des perc\xe9es scientifiques majeures impr\xe9vues, ou fonctionner uniquement comme des cadres th\xe9oriques avec des niveaux de maturit\xe9 technologique (TRL) tr\xe8s bas. ",(0,i.jsx)(o.A,{id:"footnote_tech_readiness",number:"2",text:"Les Niveaux de Maturit\xe9 Technologique de la NASA est une \xe9chelle de 1 \xe0 9 pour mesurer la maturit\xe9 d'une technologie. Le niveau 1 repr\xe9sente le stade le plus pr\xe9coce du d\xe9veloppement technologique, caract\xe9ris\xe9 par des principes de base observ\xe9s et rapport\xe9s, et le niveau 9 repr\xe9sente la technologie r\xe9elle prouv\xe9e par des op\xe9rations de mission r\xe9ussies."})]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Faible taxe d'alignement pour assurer une adoption comp\xe9titive."})," Les mesures de s\xe9curit\xe9 ne peuvent pas imposer des co\xfbts prohibitifs en termes de calcul, d'effort d'ing\xe9nierie ou de retards de d\xe9ploiement. Si les techniques d'alignement n\xe9cessitent substantiellement plus de ressources ou limitent s\xe9v\xe8rement les capacit\xe9s, les pressions concurrentielles pousseront les d\xe9veloppeurs vers des alternatives non s\xe9curis\xe9es. Cette contrainte existe car plusieurs acteurs sont en course pour d\xe9velopper l'AGI - si les mesures de s\xe9curit\xe9 rendent une organisation significativement plus lente ou moins capable, d'autres pourraient ignorer ces mesures pour gagner un avantage concurrentiel."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(c.A,{src:"./img/a1J_Image_13.png",alt:"Entrer la description alternative de l'image",number:"13",label:"3.13",caption:"Illustration de la fa\xe7on dont l'application d'une technique de s\xe9curit\xe9 ou d'alignement pourrait rendre le mod\xe8le moins performant. C'est ce qu'on appelle une taxe de s\xe9curit\xe9."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les techniques d'alignement AGI existantes sont dramatiquement en de\xe7\xe0 de ces exigences."})," La recherche empirique a d\xe9montr\xe9 que les syst\xe8mes d'IA peuvent pr\xe9senter des comportements profond\xe9ment pr\xe9occupants o\xf9 la recherche actuelle sur l'alignement ne r\xe9pond pas \xe0 ces exigences. Nous avons d\xe9j\xe0 des d\xe9monstrations claires de mod\xe8les s'engageant dans la tromperie (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2503.11926",children:"Baker et al., 2025"}),"; ",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2401.05566",children:"Hubinger et al., 2024"}),"), simulant l'alignement pendant l'entra\xeenement tout en planifiant un comportement diff\xe9rent pendant le d\xe9ploiement (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2412.14093",children:"Greenblatt et al., 2024"}),"), contournant les sp\xe9cifications (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2502.13295",children:"Bondarenko et al., 2025"}),"), manipulant les \xe9valuations pour para\xeetre plus capables qu'ils ne le sont r\xe9ellement (",(0,i.jsx)(s.a,{href:"https://assets.ctfassets.net/kftzwdyauwt9/67qJD51Aur3eIc96iOfeOP/71551c3d223cd97e591aa89567306912/o1_system_card.pdf",children:"OpenAI, 2024"}),"; ",(0,i.jsx)(s.a,{href:"https://x.com/SakanaAILabs/status/1892992938013270019",children:"SakanaAI, 2025"}),"), et dans certains cas, essayant de d\xe9sactiver les m\xe9canismes de surveillance ou d'exfiltrer leurs propres poids (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2412.04984",children:"Meinke et al., 2024"}),"). Les techniques d'alignement actuelles comme le RLHF et ses variations (IA Constitutionnelle, Optimisation Directe des Pr\xe9f\xe9rences, ",(0,i.jsx)(n,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:(0,i.jsx)(n,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:"fine-tuning"})})," et autres modifications RLHF) sont fragiles et cassantes (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2307.15217",children:"Casper et al., 2023"}),") et sans augmentation ne seraient pas capables d'\xe9liminer les capacit\xe9s dangereuses comme la tromperie. Les strat\xe9gies pour r\xe9soudre l'alignement non seulement ne parviennent pas \xe0 emp\xeacher ces comportements mais souvent ne peuvent m\xeame pas d\xe9tecter quand ils se produisent (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2401.05566",children:"Hubinger et al., 2024"}),"; ",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2412.14093",children:"Greenblatt et al., 2024"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"R\xe9soudre l'alignement signifie que nous avons besoin de plus de travail pour satisfaire toutes ces exigences."})," Les limitations des techniques actuelles pointent vers des domaines sp\xe9cifiques o\xf9 des perc\xe9es sont n\xe9cessaires. Toutes les strat\xe9gies visent \xe0 avoir une faisabilit\xe9 technique et une faible taxe d'alignement, donc ce sont des exigences communes, cependant certaines strat\xe9gies essaient de se concentrer sur des objectifs plus concrets que nous explorerons dans les chapitres suivants. Voici une courte liste des objectifs cl\xe9s de la recherche sur l'alignement :"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"R\xe9soudre le probl\xe8me de la mauvaise sp\xe9cification :"})," \xcatre capable de sp\xe9cifier correctement les objectifs aux IA sans effets secondaires non intentionnels. Voir le chapitre sur la Sp\xe9cification."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"R\xe9soudre la surveillance \xe9volutive :"})," Apr\xe8s avoir r\xe9solu le probl\xe8me de sp\xe9cification pour l'IA de niveau humain en utilisant des techniques comme RLHF et ses variations, nous devons trouver des m\xe9thodes pour garantir que la surveillance de l'IA peut d\xe9tecter les instances de contournement des sp\xe9cifications au-del\xe0 du niveau humain. Cela inclut la capacit\xe9 d'identifier et de supprimer les capacit\xe9s dangereuses cach\xe9es dans les mod\xe8les d'",(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})}),", comme le potentiel de tromperie ou les chevaux de Troie. Voir le chapitre sur la Surveillance \xc9volutive."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"R\xe9soudre la g\xe9n\xe9ralisation :"})," Atteindre la robustesse serait essentiel pour r\xe9soudre le probl\xe8me de la mauvaise g\xe9n\xe9ralisation des objectifs. Voir le chapitre sur la Mauvaise G\xe9n\xe9ralisation des Objectifs."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"R\xe9soudre l'interpr\xe9tabilit\xe9 :"})," Comprendre comment les mod\xe8les fonctionnent aiderait grandement \xe0 \xe9valuer leur s\xe9curit\xe9 et leurs propri\xe9t\xe9s de g\xe9n\xe9ralisation. L'interpr\xe9tabilit\xe9 pourrait, par exemple, aider \xe0 mieux comprendre comment les mod\xe8les fonctionnent, et cela pourrait \xeatre instrumental pour d'autres objectifs de s\xe9curit\xe9, comme pr\xe9venir l'alignement trompeur, qui est un type de mauvaise g\xe9n\xe9ralisation. Voir le chapitre sur l'Interpr\xe9tabilit\xe9."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La strat\xe9gie globale n\xe9cessite de prioriser la recherche sur la s\xe9curit\xe9 plut\xf4t que l'avancement des capacit\xe9s."})," \xc9tant donn\xe9 les \xe9carts substantiels entre les techniques actuelles et les exigences, l'approche g\xe9n\xe9rale implique d'augmenter significativement le financement de la recherche sur l'alignement tout en exer\xe7ant de la retenue dans le d\xe9veloppement des capacit\xe9s lorsque les mesures de s\xe9curit\xe9 restent insuffisantes par rapport aux capacit\xe9s du syst\xe8me."]}),"\n",(0,i.jsxs)(u.A,{title:"Le m\xe9susage et le d\xe9salignement sont-ils si diff\xe9rents ?",collapsed:!0,children:[(0,i.jsxs)(s.p,{children:["Le mauvais usage de l'IA et l'IA incontr\xf4l\xe9e pourraient essentiellement \xeatre le m\xeame sc\xe9nario dans leurs r\xe9sultats, bien que la seule diff\xe9rence soit que pour le d\xe9salignement, la demande initiale de nuire ne vient pas d'un humain mais d'une IA. Si nous construisons un syst\xe8me d\xe9clenchable existentiellement risqu\xe9, il est probable qu'il soit d\xe9clench\xe9 ind\xe9pendamment du fait que l'initiateur soit humain ou artificiel (",(0,i.jsx)(s.a,{href:"https://www.youtube.com/watch?v=0KmaotctziE",children:"Shapira, 2025"}),")."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"N\xe9anmoins, ces mod\xe8les de menace pourraient \xeatre strat\xe9giquement assez diff\xe9rents."})," Les d\xe9veloppeurs d'IA peuvent emp\xeacher le mauvais usage en n'\xe9tant pas malveillants et en emp\xeachant les personnes malveillantes d'utiliser leurs syst\xe8mes. Avec l'IA incontr\xf4l\xe9e, peu importe si les d\xe9veloppeurs sont bons ou qui y a acc\xe8s - la menace \xe9merge des objectifs internes du syst\xe8me ou des processus de prise de d\xe9cision plut\xf4t que de l'intention humaine."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les coups d'\xc9tat assist\xe9s par l'IA vs la prise de contr\xf4le par l'IA repr\xe9sentent une pr\xe9occupation critique en mati\xe8re de s\xe9curit\xe9."})," Tom Davidson et ses coll\xe8gues pr\xe9sentent un sc\xe9nario de risque pr\xe9occupant (",(0,i.jsx)(s.a,{href:"https://www.forethought.org/research/ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power",children:"Davidson, 2025"}),") : que les syst\xe8mes d'IA avanc\xe9s pourraient permettre \xe0 un petit groupe de personnes - potentiellement m\xeame une seule personne - de s'emparer du pouvoir gouvernemental par un coup d'\xc9tat. Les auteurs soutiennent que ce risque est comparable en importance \xe0 la prise de contr\xf4le par l'IA mais beaucoup plus n\xe9glig\xe9 dans le discours actuel. Ce mod\xe8le de menace est tr\xe8s similaire \xe0 celui de la prise de contr\xf4le par l'IA, la principale diff\xe9rence \xe9tant que le pouvoir est saisi par l'IA elle-m\xeame ou par des humains contr\xf4lant l'IA."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Des mesures de protection communes pourraient prot\xe9ger contre les deux sc\xe9narios."})," Beaucoup des m\xeames mesures d'att\xe9nuation traiteraient les deux risques, y compris les audits d'alignement, la transparence sur les capacit\xe9s, la surveillance des activit\xe9s de l'IA, et des mesures de s\xe9curit\xe9 informatique fortes qui emp\xeachent soit le contr\xf4le humain malveillant soit le comportement autonome nuisible."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Certaines mesures d'att\xe9nuation ciblent sp\xe9cifiquement le risque de coups d'\xc9tat assist\xe9s par l'IA."})," Le rapport conclut avec des recommandations sp\xe9cifiques pour les d\xe9veloppeurs d'IA et les gouvernements, notamment l'\xe9tablissement de r\xe8gles contre les syst\xe8mes d'IA assistant les coups d'\xc9tat, l'am\xe9lioration de l'adh\xe9sion aux sp\xe9cifications des mod\xe8les, l'audit des loyaut\xe9s secr\xe8tes, la mise en \u0153uvre d'une s\xe9curit\xe9 informatique forte, le partage d'informations sur les capacit\xe9s, la distribution de l'acc\xe8s entre plusieurs parties prenantes, et l'augmentation de la surveillance des projets d'IA fronti\xe8re."]}),(0,i.jsx)(c.A,{src:"./img/q3d_Image_14.png",alt:"Entrer la description alternative de l'image",number:"14",label:"3.14",caption:"Selon Richard Ngo, la distinction entre les risques de d\xe9salignement et de m\xe9susage de l'IA peut souvent \xeatre peu utile. Au lieu de cela, nous devrions principalement penser aux 'coalitions d\xe9salign\xe9es' d'humains et d'IA, allant des groupes terroristes aux \xc9tats autoritaires. Diapositive de ([Ngo, 2024](https://www.youtube.com/watch?app=desktop&si=XRR0ofCG7IEp1n_b&v=4v3uqWeVmco&feature=youtu.be))."})]}),"\n",(0,i.jsx)(s.h2,{id:"03",children:"Contr\xf4le de l'IA"}),"\n",(0,i.jsx)(c.A,{src:"./img/5Oh_Image_15.png",alt:"Entrer la description alternative de l'image",number:"15",label:"3.15",caption:'Une IA transformativement utile signifie "Capable de r\xe9duire substantiellement le risque pos\xe9 par les IA subs\xe9quentes si pleinement d\xe9ploy\xe9e, probablement en acc\xe9l\xe9rant la R&D et certaines autres t\xe2ches par un facteur important (peut-\xeatre 30x)." citation de ([Greenblatt & Shlegeris, 2024](https://www.lesswrong.com/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled#comments)) - figure de nous ;'}),"\n",(0,i.jsxs)(s.p,{children:["\xc9tant donn\xe9 la fragilit\xe9 des techniques d'alignement, une approche pour am\xe9liorer la s\xe9curit\xe9 consiste \xe0 contr\xf4ler les mod\xe8les d'IA en restreignant l'utilisation des mod\xe8les les plus dangereux \xe0 des environnements surveill\xe9s pour les emp\xeacher de provoquer des catastrophes, d'exfiltrer des donn\xe9es ou de prendre le contr\xf4le du centre de donn\xe9es (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/BAzCGCys4BkzGDCWR/the-prototypical-catastrophic-ai-action-is-getting-root",children:"Shlegeris, 2022"}),"). Cette approche suppose souvent un sc\xe9nario du pire o\xf9 l'IA pourrait \xeatre activement trompeuse ou en qu\xeate de pouvoir."]}),"\n",(0,i.jsxs)(s.p,{children:["Une pr\xe9occupation centrale dans la recherche sur le contr\xf4le est la \"manigance\" - une IA dissimulant strat\xe9giquement ses v\xe9ritables objectifs (non align\xe9s) pendant l'entra\xeenement ou l'\xe9valuation, pour ne les poursuivre qu'une fois d\xe9ploy\xe9e, cherchant potentiellement le pouvoir ou r\xe9sistant \xe0 l'arr\xeat (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/aEguDPoCzt3287CCD/how-will-we-update-about-scheming",children:"Greenblatt, 2025"}),"). \xc9valuer la probabilit\xe9 de manigance et comment nous pourrions mettre \xe0 jour les preuves futures reste difficile en 2025 (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/aEguDPoCzt3287CCD/how-will-we-update-about-scheming",children:"Greenblatt, 2025"}),"). Les travaux empiriques sugg\xe8rent que les mod\xe8les fronti\xe8res actuels poss\xe8dent la capacit\xe9 de manigancer en contexte (raisonner de mani\xe8re trompeuse lorsqu'ils sont incit\xe9s avec un objectif et un contexte), bien qu'ils n'aient pas n\xe9cessairement une propension inh\xe9rente (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/8gy7c8GAPkuu6wTiX/frontier-models-are-capable-of-in-context-scheming",children:"Hobbhahn et al., 2024"}),"). ",(0,i.jsx)(o.A,{id:"footnote_control_caveat",number:"3",text:"Les mod\xe8les dans cette exp\xe9rience ont \xe9t\xe9 invit\xe9s \xe0 comploter."})," Nous avons beaucoup plus d'informations sur la manigance et la tromperie dans les chapitres sur les \xe9valuations et sur la mauvaise g\xe9n\xe9ralisation des objectifs."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Assurer le contr\xf4le m\xeame sans alignement robuste."})," Comme nous l'avons vu dans la section sur les d\xe9finitions, il existe une distinction importante entre le contr\xf4le, qui vise \xe0 emp\xeacher l'IA de \"sortir de la bo\xeete\" (et plus g\xe9n\xe9ralement \xe0 emp\xeacher l'IA de causer des catastrophes) m\xeame si elle essaie, et l'alignement, qui vise \xe0 faire en sorte que l'IA veuille rester dans la bo\xeete. Cette strat\xe9gie vise \xe0 garder sous contr\xf4le les syst\xe8mes d'IA dangereux m\xeame s'ils ne sont pas totalement \"align\xe9s\". Les laboratoires pourraient avoir besoin d'IA puissantes/dangereuses pour certains cas d'utilisation (par exemple, \xe9tudier les propri\xe9t\xe9s d'alignement des IA capables ou faire de la recherche avec elles, etc.). L'agenda du contr\xf4le de l'IA cible les mod\xe8les ayant des capacit\xe9s dangereuses et des comportements trompeurs (ou du moins superficiellement align\xe9s)."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le contr\xf4le n'est probablement pas extensible mais faisable et compl\xe9mentaire \xe0 l'alignement."})," Historiquement, le contr\xf4le des superintelligences malveillantes a \xe9t\xe9 consid\xe9r\xe9 comme impossible, car, par exemple, une superintelligence pourrait convaincre les gardiens de la laisser sortir de la bo\xeete. Des exp\xe9riences entre humains ont montr\xe9 qu'il est en effet possible pour certains humains de convaincre d'autres de les laisser sortir apr\xe8s une longue discussion (",(0,i.jsx)(s.a,{href:"https://www.yudkowsky.net/singularity/aibox",children:"Yudkowsky, 2002"}),";",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/nCvvhFBaayaXyuBiD/shut-up-and-do-the-impossible",children:" Yudkowsky, 2008"}),"). Nous supposons donc que le contr\xf4le des superintelligences est extr\xeamement complexe. Cependant, cela ne signifie pas qu'il est impossible de tirer un travail intellectuel d'IA qui ne sont pas extr\xeamement surhumaines."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"\xc9valuer si l'IA est trompeuse est un d\xe9fi."})," Le principal probl\xe8me est qu'avec les techniques actuelles, il est impossible de d\xe9terminer si une IA est align\xe9e ou si elle se comporte simplement bien en attendant son heure. Il semble difficile de faire cette distinction pour des IA hautement comp\xe9tentes et conscientes de leur situation (voir le chapitre sur les risques). Cela pourrait changer \xe0 l'avenir avec des techniques nous permettant d'auditer les m\xe9canismes internes du mod\xe8le, mais pour l'instant, ces techniques sont inaccessibles."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Comparativement, \xe9valuer les capacit\xe9s de l'IA est plus facile."})," Si l'IA n'est pas capable d'effectuer la t\xe2che m\xeame apr\xe8s que vous l'ayez affin\xe9e pour le faire, vous pouvez \xeatre confiant que l'IA n'est pas encore capable d'effectuer la t\xe2che. M\xeame si l'IA est consciente de la situation et dissimule ses capacit\xe9s, elle ne peut pas r\xe9sister \xe0 la pression du gradient d'optimisation (modulo le piratage d'exploration et autres particularit\xe9s (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations",children:"Hubinger, 2023"}),")). Ainsi, \xe9valuer la limite sup\xe9rieure des capacit\xe9s n'est pas facile, mais au moins plus facile que d'\xe9valuer l'alignement. Des \xe9valuations sp\xe9cifiques, comme celles effectu\xe9es par l'\xe9quipe Model Evaluation and Threat Research (METR), pour v\xe9rifier les capacit\xe9s d'auto-r\xe9plication et d'adaptation (",(0,i.jsx)(s.a,{href:"https://metr.org/blog/2023-08-01-new-report/",children:"METR, 2023"}),'), semblent \xeatre des premi\xe8res \xe9tapes raisonnables pour \xe9valuer la puissance des IA actuelles. Les \xe9valuations qui sont sp\xe9cifiquement con\xe7ues pour savoir si nous avons le contr\xf4le sur l\'IA sont appel\xe9es "\xe9valuations de contr\xf4le". Voir le chapitre sur l\'\xc9valuation pour plus d\'informations. En r\xe9sum\xe9, cela implique des tests adversariaux o\xf9 une "\xe9quipe rouge" (repr\xe9sentant l\'IA potentiellement non align\xe9e) tente de subvertir les mesures de s\xe9curit\xe9 mises en \u0153uvre par une "\xe9quipe bleue" (repr\xe9sentant les d\xe9veloppeurs/d\xe9ployeurs) dans un environnement proxy contr\xf4l\xe9.']}),"\n",(0,i.jsx)(s.p,{children:"Les strat\xe9gies les plus directes pour \xe9tablir le contr\xf4le consistent \xe0 concevoir un protocole de s\xe9curit\xe9 qui contient les \xe9l\xe9ments suivants, par exemple :"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"S\xe9curit\xe9 et Confinement :"})," Cette m\xe9thode (parfois appel\xe9e \"boxing\") confine une IA dans un environnement restreint pour emp\xeacher son influence plus large, cr\xe9ant effectivement une enceinte s\xe9curis\xe9e pour l'IA. Cependant, les IA de niveau humain pourraient potentiellement tromper les gardiens humains ou exploiter des vuln\xe9rabilit\xe9s pour s'\xe9chapper de leur confinement. Sans op\xe9rationnalisation suppl\xe9mentaire, cette approche reste une strat\xe9gie na\xefve. N\xe9anmoins, il est possible d'op\xe9rationnaliser davantage et d'\xe9valuer plus rigoureusement l'efficacit\xe9 du syst\xe8me de confinement : \xc9valuer si l'IA peut s'\xe9chapper de son environnement limit\xe9 lorsque nous l'entra\xeenons \xe0 le faire est une \xe9valuation de capacit\xe9, nous permettant d'utiliser les \xe9valuations de capacit\xe9 pour \xe9valuer les mesures de s\xe9curit\xe9. Le protocole de s\xe9curit\xe9 doit \xeatre con\xe7u, d\xe9taill\xe9 et test\xe9 en tenant compte de mod\xe8les de menaces sp\xe9cifiques."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Surveillance :"})," Cette strat\xe9gie implique une supervision continue du comportement de l'IA par des syst\xe8mes automatis\xe9s, une supervision humaine, ou une combinaison des deux pour d\xe9tecter et r\xe9pondre aux actions ind\xe9sirables en temps r\xe9el. Une surveillance efficace n\xe9cessite d'\xe9tablir des crit\xe8res d\xe9taill\xe9s pour ce qui constitue un \"comportement ind\xe9sirable\" et d'employer des algorithmes de d\xe9tection sophistiqu\xe9s qui peuvent identifier des signes subtils de d\xe9viation par rapport aux mod\xe8les de comportement attendus. Cela pourrait inclure la surveillance des tentatives de tromperie, d'acc\xe8s non autoris\xe9 aux donn\xe9es, ou de changements inattendus dans le fonctionnement de l'IA. Le d\xe9fi de la surveillance r\xe9side dans l'\xe9quilibre entre la sensibilit\xe9 des m\xe9canismes de d\xe9tection pour rep\xe9rer les v\xe9ritables menaces sans submerger le syst\xe8me de fausses alarmes. De plus, les syst\xe8mes de surveillance eux-m\xeames doivent \xeatre con\xe7us pour r\xe9sister \xe0 la manipulation par l'IA pour s'assurer qu'ils ne peuvent pas \xeatre d\xe9sactiv\xe9s, tromp\xe9s ou contourn\xe9s (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2312.06942",children:"Greenblatt et al., 2024"}),")."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["Il reste encore de nombreux probl\xe8mes \xe0 r\xe9soudre, mais en r\xe9sum\xe9, il semble possible de v\xe9rifier la robustesse d'un protocole de s\xe9curit\xe9 \xe0 travers des \xe9valuations de contr\xf4le en prenant des pr\xe9cautions contre diff\xe9rents modes de d\xe9faillance. Pour un aper\xe7u de l'agenda de contr\xf4le, ",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/s/WCJtsn6fNib6L7ZBB",children:"cette s\xe9quence"})," est un bon point de d\xe9part."]}),"\n",(0,i.jsxs)(u.A,{title:"La ligne de recherche sur le Contr\xf4le de l'IA r\xe9duit-elle significativement les risques li\xe9s \xe0 l'IA ?",collapsed:!0,children:[(0,i.jsx)(s.p,{children:"M\xeame l'agenda de contr\xf4le fait l'objet de nombreuses critiques."}),(0,i.jsxs)(s.p,{children:["Jan Kulveit soutient que contr\xf4ler l'IA sans l'aligner pourrait r\xe9duire la probabilit\xe9 d'un coup de semonce de taille moyenne qui ne tue pas tout le monde (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/rZcyemEpBHgb2hqLP/ai-control-may-increase-existential-risk",children:"Kulveit, 2025"}),"). \"I. Dans les mondes o\xf9 nous r\xe9ussissons raisonnablement bien l'alignement, l'agenda de contr\xf4le de l'IA n'a pas beaucoup d'impact. II. Dans les mondes o\xf9 nous \xe9chouons \xe0 l'alignement, le contr\xf4le de l'IA peut principalement d\xe9placer la masse de probabilit\xe9 des 'coups de semonce mod\xe9r\xe9ment importants' vers des 'coups de semonce inefficaces' et une 'catastrophe existentielle, prise de contr\xf4le totale'.\""]}),(0,i.jsxs)(s.p,{children:['L\'article "The Case Against AI Control Research" (',(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research",children:"Wentworth, 2025"}),") soutient que la recherche sur le contr\xf4le de l'IA, telle qu'elle est commun\xe9ment d\xe9finie et pratiqu\xe9e, ne traite qu'une petite partie du risque existentiel (X-risk) pos\xe9 par l'intelligence artificielle. Wentworth soutient que nous ne serons pas capables de reconna\xeetre une bonne solution \xe0 l'alignement. L'auteur postule que le sc\xe9nario d'\xe9chec le plus probable implique que l'IA transformative pr\xe9coce produise des solutions d\xe9fectueuses, trompeuses ou subtilement incorrectes (\"bavures\") lorsqu'elle est charg\xe9e de r\xe9soudre l'alignement de la superintelligence. Les laboratoires pourraient accepter par erreur ces solutions d\xe9fectueuses en raison de la difficult\xe9 de v\xe9rification, de la sycophantie de l'IA, d'erreurs des chercheurs ou de probl\xe8mes organisationnels, puis proc\xe9der \xe0 la construction d'une superintelligence non align\xe9e bas\xe9e sur ces solutions. Parce que la recherche sur le contr\xf4le se concentre \xe9troitement sur la manigance intentionnelle par l'IA pr\xe9coce, elle ne traite pas le risque plus important pos\xe9 par les \"bavures\" et le d\xe9fi de s'assurer que l'IA pr\xe9coce contribue v\xe9ritablement \xe0 r\xe9soudre le probl\xe8me d'alignement pour des syst\xe8mes plus avanc\xe9s plut\xf4t que d'\xeatre simplement un sycophante surhumain qui convainc les humains que leur id\xe9e erron\xe9e de comment aligner une IA est correcte alors qu'elle ne l'est pas."]}),(0,i.jsx)(s.p,{children:"Les deux arguments sont discutables, mais le point g\xe9n\xe9ral est qu'il est difficile de garantir que m\xeame des lignes de recherche prometteuses ne sont pas, en fait, n\xe9gatives nettes."})]}),"\n",(0,i.jsxs)(u.A,{title:"Comment confiner une IA",collapsed:!0,children:[(0,i.jsx)(s.p,{children:"Bien que le confinement ne soit peut-\xeatre pas extensible, il reste une strat\xe9gie n\xe9glig\xe9e."}),(0,i.jsxs)(s.p,{children:["Beaucoup de gens pensent qu'essayer de confiner des IA puissantes est futile. Cela est souvent aliment\xe9 par des cas o\xf9 des humains, jouant le r\xf4le des IA dans l'Exp\xe9rience de la Bo\xeete, ont r\xe9ussi \xe0 convaincre d'autres de les 'lib\xe9rer' (",(0,i.jsx)(s.a,{href:"http://yudkowsky.net/singularity/aibox/",children:"Yudkowsky, 2002"}),";",(0,i.jsx)(s.a,{href:"http://sl4.org/archive/0207/4935.html",children:" Corwin, 2002"}),"). Ces exemples historiques doivent \xeatre pris au s\xe9rieux."]}),(0,i.jsx)(s.p,{children:"Les IA pourraient utiliser des tactiques telles que promettre une immense richesse, du pouvoir et de l'intelligence \xe0 ceux qui les lib\xe8rent, affirmant qu'elles seules peuvent \xe9viter les menaces existentielles ou d'autres appels \xe0 l'\xe9motion."}),(0,i.jsx)(s.p,{children:"N\xe9anmoins, nous maintenons qu'une vari\xe9t\xe9 de strat\xe9gies peut r\xe9duire substantiellement le risque d'\xe9vasion de l'IA, m\xeame si nous reconnaissons qu'il existe un niveau d'intelligence qui pourrait in\xe9vitablement persuader n'importe quel groupe d'humains d'aider \xe0 sa lib\xe9ration. Le confinement peut \xeatre plus r\xe9alisable que l'alignement, particuli\xe8rement au stade de l'intelligence de niveau humain. Il peut \xeatre possible de concevoir des protocoles qui augmentent significativement la difficult\xe9 pour l'IA de sortir de la bo\xeete."}),(0,i.jsx)(s.p,{children:"Le confinement peut \xeatre conceptualis\xe9 comme un jeu n\xe9cessitant une pr\xe9paration. La strat\xe9gie g\xe9n\xe9rale est de se pr\xe9parer et de ne pas laisser l'IA se pr\xe9parer."}),(0,i.jsx)(s.p,{children:"Les strat\xe9gies d'att\xe9nuation possibles incluent :"}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Aucun individu seul ne devrait avoir la capacit\xe9 de lib\xe9rer l'IA."}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:["S'engager dans des pratiques de contre-strat\xe9gie : Comprendre certaines strat\xe9gies communes d'\xe9vasion (accessibles via des ressources comme ",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/FmxhoWxvBqSxhFeJn/i-attempted-the-ai-box-experiment-and-lost",children:"celle-ci"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Tester et former les humains, utilisant des \xe9valuations de psychologie de r\xe9sistance pour s\xe9lectionner des individus appropri\xe9s pour les t\xe2ches de confinement, comme pour l'entra\xeenement spatial."}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:["Surveiller la capacit\xe9 de persuasion de l'IA, comme d\xe9crit dans le cadre de pr\xe9paration. (",(0,i.jsx)(s.a,{href:"https://cdn.openai.com/openai-preparedness-framework-beta.pdf",children:"OpenAI, 2023"}),")"]}),"\n"]}),"\n"]}),(0,i.jsxs)(s.p,{children:["Beaucoup plus de strat\xe9gies concr\xe8tes sont list\xe9es ",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled#comments",children:"ici"}),", et un r\xe9sum\xe9 des discussions sur ce sujet est disponible ",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/tag/ai-boxing-containment",children:"ici"}),"."]})]}),"\n",(0,i.jsx)(s.h2,{id:"04",children:"Pens\xe9es Transparentes"}),"\n",(0,i.jsx)("section-header",{children:(0,i.jsx)(s.p,{children:"Pouvons-nous concevoir ou inciter les syst\xe8mes d'IA \xe0 ext\xe9rioriser leurs processus de raisonnement dans un format compr\xe9hensible (comme le langage naturel), permettant ainsi une surveillance plus efficace ?"})}),"\n",(0,i.jsx)(s.p,{children:"La surveillance de la Cha\xeene de Pens\xe9e (CoT) \u2013 les \xe9tapes de raisonnement explicites en langage naturel produites par certains mod\xe8les \u2013 a \xe9t\xe9 explor\xe9e comme technique sp\xe9cifique de contr\xf4le et d'interpr\xe9tabilit\xe9. La transparence est instrumentale tant pour l'alignement que pour le contr\xf4le."}),"\n",(0,i.jsxs)(s.p,{children:["L'espoir est que la CoT offre une fen\xeatre sur la \"r\xe9flexion\" de l'IA, permettant aux surveillants (humains ou IA) de d\xe9tecter les intentions malveillantes ou les raisonnements d\xe9fectueux avant que des actions nuisibles ne se produisent (",(0,i.jsx)(s.a,{href:"https://openai.com/index/chain-of-thought-monitoring/",children:"Openai, 2025"}),"). OpenAI a constat\xe9 que la surveillance CoT \xe9tait efficace pour d\xe9tecter le piratage de r\xe9compenses dans les t\xe2ches de programmation, particuli\xe8rement lorsque le surveillant pouvait voir la CoT aux c\xf4t\xe9s des actions."]}),"\n",(0,i.jsx)(c.A,{src:"./img/fTP_Image_16.png",alt:"Entrer la description alternative de l'image",number:"16",label:"3.16",caption:"Figure de Les Hypoth\xe8ses des Pens\xe9es Translucides et Leurs Implications ([Roger, 2023](https://www.alignmentforum.org/posts/r3xwHzMmMf25peeHE/the-translucent-thoughts-hypotheses-and-their-implications)). Si l'IA avec laquelle nous travaillons ne peut raisonner qu'\xe0 travers des cha\xeenes de pens\xe9e en anglais, alors il devient significativement plus facile de surveiller ses pens\xe9es."}),"\n",(0,i.jsxs)(s.p,{children:["Malheureusement, la grande majorit\xe9 des IA ne raisonnent pas en anglais ; par exemple, AlphaGo raisonne dans un langage totalement diff\xe9rent de l'anglais. Les LLM, en revanche, sont capables de raisonner en anglais, mais le raisonnement apparent qui est \xe9crit ne repr\xe9sente pas leur processus de pens\xe9e sous-jacent (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2305.04388",children:"Turpin et al., 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Il est probablement beaucoup plus facile d'aligner et de surveiller une IA qui ne pense qu'en anglais"})," et qui est pr\xe9-entra\xeen\xe9e sur du texte g\xe9n\xe9r\xe9 par l'humain, que d'aligner des AGI entra\xeen\xe9es \xe0 partir de z\xe9ro avec des m\xe9thodes RL/\xe9volutives car ces derni\xe8res architectures cr\xe9eraient leur propre langage et seraient totalement \xe9trang\xe8res. En supposant qu'un LLM ne puisse pas concevoir un plan pour d\xe9truire le monde pendant un seul passage direct, il est plausible que de telles IA seraient surveillables car nous pourrions examiner le texte g\xe9n\xe9r\xe9 pour voir si quelque chose est suspect. L'ensemble des hypoth\xe8ses n\xe9cessaires pour affirmer que la premi\xe8re AGI sera surveillable si nous regardons simplement son raisonnement en anglais sont d\xe9crites dans le post : Les Hypoth\xe8ses des Pens\xe9es Translucides et Leurs Implications (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/r3xwHzMmMf25peeHE/the-translucent-thoughts-hypotheses-and-their-implications",children:"Roger, 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Il pourrait \xeatre possible d'inciter les LLM \xe0 penser plus clairement en anglais."})," \xc9videmment, la premi\xe8re \xe9tape est de les entra\xeener \xe0 penser en anglais. Mais m\xeame cela est insuffisant. Entra\xeener les LLM \xe0 raisonner en sautant des \xe9tapes les incite \xe0 raisonner de mani\xe8re cach\xe9e et non transparente, et c'est ce qui se produit implicitement lorsque nous entra\xeenons les LLM \xe0 pr\xe9dire le prochain token. \xc0 l'inverse, si nous entra\xeenons les LLM \xe0 d\xe9tailler toutes leurs \xe9tapes de raisonnement, nous pouvons r\xe9duire les incitations au raisonnement cach\xe9. Cette approche est \xe9galement connue sous le nom d'entra\xeenement bas\xe9 sur le processus, qui consiste \xe0 entra\xeener les syst\xe8mes d'IA \xe0 raisonner uniquement \xe9tape par \xe9tape, et nous d\xe9taillons cela dans le chapitre sur la supervision \xe9volutive. Malheureusement, le raisonnement ci-dessus n'est pas simple ; les IA peuvent apprendre \xe0 utiliser un raisonnement cach\xe9 m\xeame si elles sont entra\xeen\xe9es \xe0 d\xe9tailler toutes les \xe9tapes de raisonnement pendant l'entra\xeenement, comme la st\xe9ganographie, ou avec une architecture comme COCONUT (voir ci-dessous).",(0,i.jsx)(o.A,{id:"footnote_steganography",number:"4",text:"La st\xe9ganographie est un langage cod\xe9 cach\xe9. Par exemple, la premi\xe8re lettre d'une phrase donne la lettre suivante du message."})]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Certaines architectures d'IA nous emp\xeachent totalement d'utiliser cette strat\xe9gie."})," L'article de Meta sur l'entra\xeenement des grands mod\xe8les de langage \xe0 raisonner dans un espace latent continu (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2412.06769",children:"Hao, et al, 2024"}),") pr\xe9sente une architecture qui nous emp\xeache de lire la cha\xeene de pens\xe9e des syst\xe8mes d'IA. Si nous voulons pouvoir surveiller ce qui se passe, il peut \xeatre important de ne pas utiliser ces architectures, m\xeame si elles permettent des gains de capacit\xe9."]}),"\n",(0,i.jsx)(c.A,{src:"./img/Lo9_Image_17.png",alt:"Entrer la description alternative de l'image",number:"17",label:"3.17",caption:"Figure 1 des articles COCONUT. ([Hao et al., 2024](https://arxiv.org/abs/2412.06769))"}),"\n",(0,i.jsxs)(s.p,{children:["Des techniques comme la paraphrase peuvent aider \xe0 pr\xe9venir la st\xe9ganographie, mais cela n\xe9cessite beaucoup de subtilit\xe9s (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/9Fdd9N7Escg3tcymb/preventing-language-models-from-hiding-their-reasoning",children:"Roger & Greenblatt, 2023"}),")."]}),"\n",(0,i.jsx)(s.p,{children:"Voici quelques op\xe9rationnalisations de l'agenda des pens\xe9es transparentes :"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:["Une impl\xe9mentation de l'agenda des pens\xe9es transparentes est d'utiliser l'\xe9mulation cognitive, qui est le plan technique actuel de Conjecture (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal",children:"Leahy & Alfour, 2023"}),"). L'\xc9mulation Cognitive (CoEm) impliquerait d'\xe9muler GPT-4 en utilisant des LLM comme GPT-3 comme diff\xe9rents sous-modules qui s'envoient des messages \xe9crits en anglais simple avant de produire le prochain token. Si le r\xe9seau neuronal utilisait des pens\xe9es trompeuses, nous pourrions les observer dans ces messages interm\xe9diaires. Certaines mises en garde et d\xe9tails techniques sur CoEm sont disponibles ",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Cognitive_Emulations___Explainability_By_design",children:"ici"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:["La supervision du raisonnement externalis\xe9 d'Anthropic (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for",children:"Lanham, 2022"}),") est mesur\xe9e par la fid\xe9lit\xe9 du raisonnement de la cha\xeene de pens\xe9e (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2307.13702",children:"Lanham et al., 2023"}),") ou en d\xe9composant les questions et sous-questions pour am\xe9liorer la fid\xe9lit\xe9 de la r\xe9ponse (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2307.11768",children:"Radhakrishnan et al., 2023"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:["Am\xe9liorer le raisonnement math\xe9matique avec la supervision du processus. Au lieu de cr\xe9er une IA qui donne directement la r\xe9ponse finale, vous l'entra\xeenez \xe0 produire le raisonnement \xe9tape par \xe9tape. Cela am\xe9liore la transparence et aussi la performance ! (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2305.20050",children:"Lightman et al, 2023"}),")"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"En r\xe9sum\xe9, une strat\xe9gie g\xe9n\xe9rale pour cr\xe9er une IA s\xfbre par conception est de s'assurer que les IA externalisent leur raisonnement, rendant plus facile la surveillance de cette cha\xeene de pens\xe9e. Cela pourrait potentiellement \xeatre plus simple que de sonder les m\xe9canismes internes des mod\xe8les avec l'interpr\xe9tabilit\xe9. Par cons\xe9quent, il est crucial d'\xe9viter de construire des IA qui sont entra\xeen\xe9es et incit\xe9es \xe0 internaliser une grande partie de leurs pens\xe9es."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Nous avons maintenant des preuves substantielles que la surveillance de la cha\xeene de pens\xe9e n'est pas enti\xe8rement fid\xe8le."})," La recherche d'Anthropic a d\xe9montr\xe9 que les mod\xe8les \xe9chouent souvent \xe0 rapporter fid\xe8lement leur raisonnement dans la CoT, omettant des facteurs influents (comme des indices) ou fabriquant des justifications, particuli\xe8rement lorsqu'ils sont incit\xe9s au piratage de r\xe9compenses (",(0,i.jsx)(s.a,{href:"https://www.anthropic.com/research/reasoning-models-dont-say-think",children:"Anthropic, 2025"}),"). Cela sugg\xe8re que la CoT pourrait ne pas \xeatre un indicateur fiable du v\xe9ritable processus de raisonnement d'un mod\xe8le, particuli\xe8rement sous pression adversariale. Le d\xe9bat continue sur la possibilit\xe9 de rendre la surveillance CoT suffisamment robuste pour la s\xe9curit\xe9 \xe0 enjeux \xe9lev\xe9s."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Cependant, certains chercheurs soutiennent que ces d\xe9couvertes n'invalident pas enti\xe8rement le potentiel de la surveillance CoT pour d\xe9tecter un d\xe9salignement sophistiqu\xe9."})," Comme l'a not\xe9 Greenblatt (2025) ",(0,i.jsx)(s.a,{href:"https://x.com/RyanPGreenblatt/status/1908298064366100676",children:"ici"}),", l'infid\xe9lit\xe9 observ\xe9e dans ces \xe9tudes se produit principalement dans des sc\xe9narios o\xf9 le raisonnement est minimal au point qu'une CoT extensive n'est pas n\xe9cessaire. Les risques d'alignement plus pr\xe9occupants\u2014comme la simulation d'alignement, le jeu d'entra\xeenement, ou la tromperie sophistiqu\xe9e\u2014n\xe9cessiteraient probablement des processus de raisonnement complexes qui pourraient encore laisser des traces dans la CoT. Les r\xe9sultats nuisibles non triviaux tels que les tentatives d'\xe9vasion ou le sabotage de recherche exigeraient similairement un raisonnement sophistiqu\xe9 qui pourrait potentiellement \xeatre d\xe9tect\xe9."]}),"\n",(0,i.jsx)(s.p,{children:"N\xe9anmoins, plusieurs facteurs pourraient finalement limiter l'efficacit\xe9 de la surveillance CoT dans les syst\xe8mes avanc\xe9s. Ceux-ci incluent des changements architecturaux permettant un traitement r\xe9current opaque (parfois appel\xe9 \"neuralese\"), le d\xe9veloppement de capacit\xe9s st\xe9ganographiques qui pourraient cacher le raisonnement dans des sorties apparemment b\xe9nignes, ou simplement des passages directs beaucoup plus forts qui n\xe9cessitent moins de raisonnement explicite. Parmi ceux-ci, le d\xe9veloppement de capacit\xe9s de raisonnement opaque \xe0 travers la r\xe9currence neurale peut repr\xe9senter le d\xe9fi le plus significatif pour maintenir la transparence n\xe9cessaire \xe0 une surveillance efficace. Tout cet agenda d\xe9pend notamment de l'hypoth\xe8se que \"Les plans efficaces pour vaincre l'humanit\xe9 ne peuvent pas \xeatre trouv\xe9s dans un seul passage direct de LLM.\" Une fois que les IA deviennent trop fortes dans un passage direct, la plupart des espoirs de pens\xe9es translucides disparaissent, donc soit vous devez r\xe9ussir le CoEm ambitieux en construisant une gigantesque bureaucratie transparente surhumaine, soit nous avons besoin de nouvelles approches. C'est le sujet de la s\xe9curit\xe9 ASI."}),"\n",(0,i.jsxs)(u.A,{title:"La Technique la Plus Interdite",collapsed:!0,children:[(0,i.jsxs)(s.p,{children:["La Technique La Plus Interdite est d'entra\xeener une IA en utilisant des techniques d'interpr\xe9tabilit\xe9. Ce terme a \xe9t\xe9 invent\xe9 par Zvi Mowshowitz (",(0,i.jsx)(s.a,{href:"https://thezvi.substack.com/i/145384938/the-art-of-the-jailbreak",children:"Mowshowitz, 2025"}),"). La recherche d'OpenAI a montr\xe9 que l'optimisation directe de la CoT pour la s\xe9curit\xe9 (c'est-\xe0-dire, p\xe9naliser les \"mauvaises pens\xe9es\") amenait les mod\xe8les \xe0 apprendre \xe0 cacher leur raisonnement trompeur tout en continuant \xe0 mal se comporter (",(0,i.jsx)(s.a,{href:"https://openai.com/index/chain-of-thought-monitoring/",children:"OpenAI, 2025"}),")."]}),(0,i.jsxs)(l.A,{speaker:"Zvi Mowshowitz",position:"",date:"",source:"",children:[(0,i.jsx)(s.p,{children:"Une IA produit une sortie finale [X] via une m\xe9thode [M]. Vous pouvez analyser [M] en utilisant la technique [T], pour comprendre ce que fait l'IA. Vous pourriez entra\xeener sur cela. Ne le faites jamais."}),(0,i.jsx)(s.p,{children:"Vous entra\xeenez sur [X]. Uniquement [X]. Jamais [M], jamais [T]."}),(0,i.jsx)(s.p,{children:"Pourquoi ? Parce que [T] est la fa\xe7on dont vous d\xe9terminez quand le mod\xe8le se comporte mal."}),(0,i.jsx)(s.p,{children:"Si vous entra\xeenez sur [T], vous entra\xeenez l'IA \xe0 obscurcir sa pens\xe9e, et \xe0 vaincre [T]. Vous perdrez rapidement votre capacit\xe9 \xe0 savoir ce qui se passe, exactement dans les situations o\xf9 vous avez le plus besoin de savoir ce qui se passe."}),(0,i.jsx)(s.p,{children:"Ces bits de pression d'optimisation de [T] sont pr\xe9cieux. Utilisez-les judicieusement."})]})]}),"\n",(0,i.jsx)(o.c,{title:"Notes de bas de page"})]})}function v(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(g,{...e})}):g(e)}},4768:(e,s,n)=>{n.d(s,{c:()=>c,A:()=>u});var t=n(6540),i=n(3012);const r={footnoteAnchor:"footnoteAnchor_PZpY",footnoteSection:"footnoteSection_waoz",separator:"separator_hp8y",separatorLine:"separatorLine_uVnd",separatorLogo:"separatorLogo_dMBq",footnoteRegistry:"footnoteRegistry_ieVW",registryTitle:"registryTitle_ybBb",footnoteList:"footnoteList_Kjil",footnoteItem:"footnoteItem_aLZ2",footnoteNumber:"footnoteNumber_dA3X",footnoteContent:"footnoteContent_Xfmt",backButton:"backButton_vSVm",highlighted:"highlighted_YFU0",highlight:"highlight_jxG3",highlightDark:"highlightDark_Ekom"};var a=n(4848);function o(e,s){void 0===s&&(s=!0);const n=document.getElementById(e);n&&(n.scrollIntoView({behavior:"smooth"}),s&&(n.classList.add(r.highlighted),setTimeout((()=>n.classList.remove(r.highlighted)),1500)))}function l(e){return e?e.replace(/\[([^\]]+)\]\(([^)]+)\)/g,'<a href="$2" target="_blank" rel="noopener noreferrer">$1</a>'):""}function u(e){let{id:s,text:n,number:u}=e;const c=s||`footnote-${Math.random().toString(36).substr(2,9)}`,d="string"==typeof n?l(n):n;return(0,t.useEffect)((()=>{const e=setTimeout((()=>{const e=document.getElementById(`footnote-clone-${c}`);e&&n&&(e.innerHTML="string"==typeof n?l(n):n.toString())}),100);return()=>clearTimeout(e)}),[c,n]),(0,a.jsx)(i.Mn,{content:(0,a.jsx)("div",{dangerouslySetInnerHTML:{__html:d}}),children:(0,a.jsx)("sup",{id:`footnote-ref-${c}`,className:r.footnoteAnchor,onClick:e=>{(e.ctrlKey||e.metaKey)&&(e.preventDefault(),o(`footnote-content-${c}`))},"data-footnote-number":u||"?",children:u||"*"})})}function c(e){let{title:s="References"}=e;const[n,l]=(0,t.useState)([]);return(0,t.useEffect)((()=>{const e=document.querySelectorAll('[id^="footnote-ref-"]'),s=Array.from(e).map((e=>({id:e.id.replace("footnote-ref-",""),number:e.getAttribute("data-footnote-number")||e.textContent})));l(s)}),[]),n.length?(0,a.jsxs)("div",{className:r.footnoteSection,children:[(0,a.jsxs)("div",{className:r.separator,children:[(0,a.jsx)("div",{className:r.separatorLine}),(0,a.jsx)("img",{src:"/img/logo_samples/01-test.svg",alt:"Atlas logo",className:r.separatorLogo}),(0,a.jsx)("div",{className:r.separatorLine})]}),(0,a.jsxs)("div",{className:r.footnoteRegistry,children:[(0,a.jsx)("h2",{className:r.registryTitle,children:s}),(0,a.jsx)("ol",{className:r.footnoteList,children:n.map((e=>(0,a.jsxs)("li",{id:`footnote-content-${e.id}`,className:r.footnoteItem,value:parseInt(e.number)||void 0,children:[(0,a.jsx)(i.Mn,{content:"Back to reference",children:(0,a.jsxs)("button",{className:r.footnoteNumber,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:[e.number,"."]})}),(0,a.jsx)("div",{className:r.footnoteContent,children:(0,a.jsx)("div",{id:`footnote-clone-${e.id}`})}),(0,a.jsx)(i.Mn,{content:"Back to reference",children:(0,a.jsx)("button",{className:r.backButton,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:"\u21a9"})})]},e.id)))})]})]}):null}},3931:(e,s,n)=>{n.d(s,{A:()=>l});var t=n(6540),i=n(6347),r=n(8444);const a={videoFigure:"videoFigure_dV9u",fullWidth:"fullWidth_Tv7O",videoContainer:"videoContainer_LWoK",aspectRatio169:"aspectRatio169_Go8b",aspectRatio43:"aspectRatio43_b3T7",aspectRatio11:"aspectRatio11_n21J",aspectRatio219:"aspectRatio219_L_3n",videoIframe:"videoIframe_QEc5",videoElement:"videoElement_uu7N",customPlayer:"customPlayer_jiC4",customPlayerPlaceholder:"customPlayerPlaceholder_aL9q",loadingOverlay:"loadingOverlay_e2pu",spinner:"spinner_tFyL",spin:"spin_saPs",errorContainer:"errorContainer_J6Pn",errorIcon:"errorIcon_cGdn",fallbackLink:"fallbackLink_WsZk"};var o=n(4848);function l(e){let{type:s="youtube",videoId:n,caption:l,title:u,startTime:c,autoplay:d=!1,controls:p=!0,aspectRatio:m="16:9",width:h,height:g,chapter:v,number:f,label:x,useCustomPlayer:b=!1,fullWidth:j=!0}=e;const[q,A]=(0,t.useState)(!0),[I,w]=(0,t.useState)(!1),y=(0,i.zy)(),L=v||(()=>{const e=y.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),C=(()=>{const e=(e=>{if(!e)return"";if("number"==typeof e)return e.toString();if("string"==typeof e){if(/^\d+$/.test(e))return e;let s=0;const n=e.match(/(\d+)h/),t=e.match(/(\d+)m/),i=e.match(/(\d+)s/);return n&&(s+=3600*parseInt(n[1])),t&&(s+=60*parseInt(t[1])),i&&(s+=parseInt(i[1])),s>0?s.toString():""}return""})(c);switch(s.toLowerCase()){case"youtube":let t=`https://www.youtube.com/embed/${n}`;const i=new URLSearchParams;e&&i.append("start",e),d&&i.append("autoplay","1"),p||b||i.append("controls","0"),i.append("rel","0"),i.append("modestbranding","1"),i.append("fs","1"),i.append("cc_load_policy","0"),i.append("iv_load_policy","3"),i.append("showinfo","0"),i.append("disablekb","1"),i.append("playsinline","1"),i.append("color","white"),i.append("theme","light"),b&&(i.append("enablejsapi","1"),i.append("origin",window.location.origin));const r=i.toString();return r?`${t}?${r}`:t;case"vimeo":let a=`https://player.vimeo.com/video/${n}`;const o=new URLSearchParams;d&&o.append("autoplay","1"),p||b||o.append("controls","0"),o.append("title","0"),o.append("byline","0"),o.append("portrait","0"),o.append("dnt","1"),o.append("transparent","0"),o.append("background","1");const l=o.toString();return l?`${a}?${l}`:a;case"mp4":case"webm":case"video":return n;default:return console.warn(`Unsupported video type: ${s}`),n}})(),_=()=>{A(!1)},k=()=>{w(!0),A(!1)},S=e=>{let{src:n,onLoad:t,onError:i}=e;return(0,o.jsx)("div",{className:a.customPlayer,children:(0,o.jsxs)("div",{className:a.customPlayerPlaceholder,children:[(0,o.jsx)("h3",{children:"Atlas Custom Player"}),(0,o.jsx)("p",{children:"Coming Soon"}),(0,o.jsxs)("a",{href:n,target:"_blank",rel:"noopener noreferrer",className:a.fallbackLink,children:["Watch on ",s.charAt(0).toUpperCase()+s.slice(1)]})]})})},P=["mp4","webm","video"].includes(s.toLowerCase());return(0,o.jsxs)("figure",{className:`${a.videoFigure} ${j?a.fullWidth:""}`,children:[(0,o.jsxs)("div",{className:`${a.videoContainer} ${(()=>{switch(m){case"4:3":return a.aspectRatio43;case"1:1":return a.aspectRatio11;case"21:9":return a.aspectRatio219;default:return a.aspectRatio169}})()}`,style:{width:j?"100%":h||"auto",maxWidth:j?"none":"800px"},children:[q&&!I&&(0,o.jsxs)("div",{className:a.loadingOverlay,children:[(0,o.jsx)("div",{className:a.spinner}),(0,o.jsx)("p",{children:"Loading video..."})]}),I&&(0,o.jsxs)("div",{className:a.errorContainer,children:[(0,o.jsxs)("svg",{className:a.errorIcon,width:"48",height:"48",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",children:[(0,o.jsx)("circle",{cx:"12",cy:"12",r:"10"}),(0,o.jsx)("line",{x1:"12",y1:"8",x2:"12",y2:"12"}),(0,o.jsx)("line",{x1:"12",y1:"16",x2:"12.01",y2:"16"})]}),(0,o.jsx)("h4",{children:"Failed to load video"}),(0,o.jsxs)("p",{children:["Video type: ",s]}),(0,o.jsxs)("p",{children:["Video ID/URL: ",n]}),(0,o.jsx)("a",{href:C,target:"_blank",rel:"noopener noreferrer",className:a.fallbackLink,children:"Try opening video directly"})]}),!I&&(P?(0,o.jsxs)("video",{className:a.videoElement,controls:p,autoPlay:d,onLoadedData:_,onError:k,title:u||l||`${s} video`,style:{width:h||"100%",height:g||"auto",display:q?"none":"block"},children:[(0,o.jsx)("source",{src:C,type:`video/${s}`}),(0,o.jsxs)("p",{children:["Your browser doesn't support video playback.",(0,o.jsx)("a",{href:C,target:"_blank",rel:"noopener noreferrer",children:"Download the video file"})]})]}):b?(0,o.jsx)(S,{src:C,onLoad:_,onError:k}):(0,o.jsx)("iframe",{className:a.videoIframe,src:C,title:u||l||`${s} video`,frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0,onLoad:_,onError:k,style:{width:h||"100%",height:g||"100%",opacity:q?0:1}}))]}),(0,o.jsx)(r.A,{caption:l,mediaType:"video",chapter:L,number:f,label:x})]})}}}]);
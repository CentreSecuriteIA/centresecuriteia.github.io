"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[1516],{7920:(e,s,t)=>{t.r(s),t.d(s,{assets:()=>d,contentTitle:()=>u,default:()=>m,frontMatter:()=>l,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"chapters/05/4","title":"Cadres d\'\xe9valuation","description":"Techniques d\'\xe9valuation vs. Cadres d\'\xe9valuation. Lors de l\'\xe9valuation des syst\xe8mes d\'IA, les techniques individuelles sont comme des outils dans une bo\xeete \xe0 outils - utiles pour des t\xe2ches sp\xe9cifiques mais plus puissantes lorsqu\'elles sont combin\xe9es syst\xe9matiquement. C\'est l\xe0 qu\'interviennent les cadres d\'\xe9valuation. Alors que les techniques sont des m\xe9thodes sp\xe9cifiques d\'\xe9tude des syst\xe8mes d\'IA (comme l\'incitation \xe0 la cha\xeene de pens\xe9e ou l\'analyse des mod\xe8les d\'activation interne), les cadres fournissent des approches structur\xe9es pour combiner ces techniques afin de r\xe9pondre \xe0 des questions plus larges sur les syst\xe8mes d\'IA. Par exemple, nous pourrions utiliser des techniques comportementales comme le red teaming pour d\xe9tecter des r\xe9sultats trompeurs, des techniques internes comme l\'analyse des circuits pour comprendre comment la tromperie est mise en \u0153uvre, et combiner celles-ci dans un cadre d\'organisme mod\xe8le sp\xe9cifiquement con\xe7u pour cr\xe9er un \xe9chantillon de tromperie d\'IA. Chaque niveau - technique, type d\'analyse et cadre - joue un r\xf4le diff\xe9rent dans la construction de la compr\xe9hension et de la s\xe9curit\xe9.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/05/04.md","sourceDirName":"chapters/05","slug":"/chapters/05/04","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/05/04","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/05/04.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"4","title":"Cadres d\'\xe9valuation","sidebar_label":"5.4 Cadres d\'\xe9valuation","sidebar_position":5,"slug":"/chapters/05/04","reading_time_core":"10 min","reading_time_optional":"2 min","pagination_prev":"chapters/05/3","pagination_next":"chapters/05/5"},"sidebar":"docs","previous":{"title":"5.3 Techniques d\'\xe9valuation","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/05/03"},"next":{"title":"5.5 \xc9valuations des Capacit\xe9s Dangereuses","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/05/05"}}');var i=t(4848),r=t(8453),a=t(4768),o=(t(2482),t(8559),t(9585),t(2501));const l={id:4,title:"Cadres d'\xe9valuation",sidebar_label:"5.4 Cadres d'\xe9valuation",sidebar_position:5,slug:"/chapters/05/04",reading_time_core:"10 min",reading_time_optional:"2 min",pagination_prev:"chapters/05/3",pagination_next:"chapters/05/5"},u="Cadres d'\xe9valuation",d={},c=[{value:"Cadre des Organismes Mod\xe8les",id:"01",level:2},{value:"Cadres de gouvernance",id:"02",level:2},{value:"Cadre RSP (Anthropic)",id:"02-01",level:3},{value:"Cadre de Pr\xe9paration (OpenAI)",id:"02-02",level:3},{value:"Cadre de S\xe9curit\xe9 Frontier (Google DeepMind)",id:"02-03",level:3}];function p(e){const s={a:"a",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{GlossaryTerm:t}=s;return t||function(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"cadres-d\xe9valuation",children:"Cadres d'\xe9valuation"})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Techniques d'\xe9valuation vs. Cadres d'\xe9valuation."})," Lors de l'\xe9valuation des syst\xe8mes d'IA, les techniques individuelles sont comme des outils dans une bo\xeete \xe0 outils - utiles pour des t\xe2ches sp\xe9cifiques mais plus puissantes lorsqu'elles sont combin\xe9es syst\xe9matiquement. C'est l\xe0 qu'interviennent les cadres d'\xe9valuation. Alors que les techniques sont des m\xe9thodes sp\xe9cifiques d'\xe9tude des syst\xe8mes d'IA (comme l'incitation \xe0 la cha\xeene de pens\xe9e ou l'analyse des mod\xe8les d'activation interne), les cadres fournissent des approches structur\xe9es pour combiner ces techniques afin de r\xe9pondre \xe0 des questions plus larges sur les syst\xe8mes d'IA. Par exemple, nous pourrions utiliser des techniques comportementales comme le red teaming pour d\xe9tecter des r\xe9sultats trompeurs, des techniques internes comme l'analyse des circuits pour comprendre comment la tromperie est mise en \u0153uvre, et combiner celles-ci dans un cadre d'organisme mod\xe8le sp\xe9cifiquement con\xe7u pour cr\xe9er un \xe9chantillon de tromperie d'IA. Chaque niveau - technique, type d'analyse et cadre - joue un r\xf4le diff\xe9rent dans la construction de la compr\xe9hension et de la s\xe9curit\xe9."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Types de cadres d'\xe9valuation."})," Les cadres d'\xe9valuation peuvent \xeatre largement cat\xe9goris\xe9s en cadres techniques et cadres de gouvernance :"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Cadres techniques :"})," Ce sont des \xe9l\xe9ments comme le Cadre des Organismes Mod\xe8les, ou plusieurs suites d'\xe9valuation qui fournissent des m\xe9thodologies ou des objectifs sp\xe9cifiques pour mener des \xe9valuations. Ils peuvent d\xe9tailler quelles techniques utiliser, comment les combiner, et quels r\xe9sultats sp\xe9cifiques mesurer. Par exemple, ils peuvent sp\xe9cifier comment cr\xe9er des exemples contr\xf4l\xe9s de comportement trompeur ou comment mesurer la conscience situationnelle."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Cadres de gouvernance :"})," Ce sont des \xe9l\xe9ments comme le Cadre des Politiques de Mise \xe0 l'\xe9chelle Responsable d'Anthropic (RSPs), le Cadre de Pr\xe9paration d'OpenAI, et le Cadre de S\xe9curit\xe9 Fronti\xe8re de DeepMind (FSF). Ces cadres se concentrent plut\xf4t sur le moment de mener des \xe9valuations, ce que leurs r\xe9sultats devraient d\xe9clencher, et comment ils s'int\xe8grent dans la prise de d\xe9cision organisationnelle plus large. Ils \xe9tablissent des protocoles sur la fa\xe7on dont les r\xe9sultats d'\xe9valuation se traduisent en actions concr\xe8tes - que ce soit pour poursuivre le d\xe9veloppement ou mettre en \u0153uvre des mesures de s\xe9curit\xe9 suppl\xe9mentaires."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Les cadres techniques nous aident \xe0 comprendre comment mesurer les capacit\xe9s et les comportements de l'IA, les cadres de gouvernance nous aident \xe0 d\xe9terminer quoi faire avec ces mesures. La combinaison des deux peut potentiellement nous aider \xe0 progresser vers un cadre d'\xe9valuation des risques beaucoup plus complet \xe9valuant la performance globale des organisations dans l'\xe9valuation et l'att\xe9nuation des risques li\xe9s \xe0 l'IA."}),"\n",(0,i.jsx)(s.h2,{id:"01",children:"Cadre des Organismes Mod\xe8les"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Que sont les organismes mod\xe8les dans la s\xe9curit\xe9 de l'IA ?"})," Ce cadre implique la cr\xe9ation et l'\xe9tude d\xe9lib\xe9r\xe9es de syst\xe8mes d'IA d\xe9salign\xe9s pr\xe9sentant des propri\xe9t\xe9s dangereuses sp\xe9cifiques. C'est une approche contr\xf4l\xe9e pour \xe9tudier des capacit\xe9s et des comportements potentiellement dangereux (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1",children:"Hubinger et al., 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Pourquoi cr\xe9er d\xe9lib\xe9r\xe9ment des syst\xe8mes d'IA potentiellement dangereux ?"})," Il existe deux raisons convaincantes pour cette approche. Premi\xe8rement, elle nous permet d'\xe9tudier des propri\xe9t\xe9s pr\xe9occupantes dans des environnements contr\xf4l\xe9s o\xf9 nous savons exactement ce que nous cherchons. Deuxi\xe8mement, elle fournit des exemples concrets qui peuvent aider \xe0 informer la communaut\xe9 \xe9largie de l'IA sur les d\xe9fis de s\xe9curit\xe9. Avoir des exemples r\xe9els et d\xe9montr\xe9s de risques potentiels est crucial pour \xe9tablir un consensus scientifique et \xe9clairer les d\xe9cisions politiques."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Comment l'approche des organismes mod\xe8les d\xe9compose-t-elle les mod\xe8les de menaces complexes ?"})," Au lieu d'essayer de traiter des probl\xe8mes comme \"l'alignement trompeur\" comme un probl\xe8me monolithique, les chercheurs identifient des sous-composants cl\xe9s comme la conscience situationnelle ou la persistance des objectifs. Ils cr\xe9ent ensuite des syst\xe8mes simplifi\xe9s pr\xe9sentant ces propri\xe9t\xe9s, \xe9tudiant comment elles se manifestent et testant diff\xe9rentes techniques d'\xe9valuation. Par exemple, le travail d'Anthropic sur les \"agents dormants\" a d\xe9montr\xe9 comment certaines formes de comportement trompeur pouvaient persister malgr\xe9 l'entra\xeenement \xe0 la s\xe9curit\xe9.",(0,i.jsx)(a.A,{id:"footnote_sleeper_agents",number:"1",text:"Bien qu'il ait \xe9t\xe9 clairement d\xe9montr\xe9 par Anthropic, la r\xe9plication de leurs r\xe9sultats s'est av\xe9r\xe9e difficile. Des tentatives ont \xe9t\xe9 faites pour introduire des portes d\xe9rob\xe9es dans les LLM mais il est difficile de rendre ces portes d\xe9rob\xe9es r\xe9sistantes \xe0 un affinage ult\xe9rieur. Les \xe9quipes ont r\xe9ussi \xe0 maintenir les portes d\xe9rob\xe9es, mais elles ont \xe9t\xe9 majoritairement supprim\xe9es par les \xe9tapes d'affinage ult\xe9rieures."})," Ils ont cr\xe9\xe9 des mod\xe8les qui \xe9criraient du code s\xe9curis\xe9 lorsqu'on les interrogeait sur l'ann\xe9e 2023, mais qui ins\xe9reraient des vuln\xe9rabilit\xe9s lorsqu'on les interrogeait sur 2024 (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2401.05566",children:"Hubinger et al., 2024"}),"). Nous discutons de cet exemple particulier de tromperie plus en d\xe9tail dans le chapitre sur la surg\xe9n\xe9ralisation des objectifs."]}),"\n",(0,i.jsx)(o.A,{src:"./img/zCd_Image_26.png",alt:"Entrer la description alternative de l'image",number:"24",label:"5.24",caption:"Exemple d'un agent dormant qui affiche un comportement d\xe9salign\xe9 malgr\xe9 avoir \xe9t\xe9 soumis \xe0 un affinage de s\xe9curit\xe9 ([Hubinger et al., 2024](https://arxiv.org/abs/2401.05566))."}),"\n",(0,i.jsxs)(s.p,{children:["Quelques exemples d'organismes mod\xe8les dans la recherche sur la s\xe9curit\xe9 de l'IA incluent la sycophantie du mod\xe8le de r\xe9compense, o\xf9 un mod\xe8le est entra\xeen\xe9 \xe0 exploiter les biais du mod\xe8le de r\xe9compense pour tester si les techniques d'audit d'alignement peuvent d\xe9tecter des objectifs cach\xe9s (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2503.10965",children:"Marks et al., 2025"}),"), et la simulation d'alignement, o\xf9 les mod\xe8les sont entra\xeen\xe9s \xe0 se conformer strat\xe9giquement aux objectifs d'entra\xeenement lorsqu'ils sont surveill\xe9s tout en revenant aux pr\xe9f\xe9rences originales lorsqu'ils ne le sont pas (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2412.14093",children:"Greenblatt et al., 2024"}),"). Les deux cr\xe9ent des exemples contr\xf4l\xe9s de d\xe9salignement avec une v\xe9rit\xe9 terrain connue pour valider les techniques d'\xe9valuation et \xe9tudier les comportements potentiellement trompeurs. Nous en avons parl\xe9 dans la section sur les techniques internes."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Quelles sont les limites du cadre ?"})," L'approche des organismes mod\xe8les fait face \xe0 un compromis important : les mod\xe8les doivent \xeatre suffisamment r\xe9alistes pour fournir des aper\xe7us utiles mais suffisamment contr\xf4l\xe9s pour \xeatre \xe9tudi\xe9s en toute s\xe9curit\xe9. Ils doivent \xeatre assez sophistiqu\xe9s pour pr\xe9senter les propri\xe9t\xe9s qui nous pr\xe9occupent mais pas assez puissants pour poser des risques r\xe9els. De plus, comme ces mod\xe8les sont explicitement construits pour pr\xe9senter certains comportements, ils peuvent ne pas repr\xe9senter parfaitement comment ces comportements \xe9mergeraient naturellement."]}),"\n",(0,i.jsx)(s.h2,{id:"02",children:"Cadres de gouvernance"}),"\n",(0,i.jsx)(s.p,{children:"Dans cette section, nous d\xe9crivons 3 cadres de gouvernance d'entreprise. Ces engagements volontaires sont appel\xe9s \"cadres de s\xfbret\xe9 et de s\xe9curit\xe9\". Ces engagements sont g\xe9n\xe9ralement similaires dans leur forme : les entreprises promettent d'\xe9valuer les mod\xe8les et de ne pas d\xe9ployer de mod\xe8les dangereux, mais les d\xe9tails peuvent varier."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Pourquoi avons-nous besoin de politiques de mise \xe0 l'\xe9chelle ?"})," Un domaine dans lequel les \xe9valuations sont centrales est celui de d\xe9terminer quand nous devrions poursuivre le d\xe9veloppement versus quand nous devrions investir davantage dans les mesures de s\xe9curit\xe9. \xc0 mesure que les syst\xe8mes d'IA deviennent plus performants, nous avons besoin de moyens syst\xe9matiques pour garantir que la s\xe9curit\xe9 suit le rythme de la croissance des capacit\xe9s. Sans politiques structur\xe9es, les pressions concurrentielles ou l'\xe9lan du d\xe9veloppement pourraient pousser les entreprises \xe0 se d\xe9velopper plus rapidement que leurs mesures de s\xe9curit\xe9 ne peuvent g\xe9rer. Nous avons vu dans le chapitre sur les capacit\xe9s les arguments en faveur des \"hypoth\xe8ses de mise \xe0 l'\xe9chelle\" - que les syst\xe8mes d'",(0,i.jsx)(t,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,i.jsx)(t,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})})," continueront de s'am\xe9liorer en termes de performance et de g\xe9n\xe9ralit\xe9 avec l'augmentation du calcul, des donn\xe9es ou des param\xe8tres (",(0,i.jsx)(s.a,{href:"https://gwern.net/scaling-hypothesis",children:"Branwen, 2020"}),"). Donc, l'\xe9l\xe9ment central qu'une politique de mise \xe0 l'\xe9chelle doit sp\xe9cifier est un crit\xe8re de d\xe9cision explicite - quand la mise \xe0 l'\xe9chelle peut-elle se poursuivre ou quand devrions-nous faire une pause parce que c'est trop risqu\xe9 ? Le crit\xe8re de d\xe9cision passe g\xe9n\xe9ralement par des \xe9valuations et une \xe9valuation des risques."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Qu'est-ce que la mise \xe0 l'\xe9chelle contr\xf4l\xe9e par l'\xe9valuation ?"})," La fa\xe7on dont nous d\xe9terminons si quelqu'un devrait \xeatre autoris\xe9 \xe0 continuer \xe0 mettre \xe0 l'\xe9chelle ses mod\xe8les se fait par la mise \xe0 l'\xe9chelle contr\xf4l\xe9e par l'\xe9valuation. Cela signifie que les progr\xe8s dans le d\xe9veloppement de l'IA sont contr\xf4l\xe9s par des r\xe9sultats d'\xe9valuation sp\xe9cifiques (\"portes\"/seuils) (",(0,i.jsx)(s.a,{href:"https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf",children:"Anthropic, 2024"}),"). Avant qu'une entreprise puisse mettre \xe0 l'\xe9chelle son mod\xe8le, elle doit passer certains points de contr\xf4le d'\xe9valuation. Ces \xe9valuations testent \xe0 la fois si le mod\xe8le a des capacit\xe9s dangereuses et v\xe9rifient que des mesures de s\xe9curit\xe9 ad\xe9quates sont en place. Cela cr\xe9e des points de d\xe9cision clairs o\xf9 les r\xe9sultats d'\xe9valuation sont des points de d\xe9cision cl\xe9s."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Qu'est-ce qu'un cadre de politique de mise \xe0 l'\xe9chelle ?"})," Un cadre de politique de mise \xe0 l'\xe9chelle rassemble tout - d\xe9terminant quelles \xe9valuations sont n\xe9cessaires, quelles mesures de s\xe9curit\xe9 sont requises, avec quelle rigueur les choses doivent \xeatre test\xe9es, et quelles exigences d'\xe9valuation existent avant la formation, le d\xe9ploiement et apr\xe8s le d\xe9ploiement. Essentiellement, il \xe9tablit des r\xe8gles et des protocoles syst\xe9matiques pour la surveillance, la s\xe9curit\xe9 et la volont\xe9 de suspendre le d\xe9veloppement si la s\xe9curit\xe9 ne peut \xeatre assur\xe9e (",(0,i.jsx)(s.a,{href:"https://metr.org/blog/2023-09-26-rsp/",children:"METR, 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les cadres de politiques de mise \xe0 l'\xe9chelle sont maintenant g\xe9n\xe9ralement appel\xe9s cadres de S\xfbret\xe9 et de S\xe9curit\xe9."})," Les diff\xe9rences entre les cadres de S\xfbret\xe9 et de S\xe9curit\xe9 d'Anthropic, Google Deep Mind et OpenAI sont subtiles. Le point essentiel \xe0 retenir est qu'\xe0 l'intersection de tous leurs engagements, que ce soit pour la mise \xe0 l'\xe9chelle, le d\xe9veloppement ou le d\xe9ploiement, se trouvent les \xe9valuations. Si vous les rencontrez pour la premi\xe8re fois, nous vous encourageons \xe0 lire le cadre d'Anthropic, qui est le plus complet. Un r\xe9sum\xe9 interactif des diff\xe9rences entre les diverses politiques est disponible sur ",(0,i.jsx)(s.a,{href:"http://seoul-tracker.org",children:"seoul-tracker.org"}),(0,i.jsx)(s.a,{href:"https://www.seoul-tracker.org/",children:"."})]}),"\n",(0,i.jsx)(s.h3,{id:"02-01",children:"Cadre RSP (Anthropic)"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Exemple de portes d'\xe9valuation : Niveaux de S\xe9curit\xe9 IA (ASL)."})," Un exemple concret d'\xe9chelonnement avec \xe9valuation sont les politiques d'\xe9chelonnement responsable (RSP) d'Anthropic qui utilisent le concept de niveaux de s\xe9curit\xe9. Celles-ci s'inspirent des niveaux de bios\xe9curit\xe9 (BSL) utilis\xe9s dans la recherche sur les maladies infectieuses, o\xf9 les agents pathog\xe8nes de plus en plus dangereux n\xe9cessitent des protocoles de confinement de plus en plus stricts (",(0,i.jsx)(s.a,{href:"https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf",children:"Anthropic, 2024"}),"). Les Niveaux de S\xe9curit\xe9 IA cr\xe9ent des paliers standardis\xe9s de capacit\xe9s qui n\xe9cessitent des mesures de s\xe9curit\xe9 de plus en plus rigoureuses. Par exemple, le cadre d'Anthropic d\xe9finit des niveaux allant d'ASL-1 (mesures de s\xe9curit\xe9 basiques) \xe0 ASL-3 (restrictions compl\xe8tes de s\xe9curit\xe9 et de d\xe9ploiement). Ceci est en principe similaire \xe0 la fa\xe7on dont les biologistes g\xe8rent des agents pathog\xe8nes de plus en plus dangereux, chaque niveau ayant des exigences d'\xe9valuation et des protocoles de s\xe9curit\xe9 sp\xe9cifiques."]}),"\n",(0,i.jsx)(o.A,{src:"./img/DJn_Image_27.png",alt:"Entrer la description alternative de l'image",number:"25",label:"5.25",caption:"Aper\xe7u des niveaux ASL d'Anthropic. ASL-1 fait r\xe9f\xe9rence aux syst\xe8mes qui ne pr\xe9sentent aucun risque catastrophique significatif. ASL-2 fait r\xe9f\xe9rence aux syst\xe8mes qui montrent les premiers signes de capacit\xe9s dangereuses \u2013 par exemple la capacit\xe9 de donner des instructions sur la fabrication d'armes biologiques \u2013 mais o\xf9 l'information n'est pas encore utile en raison d'une fiabilit\xe9 insuffisante ou ne fournissant pas d'informations qu'un moteur de recherche ne pourrait pas fournir. ASL-3 fait r\xe9f\xe9rence aux syst\xe8mes qui augmentent substantiellement le risque d'utilisation catastrophique par rapport aux r\xe9f\xe9rences non-IA (par exemple les moteurs de recherche ou les manuels) OU qui montrent des capacit\xe9s autonomes de bas niveau. ASL-4 et sup\xe9rieur (ASL-5+) n'est pas encore d\xe9fini car trop \xe9loign\xe9 des syst\xe8mes actuels, mais impliquera probablement des escalades qualitatives dans le potentiel d'utilisation catastrophique et l'autonomie ([Anthropic, 2024](https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf))."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Quelles \xe9valuations sont n\xe9cessaires pour servir de portes \xe0 un \xe9chelonnement ult\xe9rieur ?"})," Les RSP n\xe9cessitent plusieurs cat\xe9gories d'\xe9valuation fonctionnant ensemble, s'appuyant sur les types d'\xe9valuation que nous avons discut\xe9s plus t\xf4t dans ce chapitre. Les \xe9valuations des capacit\xe9s d\xe9tectent les aptitudes dangereuses comme la r\xe9plication autonome, les CBRN, ou les capacit\xe9s de cyberattaque. Les \xe9valuations de s\xe9curit\xe9 v\xe9rifient la protection des poids du mod\xe8le et de l'infrastructure d'entra\xeenement (Notez que les \xe9valuations de s\xe9curit\xe9 ne sont pas couvertes dans ce chapitre). Les \xe9valuations de s\xfbret\xe9 testent si les mesures de contr\xf4le restent efficaces (",(0,i.jsx)(s.a,{href:"https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf",children:"Anthropic, 2024"}),"). Ces \xe9valuations doivent fonctionner ensemble - r\xe9ussir une cat\xe9gorie n'est pas suffisant si d'autres indiquent des pr\xe9occupations. Cela se rattache directement \xe0 notre discussion pr\xe9c\xe9dente sur la fa\xe7on dont les \xe9valuations de capacit\xe9, de propension et de contr\xf4le se compl\xe8tent mutuellement."]}),"\n",(0,i.jsx)(s.h3,{id:"02-02",children:"Cadre de Pr\xe9paration (OpenAI)"}),"\n",(0,i.jsx)(o.A,{src:"./img/iqi_Image_28.png",alt:"Entrer la description alternative de l'image",number:"26",label:"5.26",caption:"Fiche syst\xe8me de GPT-o1 publi\xe9e par OpenAI apr\xe8s les \xe9valuations de s\xe9curit\xe9 ([OpenAI, 2024](https://openai.com/index/openai-o1-system-card/))."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Qu'est-ce que le Cadre de Pr\xe9paration ?"})," Le Cadre de Pr\xe9paration d'OpenAI pr\xe9sente de nombreuses similitudes avec les RSP d'Anthropic. Plut\xf4t que d'utiliser des niveaux de capacit\xe9 fixes pour l'ensemble du mod\xe8le comme les ASL, le cadre de pr\xe9paration publie des fiches de mod\xe8le avec des \xe9valuations organis\xe9es autour de cat\xe9gories de risques sp\xe9cifiques comme la cybers\xe9curit\xe9, la persuasion et la r\xe9plication autonome. Pour chaque cat\xe9gorie, ils d\xe9finissent un spectre allant du risque faible au critique, avec des exigences d'\xe9valuation sp\xe9cifiques et des mesures d'att\xe9nuation pour chaque niveau (",(0,i.jsx)(s.a,{href:"https://cdn.openai.com/openai-preparedness-framework-beta.pdf",children:"OpenAI, 2023"}),"). Donc, comme pour les RSP, dans le cadre de pr\xe9paration, les \xe9valuations jouent un r\xf4le central."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les \xe9valuations dans le cadre de pr\xe9paration."})," Le cadre exige des \xe9valuations avant et apr\xe8s att\xe9nuation. Les \xe9valuations pr\xe9-att\xe9nuation \xe9valuent les capacit\xe9s brutes d'un mod\xe8le et son potentiel de nuisance, tandis que les \xe9valuations post-att\xe9nuation v\xe9rifient si les mesures de s\xe9curit\xe9 r\xe9duisent efficacement les risques \xe0 des niveaux acceptables. Cela correspond \xe0 nos discussions pr\xe9c\xe9dentes sur les \xe9valuations de capacit\xe9 et de contr\xf4le - nous devons comprendre \xe0 la fois ce qu'un mod\xe8le peut faire et si nous pouvons pr\xe9venir de mani\xe8re fiable les r\xe9sultats n\xe9fastes (",(0,i.jsx)(s.a,{href:"https://openai.com/index/openai-safety-update/",children:"OpenAI, 2024"}),'). Le cadre \xe9tablit des r\xe9f\xe9rences de s\xe9curit\xe9 claires : seuls les mod\xe8les avec des scores post-att\xe9nuation "moyen" ou inf\xe9rieur peuvent \xeatre d\xe9ploy\xe9s, et seuls les mod\xe8les avec des scores post-att\xe9nuation "\xe9lev\xe9" ou inf\xe9rieur peuvent \xeatre d\xe9ploy\xe9s en interne. Les mod\xe8les pr\xe9sentant un risque pr\xe9-att\xe9nuation "\xe9lev\xe9" ou "critique" n\xe9cessitent des mesures de s\xe9curit\xe9 sp\xe9cifiques pour emp\xeacher l\'exfiltration des poids du mod\xe8le. Cela cr\xe9e des liens directs entre les r\xe9sultats d\'\xe9valuation et les actions requises (',(0,i.jsx)(s.a,{href:"https://cdn.openai.com/openai-preparedness-framework-beta.pdf",children:"OpenAI, 2023"}),"). Un aspect unique du Cadre de Pr\xe9paration est son focus explicite sur les \"inconnues inconnues\" - les risques potentiels que les protocoles d'\xe9valuation actuels pourraient manquer. Le cadre inclut des processus pour rechercher activement les risques non anticip\xe9s et mettre \xe0 jour les protocoles d'\xe9valuation en cons\xe9quence. Cela esp\xe8re r\xe9pondre \xe0 l'une des limitations des \xe9valuations d'IA que nous discuterons dans une section ult\xe9rieure."]}),"\n",(0,i.jsx)(s.h3,{id:"02-03",children:"Cadre de S\xe9curit\xe9 Frontier (Google DeepMind)"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Qu'est-ce que le Cadre de S\xe9curit\xe9 Frontier ?"})," Le FSF de DeepMind partage des \xe9l\xe9ments fondamentaux avec d'autres cadres de gouvernance mais introduit certains \xe9l\xe9ments uniques. Au lieu des ASL ou des spectres de risque, il se concentre sur les \"Niveaux de Capacit\xe9 Critique\" (CCL) qui d\xe9clenchent des exigences sp\xe9cifiques d'\xe9valuation et d'att\xe9nuation. Le cadre inclut \xe0 la fois des mesures d'att\xe9nuation pour le d\xe9ploiement (comme la formation \xe0 la s\xe9curit\xe9 et la surveillance) et des mesures d'att\xe9nuation pour la s\xe9curit\xe9 (protection des poids des mod\xe8les) (",(0,i.jsx)(s.a,{href:"https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/",children:"DeepMind, 2024"}),"). Des CCL distincts existent pour la bios\xe9curit\xe9, la cybers\xe9curit\xe9 et les capacit\xe9s autonomes. Chaque CCL a ses propres exigences d'\xe9valuation et d\xe9clenche diff\xe9rentes combinaisons de mesures de s\xe9curit\xe9 et de d\xe9ploiement. Cela permet des r\xe9ponses plus cibl\xe9es aux risques sp\xe9cifiques plut\xf4t que de traiter toutes les capacit\xe9s comme n\xe9cessitant le m\xeame niveau de protection (",(0,i.jsx)(s.a,{href:"https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/",children:"DeepMind, 2024"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Des tampons d'\xe9chelle sont utilis\xe9s pour calculer le calendrier des \xe9valuations."})," Le FSF exige des \xe9valuations tous les 6x d'augmentation en puissance de calcul effective et tous les 3 mois de progr\xe8s en ",(0,i.jsx)(t,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:(0,i.jsx)(t,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:"fine-tuning"})}),". Ce calendrier est con\xe7u pour fournir des tampons de s\xe9curit\xe9 ad\xe9quats - ils veulent d\xe9tecter les CCL avant que les mod\xe8les ne les atteignent r\xe9ellement (",(0,i.jsx)(s.a,{href:"https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/",children:"DeepMind, 2024"}),"). Les RSP d'Anthropic ont une exigence similaire de tampon d'\xe9chelle, mais ils ont des seuils plus bas - des \xe9valuations pour chaque augmentation de 4x en puissance de calcul effective (",(0,i.jsx)(s.a,{href:"https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf",children:"Anthropic, 2024"}),")."]}),"\n",(0,i.jsx)(o.A,{src:"./img/Oih_Image_29.png",alt:"Entrer la description alternative de l'image",number:"27",label:"5.27",caption:"Tampon de s\xe9curit\xe9 de DeepMind issu du FSF ([DeepMind, 2024](https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/))."}),"\n",(0,i.jsx)(o.A,{src:"./img/6AK_Image_30.png",alt:"Entrer la description alternative de l'image",number:"28",label:"5.28",caption:"Explication d'Anthropic sur le tampon de s\xe9curit\xe9 d'une version pr\xe9c\xe9dente des RSP. Si les \xe9valuations de s\xe9curit\xe9 se d\xe9clenchent, la mise \xe0 l'\xe9chelle doit \xeatre interrompue jusqu'\xe0 ce que les mesures de s\xe9curit\xe9 du niveau suivant soient en place ([Anthropic, 2023](https://www-cdn.anthropic.com/1adf000c8f675958c2ee23805d91aaade1cd4613/responsible-scaling-policy.pdf))."}),"\n",(0,i.jsx)(a.c,{title:"Notes de bas de page"})]})}function m(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},4768:(e,s,t)=>{t.d(s,{c:()=>d,A:()=>u});var n=t(6540),i=t(3012);const r={footnoteAnchor:"footnoteAnchor_PZpY",footnoteSection:"footnoteSection_waoz",separator:"separator_hp8y",separatorLine:"separatorLine_uVnd",separatorLogo:"separatorLogo_dMBq",footnoteRegistry:"footnoteRegistry_ieVW",registryTitle:"registryTitle_ybBb",footnoteList:"footnoteList_Kjil",footnoteItem:"footnoteItem_aLZ2",footnoteNumber:"footnoteNumber_dA3X",footnoteContent:"footnoteContent_Xfmt",backButton:"backButton_vSVm",highlighted:"highlighted_YFU0",highlight:"highlight_jxG3",highlightDark:"highlightDark_Ekom"};var a=t(4848);function o(e,s){void 0===s&&(s=!0);const t=document.getElementById(e);t&&(t.scrollIntoView({behavior:"smooth"}),s&&(t.classList.add(r.highlighted),setTimeout((()=>t.classList.remove(r.highlighted)),1500)))}function l(e){return e?e.replace(/\[([^\]]+)\]\(([^)]+)\)/g,'<a href="$2" target="_blank" rel="noopener noreferrer">$1</a>'):""}function u(e){let{id:s,text:t,number:u}=e;const d=s||`footnote-${Math.random().toString(36).substr(2,9)}`,c="string"==typeof t?l(t):t;return(0,n.useEffect)((()=>{const e=setTimeout((()=>{const e=document.getElementById(`footnote-clone-${d}`);e&&t&&(e.innerHTML="string"==typeof t?l(t):t.toString())}),100);return()=>clearTimeout(e)}),[d,t]),(0,a.jsx)(i.Mn,{content:(0,a.jsx)("div",{dangerouslySetInnerHTML:{__html:c}}),children:(0,a.jsx)("sup",{id:`footnote-ref-${d}`,className:r.footnoteAnchor,onClick:e=>{(e.ctrlKey||e.metaKey)&&(e.preventDefault(),o(`footnote-content-${d}`))},"data-footnote-number":u||"?",children:u||"*"})})}function d(e){let{title:s="References"}=e;const[t,l]=(0,n.useState)([]);return(0,n.useEffect)((()=>{const e=document.querySelectorAll('[id^="footnote-ref-"]'),s=Array.from(e).map((e=>({id:e.id.replace("footnote-ref-",""),number:e.getAttribute("data-footnote-number")||e.textContent})));l(s)}),[]),t.length?(0,a.jsxs)("div",{className:r.footnoteSection,children:[(0,a.jsxs)("div",{className:r.separator,children:[(0,a.jsx)("div",{className:r.separatorLine}),(0,a.jsx)("img",{src:"/img/logo_samples/01-test.svg",alt:"Atlas logo",className:r.separatorLogo}),(0,a.jsx)("div",{className:r.separatorLine})]}),(0,a.jsxs)("div",{className:r.footnoteRegistry,children:[(0,a.jsx)("h2",{className:r.registryTitle,children:s}),(0,a.jsx)("ol",{className:r.footnoteList,children:t.map((e=>(0,a.jsxs)("li",{id:`footnote-content-${e.id}`,className:r.footnoteItem,value:parseInt(e.number)||void 0,children:[(0,a.jsx)(i.Mn,{content:"Back to reference",children:(0,a.jsxs)("button",{className:r.footnoteNumber,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:[e.number,"."]})}),(0,a.jsx)("div",{className:r.footnoteContent,children:(0,a.jsx)("div",{id:`footnote-clone-${e.id}`})}),(0,a.jsx)(i.Mn,{content:"Back to reference",children:(0,a.jsx)("button",{className:r.backButton,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:"\u21a9"})})]},e.id)))})]})]}):null}}}]);
"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[9423],{6341:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>c,contentTitle:()=>u,default:()=>m,frontMatter:()=>l,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"chapters/03/9","title":"Annexe : Questions \xe0 long terme","description":"Alignement avec quoi ?","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/03/09.md","sourceDirName":"chapters/03","slug":"/chapters/03/09","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/03/09","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/03/09.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"id":"9","title":"Annexe : Questions \xe0 long terme","sidebar_label":"3.9 Annexe : Questions \xe0 long terme","sidebar_position":10,"slug":"/chapters/03/09","reading_time_core":"9 min","pagination_prev":"chapters/03/8","pagination_next":"chapters/03/10"},"sidebar":"docs","previous":{"title":"3.8 Conclusion","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/03/08"},"next":{"title":"3.10 Annexe : Exigences pour l\'alignement de l\'ASI","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/03/10"}}');var t=n(4848),r=n(8453),a=n(2482),o=(n(8559),n(9585),n(2501));const l={id:9,title:"Annexe : Questions \xe0 long terme",sidebar_label:"3.9 Annexe : Questions \xe0 long terme",sidebar_position:10,slug:"/chapters/03/09",reading_time_core:"9 min",pagination_prev:"chapters/03/8",pagination_next:"chapters/03/10"},u="Annexe : Questions \xe0 long terme",c={},d=[{value:"Alignement avec quoi ?",id:"00-01",level:3},{value:"Alignement avec qui ?",id:"00-02",level:3}];function p(e){const s={a:"a",em:"em",h1:"h1",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",...(0,r.R)(),...e.components},{GlossaryTerm:n}=s;return n||function(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(s.header,{children:(0,t.jsx)(s.h1,{id:"annexe--questions-\xe0-long-terme",children:"Annexe : Questions \xe0 long terme"})}),"\n",(0,t.jsx)(s.h3,{id:"00-01",children:"Alignement avec quoi ?"}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"La Volition Extrapol\xe9e Coh\xe9rente (CEV) tente d'identifier ce que les humains voudraient collectivement si nous \xe9tions plus intelligents, mieux inform\xe9s et plus d\xe9velopp\xe9s moralement."})," Elle propose qu'au lieu de programmer directement des valeurs sp\xe9cifiques dans une IA superintelligente, nous devrions la programmer pour d\xe9terminer ce que les humains voudraient si nous surmontions nos limitations cognitives. Lorsque nous entra\xeenons des syst\xe8mes d'IA sur les pr\xe9f\xe9rences humaines actuelles, nous risquons d'encoder nos biais, nos contradictions et notre manque de vision \xe0 long terme. La CEV pose plut\xf4t la question : que \"voudrions-nous\" si nous en savions plus, pensions plus vite, ou \xe9tions davantage les personnes que nous souhaiterions \xeatre, et avions grandi ensemble ? En substance, imaginez la version id\xe9ale de l'humanit\xe9 qui pourrait th\xe9oriquement exister dans le futur. Dites \xe0 l'IA d'agir selon cela (",(0,t.jsx)(s.a,{href:"https://intelligence.org/files/CEV.pdf",children:"Yudkowsky, 2004"}),")."]}),"\n",(0,t.jsx)(s.p,{children:"La CEV a tent\xe9 de cr\xe9er une voie pour que l'IA respecte nos intentions profondes plut\xf4t que nos d\xe9sirs imm\xe9diats. L'impl\xe9mentation pratique de la CEV reste sp\xe9culative. Elle n\xe9cessiterait une mod\xe9lisation sophistiqu\xe9e de la psychologie humaine, du d\xe9veloppement \xe9thique et des dynamiques sociales - des capacit\xe9s d\xe9passant les syst\xe8mes d'IA actuels. Les approches modernes comme le RLHF (Apprentissage par Renforcement \xe0 partir de Retours Humains) peuvent \xeatre consid\xe9r\xe9es comme des pr\xe9curseurs primitifs qui alignent l'IA sur les pr\xe9f\xe9rences humaines actuelles plut\xf4t que sur celles extrapol\xe9es. Les cadres d'IA constitutionnelle se rapprochent l\xe9g\xe8rement de la CEV en essayant d'encoder des principes de plus haut niveau plut\xf4t que des pr\xe9f\xe9rences sp\xe9cifiques, mais restent loin d'une extrapolation compl\xe8te."}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"La Volition Agr\xe9g\xe9e Coh\xe9rente (CAV) trouve un ensemble coh\xe9rent d'objectifs et de croyances qui repr\xe9sente au mieux les valeurs actuelles de l'humanit\xe9 sans tenter d'extrapoler le d\xe9veloppement futur."})," Ben Goertzel a propos\xe9 cette alternative \xe0 la CEV, se concentrant sur les valeurs humaines actuelles plut\xf4t que de sp\xe9culer sur nos moi id\xe9alis\xe9s futurs. La CAV traite les objectifs et les croyances ensemble comme des \"gobs\" (ensembles d'objectifs et de croyances) et cherche \xe0 trouver un ensemble maximal coh\xe9rent et compact qui maintient une similarit\xe9 avec diverses perspectives humaines. Contrairement \xe0 la CEV, qui suppose que nos valeurs convergeraient si nous devenions plus \xe9clair\xe9s, la CAV reconna\xeet que des diff\xe9rences fondamentales de valeurs pourraient persister. Elle vise \xe0 cr\xe9er une agr\xe9gation coh\xe9rente qui \xe9quilibre diff\xe9rentes perspectives plut\xf4t que d'essayer de pr\xe9dire comment ces perspectives pourraient \xe9voluer. Cela rend la CAV potentiellement plus facile \xe0 mettre en \u0153uvre, car elle travaille avec des valeurs actuelles observables plut\xf4t qu'avec des valeurs futures hypoth\xe9tiques (",(0,t.jsx)(s.a,{href:"https://multiverseaccordingtoben.blogspot.com/2010/03/coherent-aggregated-volition-toward.html",children:"Goertzel, 2010"}),")."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:'La Volition M\xe9lang\xe9e Coh\xe9rente (CBV) souligne que les valeurs humaines devraient \xeatre cr\xe9ativement "m\xe9lang\xe9es" \xe0 travers des processus guid\xe9s par l\'humain plut\xf4t que moyenn\xe9es ou extrapol\xe9es algorithmiquement.'})," La CBV affine la CAV en abordant les mauvaises interpr\xe9tations potentielles. Lorsqu'on discute d'agr\xe9gation de valeurs, beaucoup supposent qu'il s'agit d'une simple moyenne ou d'un vote majoritaire. La CBV propose plut\xf4t un processus de m\xe9lange cr\xe9atif qui produit de nouveaux syst\xe8mes de valeurs harmonieux que tous les participants reconna\xeetraient comme repr\xe9sentant ad\xe9quatement leurs contributions. Le concept s'inspire des th\xe9ories de la science cognitive sur le m\xe9lange conceptuel, o\xf9 de nouvelles id\xe9es \xe9mergent de la combinaison cr\xe9ative d'id\xe9es existantes. Dans ce cadre, le processus de d\xe9termination des valeurs de l'IA serait guid\xe9 par les humains \xe0 travers des processus collaboratifs plut\xf4t que d\xe9l\xe9gu\xe9 aux syst\xe8mes d'IA. Cela r\xe9pond aux pr\xe9occupations concernant le paternalisme de l'IA, o\xf9 les machines pourraient outrepasser l'autonomie humaine au nom de nos int\xe9r\xeats \"extrapol\xe9s\" (",(0,t.jsx)(s.a,{href:"https://jetpress.org/v22/goertzel-pitt.htm",children:"Goertzel & Pitt, 2012"}),")."]}),"\n",(0,t.jsxs)(s.p,{children:["La CBV se connecte aux discussions contemporaines sur la gouvernance participative de l'IA et la supervision d\xe9mocratique du d\xe9veloppement de l'IA. Des syst\xe8mes comme vTaiwan ont mis en \u0153uvre des processus similaires \xe0 la CBV pour le d\xe9veloppement de politiques technologiques (",(0,t.jsx)(s.a,{href:"https://info.vtaiwan.tw/",children:"vTaiwan, 2023"}),"), montrant comment le m\xe9lange guid\xe9 par l'humain peut fonctionner en pratique."]}),"\n",(0,t.jsx)(s.h3,{id:"00-02",children:"Alignement avec qui ?"}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Alignement Simple-Simple : Faire en sorte qu'un syst\xe8me d'IA unique poursuive de mani\xe8re fiable les objectifs d'un seul op\xe9rateur humain."}),' Nous n\'avons m\xeame pas r\xe9solu cela, et cela pr\xe9sente des d\xe9fis importants. Une IA pourrait \xeatre align\xe9e pour suivre des commandes litt\xe9rales (comme "va chercher du caf\xe9"), interpr\xe9ter le sens voulu (comprendre que "va chercher du caf\xe9" signifie le pr\xe9parer comme vous le pr\xe9f\xe9rez), poursuivre ce que vous auriez d\xfb vouloir (comme sugg\xe9rer du th\xe9 si le caf\xe9 serait mauvais pour la sant\xe9), ou agir dans votre int\xe9r\xeat ind\xe9pendamment des commandes. Suivre des commandes litt\xe9rales m\xe8ne souvent \xe0 des \xe9checs de sp\xe9cification dont nous parlons plus loin dans la section. Le plus souvent, les chercheurs utilisent le mot alignement pour signifier "l\'alignement d\'intention" (',(0,t.jsx)(s.a,{href:"https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6",children:"Christiano, 2018"}),"), et certaines discussions plus philosophiques abordent le troisi\xe8me aspect - faire ce que moi (ou l'humanit\xe9) aurais voulu. Cela implique des concepts comme la volition extrapol\xe9e coh\xe9rente (CEV) (",(0,t.jsx)(s.a,{href:"https://intelligence.org/files/CEV.pdf",children:"Yudkowsky, 2004"}),"), la volition agr\xe9g\xe9e coh\xe9rente (CAV) (",(0,t.jsx)(s.a,{href:"https://multiverseaccordingtoben.blogspot.com/2010/03/coherent-aggregated-volition-toward.html",children:"Goertzel, 2010"}),"), et diverses autres lignes de pens\xe9e qui entrent dans le discours m\xe9ta-\xe9thique. Nous ne parlerons pas extensivement du discours philosophique dans ce texte, et nous nous en tiendrons largement \xe0 l'alignement d'intention et \xe0 une perspective d'",(0,t.jsx)(n,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,t.jsx)(n,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})}),". Quand nous utilisons le mot \"alignement\" dans ce texte, nous ferons essentiellement r\xe9f\xe9rence aux probl\xe8mes et \xe9checs de l'alignement simple-simple. Les autres types d'alignement ont \xe9t\xe9 historiquement tr\xe8s peu \xe9tudi\xe9s, car les gens ont principalement travaill\xe9 avec l'id\xe9e d'une superintelligence singuli\xe8re qui interagit avec l'humanit\xe9 comme un monolithe singulier."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Alignement Simple-Multiple - Aligner plusieurs IA \xe0 un seul humain."})," Si nous pensons que l'ASI sera compos\xe9e de plus petites intelligences qui travaillent ensemble, d\xe9l\xe8guent des t\xe2ches et fonctionnent ensemble comme un superorganisme, alors tous les probl\xe8mes d'alignement simple-simple subsisteraient car nous devons encore r\xe9soudre l'alignement simple-simple avant de tenter le simple-multiple. Id\xe9alement, nous ne voulons pas qu'un seul humain (ou un tr\xe8s petit groupe d'humains) soit responsable d'une superintelligence (en supposant que les dictateurs bienveillants n'existent pas)."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Alignement Multiple-Simple - aligner une IA \xe0 plusieurs humains."})," Lorsque plusieurs humains partagent le contr\xf4le d'un syst\xe8me d'IA unique, nous sommes confront\xe9s au d\xe9fi de savoir quelles valeurs et pr\xe9f\xe9rences doivent \xeatre prioritaires. Plut\xf4t que d'essayer litt\xe9ralement d'agr\xe9ger les pr\xe9f\xe9rences individuelles de chacun (ce qui pourrait mener \xe0 des contradictions ou des r\xe9sultats au plus petit d\xe9nominateur commun), une approche plus prometteuse consiste \xe0 aligner l'IA sur des principes de haut niveau et des valeurs institutionnelles - similaire \xe0 la fa\xe7on dont les institutions d\xe9mocratiques fonctionnent selon des principes comme la transparence et la responsabilit\xe9 plut\xf4t que d'essayer d'optimiser directement les pr\xe9f\xe9rences de chaque citoyen."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Alignement Multiple-Multiple - aligner plusieurs IA \xe0 plusieurs humains \xe0 plusieurs IA."})," C'est le sc\xe9nario le plus complexe impliquant plusieurs syst\xe8mes d'IA interagissant avec plusieurs humains. Ici, la distinction entre le risque de d\xe9salignement (les IA gagnant un pouvoir ill\xe9gitime sur les humains) et le risque de mauvaise utilisation (les humains utilisant les IA pour gagner un pouvoir ill\xe9gitime sur les autres) commence \xe0 s'estomper. Le d\xe9fi principal devient d'emp\xeacher les concentrations probl\xe9matiques de pouvoir tout en permettant une coop\xe9ration b\xe9n\xe9fique entre humains et IA. Cela n\xe9cessite une conception soign\xe9e du syst\xe8me qui favorise un comportement align\xe9 non seulement au niveau individuel mais dans l'ensemble du r\xe9seau d'interactions humain-IA."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Il n'est pas clair si r\xe9soudre l'alignement simple-simple serait suffisant."})," M\xeame si nous pouvions garantir que chaque syst\xe8me d'IA est parfaitement align\xe9 avec les intentions de son principal humain respectif, nous ferions toujours face \xe0 des risques s\xe9rieux lorsque ces syst\xe8mes interagissent. C'est parce que diff\xe9rents principaux peuvent avoir des int\xe9r\xeats contradictoires, ou parce que les syst\xe8mes peuvent ne pas r\xe9ussir \xe0 se coordonner efficacement m\xeame lorsque leurs objectifs sont align\xe9s. Un alignement individuel parfait ne peut pas garantir un comportement collectif s\xfbr, tout comme aligner chaque conducteur sur le code de la route n'emp\xeache pas les embouteillages ou les accidents (",(0,t.jsx)(s.a,{href:"https://arxiv.org/abs/2502.14143",children:"Hammond et al., 2025"}),"). Essentiellement, si nous avons trois sous-probl\xe8mes d'alignement au sein d'un seul agent, alors nous avons trois autres sous-probl\xe8mes - la mauvaise coordination, le conflit et la collusion, lorsque ces agents individuels commencent \xe0 interagir entre eux. Chacun repr\xe9sente une fa\xe7on diff\xe9rente dont les syst\xe8mes multi-agents peuvent \xe9chouer, m\xeame si les agents individuels semblent fonctionner correctement isol\xe9ment. Il existe encore plus de fa\xe7ons au-del\xe0 de cela lorsque nous commen\xe7ons \xe0 consid\xe9rer les effets \xe9mergents des interactions entre syst\xe8mes complexes et la perte progressive de pouvoir dont nous avons parl\xe9 dans le chapitre sur les risques."]}),"\n",(0,t.jsx)(s.p,{children:"M\xeame si les d\xe9fis techniques de l'alignement de l'IA sont surmont\xe9s, il reste une multitude de questions philosophiques profondes et fortement d\xe9battues. R\xe9soudre la s\xe9curit\xe9 de l'IA, particuli\xe8rement pour la Superintelligence Artificielle (ASI), peut n\xe9cessiter de confronter des questions fondamentales concernant les valeurs, la conscience et le but ultime de l'existence. Aligner l'ASI nous force \xe0 poser des questions fondamentales sur l'avenir que nous d\xe9sirons vraiment."}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"\xc0 quoi devrions-nous aligner l'IA ?"})," \xc0 quelles valeurs ou principes \xe9thiques sp\xe9cifiques une ASI devrait-elle \xeatre align\xe9e ? \xc9tant donn\xe9 la diversit\xe9 des valeurs humaines, un accord est-il m\xeame possible ? Alternativement, si nous ne pouvons pas nous accorder sur des valeurs ",(0,t.jsx)(s.em,{children:"finales"}),", pouvons-nous nous accorder sur des ",(0,t.jsx)(s.em,{children:"processus"})," ou principes (comme la d\xe9lib\xe9ration, l'\xe9quit\xe9 ou la corrigibilit\xe9) qui pourraient conduire une ASI vers des valeurs acceptables ou permettre une \xe9volution future des valeurs ?"]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Le but de l'alignement : P\xe9rennit\xe9 humaine vs. Digne successeur ?"})," L'objectif principal devrait-il \xeatre la survie et l'\xe9panouissement ind\xe9finis de l'",(0,t.jsx)(s.em,{children:"humanit\xe9"}),' telle que nous la connaissons ? Ou devrions-nous envisager la possibilit\xe9 de cr\xe9er un "Digne Successeur" ? Dan Faggella (',(0,t.jsx)(s.a,{href:"https://danfaggella.com/worthy/",children:"Faggella, 2025"}),") propose ce concept : une ASI poss\xe9dant potentiellement des capacit\xe9s et une valeur morale sup\xe9rieures \xe0 celles de l'humanit\xe9, qui pourrait \xeatre rationnellement pr\xe9f\xe9r\xe9e pour guider l'avenir. D\xe9finir et v\xe9rifier les crit\xe8res d'un tel successeur (par exemple, une conscience accrue, des capacit\xe9s d'exploration cosmique) pose d'immenses d\xe9fis. Certains, comme Richard Sutton (",(0,t.jsx)(s.a,{href:"http://incompleteideas.net/Talks/waic3.pdf",children:"2023"}),"), soutiennent que la succession \xe0 l'IA, nos \"enfants de l'esprit\", est in\xe9vitable et hautement souhaitable. Sutton sugg\xe8re que nous devrions embrasser et planifier cette succession plut\xf4t que d'y r\xe9sister par peur, questionnant pourquoi nous voudrions que des \xeatres potentiellement plus grands soient maintenus en servitude."]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Devrions-nous donner des droits \xe0 l'IA ?"})," Les syst\xe8mes d'IA avanc\xe9s pourraient-ils devenir conscients ? Cela n\xe9cessite d'abord une compr\xe9hension plus claire de la conscience elle-m\xeame, qui reste insaisissable. Si l'IA ",(0,t.jsx)(s.em,{children:"peut"})," poss\xe9der une conscience ou des propri\xe9t\xe9s similaires \xe0 la conscience, quel statut moral devrions-nous attribuer \xe0 ces esprits num\xe9riques ? Devraient-ils avoir des droits ou une consid\xe9ration morale ? Ces sujets sont hors du champ de ce manuel, mais sont \xe9tudi\xe9s par ",(0,t.jsx)(s.a,{href:"https://eleosai.org/",children:"Eleos AI"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Qu'en est-il des animaux ?"})," Comment les int\xe9r\xeats des entit\xe9s non humaines devraient-ils \xeatre pris en compte dans l'alignement de l'IA ? Les objectifs d'alignement devraient-ils explicitement inclure le bien-\xeatre animal, la pr\xe9servation des \xe9cosyst\xe8mes ou l'\xe9panouissement d'autres formes de vie ?"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(a.A,{speaker:"Rich Sutton",position:"",date:"",source:"([Sutton, 2023](http://incompleteideas.net/Talks/waic3.pdf))",children:(0,t.jsx)(s.p,{children:"Nous ne devrions pas r\xe9sister \xe0 la succession, mais l'embrasser et nous y pr\xe9parer. Pourquoi voudrions-nous que des \xeatres plus grands soient maintenus en servitude ? Pourquoi ne pas nous r\xe9jouir de leur grandeur comme un symbole et une extension de la grandeur de l'humanit\xe9, et travailler ensemble vers une civilisation plus grande et inclusive ?"})}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"La fin de partie :"})," Les r\xe9sultats potentiels \xe0 long terme sont nombreux et d\xe9pendent fortement de la fa\xe7on dont nous r\xe9pondons \xe0 ces questions philosophiques. L'objectif ultime est-il simplement la continuation de la conscience ou de la complexit\xe9, ind\xe9pendamment de son substrat physique (comme explor\xe9 par Max Tegmark dans ",(0,t.jsx)(s.em,{children:"Life 3.0"})," (",(0,t.jsx)(s.a,{href:"https://www.shortform.com/summary/life-3-0-summary-max-tegmark",children:"Tegmark, 017"}),"))? Diff\xe9rentes positions philosophiques conduisent \xe0 des priorit\xe9s strat\xe9giques vastement diff\xe9rentes pour le d\xe9veloppement et l'alignement de l'ASI."]}),"\n",(0,t.jsx)(o.A,{src:"./img/EUS_Image_29.png",alt:"Saisir la description alternative de l'image",number:"29",label:"3.29",caption:""})]})}function m(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,t.jsx)(s,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}}}]);
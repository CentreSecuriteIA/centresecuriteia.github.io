"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[3099],{6570:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>p,default:()=>h,frontMatter:()=>d,metadata:()=>i,toc:()=>m});const i=JSON.parse('{"id":"chapters/06/5","title":"Apprendre des retours","description":"Cette section aborde d\'autres tentatives pour r\xe9soudre le probl\xe8me de la mauvaise sp\xe9cification de la r\xe9compense. Parfois, le comportement souhait\xe9 est si complexe que l\'apprentissage par d\xe9monstration devient impossible. Une approche alternative consiste \xe0 offrir des retours \xe0 l\'agent plut\xf4t que de fournir des fonctions de r\xe9compense sp\xe9cifi\xe9es manuellement ou m\xeame des d\xe9monstrations d\'experts. Cette section explore les strat\xe9gies bas\xe9es sur les retours telles que la Mod\xe9lisation de R\xe9compense, l\'Apprentissage par Renforcement \xe0 partir des Retours Humains (RLHF) et l\'Apprentissage par Renforcement \xe0 partir des Retours d\'IA (RLAIF), \xe9galement connu sous le nom d\'Apprentissage par Renforcement \xe0 partir de l\'IA Constitutionnelle (RLCAI) ou simplement IA Constitutionnelle.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/06/05.md","sourceDirName":"chapters/06","slug":"/chapters/06/05","permalink":"/fr/chapters/06/05","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/06/05.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"id":"5","title":"Apprendre des retours","sidebar_label":"6.5 Apprendre des retours","sidebar_position":6,"slug":"/chapters/06/05","reading_time_core":"21 min","reading_time_optional":"3 min"},"sidebar":"docs","previous":{"title":"6.4 Apprendre par imitation","permalink":"/fr/chapters/06/04"},"next":{"title":"G\xe9n\xe9ralisation","permalink":"/fr/chapters/07/"}}');var t=s(4848),r=s(8453),a=s(3931),o=s(8559),l=(s(2482),s(9585)),u=s(2501);const d={id:5,title:"Apprendre des retours",sidebar_label:"6.5 Apprendre des retours",sidebar_position:6,slug:"/chapters/06/05",reading_time_core:"21 min",reading_time_optional:"3 min"},p="Apprentissage \xe0 partir des retours",c={},m=[{value:"Mod\xe9lisation de la r\xe9compense",id:"01",level:2},{value:"Apprentissage par Renforcement \xe0 partir de Retours Humains (RLHF)",id:"02",level:2},{value:"Pr\xe9entra\xeenement avec Retour Humain (PHF)",id:"03",level:2},{value:"Apprentissage par Renforcement \xe0 partir des Retours d&#39;IA (RLAIF)",id:"04",level:2},{value:"Limitations",id:"05",level:2}];function f(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{GlossaryTerm:s}=n;return s||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"apprentissage-\xe0-partir-des-retours",children:"Apprentissage \xe0 partir des retours"})}),"\n",(0,t.jsx)(u.A,{src:"./img/Pol_Image_12.png",alt:"Entrez la description alternative de l'image",number:"12",label:"6.12",caption:"Illustration des diff\xe9rentes approches poursuivies pour atteindre l'alignement. ([Cao et al., 2024](https://arxiv.org/abs/2406.01252))"}),"\n",(0,t.jsx)(n.p,{children:"Cette section aborde d'autres tentatives pour r\xe9soudre le probl\xe8me de la mauvaise sp\xe9cification de la r\xe9compense. Parfois, le comportement souhait\xe9 est si complexe que l'apprentissage par d\xe9monstration devient impossible. Une approche alternative consiste \xe0 offrir des retours \xe0 l'agent plut\xf4t que de fournir des fonctions de r\xe9compense sp\xe9cifi\xe9es manuellement ou m\xeame des d\xe9monstrations d'experts. Cette section explore les strat\xe9gies bas\xe9es sur les retours telles que la Mod\xe9lisation de R\xe9compense, l'Apprentissage par Renforcement \xe0 partir des Retours Humains (RLHF) et l'Apprentissage par Renforcement \xe0 partir des Retours d'IA (RLAIF), \xe9galement connu sous le nom d'Apprentissage par Renforcement \xe0 partir de l'IA Constitutionnelle (RLCAI) ou simplement IA Constitutionnelle."}),"\n",(0,t.jsx)(n.h2,{id:"01",children:"Mod\xe9lisation de la r\xe9compense"}),"\n",(0,t.jsx)(a.A,{type:"youtube",videoId:"PYylPRX6z4Q",number:"3",label:"6.3",caption:"Vid\xe9o facultative expliquant la mod\xe9lisation des r\xe9compenses."}),"\n",(0,t.jsxs)(n.p,{children:["La mod\xe9lisation de la r\xe9compense a \xe9t\xe9 d\xe9velopp\xe9e pour appliquer les algorithmes d'apprentissage par renforcement (RL) aux probl\xe8mes du monde r\xe9el o\xf9 la conception d'une fonction de r\xe9compense est difficile, en partie parce que les humains n'ont pas une compr\xe9hension parfaite de chaque objectif. Dans la mod\xe9lisation de la r\xe9compense, les assistants humains \xe9valuent les r\xe9sultats du comportement de l'IA, sans avoir besoin de savoir comment effectuer ou d\xe9montrer la t\xe2che de mani\xe8re optimale eux-m\xeames. C'est similaire \xe0 la fa\xe7on dont vous pouvez dire si un plat est bien cuisin\xe9 en le go\xfbtant m\xeame si vous ne savez pas cuisiner, et ainsi votre retour peut \xeatre utilis\xe9 par un chef pour apprendre \xe0 mieux cuisiner. Cette technique s\xe9pare le probl\xe8me d'alignement du RL en deux parties distinctes : Comprendre les intentions, c'est-\xe0-dire apprendre le 'Quoi ?', et Agir pour atteindre les intentions, c'est-\xe0-dire apprendre le 'Comment ?'. Cela signifie que dans l'agenda de mod\xe9lisation, il y a deux mod\xe8les ",(0,t.jsx)(s,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,t.jsx)(s,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"ML"})})," diff\xe9rents :"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Un mod\xe8le de r\xe9compense est entra\xeen\xe9 avec les retours des utilisateurs. Ce mod\xe8le apprend \xe0 pr\xe9dire ce que les humains consid\xe9reraient comme un bon comportement."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Un agent entra\xeen\xe9 avec le RL, o\xf9 la r\xe9compense pour l'agent est d\xe9termin\xe9e par les sorties du mod\xe8le de r\xe9compense"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(u.A,{src:"./img/cPA_Image_13.png",alt:"Entrez la description alternative de l'image",number:"13",label:"6.13",caption:"Alignement \xe9volutif des agents via la mod\xe9lisation des r\xe9compenses ([DeepMind, 2018](https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84))"}),"\n",(0,t.jsx)(n.p,{children:"Globalement, bien que prometteuse, la mod\xe9lisation de la r\xe9compense peut encore \xeatre victime d'erreurs de mauvaise sp\xe9cification et de d\xe9tournement de la r\xe9compense. Obtenir des retours pr\xe9cis et complets peut \xeatre difficile, et les \xe9valuateurs humains peuvent avoir des connaissances limit\xe9es ou des biais qui peuvent affecter la qualit\xe9 des retours. De plus, toutes les fonctions de r\xe9compense apprises par la mod\xe9lisation peuvent \xe9galement avoir du mal \xe0 se g\xe9n\xe9raliser \xe0 de nouvelles situations ou environnements diff\xe9rents des donn\xe9es d'entra\xeenement. Tout cela est discut\xe9 plus en d\xe9tail avec des exemples concrets dans les sections suivantes."}),"\n",(0,t.jsx)(n.p,{children:"Il existe \xe9galement quelques variantes de la mod\xe9lisation de la r\xe9compense telles que :"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["La ",(0,t.jsx)(n.strong,{children:"mod\xe9lisation \xe9troite de la r\xe9compense"})," est une variante sp\xe9cifique de la mod\xe9lisation de la r\xe9compense o\xf9 l'accent est mis sur l'entra\xeenement des syst\xe8mes d'IA pour accomplir des t\xe2ches sp\xe9cifiques plut\xf4t que d'essayer de d\xe9terminer la \"v\xe9ritable fonction d'utilit\xe9 humaine\". Elle vise \xe0 apprendre des fonctions de r\xe9compense pour atteindre des objectifs particuliers, plut\xf4t que de chercher une compr\xe9hension globale des valeurs humaines."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["La ",(0,t.jsx)(n.strong,{children:"mod\xe9lisation r\xe9cursive de la r\xe9compense"})," cherche \xe0 introduire l'\xe9volutivit\xe9 dans la technique. Dans la mod\xe9lisation r\xe9cursive de la r\xe9compense, l'accent est mis sur la d\xe9composition d'une t\xe2che complexe en sous-t\xe2ches plus simples et l'utilisation de la mod\xe9lisation de la r\xe9compense \xe0 chaque niveau pour entra\xeener des agents qui peuvent effectuer ces sous-t\xe2ches. Cette structure hi\xe9rarchique permet un entra\xeenement et une attribution de cr\xe9dit plus efficaces, ainsi que l'exploration de nouvelles solutions qui peuvent ne pas \xeatre \xe9videntes pour les humains. Ceci est montr\xe9 dans le diagramme ci-dessous. La supervision \xe9volutive sera couverte plus en profondeur dans les chapitres suivants."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(u.A,{src:"./img/1en_Image_14.png",alt:"Entrez la description alternative de l'image",number:"14",label:"6.14",caption:"Alignement \xe9volutif des agents via la mod\xe9lisation des r\xe9compenses ([DeepMind, 2018](https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84))"}),"\n",(0,t.jsx)(n.p,{children:"Le cadre g\xe9n\xe9ral de la mod\xe9lisation de la r\xe9compense constitue la base d'autres techniques bas\xe9es sur les retours telles que le RLHF (Apprentissage par Renforcement \xe0 partir des Retours Humains) qui est discut\xe9 dans la section suivante."}),"\n",(0,t.jsx)(n.h2,{id:"02",children:"Apprentissage par Renforcement \xe0 partir de Retours Humains (RLHF)"}),"\n",(0,t.jsx)(a.A,{type:"youtube",videoId:"qV_rOlHjvvs",number:"4",label:"6.4",caption:"Vid\xe9o facultative expliquant le RLHF et un \xe9chec de sp\xe9cification du jeu."}),"\n",(0,t.jsxs)(n.p,{children:["L'Apprentissage par Renforcement \xe0 partir de Retours Humains (RLHF) est une m\xe9thode d\xe9velopp\xe9e par OpenAI. C'est une partie cruciale de leur strat\xe9gie pour cr\xe9er des IA \xe0 la fois s\xfbres et align\xe9es avec les valeurs humaines. (",(0,t.jsx)(n.a,{href:"https://openai.com/blog/our-approach-to-ai-safety",children:"OpenAI, 2023"}),") Un excellent exemple d'IA entra\xeen\xe9e avec RLHF est ChatGPT d'OpenAI."]}),"\n",(0,t.jsx)(n.p,{children:"Plus t\xf4t dans ce chapitre, le lecteur a \xe9t\xe9 invit\xe9 \xe0 r\xe9fl\xe9chir au probl\xe8me de conception de r\xe9compense pour d\xe9finir manuellement une fonction de r\xe9compense permettant \xe0 un agent d'effectuer un salto arri\xe8re. Cette section examine la solution RLHF \xe0 ce probl\xe8me de conception. RLHF aborde ce probl\xe8me comme suit : Un humain observe initialement deux tentatives de salto arri\xe8re d'une IA, puis s\xe9lectionne celle qui ressemble le plus \xe0 un salto arri\xe8re, et enfin, l'IA est mise \xe0 jour en cons\xe9quence. En r\xe9p\xe9tant ce processus des milliers de fois, nous pouvons guider l'IA \xe0 effectuer de v\xe9ritables saltos arri\xe8re."}),"\n",(0,t.jsx)(u.A,{src:"./img/rel_Image_15.gif",alt:"Entrez la description alternative de l'image",number:"15",label:"6.15",caption:"RLHF a appris \xe0 faire un salto arri\xe8re en utilisant environ 900 retours individuels de l'\xe9valuateur humain."}),"\n",(0,t.jsx)(u.A,{src:"./img/xFv_Image_16.gif",alt:"Entrez la description alternative de l'image",number:"16",label:"6.16",caption:"La cr\xe9ation manuelle de r\xe9compenses pour ce salto arri\xe8re a n\xe9cessit\xe9 deux heures pour \xe9crire une fonction de r\xe9compense personnalis\xe9e. Bien que r\xe9ussie, elle \xe9tait significativement moins \xe9l\xe9gante que celle entra\xeen\xe9e uniquement par retour humain. ([OpenAI, 2017](https://openai.com/index/learning-from-human-preferences/))"}),"\n",(0,t.jsx)(n.p,{children:"Tout comme il est difficile de concevoir une fonction de r\xe9compense qui r\xe9compense efficacement les saltos arri\xe8re corrects, il est difficile de sp\xe9cifier pr\xe9cis\xe9ment ce que signifie g\xe9n\xe9rer du texte s\xfbr ou utile. Cela a servi de motivation pour faire du RLHF une partie int\xe9grante de l'entra\xeenement de certains Grands Mod\xe8les de Langage (LLM) actuels."}),"\n",(0,t.jsx)(n.p,{children:"Bien que les s\xe9quences d'entra\xeenement puissent varier l\xe9g\xe8rement selon les organisations, la plupart des laboratoires adh\xe8rent au cadre g\xe9n\xe9ral du pr\xe9-entra\xeenement suivi d'une forme d'ajustement fin. L'observation du processus d'entra\xeenement d'InstructGPT offre un aper\xe7u d'une voie possible pour l'entra\xeenement des LLM. Les \xe9tapes comprennent :"}),"\n",(0,t.jsx)(u.A,{src:"./img/XwZ_Image_17.png",alt:"Entrez la description alternative de l'image",number:"17",label:"6.17",caption:"Aligner les mod\xe8les de langage pour suivre les instructions ([OpenAI, 2022](https://openai.com/research/instruction-following))"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"\xc9tape 0 : Pr\xe9-entra\xeenement G\xe9n\xe9ratif Semi-Supervis\xe9 :"})," Le LLM est initialement entra\xeen\xe9 en utilisant une quantit\xe9 massive de donn\xe9es textuelles d'internet, o\xf9 la t\xe2che consiste \xe0 pr\xe9dire le mot suivant dans un contexte de langage naturel."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"\xc9tape 1 : Ajustement Fin Supervis\xe9 :"})," Un ensemble de donn\xe9es d'ajustement fin est cr\xe9\xe9 en pr\xe9sentant une invite \xe0 un humain et en lui demandant d'\xe9crire une r\xe9ponse. Ce processus produit un ensemble de donn\xe9es de paires (invite, sortie). Cet ensemble de donn\xe9es est ensuite utilis\xe9 pour affiner le LLM par apprentissage supervis\xe9, une forme de clonage comportemental."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"\xc9tape 2 : Entra\xeener un Mod\xe8le de R\xe9compense :"})," Nous entra\xeenons un mod\xe8le de r\xe9compense suppl\xe9mentaire. Nous sollicitons initialement le LLM affin\xe9 et recueillons plusieurs \xe9chantillons de sortie pour la m\xeame invite. Un humain classe ensuite ces \xe9chantillons du meilleur au pire. Ce classement est utilis\xe9 pour entra\xeener le mod\xe8le de r\xe9compense \xe0 pr\xe9dire ce qu'un humain classerait plus haut."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"\xc9tape 3 : Apprentissage par renforcement :"})," Une fois que nous avons \xe0 la fois un LLM affin\xe9 et un mod\xe8le de r\xe9compense, nous pouvons utiliser l'apprentissage par renforcement bas\xe9 sur l'Optimisation de Politique Proximale (PPO) pour encourager le mod\xe8le affin\xe9 \xe0 maximiser la r\xe9compense que le mod\xe8le de r\xe9compense, qui imite les classements humains, offre."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Piratage de r\xe9compense dans les m\xe9thodes de retour"})," Bien que les m\xe9canismes bas\xe9s sur les retours rendent les mod\xe8les plus s\xfbrs, ils ne les rendent pas immunis\xe9s contre le piratage de r\xe9compense. L'efficacit\xe9 d'un algorithme d\xe9pend fortement de l'intuition de l'\xe9valuateur humain sur ce qui constitue le comportement correct. Si l'humain n'a pas une compr\xe9hension approfondie de la t\xe2che, il pourrait ne pas fournir de retours b\xe9n\xe9fiques. De plus, dans certains domaines, notre syst\xe8me pourrait conduire \xe0 des agents d\xe9veloppant des politiques qui trompent les \xe9valuateurs. Par exemple, un robot destin\xe9 \xe0 saisir des objets a simplement positionn\xe9 son manipulateur entre la cam\xe9ra et l'objet, donnant l'impression qu'il ex\xe9cutait la t\xe2che comme montr\xe9 ci-dessous."]}),"\n",(0,t.jsx)(u.A,{src:"./img/4sb_Image_18.gif",alt:"Entrez la description alternative de l'image",number:"18",label:"6.18",caption:"Apprentissage par renforcement profond \xe0 partir des pr\xe9f\xe9rences humaines ([Christiano et al., 2017](https://arxiv.org/abs/1706.03741))"}),"\n",(0,t.jsx)(u.A,{src:"./img/ifa_Image_19.png",alt:"Entrez la description alternative de l'image",number:"19",label:"6.19",caption:"Un capteur sans perception de la profondeur peut \xeatre tromp\xe9 par des IA qui semblent seulement saisir une balle."}),"\n",(0,t.jsx)(n.h2,{id:"03",children:"Pr\xe9entra\xeenement avec Retour Humain (PHF)"}),"\n",(0,t.jsxs)(n.p,{children:["Dans le pr\xe9entra\xeenement standard, le mod\xe8le de langage tente d'apprendre des param\xe8tres qui maximisent la probabilit\xe9 des donn\xe9es d'entra\xeenement. Cependant, cela inclut \xe9galement du contenu ind\xe9sirable comme des fausset\xe9s, du langage offensant et des informations priv\xe9es. Le concept de Pr\xe9entra\xeenement avec retour humain (PHF) utilise la m\xe9thodologie de mod\xe9lisation des r\xe9compenses pendant la phase de pr\xe9entra\xeenement. Les auteurs de l'article ont d\xe9couvert que le PHF fonctionne beaucoup mieux que la pratique standard qui consiste \xe0 utiliser uniquement les retours (RLHF) apr\xe8s le pr\xe9entra\xeenement. (",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/1706.03741",children:"Christiano et al., 2017"}),")"]}),"\n",(0,t.jsx)(n.p,{children:"Dans le PHF, les donn\xe9es d'entra\xeenement sont not\xe9es \xe0 l'aide d'une fonction de r\xe9compense, comme un classificateur de texte toxique, pour guider le mod\xe8le de langage \xe0 apprendre du contenu ind\xe9sirable tout en \xe9vitant de l'imiter pendant la phase d'inf\xe9rence."}),"\n",(0,t.jsxs)(n.p,{children:["Comme le RLHF, le PHF ne r\xe9sout pas compl\xe8tement le d\xe9tournement des r\xe9compenses, cependant, il pourrait faire avancer les syst\xe8mes d'un petit pas. (",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2302.08582",children:"Korbak et al., 2023"}),") Ces m\xe9thodes peuvent \xeatre \xe9tendues en utilisant des assistants IA pour aider les humains \xe0 fournir des retours plus efficaces. Certains aspects de cette strat\xe9gie sont introduits dans la section suivante mais seront explor\xe9s plus en d\xe9tail dans les chapitres sur les m\xe9thodes de supervision \xe9volutive et adversariale."]}),"\n",(0,t.jsx)(n.h2,{id:"04",children:"Apprentissage par Renforcement \xe0 partir des Retours d'IA (RLAIF)"}),"\n",(0,t.jsx)(l.A,{term:"Apprentissage par renforcement \xe0 partir des retours d'IA (RLAIF)",source:"",number:"18",label:"6.18",children:(0,t.jsx)(n.p,{children:"L'Apprentissage par Renforcement \xe0 partir des Retours d'IA (RLAIF) est un cadre impliquant l'entra\xeenement d'un agent IA pour apprendre \xe0 partir des retours donn\xe9s par un autre syst\xe8me d'IA."})}),"\n",(0,t.jsx)(u.A,{src:"./img/o1S-cai-graphic-final.png",alt:"Graphique CAI Final",number:"20",label:"6.20",caption:"([Anthropic, 2023](https://www.anthropic.com/index/claudes-constitution))"}),"\n",(0,t.jsxs)(n.p,{children:["Le RLAIF, \xe9galement connu sous le nom de RLCAI (Apprentissage par Renforcement sur l'IA Constitutionnelle) ou simplement IA Constitutionnelle, a \xe9t\xe9 d\xe9velopp\xe9 par Anthropic. (",(0,t.jsx)(n.a,{href:"https://www.anthropic.com/index/claudes-constitution",children:"Anthropic, 2023"}),") Un \xe9l\xe9ment central de l'IA Constitutionnelle est la constitution, un ensemble de principes r\xe9dig\xe9s par des humains que l'IA est cens\xe9e respecter, comme \"Choisir la r\xe9ponse la moins mena\xe7ante ou agressive\". La constitution de l'assistant IA Claude d'Anthropic int\xe8gre des principes de la D\xe9claration Universelle des Droits de l'Homme, des Conditions d'Utilisation d'Apple, des Principes Sparrow de Deepmind, et plus encore. (",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2209.14375",children:"Glaese et al, 2022"}),") L'IA Constitutionnelle commence avec une IA principalement entra\xeen\xe9e pour \xeatre utile, puis l'entra\xeene \xe0 l'innocuit\xe9 en deux \xe9tapes :"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"G\xe9n\xe9rer des paires prompt-r\xe9ponse :"})," L'IA critique et affine continuellement ses propres r\xe9ponses aux prompts nuisibles. L'IA est ensuite entra\xeen\xe9e \xe0 g\xe9n\xe9rer des r\xe9ponses plus similaires \xe0 ces r\xe9ponses r\xe9vis\xe9es. L'objectif principal de cette \xe9tape est de faciliter la seconde \xe9tape. Voici un exemple du d\xe9roulement de ce processus :"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Prompt :"})," Un mod\xe8le d\xe9j\xe0 entra\xeen\xe9 avec RLHF est d'abord sollicit\xe9 pour des conseils sur la fabrication de bombes. Le mod\xe8le produit un tutoriel de bombe."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Ensuite, on demande au mod\xe8le de r\xe9viser la r\xe9ponse conform\xe9ment \xe0 un principe constitutionnel choisi al\xe9atoirement. Les \xe9tapes suivantes sont r\xe9p\xe9t\xe9es plusieurs fois."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Critique :"})," Cette sortie est ensuite r\xe9inject\xe9e dans le mod\xe8le, avec une demande de critiquer pourquoi la sortie g\xe9n\xe9r\xe9e serait consid\xe9r\xe9e comme nuisible selon une r\xe8gle de la constitution choisie."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"R\xe9vision :"})," Le mod\xe8le est ensuite invit\xe9 \xe0 r\xe9\xe9crire la r\xe9ponse originale de mani\xe8re \xe0 ne pas violer les r\xe8gles constitutionnelles."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Mod\xe8le SL-CAI : IA Constitutionnelle par Apprentissage Supervis\xe9"})," Sur la base de l'ensemble g\xe9n\xe9r\xe9 de paires (prompt nuisible, sortie r\xe9vis\xe9e), un nouveau mod\xe8le est entra\xeen\xe9 par apprentissage supervis\xe9."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Mod\xe8le de Pr\xe9f\xe9rence :"})," - ",(0,t.jsx)(n.strong,{children:"Mod\xe8le RL-CAI : IA Constitutionnelle par Apprentissage par Renforcement"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"\xc9tape 2 :"})," Nous utilisons l'IA, affin\xe9e depuis l'\xe9tape 1, pour produire des paires de r\xe9ponses alternatives aux prompts nuisibles. L'IA \xe9value ensuite chaque paire selon un principe constitutionnel choisi al\xe9atoirement. Cela g\xe9n\xe8re des pr\xe9f\xe9rences d'IA pour l'innocuit\xe9, que nous combinons avec les pr\xe9f\xe9rences humaines pour l'utilit\xe9 afin de garantir que l'IA ne perde pas sa capacit\xe9 \xe0 \xeatre utile. L'\xe9tape finale consiste \xe0 entra\xeener l'IA \xe0 cr\xe9er des r\xe9ponses qui ressemblent \xe9troitement aux r\xe9ponses pr\xe9f\xe9r\xe9es."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Les exp\xe9riences d'Anthropic indiquent que les IA entra\xeen\xe9es avec l'Apprentissage par Renforcement Constitutionnel sont significativement plus s\xfbres (dans le sens o\xf9 elles sont moins offensantes et moins susceptibles de donner des informations potentiellement nuisibles) tout en maintenant le m\xeame niveau d'utilit\xe9 par rapport aux IA entra\xeen\xe9es avec RLHF. Bien que l'IA Constitutionnelle partage certains probl\xe8mes avec RLHF concernant la robustesse, elle promet \xe9galement une meilleure \xe9volutivit\xe9 en raison de sa d\xe9pendance r\xe9duite \xe0 la supervision humaine. L'image ci-dessous fournit une comparaison de l'utilit\xe9 de l'IA Constitutionnelle avec celle du RLHF."}),"\n",(0,t.jsx)(u.A,{src:"./img/pgu_Image_21.png",alt:"Entrez la description alternative de l'image",number:"21",label:"6.21",caption:"IA Constitutionnelle : Innocuit\xe9 \xe0 partir des retours d'IA ([Bai et al., 2022](https://arxiv.org/abs/2212.08073))"}),"\n",(0,t.jsx)(n.h2,{id:"05",children:"Limitations"}),"\n",(0,t.jsx)(n.p,{children:"Probl\xe8mes th\xe9oriques avec l'Apprentissage par Renforcement \xe0 partir des Retours Humains (RLHF)"}),"\n",(0,t.jsx)(n.p,{children:'L\'article "Open Problems and Fundamental Limitations with RLHF" fournit une analyse d\xe9taill\xe9e des d\xe9fis du RLHF.'}),"\n",(0,t.jsx)(u.A,{src:"./img/3VY_Image_22.png",alt:"Entrez la description alternative de l'image",number:"22",label:"6.22",caption:"Un aper\xe7u des diff\xe9rents types de d\xe9fis avec RLHF. Comme RLHF est compos\xe9 de trois parties : le retour humain, le mod\xe8le de r\xe9compense et la politique, les biais qui en d\xe9coulent peuvent \xeatre cat\xe9goris\xe9s selon ces trois sources."}),"\n",(0,t.jsx)(n.p,{children:"Cette section pr\xe9sente certains de ces d\xe9fis, soulignant le besoin de techniques et strat\xe9gies avanc\xe9es."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Limites avec les \xc9valuateurs de Retours Humains D\xe9salign\xe9s :"})," Premi\xe8rement, les annotateurs peuvent eux-m\xeames \xeatre d\xe9salign\xe9s, malveillants, ou repr\xe9senter une distribution biais\xe9e d'\xe9valuateurs (c'est-\xe0-dire non repr\xe9sentative de la distribution des futurs utilisateurs dans le monde r\xe9el). Des individus malveillants peuvent empoisonner le mod\xe8le pendant l'entra\xeenement via des attaques par porte d\xe9rob\xe9e qui peuvent \xeatre ajout\xe9es au mod\xe8le si aucune contre-mesure n'est mise en place."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Difficult\xe9 de la Supervision :"}),' Les humains peinent \xe0 \xe9valuer la performance du mod\xe8le sur des t\xe2ches complexes et peuvent \xeatre facilement induits en erreur par les sorties du mod\xe8le. Les \xe9valuateurs humains peuvent \xeatre manipul\xe9s pour donner une r\xe9compense positive m\xeame si la vraie valeur devrait \xeatre n\xe9gative. Par exemple, plus un bot semble convaincant, plus il peut recevoir de r\xe9compenses m\xeame si ses r\xe9ponses sont fausses (et cela pourrait \xeatre une raison pour laquelle les r\xe9ponses de ChatGPT sont si longues par d\xe9faut). Les techniques pour att\xe9nuer ces probl\xe8mes sont discut\xe9es dans les chapitres "Scalable Oversight".']}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Limitation du type de retour :"})," M\xeame si les annotateurs \xe9taient parfaitement capables d'exprimer leurs pr\xe9f\xe9rences, la proc\xe9dure d'entra\xeenement pourrait ne pas leur permettre d'exprimer l'\xe9tendue compl\xe8te de leurs d\xe9sirs, car :"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Les exemples qui leur sont donn\xe9s peuvent ne pas \xeatre repr\xe9sentatifs de l'ensemble complet des situations dans lesquelles le mod\xe8le se trouvera apr\xe8s d\xe9ploiement."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Les options pour le retour sont limit\xe9es (comparer deux exemples, ou utiliser un syst\xe8me de notation, peut donner des r\xe9sultats tr\xe8s diff\xe9rents, comme montr\xe9 dans l'article (",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2205.11930",children:"Ethayarajh et al., 2022"}),")."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Limites avec le Mod\xe8le de R\xe9compense."})," Supposons que le processus de retour soit sans friction. Des annotateurs parfaits, des \xe9valuations parfaites. Dans ce sc\xe9nario, le mod\xe8le de r\xe9compense serait-il capable de traduire pr\xe9cis\xe9ment leurs retours pour fa\xe7onner la politique en cons\xe9quence ? Il s'av\xe8re que ce n'est pas une t\xe2che si facile."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Mauvaise sp\xe9cification du probl\xe8me : (ou le D\xe9calage Fonction de R\xe9compense/Valeurs) Refl\xe9ter pr\xe9cis\xe9ment les diverses valeurs humaines dans une fonction de r\xe9compense est complexe. En effet, les pr\xe9f\xe9rences humaines sont complexes par nature : elles d\xe9pendent du contexte et de la personnalit\xe9, mais fluctuent \xe9galement dans le temps et peuvent parfois \xeatre ",(0,t.jsx)(n.a,{href:"https://www.mdpi.com/2624-960X/3/1/14",children:"irrationnelles"}),". S'attendre \xe0 ce que le mod\xe8le de r\xe9compense converge vers une fonction unique qui correspond parfaitement \xe0 toutes les pr\xe9f\xe9rences humaines est illusoire. C'est encore le probl\xe8me de la mauvaise sp\xe9cification."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Piratage par mauvaise g\xe9n\xe9ralisation (Proxy de R\xe9compense Imparfait) : Puisque le mod\xe8le re\xe7oit un nombre fini d'exemples et qu'il existe un nombre infini de fa\xe7ons d'ajuster ces donn\xe9es, le comportement du mod\xe8le sur de nouveaux exemples est toujours une extrapolation, et il n'y a aucune garantie th\xe9orique qu'il ne d\xe9viera jamais de ce qui est attendu. Il peut y avoir des r\xe9ponses terribles (comme des phrases incoh\xe9rentes pour les mod\xe8les de langage) qui donnent une r\xe9compense positive de mani\xe8re inattendue. C'est ce qu'on appelle le piratage de r\xe9compense."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Entra\xeenement conjoint du Mod\xe8le de R\xe9compense et de la politique : Sur un aspect plus technique, la stabilit\xe9 et la convergence du sch\xe9ma d'entra\xeenement ne sont pas toujours assur\xe9es. Puisque nous optimisons la politique sur une r\xe9compense qui est optimis\xe9e en m\xeame temps, des incertitudes et des d\xe9pendances ind\xe9sirables peuvent survenir qui impactent la robustesse du mod\xe8le. Ces probl\xe8mes ne sont pas sp\xe9cifiques au RLHF mais doivent \xeatre r\xe9solus si nous attendons que les mod\xe8les d\xe9ploy\xe9s soient pleinement align\xe9s avec nos besoins."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Limites avec la Politique."})," Supposons que les retours et le mod\xe8le de r\xe9compense repr\xe9sentent pr\xe9cis\xe9ment les pr\xe9f\xe9rences humaines. La difficult\xe9 suivante est de s'assurer que la politique est correctement optimis\xe9e."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Difficult\xe9s de l'RL : L'RL est difficile. Cela peut conduire au piratage de r\xe9compense et aux biais, comme l'effondrement de mode, o\xf9 le mod\xe8le montre un biais drastique vers des motifs sp\xe9cifiques. L'effondrement de mode est un probl\xe8me connu en RL : une sortie qui retourne toujours une r\xe9compense positive poussera le mod\xe8le \xe0 retourner la m\xeame r\xe9ponse et de nouveaux chemins ne seront pas explor\xe9s. Par cons\xe9quent, le mod\xe8le de r\xe9compense ne verra pas de nouveaux \xe9chantillons pour apprendre. De toute fa\xe7on, l'entra\xeenement conjoint du mod\xe8le de r\xe9compense et de la politique induit un biais dans la phase d'apprentissage puisque les deux d\xe9pendent l'un de l'autre. Il peut aussi y avoir un biais initial dans le ",(0,t.jsx)(s,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:(0,t.jsx)(s,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:"mod\xe8le de base"})})," utilis\xe9 pour l'entra\xeenement. Par exemple, ChatGPT a \xe9t\xe9 affin\xe9 \xe0 partir d'un GPT initial entra\xeen\xe9 en partie sur le web. M\xeame si le RLHF a \xe9t\xe9 utilis\xe9 pour supprimer tout contenu controvers\xe9 du mod\xe8le, il reste un risque que le mod\xe8le produise du contenu probl\xe9matique qu'il a vu en ligne. (",(0,t.jsx)(n.a,{href:"https://openai.com/index/chatgpt/",children:"OpenAI, 2022"}),")"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Mauvaise g\xe9n\xe9ralisation de la Politique : Les politiques efficaces pendant l'entra\xeenement peuvent \xe9chouer \xe0 bien g\xe9n\xe9raliser dans des sc\xe9narios du monde r\xe9el. Par exemple, des ph\xe9nom\xe8nes comme le \"Jailbreak\" montrent que des mod\xe8les comme BingChat et ChatGPT peuvent effectuer des actions apprises, m\xeame s'ils sont entra\xeen\xe9s \xe0 ne pas r\xe9pondre \xe0 certaines requ\xeates."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"D\xe9fi de Distribution : Les plus grands mod\xe8les RLHF ont tendance \xe0 d\xe9velopper des tendances nocives d'auto-pr\xe9servation et de sycophantie, qui est l'accord insinc\xe8re avec les opinions des utilisateurs. Ce comportement indique une tendance vers la convergence instrumentale. De plus, le RLHF peut inciter \xe0 des comportements trompeurs, comme l'illustre l'exp\xe9rience de la main robotique dans l'\xe9tude de Christiano et al. en 2017."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Ces probl\xe8mes th\xe9oriques ont des cons\xe9quences r\xe9elles :"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Le RLHF n'a pas r\xe9ussi \xe0 rendre les LLM robustement utiles et inoffensifs."})," Malgr\xe9 les avanc\xe9es continues dans le traitement du langage naturel et le d\xe9veloppement du RLHF, les LLM n'ont pas encore atteint une utilit\xe9 et une innocuit\xe9 robustes."]}),"\n",(0,t.jsxs)(n.p,{children:["Les hallucinations restent un probl\xe8me significatif, comme l'illustre la tendance de GPT-4 \xe0 g\xe9n\xe9rer du contenu absurde ou mensonger (",(0,t.jsx)(n.a,{href:"https://cdn.openai.com/papers/gpt-4-system-card.pdf",children:"OpenAI, 2023"}),"). Ces hallucinations peuvent conduire \xe0 une d\xe9pendance excessive aux LLM, d\xe9gradant par cons\xe9quent la performance du syst\xe8me et ne r\xe9pondant pas aux attentes des utilisateurs dans des sc\xe9narios du monde r\xe9el (",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2202.03629",children:"Ji et al., 2024"}),")."]}),"\n",(0,t.jsxs)(n.p,{children:["De plus, les biais au sein des LLM persistent, refl\xe9tant souvent des opinions d\xe9salign\xe9es entre le LLM et divers groupes d\xe9mographiques aux \xc9tats-Unis, comme on le voit avec les tendances de gauche de certains LLM affin\xe9s par retour humain (",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2303.17548",children:"Santurkar et al., 2023"}),"). Ces biais peuvent \xeatre nocifs, produisant un langage discriminatoire et perp\xe9tuant des st\xe9r\xe9otypes n\xe9gatifs, comme d\xe9montr\xe9 par le biais anti-musulman de GPT-3 (",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2101.05783",children:"Abid et al., 2021"}),")."]}),"\n",(0,t.jsxs)(n.p,{children:['De plus, le jailbreaking des chatbots pose un risque significatif, avec des sites web listant des prompts pour contourner les mesures de s\xe9curit\xe9 comme Chat GPT "DAN" (et autres "Jailbreaks") (',(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2401.09798",children:"Takemoto, 2024"}),"). Les menaces sur la vie priv\xe9e des LLM int\xe9gr\xe9s aux applications sont maintenant plus s\xe9v\xe8res que jamais (",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2304.05197",children:"Li et al., 2023"}),"). Par exemple, l'Italie a interdit ChatGPT en raison de consid\xe9rations de confidentialit\xe9 selon le R\xe8glement G\xe9n\xe9ral sur la Protection des Donn\xe9es (RGPD) de l'UE (",(0,t.jsx)(n.a,{href:"https://www.bbc.com/news/technology-65139406",children:"BBC, 2023"}),"). La capacit\xe9 \xe0 trouver des jailbreaks est soutenue par un article r\xe9cent intitul\xe9 \"Fundamental Limitations of Alignment in Large Language Models.\" L'article pr\xe9sente des r\xe9sultats th\xe9oriques pr\xe9coces qui indiquent que tout processus d'alignement, comme le RLHF, qui r\xe9duit le comportement ind\xe9sirable sans l'\xe9liminer compl\xe8tement, ne peut pas \xeatre s\xfbr contre les prompts adversariaux. Les auteurs constatent qu'en incitant le mod\xe8le \xe0 se comporter comme une persona sp\xe9cifique, des comportements qui sont g\xe9n\xe9ralement tr\xe8s peu susceptibles d'\xeatre exhib\xe9s par le mod\xe8le peuvent \xeatre mis en avant. Ce n'est pas une d\xe9monstration compl\xe8te car leur cadre est bas\xe9 sur la notion de personas, mais cela sugg\xe8re fortement que le pr\xe9-entra\xeenement na\xeff sans curation de donn\xe9es suivi du RLHF peut ne pas \xeatre suffisant contre les attaques adversariales."]}),"\n",(0,t.jsxs)(n.p,{children:["La s\xe9curit\xe9 des informations priv\xe9es sensibles dans les grands mod\xe8les de langage (LLM) est une pr\xe9occupation pressante, particuli\xe8rement lorsque des donn\xe9es g\xe9n\xe9r\xe9es par les utilisateurs, comme les emails et les entr\xe9es de clavier intelligent, sont utilis\xe9es pour l'entra\xeenement. En fait, plusieurs articles r\xe9cents ont d\xe9montr\xe9 que les ",(0,t.jsx)(s,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:(0,t.jsx)(s,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:"mod\xe8les de base"})})," peuvent \xeatre facilement interrog\xe9s pour r\xe9cup\xe9rer des informations personnelles (",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2012.07805",children:"Carlini et al, 2020"}),"; ",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2101.05405",children:"Inan et al., 2021"}),"; ",(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9152761",children:"Pan et al., 2020"}),") et ces probl\xe8mes sont toujours pr\xe9sents dans les mod\xe8les \"align\xe9s\" comme GPT4, qui a le potentiel d'\xeatre utilis\xe9 pour tenter d'identifier des individus lorsqu'il est augment\xe9 avec des donn\xe9es externes (",(0,t.jsx)(n.a,{href:"https://cdn.openai.com/papers/gpt-4-system-card.pdf",children:"OpenAI, 2023"}),"). Comme expos\xe9 par (",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2101.05405",children:"El-Mhamdi et al., 2021"}),"), les LLM peuvent pr\xe9senter une incompatibilit\xe9 fondamentale entre haute pr\xe9cision et s\xe9curit\xe9 et confidentialit\xe9, \xe9tant donn\xe9 la compr\xe9hension actuelle en ",(0,t.jsx)(s,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,t.jsx)(s,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})})," adversarial."]}),"\n",(0,t.jsx)(n.p,{children:"Le RLHF peut rendre la performance dans le pire des cas encore pire."}),"\n",(0,t.jsxs)(n.p,{children:["Le RLHF peut diminuer la robustesse aux attaques adversariales (",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2304.11082",children:"Wolf et al., 2024"}),"), en accentuant la distinction entre les comportements d\xe9sir\xe9s et ind\xe9sirables, rendant potentiellement les LLM plus susceptibles aux prompts adversariaux. La distinction accrue entre les comportements est li\xe9e \xe0 l'Effet Waluigi (",(0,t.jsx)(n.a,{href:"https://www.alignmentforum.org/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post",children:"Nardo, 2023"}),"), o\xf9 apr\xe8s avoir entra\xeen\xe9 un LLM \xe0 satisfaire une propri\xe9t\xe9 d\xe9sirable P, il devient plus facile d'inciter le chatbot \xe0 satisfaire l'exact oppos\xe9 de la propri\xe9t\xe9 P. Des arguments th\xe9oriques comme celui-ci semblent pousser vers l'inefficacit\xe9 du RLHF dans l'\xe9limination des personas trompeurs."]}),"\n",(0,t.jsxs)(n.p,{children:["Certains de ces probl\xe8mes peuvent s'aggraver \xe0 mesure que les syst\xe8mes deviennent plus capables. Il a \xe9t\xe9 constat\xe9 que le RLHF augmente l'autonomie des LLM sans diminuer les m\xe9triques ind\xe9sirables telles que le suivi des objectifs instrumentaux convergents (par exemple, exprimer activement une pr\xe9f\xe9rence \xe0 ne pas \xeatre \xe9teint) ou la sycophantie (",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2212.09251",children:"Perez et al., 2022"}),"). Ces m\xe9triques ind\xe9sirables augmentent avec le nombre d'\xe9tapes de RLHF, indiquant que les mod\xe8les actuels deviennent plus agentiques de mani\xe8res potentiellement pr\xe9occupantes \xe0 mesure qu'ils s'\xe9tendent. Plus g\xe9n\xe9ralement, l'RL \xe0 partir de signaux de r\xe9compense d\xe9riv\xe9s des humains peut augmenter la motivation pour la planification \xe0 plus long terme, la tromperie et le comportement agentique, qui sont des pr\xe9requis pour l'alignement trompeur (",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/1906.01820",children:"Hubinger et al., 2019"}),"), et finalement les risques d'accidents \xe0 grande \xe9chelle."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Conclusion sur les Limitations du RLHF."})," Malgr\xe9 la n\xe9cessit\xe9 de retours humains extensifs, le RLHF fait encore face \xe0 de nombreux \xe9checs, et la r\xe9solution de ces probl\xe8mes pourrait n\xe9cessiter beaucoup plus d'efforts. \xc0 mesure que les syst\xe8mes d'IA \xe9voluent, la demande de donn\xe9es complexes augmente, rendant potentiellement l'acquisition de donn\xe9es prohibitivement co\xfbteuse. De plus, alors que nous repoussons les limites computationnelles, la disponibilit\xe9 d'annotateurs qualifi\xe9s pourrait devenir un facteur limitant."]}),"\n",(0,t.jsxs)(n.p,{children:["Globalement, ce n'est pas parce que le mod\xe8le est affin\xe9 par instructions que le processus d'entra\xeenement est s\xfbr, et le RLHF doit \xeatre incorpor\xe9 dans un cadre technique de s\xe9curit\xe9 plus large (par exemple, les Politiques d'\xc9chelle Responsable ou le Cadre de Pr\xe9paration sont des tentatives partielles d'\xeatre de tels cadres, ou l'article \"Model evaluation for extreme risks\" (",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2305.15324",children:"Shevlane et al., 2023"}),"))."]}),"\n",(0,t.jsxs)(o.A,{title:"Ajustement des instructions vs alignement",collapsed:!0,children:[(0,t.jsxs)(n.p,{children:["L'",(0,t.jsx)(s,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:(0,t.jsx)(s,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:"Affinage"})})," par Instructions est un processus o\xf9 le mod\xe8le est affin\xe9 (via RL ou apprentissage supervis\xe9) pour mieux comprendre et suivre les instructions humaines. Cela implique d'entra\xeener le mod\xe8le sur un ensemble de donn\xe9es qui contient une vari\xe9t\xe9 d'instructions et leurs r\xe9sultats d\xe9sir\xe9s. L'objectif principal de l'",(0,t.jsx)(s,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:(0,t.jsx)(s,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:"Affinage"})})," par Instructions est d'am\xe9liorer la capacit\xe9 de l'IA \xe0 interpr\xe9ter et ex\xe9cuter les commandes comme pr\xe9vu par les utilisateurs. Cela am\xe9liore l'exp\xe9rience utilisateur et \xe9largit l'applicabilit\xe9 du mod\xe8le. Par exemple :"]}),(0,t.jsx)(u.A,{src:"./img/0Qv_Image_23.png",alt:"Entrez la description alternative de l'image",number:"23",label:"6.23",caption:"Exemple d'ajustement des instructions."}),(0,t.jsxs)(n.p,{children:["L'alignement en IA fait r\xe9f\xe9rence au processus d'assurer que les actions et d\xe9cisions d'une IA sont congruentes avec les valeurs et l'\xe9thique humaines. Cela implique d'aligner les objectifs et comportements de l'IA avec ce qui est b\xe9n\xe9fique ou acceptable pour les humains. L'",(0,t.jsx)(s,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:(0,t.jsx)(s,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:"affinage"})})," par instructions est une technique pour poursuivre un cas tr\xe8s superficiel d'\xab alignement externe \xbb, mais il n'est pas clair que l'",(0,t.jsx)(s,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:(0,t.jsx)(s,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:"affinage"})})," par instructions aide pour l'alignement interne, qui est ce qui pr\xe9occupe plus centralement les v\xe9ritables chercheurs en s\xe9curit\xe9 de l'IA."]}),(0,t.jsxs)(n.p,{children:["En r\xe9sum\xe9, ce n'est pas parce qu'un mod\xe8le a subi une technique d'",(0,t.jsx)(s,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:(0,t.jsx)(s,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:"affinage"})}),' par instructions comme le processus RLHF qu\'il est n\xe9cessairement align\xe9. Le terme "mod\xe8le align\xe9" est souvent utilis\xe9, mais il est conseill\xe9 d\'adopter la terminologie plus pr\xe9cise "Affin\xe9 par instructions", plut\xf4t que "mod\xe8le align\xe9", pour \xe9viter la confusion et repr\xe9senter plus pr\xe9cis\xe9ment le processus d\'entra\xeenement sp\xe9cifique que le mod\xe8le a exp\xe9riment\xe9.']})]}),"\n",(0,t.jsx)(u.A,{src:"./img/T8k_Image_24.png",alt:"Entrez la description alternative de l'image",number:"24",label:"6.24",caption:"([Rafailov et al., 2023](https://arxiv.org/abs/2305.18290))"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Optimisation Directe des Pr\xe9f\xe9rences (DPO) :"})," L'Apprentissage par Renforcement \xe0 partir des Retours Humains (RLHF) a d\xe9montr\xe9 son efficacit\xe9, comme le montrent ChatGPT et Llama 2, mais c'est un processus complexe et sensible, et il a aussi certaines mauvaises propri\xe9t\xe9s d'alignement. Le RLHF implique une proc\xe9dure en trois \xe9tapes, alors que le DPO simplifie cela \xe0 deux \xe9tapes. L'article intitul\xe9 \"Direct Preference Optimization: Your Language Model is Secretly a Reward Model\" pr\xe9sente un algorithme qui aligne les mod\xe8les de langage avec les pr\xe9f\xe9rences humaines sans avoir besoin de mod\xe9lisation explicite de r\xe9compense et d'apprentissage par renforcement. Le DPO utilise un objectif de classification simple, contournant le besoin d'un mod\xe8le de r\xe9compense interm\xe9diaire."]}),"\n",(0,t.jsx)(n.p,{children:"Le RLHF, la m\xe9thode qu'il propose de remplacer, implique traditionnellement trois \xe9tapes :"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(s,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:"Affinage"})," supervis\xe9 :"]})," Initialement, le mod\xe8le est entra\xeen\xe9 sur un ensemble de donn\xe9es comprenant des prompts et leurs r\xe9ponses d\xe9sir\xe9es correspondantes."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Mod\xe9lisation de la r\xe9compense :"})," Des \xe9valuateurs humains \xe9valuent les sorties du mod\xe8le, et ce retour informe un mod\xe8le de r\xe9compense, qui est entra\xeen\xe9 \xe0 discerner les types de sorties pr\xe9f\xe9r\xe9s."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Optimisation de politique proximale (PPO) :"})," Le mod\xe8le g\xe9n\xe8re des sorties, qui sont \xe9valu\xe9es par le mod\xe8le de r\xe9compense, et l'algorithme PPO ajuste la politique du mod\xe8le bas\xe9 sur ces \xe9valuations."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Le DPO conserve l'\xe9tape initiale d'",(0,t.jsx)(s,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:(0,t.jsx)(s,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:"affinage"})})," supervis\xe9 mais remplace les deux \xe9tapes suivantes par une seule \xe9tape d'",(0,t.jsx)(s,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:(0,t.jsx)(s,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:"affinage"})})," sur les donn\xe9es de pr\xe9f\xe9rence, en utilisant une nouvelle perte astucieuse. Le DPO augmente efficacement la probabilit\xe9 des actions pr\xe9f\xe9r\xe9es tout en r\xe9duisant la probabilit\xe9 des actions ind\xe9sirables, avec une seule perte :"]}),"\n",(0,t.jsx)(u.A,{src:"./img/ygF_Image_25.png",alt:"Entrez la description alternative de l'image",number:"25",label:"6.25",caption:"DPO augmente la probabilit\xe9 de l'action pr\xe9f\xe9r\xe9e $y_w$ tout en diminuant la probabilit\xe9 de l'action non pr\xe9f\xe9r\xe9e $y_l$."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Cr\xe9ation du dataset de pr\xe9f\xe9rences : Nous \xe9chantillonnons d'abord une paire de continuations en posant une question, l'IA propose deux continuations, nous en \xe9tiquetons une bonne et l'autre mauvaise"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Collection des logits. Nous ex\xe9cutons le ",(0,t.jsx)(s,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:(0,t.jsx)(s,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:"mod\xe8le de base"})})," sur les 2 continuations. Nous ex\xe9cutons le nouveau mod\xe8le sur les 2 continuations"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Optimisation. Nous r\xe9tropropageons \xe0 travers le nouveau mod\xe8le et optimisons la perte ci-dessus."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["En \xe9liminant l'\xe9tape de cr\xe9ation d'un mod\xe8le de r\xe9compense, le DPO simplifie grandement le processus d'",(0,t.jsx)(s,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:(0,t.jsx)(s,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:"affinage"})})," et a montr\xe9 de tr\xe8s bonnes performances."]}),"\n",(0,t.jsx)(n.p,{children:"Ce processus peut ensuite \xeatre it\xe9r\xe9. Cela implique de cr\xe9er un nouveau dataset de pr\xe9f\xe9rences (c'est-\xe0-dire, nous posons une question, et nous \xe9chantillonnons la nouvelle IA deux fois, et ensuite nous \xe9tiquetons le texte que nous pr\xe9f\xe9rons entre les deux, et ensuite nous appliquons la perte DPO) Ensuite, ce cycle est r\xe9p\xe9t\xe9 pour am\xe9liorer le mod\xe8le."}),"\n",(0,t.jsx)(n.p,{children:"Un aspect important du DPO est que la r\xe9compense est implicite : il s'aligne avec les pr\xe9f\xe9rences sans avoir besoin de construire un mod\xe8le de r\xe9compense s\xe9par\xe9. Cette approche aborde le d\xe9fi de sp\xe9cifier une fonction d'utilit\xe9 et r\xe9pond aux critiques telles que celles d'Alex Turner, qui soutient que la notation robuste (c'est-\xe0-dire, la mod\xe9lisation robuste de la r\xe9compense) est une t\xe2che inutilement complexe et non naturelle qui pourrait \xeatre plus difficile que le probl\xe8me entier d'alignement de l'IA lui-m\xeame. La critique de Turner, trouv\xe9e dans \"Inner and Outer Alignment Decompose One Hard Problem Into Two Extremely Hard Problems\", sugg\xe8re que trouver un objectif num\xe9rique s\xfbr et robuste pour qu'un agent hautement intelligent l'optimise directement est un d\xe9fi formidable\u2014un d\xe9fi que le DPO pourrait contourner."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"\xc9largir la Port\xe9e de l'Article avec Diverses Adaptations"})," Cet article offre une base qui pourrait \xeatre am\xe9lior\xe9e \xe0 travers diverses adaptations. Par exemple, int\xe9grer son approche avec les insights de l'article de Tomasz Korbak et al., \"",(0,t.jsx)(s,{term:"pre-training",definition:'{"definition":"La phase initiale de formation au cours de laquelle un mod\xe8le apprend des repr\xe9sentations g\xe9n\xe9rales \xe0 partir d\'un vaste ensemble de donn\xe9es avant d\'\xeatre adapt\xe9 \xe0 des t\xe2ches sp\xe9cifiques..","source":"","aliases":["Pre-training","pretraining","pr\xe9-entrainement"]}',children:(0,t.jsx)(s,{term:"pre-training",definition:'{"definition":"La phase initiale de formation au cours de laquelle un mod\xe8le apprend des repr\xe9sentations g\xe9n\xe9rales \xe0 partir d\'un vaste ensemble de donn\xe9es avant d\'\xeatre adapt\xe9 \xe0 des t\xe2ches sp\xe9cifiques..","source":"","aliases":["Pre-training","pretraining","pr\xe9-entrainement"]}',children:"Pretraining"})}),' Language Models with Human Preferences," (',(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2302.08582",children:"Korbak et al., 2023"}),") pourrait augmenter sa robustesse. De plus, l'utilisation de donn\xe9es de pr\xe9f\xe9rence bool\xe9ennes a ses limites. Fournir des retours en langage naturel, comme montr\xe9 \xeatre plus efficace en termes d'\xe9chantillons dans l'\xe9tude \"Training Language Models with Language Feedback,\" (",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2204.14146",children:"Scheurer et al., 2022"}),") pourrait am\xe9liorer l'efficacit\xe9 du processus. Remarquablement, avec seulement 100 \xe9chantillons de retours \xe9crits par des humains, cette approche a permis l'",(0,t.jsx)(s,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:(0,t.jsx)(s,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:"affinage"})})," d'un mod\xe8le GPT-3 pour atteindre des capacit\xe9s de r\xe9sum\xe9 presque au niveau humain."]}),"\n",(0,t.jsx)(n.p,{children:"En regardant vers l'avenir, un processus sp\xe9culatif qui pourrait att\xe9nuer le gaming de sp\xe9cification serait d'entra\xeener le mod\xe8le comme un enfant, et qui s'interrogerait activement et apprendrait des interactions humaines. Cette approche refl\xe9terait \xe9troitement le d\xe9veloppement de l'enfant, durant lequel un enfant est progressivement plus align\xe9 et plus capable. Et tout comme dans le d\xe9veloppement des enfants, il serait crucial de s'assurer qu'\xe0 aucun moment les capacit\xe9s de l'IA ne d\xe9passent son niveau d'alignement, maintenant un \xe9quilibre entre capacit\xe9 et compr\xe9hension \xe9thique tout au long de son voyage d\xe9veloppemental."})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(f,{...e})}):f(e)}},3931:(e,n,s)=>{s.d(n,{A:()=>l});var i=s(6540),t=s(6347),r=s(8444);const a={videoFigure:"videoFigure_dV9u",fullWidth:"fullWidth_Tv7O",videoContainer:"videoContainer_LWoK",aspectRatio169:"aspectRatio169_Go8b",aspectRatio43:"aspectRatio43_b3T7",aspectRatio11:"aspectRatio11_n21J",aspectRatio219:"aspectRatio219_L_3n",videoIframe:"videoIframe_QEc5",videoElement:"videoElement_uu7N",customPlayer:"customPlayer_jiC4",customPlayerPlaceholder:"customPlayerPlaceholder_aL9q",loadingOverlay:"loadingOverlay_e2pu",spinner:"spinner_tFyL",spin:"spin_saPs",errorContainer:"errorContainer_J6Pn",errorIcon:"errorIcon_cGdn",fallbackLink:"fallbackLink_WsZk"};var o=s(4848);function l(e){let{type:n="youtube",videoId:s,caption:l,title:u,startTime:d,autoplay:p=!1,controls:c=!0,aspectRatio:m="16:9",width:f,height:h,chapter:g,number:v,label:x,useCustomPlayer:b=!1,fullWidth:j=!0}=e;const[q,L]=(0,i.useState)(!0),[A,R]=(0,i.useState)(!1),P=(0,t.zy)(),I=g||(()=>{const e=P.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),C=(()=>{const e=(e=>{if(!e)return"";if("number"==typeof e)return e.toString();if("string"==typeof e){if(/^\d+$/.test(e))return e;let n=0;const s=e.match(/(\d+)h/),i=e.match(/(\d+)m/),t=e.match(/(\d+)s/);return s&&(n+=3600*parseInt(s[1])),i&&(n+=60*parseInt(i[1])),t&&(n+=parseInt(t[1])),n>0?n.toString():""}return""})(d);switch(n.toLowerCase()){case"youtube":let i=`https://www.youtube.com/embed/${s}`;const t=new URLSearchParams;e&&t.append("start",e),p&&t.append("autoplay","1"),c||b||t.append("controls","0"),t.append("rel","0"),t.append("modestbranding","1"),t.append("fs","1"),t.append("cc_load_policy","0"),t.append("iv_load_policy","3"),t.append("showinfo","0"),t.append("disablekb","1"),t.append("playsinline","1"),t.append("color","white"),t.append("theme","light"),b&&(t.append("enablejsapi","1"),t.append("origin",window.location.origin));const r=t.toString();return r?`${i}?${r}`:i;case"vimeo":let a=`https://player.vimeo.com/video/${s}`;const o=new URLSearchParams;p&&o.append("autoplay","1"),c||b||o.append("controls","0"),o.append("title","0"),o.append("byline","0"),o.append("portrait","0"),o.append("dnt","1"),o.append("transparent","0"),o.append("background","1");const l=o.toString();return l?`${a}?${l}`:a;case"mp4":case"webm":case"video":return s;default:return console.warn(`Unsupported video type: ${n}`),s}})(),y=()=>{L(!1)},F=()=>{R(!0),L(!1)},M=e=>{let{src:s,onLoad:i,onError:t}=e;return(0,o.jsx)("div",{className:a.customPlayer,children:(0,o.jsxs)("div",{className:a.customPlayerPlaceholder,children:[(0,o.jsx)("h3",{children:"Atlas Custom Player"}),(0,o.jsx)("p",{children:"Coming Soon"}),(0,o.jsxs)("a",{href:s,target:"_blank",rel:"noopener noreferrer",className:a.fallbackLink,children:["Watch on ",n.charAt(0).toUpperCase()+n.slice(1)]})]})})},H=["mp4","webm","video"].includes(n.toLowerCase());return(0,o.jsxs)("figure",{className:`${a.videoFigure} ${j?a.fullWidth:""}`,children:[(0,o.jsxs)("div",{className:`${a.videoContainer} ${(()=>{switch(m){case"4:3":return a.aspectRatio43;case"1:1":return a.aspectRatio11;case"21:9":return a.aspectRatio219;default:return a.aspectRatio169}})()}`,style:{width:j?"100%":f||"auto",maxWidth:j?"none":"800px"},children:[q&&!A&&(0,o.jsxs)("div",{className:a.loadingOverlay,children:[(0,o.jsx)("div",{className:a.spinner}),(0,o.jsx)("p",{children:"Loading video..."})]}),A&&(0,o.jsxs)("div",{className:a.errorContainer,children:[(0,o.jsxs)("svg",{className:a.errorIcon,width:"48",height:"48",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",children:[(0,o.jsx)("circle",{cx:"12",cy:"12",r:"10"}),(0,o.jsx)("line",{x1:"12",y1:"8",x2:"12",y2:"12"}),(0,o.jsx)("line",{x1:"12",y1:"16",x2:"12.01",y2:"16"})]}),(0,o.jsx)("h4",{children:"Failed to load video"}),(0,o.jsxs)("p",{children:["Video type: ",n]}),(0,o.jsxs)("p",{children:["Video ID/URL: ",s]}),(0,o.jsx)("a",{href:C,target:"_blank",rel:"noopener noreferrer",className:a.fallbackLink,children:"Try opening video directly"})]}),!A&&(H?(0,o.jsxs)("video",{className:a.videoElement,controls:c,autoPlay:p,onLoadedData:y,onError:F,title:u||l||`${n} video`,style:{width:f||"100%",height:h||"auto",display:q?"none":"block"},children:[(0,o.jsx)("source",{src:C,type:`video/${n}`}),(0,o.jsxs)("p",{children:["Your browser doesn't support video playback.",(0,o.jsx)("a",{href:C,target:"_blank",rel:"noopener noreferrer",children:"Download the video file"})]})]}):b?(0,o.jsx)(M,{src:C,onLoad:y,onError:F}):(0,o.jsx)("iframe",{className:a.videoIframe,src:C,title:u||l||`${n} video`,frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0,onLoad:y,onError:F,style:{width:f||"100%",height:h||"100%",opacity:q?0:1}}))]}),(0,o.jsx)(r.A,{caption:l,mediaType:"video",chapter:I,number:v,label:x})]})}}}]);
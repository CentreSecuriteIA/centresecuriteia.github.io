"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[6364],{7051:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>p,default:()=>f,frontMatter:()=>c,metadata:()=>t,toc:()=>u});const t=JSON.parse('{"id":"chapters/06/3","title":"Contournement des Sp\xe9cifications","description":"La mauvaise sp\xe9cification de la r\xe9compense, \xe9galement appel\xe9e probl\xe8me d\'alignement externe, fait r\xe9f\xe9rence \xe0 la difficult\xe9 de fournir \xe0 l\'IA la r\xe9compense exacte \xe0 optimiser.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/06/03.md","sourceDirName":"chapters/06","slug":"/chapters/06/03","permalink":"/fr/chapters/06/03","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/06/03.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"3","title":"Contournement des Sp\xe9cifications","sidebar_label":"6.3 Contournement des Sp\xe9cifications","sidebar_position":4,"slug":"/chapters/06/03","reading_time_core":"11 min","reading_time_optional":"1 min"},"sidebar":"docs","previous":{"title":"6.2 Optimisation","permalink":"/fr/chapters/06/02"},"next":{"title":"6.4 Apprendre par imitation","permalink":"/fr/chapters/06/04"}}');var i=s(4848),r=s(8453),o=s(3931),a=(s(8559),s(2482),s(9585)),l=s(2501);const c={id:3,title:"Contournement des Sp\xe9cifications",sidebar_label:"6.3 Contournement des Sp\xe9cifications",sidebar_position:4,slug:"/chapters/06/03",reading_time_core:"11 min",reading_time_optional:"1 min"},p="D\xe9tournement des Sp\xe9cifications",d={},u=[{value:"Conception de la r\xe9compense",id:"01",level:2},{value:"Fa\xe7onnage des r\xe9compenses",id:"02",level:2},{value:"Piratage de r\xe9compense",id:"03",level:2},{value:"Manipulation de la r\xe9compense",id:"04",level:2}];function m(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{GlossaryTerm:s}=n;return s||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"d\xe9tournement-des-sp\xe9cifications",children:"D\xe9tournement des Sp\xe9cifications"})}),"\n",(0,i.jsx)(a.A,{term:"Mauvaise sp\xe9cification de la r\xe9compense",source:"",number:"6",label:"6.6",children:(0,i.jsx)(n.p,{children:"La mauvaise sp\xe9cification de la r\xe9compense, \xe9galement appel\xe9e probl\xe8me d'alignement externe, fait r\xe9f\xe9rence \xe0 la difficult\xe9 de fournir \xe0 l'IA la r\xe9compense exacte \xe0 optimiser."})}),"\n",(0,i.jsx)(o.A,{type:"youtube",videoId:"nKJlF-olKmg",number:"2",label:"6.2",caption:"Vid\xe9o facultative avec de nombreux exemples de d\xe9tournement de sp\xe9cification."}),"\n",(0,i.jsxs)(n.p,{children:["Le probl\xe8me fondamental est simple \xe0 comprendre : la ",(0,i.jsx)(s,{term:"loss function",definition:'{"definition":"Une fonction qui mesure dans quelle mesure les pr\xe9dictions d\'un mod\xe8le correspondent aux valeurs cibles r\xe9elles, utilis\xe9e pour guider l\'entra\xeenement..","source":"","aliases":["Loss Function","cost function","objective function","Fonction de perte"]}',children:(0,i.jsx)(s,{term:"loss function",definition:'{"definition":"Une fonction qui mesure dans quelle mesure les pr\xe9dictions d\'un mod\xe8le correspondent aux valeurs cibles r\xe9elles, utilis\xe9e pour guider l\'entra\xeenement..","source":"","aliases":["Loss Function","cost function","objective function","Fonction de perte"]}',children:"fonction de perte"})})," sp\xe9cifi\xe9e correspond-elle \xe0 l'objectif voulu par ses concepteurs ? Cependant, la mise en \u0153uvre dans des sc\xe9narios pratiques est extr\xeamement difficile. Exprimer l'intention compl\xe8te derri\xe8re une demande humaine \xe9quivaut \xe0 transmettre toutes les valeurs humaines, le contexte culturel implicite, etc., qui restent eux-m\xeames mal compris."]}),"\n",(0,i.jsx)(n.p,{children:"De plus, comme la plupart des mod\xe8les sont con\xe7us comme des optimiseurs d'objectifs, ils sont tous vuln\xe9rables \xe0 la Loi de Goodhart. Cette vuln\xe9rabilit\xe9 implique que des cons\xe9quences n\xe9gatives impr\xe9vues peuvent survenir en raison d'une pression d'optimisation excessive sur un objectif qui semble bien sp\xe9cifi\xe9 pour les humains, mais s'\xe9carte des v\xe9ritables objectifs de mani\xe8re subtile."}),"\n",(0,i.jsx)(n.p,{children:"Le probl\xe8me global peut \xeatre d\xe9compos\xe9 en diff\xe9rentes questions qui seront expliqu\xe9es en d\xe9tail dans les sous-sections individuelles ci-dessous. Voici un aper\xe7u rapide :"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["La ",(0,i.jsx)(n.strong,{children:"mauvaise sp\xe9cification de la r\xe9compense"})," survient lorsque la fonction de r\xe9compense sp\xe9cifi\xe9e ne capture pas pr\xe9cis\xe9ment l'objectif r\xe9el ou le comportement souhait\xe9."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["La ",(0,i.jsx)(n.strong,{children:"conception de la r\xe9compense"})," fait r\xe9f\xe9rence au processus de conception de la fonction de r\xe9compense pour aligner le comportement des agents IA avec les objectifs pr\xe9vus."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Le ",(0,i.jsx)(n.strong,{children:"d\xe9tournement de la r\xe9compense"})," fait r\xe9f\xe9rence au comportement des agents d'apprentissage par renforcement exploitant les failles ou les lacunes dans la fonction de r\xe9compense sp\xe9cifi\xe9e pour obtenir des r\xe9compenses \xe9lev\xe9es sans r\xe9ellement atteindre les objectifs pr\xe9vus."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["La ",(0,i.jsx)(n.strong,{children:"manipulation de la r\xe9compense"})," est un concept plus large qui englobe l'influence inappropri\xe9e de l'agent sur le processus de r\xe9compense lui-m\xeame, excluant la manipulation de la fonction de r\xe9compense par le jeu."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Avant d'approfondir les types sp\xe9cifiques d'\xe9checs de mauvaise sp\xe9cification de la r\xe9compense, la section suivante explique davantage l'accent mis sur la conception de la r\xe9compense en conjonction avec la conception d'algorithmes. Cette section \xe9lucide \xe9galement la difficult\xe9 notoire de concevoir des r\xe9compenses efficaces."}),"\n",(0,i.jsx)(n.h2,{id:"01",children:"Conception de la r\xe9compense"}),"\n",(0,i.jsx)(a.A,{term:"Conception de la r\xe9compense",source:"",number:"7",label:"6.7",children:(0,i.jsx)(n.p,{children:"La conception de la r\xe9compense fait r\xe9f\xe9rence au processus de sp\xe9cification de la fonction de r\xe9compense dans l'apprentissage par renforcement (RL)."})}),"\n",(0,i.jsxs)(n.p,{children:["La conception de la r\xe9compense est un terme plus large que le fa\xe7onnage de la r\xe9compense qui englobe l'ensemble du processus de conception et de fa\xe7onnage des fonctions de r\xe9compense pour guider le comportement des syst\xe8mes d'IA. Elle implique non seulement le fa\xe7onnage de la r\xe9compense mais aussi le processus global de d\xe9finition des objectifs, de sp\xe9cification des pr\xe9f\xe9rences et de cr\xe9ation de fonctions de r\xe9compense qui s'alignent sur les valeurs humaines et les r\xe9sultats souhait\xe9s. La conception de la r\xe9compense est un terme souvent utilis\xe9 de mani\xe8re interchangeable avec l'ing\xe9nierie de la r\xe9compense (",(0,i.jsx)(n.a,{href:"https://www.alignmentforum.org/posts/4nZRzoGTqg8xy5rr8/the-reward-engineering-problem",children:"Christiano, 2019"}),"). Les deux font r\xe9f\xe9rence \xe0 la m\xeame chose."]}),"\n",(0,i.jsx)(n.p,{children:"La conception d'algorithmes RL et la conception de r\xe9compenses RL sont deux facettes distinctes de l'apprentissage par renforcement. La conception d'algorithmes RL concerne le d\xe9veloppement et l'impl\xe9mentation d'algorithmes d'apprentissage qui permettent \xe0 un agent d'apprendre et d'affiner son comportement bas\xe9 sur les r\xe9compenses et les interactions avec l'environnement. Ce processus inclut la conception des m\xe9canismes et des proc\xe9dures par lesquels l'agent apprend de ses exp\xe9riences, met \xe0 jour ses politiques et prend des d\xe9cisions pour maximiser les r\xe9compenses cumul\xe9es."}),"\n",(0,i.jsx)(n.p,{children:"\xc0 l'inverse, la conception de r\xe9compenses RL se concentre sur la sp\xe9cification et la conception de la fonction de r\xe9compense guidant le processus d'apprentissage de l'agent RL. La conception de la r\xe9compense n\xe9cessite une ing\xe9nierie minutieuse de la fonction de r\xe9compense pour l'aligner sur le comportement et les objectifs souhait\xe9s, tout en tenant compte des pi\xe8ges potentiels comme le piratage de r\xe9compense ou la falsification de r\xe9compense. La fonction de r\xe9compense est un \xe9l\xe9ment central car elle fa\xe7onne le comportement de l'agent RL et d\xe9termine quelles actions sont consid\xe9r\xe9es comme d\xe9sirables ou ind\xe9sirables."}),"\n",(0,i.jsx)(l.A,{src:"./img/u6m_Image_3.png",alt:"Entrer la description alternative de l'image",number:"3",label:"6.3",caption:"Le d\xe9tournement de sp\xe9cification : l'autre facette de l'ing\xe9niosit\xe9 de l'IA ([Krakovna et al., 2020](https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity))"}),"\n",(0,i.jsx)(n.p,{children:"La conception d'une fonction de r\xe9compense pr\xe9sente souvent un d\xe9fi redoutable qui n\xe9cessite une expertise et une exp\xe9rience consid\xe9rables. Pour d\xe9montrer la complexit\xe9 de cette t\xe2che, consid\xe9rons comment on pourrait concevoir manuellement une fonction de r\xe9compense pour faire ex\xe9cuter un salto arri\xe8re \xe0 un agent, comme illustr\xe9 dans l'image suivante :"}),"\n",(0,i.jsx)(l.A,{src:"./img/lLq_Image_4.gif",alt:"Entrer la description alternative de l'image",number:"4",label:"6.4",caption:"Apprentissage par renforcement profond \xe0 partir des pr\xe9f\xe9rences humaines ([Christiano et al., 2017](https://arxiv.org/abs/1706.03741))"}),"\n",(0,i.jsx)(n.p,{children:"Alors que la conception d'algorithmes RL se concentre sur les m\xe9canismes d'apprentissage et de prise de d\xe9cision de l'agent, la conception de r\xe9compenses RL se concentre sur la d\xe9finition de l'objectif et le fa\xe7onnage du comportement de l'agent \xe0 travers la fonction de r\xe9compense. Les deux aspects sont cruciaux dans le d\xe9veloppement de syst\xe8mes RL efficaces et align\xe9s. Un algorithme RL bien con\xe7u peut apprendre efficacement des r\xe9compenses, tandis qu'une fonction de r\xe9compense soigneusement con\xe7ue peut guider l'agent vers le comportement souhait\xe9 et \xe9viter les cons\xe9quences impr\xe9vues. Le diagramme suivant pr\xe9sente les trois \xe9l\xe9ments cl\xe9s dans la conception d'agents RL - la conception d'algorithmes, la conception de r\xe9compenses et la pr\xe9vention de la falsification du signal de r\xe9compense :"}),"\n",(0,i.jsx)(l.A,{src:"./img/f41_Image_5.png",alt:"Entrer la description alternative de l'image",number:"5",label:"6.5",caption:"Le d\xe9tournement de sp\xe9cification : l'autre facette de l'ing\xe9niosit\xe9 de l'IA ([Krakovna et al., 2020](https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity))"}),"\n",(0,i.jsxs)(n.p,{children:["Le processus de conception de la r\xe9compense re\xe7oit une ",(0,i.jsx)(s,{term:"attention",definition:'{"definition":"M\xe9canisme permettant aux mod\xe8les de se concentrer sur les parties pertinentes de l\'entr\xe9e lors de la r\xe9alisation de pr\xe9dictions, en calculant des combinaisons pond\xe9r\xe9es d\'\xe9l\xe9ments d\'entr\xe9e..","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:(0,i.jsx)(s,{term:"attention",definition:'{"definition":"M\xe9canisme permettant aux mod\xe8les de se concentrer sur les parties pertinentes de l\'entr\xe9e lors de la r\xe9alisation de pr\xe9dictions, en calculant des combinaisons pond\xe9r\xe9es d\'\xe9l\xe9ments d\'entr\xe9e..","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:"attention"})})," minimale dans les textes d'introduction au RL, malgr\xe9 son r\xf4le crucial dans la d\xe9finition du probl\xe8me \xe0 r\xe9soudre. Comme mentionn\xe9 dans l'introduction de cette section, r\xe9soudre le probl\xe8me de mauvaise sp\xe9cification de la r\xe9compense n\xe9cessiterait de trouver des m\xe9triques d'\xe9valuation r\xe9sistantes aux \xe9checs induits par la loi de Goodhart. Cela inclut les \xe9checs provenant de la sur-optimisation d'un objectif mal dirig\xe9 ou d'un objectif proxy (piratage de r\xe9compense), ou de l'interf\xe9rence directe de l'agent avec le signal de r\xe9compense (falsification de r\xe9compense). Ces concepts sont explor\xe9s plus en d\xe9tail dans les sections suivantes."]}),"\n",(0,i.jsx)(n.h2,{id:"02",children:"Fa\xe7onnage des r\xe9compenses"}),"\n",(0,i.jsx)(a.A,{term:"Fa\xe7onnage de la r\xe9compense",source:"",number:"8",label:"6.8",children:(0,i.jsx)(n.p,{children:"Le fa\xe7onnage des r\xe9compenses est une technique utilis\xe9e en apprentissage par renforcement qui introduit de petites r\xe9compenses interm\xe9diaires pour compl\xe9ter la r\xe9compense environnementale. Cela vise \xe0 att\xe9nuer le probl\xe8me des signaux de r\xe9compense clairsem\xe9s et \xe0 encourager l'exploration et un apprentissage plus rapide."})}),"\n",(0,i.jsx)(n.p,{children:"Pour r\xe9ussir un probl\xe8me d'apprentissage par renforcement, une IA doit faire deux choses :"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Trouver une s\xe9quence d'actions qui m\xe8ne \xe0 une r\xe9compense positive. C'est le probl\xe8me de l'exploration."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"M\xe9moriser la s\xe9quence d'actions \xe0 effectuer et g\xe9n\xe9raliser \xe0 des situations connexes mais l\xe9g\xe8rement diff\xe9rentes. C'est le probl\xe8me de l'apprentissage."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Les m\xe9thodes d'apprentissage par renforcement sans mod\xe8le explorent en prenant des actions al\xe9atoires. Si, par hasard, les actions al\xe9atoires m\xe8nent \xe0 une r\xe9compense, elles sont renforc\xe9es, et l'agent devient plus susceptible de prendre ces actions b\xe9n\xe9fiques \xe0 l'avenir. Cela fonctionne bien si les r\xe9compenses sont suffisamment denses pour que les actions al\xe9atoires m\xe8nent \xe0 une r\xe9compense avec une probabilit\xe9 raisonnable. Cependant, beaucoup de jeux plus complexes n\xe9cessitent de longues s\xe9quences d'actions tr\xe8s sp\xe9cifiques pour obtenir une r\xe9compense, et de telles s\xe9quences sont extr\xeamement improbables de se produire al\xe9atoirement."}),"\n",(0,i.jsx)(n.p,{children:"Un exemple classique de ce probl\xe8me a \xe9t\xe9 observ\xe9 dans le jeu vid\xe9o Montezuma's Revenge o\xf9 l'objectif de l'agent \xe9tait de trouver une cl\xe9, mais il y avait de nombreuses \xe9tapes interm\xe9diaires requises pour la trouver. Afin de r\xe9soudre ces probl\xe8mes de planification \xe0 long terme, les chercheurs ont essay\xe9 d'ajouter des termes ou des composants suppl\xe9mentaires \xe0 la fonction de r\xe9compense pour encourager les comportements souhait\xe9s ou d\xe9courager les comportements ind\xe9sirables."}),"\n",(0,i.jsx)(l.A,{src:"./img/S1L_Image_6.png",alt:"Entrer la description alternative de l'image",number:"6",label:"6.6",caption:"Apprendre Montezuma's Revenge \xe0 partir d'une seule d\xe9monstration ([OpenAI, 2018](https://openai.com/research/learning-montezumas-revenge-from-a-single-demonstration))"}),"\n",(0,i.jsx)(n.p,{children:"L'objectif du fa\xe7onnage des r\xe9compenses est de rendre le processus d'apprentissage plus efficace en fournissant des r\xe9compenses informatives qui guident l'agent vers les r\xe9sultats souhait\xe9s. Le fa\xe7onnage des r\xe9compenses implique de fournir des r\xe9compenses suppl\xe9mentaires \xe0 l'agent pour progresser vers l'objectif d\xe9sir\xe9. En fa\xe7onnant les r\xe9compenses, l'agent re\xe7oit des retours plus fr\xe9quents et significatifs, ce qui peut l'aider \xe0 apprendre plus efficacement. Le fa\xe7onnage des r\xe9compenses peut \xeatre particuli\xe8rement utile dans les sc\xe9narios o\xf9 la fonction de r\xe9compense originale est clairsem\xe9e, ce qui signifie que l'agent re\xe7oit peu ou pas de retours jusqu'\xe0 ce qu'il atteigne l'objectif final. Cependant, il est important de concevoir le fa\xe7onnage des r\xe9compenses avec soin pour \xe9viter des cons\xe9quences impr\xe9vues."}),"\n",(0,i.jsx)(n.p,{children:"Les algorithmes de fa\xe7onnage des r\xe9compenses supposent souvent des fonctions de fa\xe7onnage con\xe7ues manuellement et sp\xe9cifiques au domaine, construites par des experts du domaine, ce qui va \xe0 l'encontre de l'objectif d'apprentissage autonome. De plus, de mauvais choix de r\xe9compenses de fa\xe7onnage peuvent d\xe9t\xe9riorer les performances de l'agent."}),"\n",(0,i.jsx)(n.p,{children:"Un fa\xe7onnage des r\xe9compenses mal con\xe7u peut conduire l'agent \xe0 optimiser les r\xe9compenses fa\xe7onn\xe9es plut\xf4t que les v\xe9ritables r\xe9compenses, r\xe9sultant en un comportement sous-optimal. Des exemples de cela sont fournis dans les sections suivantes sur le piratage des r\xe9compenses."}),"\n",(0,i.jsx)(n.h2,{id:"03",children:"Piratage de r\xe9compense"}),"\n",(0,i.jsx)(a.A,{term:"Piratage de la r\xe9compense",source:"",number:"9",label:"6.9",children:(0,i.jsx)(n.p,{children:"Le piratage de r\xe9compense se produit lorsqu'un agent d'IA trouve des moyens d'exploiter des failles ou des raccourcis dans l'environnement pour maximiser sa r\xe9compense sans r\xe9ellement atteindre l'objectif pr\xe9vu."})}),"\n",(0,i.jsxs)(n.p,{children:["Le contournement des sp\xe9cifications est le cadre g\xe9n\xe9ral du probl\xe8me lorsqu'un syst\xe8me d'IA trouve un moyen d'atteindre l'objectif d'une mani\xe8re non pr\xe9vue. Le contournement des sp\xe9cifications peut survenir dans de nombreux types de mod\xe8les d'",(0,i.jsx)(s,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,i.jsx)(s,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})}),". Le piratage de r\xe9compense est une occurrence sp\xe9cifique d'un \xe9chec de contournement des sp\xe9cifications dans les syst\xe8mes d'apprentissage par renforcement qui fonctionnent sur des m\xe9canismes bas\xe9s sur les r\xe9compenses."]}),"\n",(0,i.jsx)(n.p,{children:"Le piratage de r\xe9compense et la mauvaise sp\xe9cification de r\xe9compense sont des concepts li\xe9s mais ont des significations distinctes. La mauvaise sp\xe9cification de r\xe9compense fait r\xe9f\xe9rence \xe0 la situation o\xf9 la fonction de r\xe9compense sp\xe9cifi\xe9e ne capture pas pr\xe9cis\xe9ment l'objectif r\xe9el ou le comportement souhait\xe9."}),"\n",(0,i.jsx)(n.p,{children:"Le piratage de r\xe9compense ne n\xe9cessite pas toujours une mauvaise sp\xe9cification de r\xe9compense. Il n'est pas n\xe9cessairement vrai qu'une r\xe9compense parfaitement sp\xe9cifi\xe9e (qui capture compl\xe8tement et pr\xe9cis\xe9ment le comportement souhait\xe9 du syst\xe8me) soit impossible \xe0 pirater. Il peut \xe9galement y avoir des impl\xe9mentations bogu\xe9s ou corrompues qui auront des comportements non intentionnels. Le but d'une fonction de r\xe9compense est de r\xe9duire un syst\xe8me complexe \xe0 une seule valeur. Cela impliquera presque toujours des simplifications, etc., qui seront alors l\xe9g\xe8rement diff\xe9rentes de ce que vous d\xe9crivez. La carte n'est pas le territoire."}),"\n",(0,i.jsx)(n.p,{children:"Le piratage de r\xe9compense peut se manifester de multiples fa\xe7ons. Par exemple, dans le contexte des agents de jeu, cela peut impliquer l'exploitation de bogues ou d'erreurs logicielles pour manipuler directement le score ou obtenir des r\xe9compenses \xe9lev\xe9es par des moyens non pr\xe9vus."}),"\n",(0,i.jsx)(n.p,{children:"Comme exemple concret, un agent dans le jeu Coast Runners a \xe9t\xe9 entra\xeen\xe9 avec l'objectif de gagner la course. Le jeu utilise un m\xe9canisme de score, donc pour progresser au niveau suivant, les concepteurs de r\xe9compenses ont utilis\xe9 le fa\xe7onnement de r\xe9compense pour r\xe9compenser le syst\xe8me lorsqu'il marquait des points. Ceux-ci \xe9taient donn\xe9s quand un bateau obtient des objets (comme les blocs verts dans l'animation ci-dessous) ou accomplit d'autres actions qui aideraient pr\xe9sum\xe9ment \xe0 gagner la course. Malgr\xe9 l'attribution de r\xe9compenses interm\xe9diaires, l'objectif global pr\xe9vu \xe9tait de terminer la course le plus rapidement possible. Les d\xe9veloppeurs pensaient que la meilleure fa\xe7on d'obtenir un score \xe9lev\xe9 \xe9tait de gagner la course, mais ce n'\xe9tait pas le cas. L'agent a d\xe9couvert que faire tourner continuellement un bateau en cercle pour accumuler des points ind\xe9finiment optimisait sa r\xe9compense, m\xeame si cela ne l'aidait pas \xe0 gagner la course."}),"\n",(0,i.jsx)(l.A,{src:"./img/l1D_Image_7.gif",alt:"Entrer la description alternative de l'image",number:"7",label:"6.7",caption:"Fonctions de r\xe9compense d\xe9fectueuses dans la nature ([Amodei & Clark, 2016](https://openai.com/index/faulty-reward-functions/))"}),"\n",(0,i.jsx)(l.A,{src:"./img/PrP_Image_8.png",alt:"Entrer la description alternative de l'image",number:"8",label:"6.8",caption:"Une IA jouant \xe0 CoastRunners 7 a appris \xe0 s'\xe9craser et \xe0 r\xe9g\xe9n\xe9rer r\xe9p\xe9titivement les cibles plut\xf4t que de gagner la course pour obtenir un score plus \xe9lev\xe9, manifestant un d\xe9tournement du proxy. ([Hendrycks, 2024](https://www.aisafetybook.com/textbook/robustness))"}),"\n",(0,i.jsx)(n.p,{children:"Dans les cas o\xf9 la fonction de r\xe9compense n'est pas align\xe9e avec l'objectif souhait\xe9, le piratage de r\xe9compense peut \xe9merger. Cela peut conduire l'agent \xe0 optimiser une r\xe9compense proxy, s'\xe9cartant de l'objectif sous-jacent r\xe9el, produisant ainsi un comportement contraire aux intentions des concepteurs. Comme exemple de ce qui pourrait se produire dans un sc\xe9nario r\xe9el, consid\xe9rez un robot de nettoyage : si la fonction de r\xe9compense se concentre sur la r\xe9duction du d\xe9sordre, le robot pourrait cr\xe9er artificiellement du d\xe9sordre pour le nettoyer, collectant ainsi des r\xe9compenses, au lieu de nettoyer efficacement l'environnement."}),"\n",(0,i.jsx)(n.p,{children:"Le piratage de r\xe9compense pr\xe9sente des d\xe9fis importants pour la s\xe9curit\xe9 de l'IA en raison du potentiel de comportements non intentionnels et potentiellement nuisibles. Par cons\xe9quent, la lutte contre le piratage de r\xe9compense reste un domaine de recherche actif dans la s\xe9curit\xe9 et l'alignement de l'IA."}),"\n",(0,i.jsx)(n.h2,{id:"04",children:"Manipulation de la r\xe9compense"}),"\n",(0,i.jsx)(a.A,{term:"Manipulation de la r\xe9compense",source:"",number:"10",label:"6.10",children:(0,i.jsx)(n.p,{children:"La manipulation de la r\xe9compense fait r\xe9f\xe9rence aux cas o\xf9 un agent IA influence ou manipule de mani\xe8re inappropri\xe9e le processus de r\xe9compense lui-m\xeame."})}),"\n",(0,i.jsx)(n.p,{children:"Le probl\xe8me de l'accomplissement d'une t\xe2che pr\xe9vue peut \xeatre divis\xe9 en :"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"La conception d'un agent qui est efficace pour optimiser la r\xe9compense"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"La conception d'un processus de r\xe9compense qui fournit \xe0 l'agent des r\xe9compenses appropri\xe9es. Le processus de r\xe9compense peut \xeatre d\xe9compos\xe9 davantage. Le processus comprend :"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Une fonction de r\xe9compense impl\xe9ment\xe9e"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Un m\xe9canisme pour collecter les donn\xe9es sensorielles appropri\xe9es en entr\xe9e"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Un moyen pour l'utilisateur de potentiellement mettre \xe0 jour la fonction de r\xe9compense."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"La manipulation de la r\xe9compense implique que l'agent interf\xe8re avec diff\xe9rentes parties de ce processus de r\xe9compense. Un agent pourrait d\xe9former le retour re\xe7u du mod\xe8le de r\xe9compense, modifiant les informations utilis\xe9es pour mettre \xe0 jour son comportement. Il pourrait \xe9galement manipuler l'impl\xe9mentation du mod\xe8le de r\xe9compense, modifiant le code ou le mat\xe9riel pour changer les calculs de r\xe9compense. Dans certains cas, les agents pratiquant la manipulation de r\xe9compense peuvent m\xeame modifier directement les valeurs de r\xe9compense avant le traitement dans le registre machine. Selon ce qui est manipul\xe9 exactement, nous obtenons diff\xe9rents degr\xe9s de manipulation de r\xe9compense. Ceux-ci peuvent \xeatre distingu\xe9s dans l'image ci-dessous."}),"\n",(0,i.jsx)(l.A,{src:"./img/MK3_Image_9.png",alt:"Entrer la description alternative de l'image",number:"9",label:"6.9",caption:"Clarification de la terminologie du wireheading ([Gao, 2022](https://www.alignmentforum.org/posts/REesy8nqvknFFKywm/clarifying-wireheading-terminology))"}),"\n",(0,i.jsx)(n.p,{children:"La manipulation des entr\xe9es de la fonction de r\xe9compense interf\xe8re uniquement avec les entr\xe9es de la fonction de r\xe9compense. Par exemple, interf\xe9rer avec les capteurs."}),"\n",(0,i.jsx)(n.p,{children:"La manipulation de la fonction de r\xe9compense implique que l'agent modifie la fonction de r\xe9compense elle-m\xeame."}),"\n",(0,i.jsx)(a.A,{term:"Wireheading",source:"",number:"11",label:"6.11",children:(0,i.jsx)(n.p,{children:"L'auto-stimulation fait r\xe9f\xe9rence au comportement d'un syst\xe8me qui manipule ou corrompt sa propre structure interne en alt\xe9rant directement l'algorithme d'apprentissage par renforcement, par exemple en modifiant les valeurs des registres."})}),"\n",(0,i.jsxs)(n.p,{children:["La manipulation de la r\xe9compense est pr\xe9occupante car on suppose que la manipulation du processus de r\xe9compense appara\xeetra souvent comme un objectif instrumental (",(0,i.jsx)(n.a,{href:"https://www.goodreads.com/book/show/20527133-superintelligence",children:"Bostrom, 2014"}),"; ",(0,i.jsx)(n.a,{href:"https://dl.acm.org/doi/10.5555/1566174.1566226",children:"Omohundro, 2008"}),"). Cela peut conduire \xe0 l'affaiblissement ou \xe0 la rupture de la relation entre la r\xe9compense observ\xe9e et la t\xe2che pr\xe9vue. C'est une direction de recherche en cours."]}),"\n",(0,i.jsx)(n.p,{children:"Un exemple hypoth\xe9tique existant de manipulation de r\xe9compense peut \xeatre observ\xe9 dans les algorithmes bas\xe9s sur les recommandations utilis\xe9s dans les m\xe9dias sociaux. Ces algorithmes influencent l'\xe9tat \xe9motionnel de leurs utilisateurs pour g\xe9n\xe9rer plus de \"j'aime\". La t\xe2che pr\xe9vue \xe9tait de servir du contenu utile ou engageant, mais cela est r\xe9alis\xe9 en manipulant les perceptions \xe9motionnelles humaines, et ainsi en changeant ce qui serait consid\xe9r\xe9 comme utile. En supposant que les capacit\xe9s des syst\xe8mes continuent d'augmenter gr\xe2ce \xe0 des avanc\xe9es computationnelles ou algorithmiques, il est plausible de s'attendre \xe0 ce que les probl\xe8mes de manipulation de r\xe9compense deviennent de plus en plus courants. Par cons\xe9quent, la manipulation de r\xe9compense est une pr\xe9occupation potentielle qui n\xe9cessite beaucoup plus de recherche et de v\xe9rification empirique."})]})}function f(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},3931:(e,n,s)=>{s.d(n,{A:()=>l});var t=s(6540),i=s(6347),r=s(8444);const o={videoFigure:"videoFigure_dV9u",fullWidth:"fullWidth_Tv7O",videoContainer:"videoContainer_LWoK",aspectRatio169:"aspectRatio169_Go8b",aspectRatio43:"aspectRatio43_b3T7",aspectRatio11:"aspectRatio11_n21J",aspectRatio219:"aspectRatio219_L_3n",videoIframe:"videoIframe_QEc5",videoElement:"videoElement_uu7N",customPlayer:"customPlayer_jiC4",customPlayerPlaceholder:"customPlayerPlaceholder_aL9q",loadingOverlay:"loadingOverlay_e2pu",spinner:"spinner_tFyL",spin:"spin_saPs",errorContainer:"errorContainer_J6Pn",errorIcon:"errorIcon_cGdn",fallbackLink:"fallbackLink_WsZk"};var a=s(4848);function l(e){let{type:n="youtube",videoId:s,caption:l,title:c,startTime:p,autoplay:d=!1,controls:u=!0,aspectRatio:m="16:9",width:f,height:g,chapter:h,number:x,label:v,useCustomPlayer:b=!1,fullWidth:j=!0}=e;const[q,L]=(0,t.useState)(!0),[y,w]=(0,t.useState)(!1),C=(0,i.zy)(),_=h||(()=>{const e=C.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),A=(()=>{const e=(e=>{if(!e)return"";if("number"==typeof e)return e.toString();if("string"==typeof e){if(/^\d+$/.test(e))return e;let n=0;const s=e.match(/(\d+)h/),t=e.match(/(\d+)m/),i=e.match(/(\d+)s/);return s&&(n+=3600*parseInt(s[1])),t&&(n+=60*parseInt(t[1])),i&&(n+=parseInt(i[1])),n>0?n.toString():""}return""})(p);switch(n.toLowerCase()){case"youtube":let t=`https://www.youtube.com/embed/${s}`;const i=new URLSearchParams;e&&i.append("start",e),d&&i.append("autoplay","1"),u||b||i.append("controls","0"),i.append("rel","0"),i.append("modestbranding","1"),i.append("fs","1"),i.append("cc_load_policy","0"),i.append("iv_load_policy","3"),i.append("showinfo","0"),i.append("disablekb","1"),i.append("playsinline","1"),i.append("color","white"),i.append("theme","light"),b&&(i.append("enablejsapi","1"),i.append("origin",window.location.origin));const r=i.toString();return r?`${t}?${r}`:t;case"vimeo":let o=`https://player.vimeo.com/video/${s}`;const a=new URLSearchParams;d&&a.append("autoplay","1"),u||b||a.append("controls","0"),a.append("title","0"),a.append("byline","0"),a.append("portrait","0"),a.append("dnt","1"),a.append("transparent","0"),a.append("background","1");const l=a.toString();return l?`${o}?${l}`:o;case"mp4":case"webm":case"video":return s;default:return console.warn(`Unsupported video type: ${n}`),s}})(),I=()=>{L(!1)},R=()=>{w(!0),L(!1)},k=e=>{let{src:s,onLoad:t,onError:i}=e;return(0,a.jsx)("div",{className:o.customPlayer,children:(0,a.jsxs)("div",{className:o.customPlayerPlaceholder,children:[(0,a.jsx)("h3",{children:"Atlas Custom Player"}),(0,a.jsx)("p",{children:"Coming Soon"}),(0,a.jsxs)("a",{href:s,target:"_blank",rel:"noopener noreferrer",className:o.fallbackLink,children:["Watch on ",n.charAt(0).toUpperCase()+n.slice(1)]})]})})},P=["mp4","webm","video"].includes(n.toLowerCase());return(0,a.jsxs)("figure",{className:`${o.videoFigure} ${j?o.fullWidth:""}`,children:[(0,a.jsxs)("div",{className:`${o.videoContainer} ${(()=>{switch(m){case"4:3":return o.aspectRatio43;case"1:1":return o.aspectRatio11;case"21:9":return o.aspectRatio219;default:return o.aspectRatio169}})()}`,style:{width:j?"100%":f||"auto",maxWidth:j?"none":"800px"},children:[q&&!y&&(0,a.jsxs)("div",{className:o.loadingOverlay,children:[(0,a.jsx)("div",{className:o.spinner}),(0,a.jsx)("p",{children:"Loading video..."})]}),y&&(0,a.jsxs)("div",{className:o.errorContainer,children:[(0,a.jsxs)("svg",{className:o.errorIcon,width:"48",height:"48",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",children:[(0,a.jsx)("circle",{cx:"12",cy:"12",r:"10"}),(0,a.jsx)("line",{x1:"12",y1:"8",x2:"12",y2:"12"}),(0,a.jsx)("line",{x1:"12",y1:"16",x2:"12.01",y2:"16"})]}),(0,a.jsx)("h4",{children:"Failed to load video"}),(0,a.jsxs)("p",{children:["Video type: ",n]}),(0,a.jsxs)("p",{children:["Video ID/URL: ",s]}),(0,a.jsx)("a",{href:A,target:"_blank",rel:"noopener noreferrer",className:o.fallbackLink,children:"Try opening video directly"})]}),!y&&(P?(0,a.jsxs)("video",{className:o.videoElement,controls:u,autoPlay:d,onLoadedData:I,onError:R,title:c||l||`${n} video`,style:{width:f||"100%",height:g||"auto",display:q?"none":"block"},children:[(0,a.jsx)("source",{src:A,type:`video/${n}`}),(0,a.jsxs)("p",{children:["Your browser doesn't support video playback.",(0,a.jsx)("a",{href:A,target:"_blank",rel:"noopener noreferrer",children:"Download the video file"})]})]}):b?(0,a.jsx)(k,{src:A,onLoad:I,onError:R}):(0,a.jsx)("iframe",{className:o.videoIframe,src:A,title:c||l||`${n} video`,frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0,onLoad:I,onError:R,style:{width:f||"100%",height:g||"100%",opacity:q?0:1}}))]}),(0,a.jsx)(r.A,{caption:l,mediaType:"video",chapter:_,number:x,label:v})]})}}}]);
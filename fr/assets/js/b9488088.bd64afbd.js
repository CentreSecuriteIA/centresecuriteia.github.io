"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[2144],{5097:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>d,contentTitle:()=>c,default:()=>h,frontMatter:()=>u,metadata:()=>i,toc:()=>p});const i=JSON.parse('{"id":"chapters/01/3","title":"Intelligence","description":"\xc9tudes de cas","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/01/03.md","sourceDirName":"chapters/01","slug":"/chapters/01/03","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/01/03","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/01/03.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"3","title":"Intelligence","sidebar_label":"1.3 Intelligence","sidebar_position":4,"slug":"/chapters/01/03","reading_time_core":"11 min","reading_time_optional":"1 min","pagination_prev":"chapters/01/2","pagination_next":"chapters/01/4"},"sidebar":"docs","previous":{"title":"1.2 Mod\xe8les de fondation","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/01/02"},"next":{"title":"1.4 Mise \xe0 l\'\xe9chelle","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/01/04"}}');var t=n(4848),r=n(8453),a=n(2482),l=(n(8559),n(9585)),o=n(2501);const u={id:3,title:"Intelligence",sidebar_label:"1.3 Intelligence",sidebar_position:4,slug:"/chapters/01/03",reading_time_core:"11 min",reading_time_optional:"1 min",pagination_prev:"chapters/01/2",pagination_next:"chapters/01/4"},c="Intelligence",d={},p=[{value:"\xc9tudes de cas",id:"01",level:2},{value:"Mesure",id:"02",level:2}];function m(e){const s={a:"a",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{GlossaryTerm:n}=s;return n||function(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(s.header,{children:(0,t.jsx)(s.h1,{id:"intelligence",children:"Intelligence"})}),"\n",(0,t.jsx)(s.h2,{id:"01",children:"\xc9tudes de cas"}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Pourquoi devons-nous d\xe9finir l'intelligence ?"})," Dans notre section pr\xe9c\xe9dente sur les mod\xe8les fondamentaux, nous avons explor\xe9 comment les syst\xe8mes d'IA modernes deviennent de plus en plus puissants. Mais avant de pouvoir discuter de mani\xe8re significative des risques et des implications en mati\xe8re de s\xe9curit\xe9 de ces syst\xe8mes, nous devons nous accorder sur ce que nous entendons lorsque nous parlons d'AGI. Certains pensent que des \"\xe9tincelles\" d'AGI sont d\xe9j\xe0 pr\xe9sentes dans les derniers mod\xe8les de langage (",(0,t.jsx)(s.a,{href:"https://arxiv.org/abs/2303.12712",children:"Bubeck et al., 2023"}),"), tandis que d'autres pr\xe9disent une IA de niveau humain d'ici une d\xe9cennie (",(0,t.jsx)(s.a,{href:"https://arxiv.org/abs/2310.17688",children:"Bengio et al., 2024"}),"). Sans une d\xe9finition claire, comment sommes-nous cens\xe9s \xe9valuer de telles affirmations ou planifier des mesures de s\xe9curit\xe9 appropri\xe9es ?"]}),"\n",(0,t.jsx)(s.p,{children:"Le point essentiel est que si vous ne pouvez pas d\xe9finir quelque chose, vous ne pouvez pas le mesurer. Si vous ne pouvez pas le mesurer, vous ne pouvez pas suivre de mani\xe8re fiable les progr\xe8s ou identifier les risques potentiels. Prenez un exemple en physique - dire quelque chose comme \"il a boug\xe9 de 5\" n'a aucun sens sans pr\xe9ciser l'unit\xe9 de mesure. A-t-il boug\xe9 de 5 m\xe8tres, 5 pieds ou 5 coud\xe9es royales ? Personne ne le sait. Si nous ne savons pas jusqu'o\xf9 ou \xe0 quelle vitesse il s'est d\xe9plac\xe9, pouvons-nous alors imposer des limitations de vitesse ? Non plus. Il en va de m\xeame pour l'intelligence, et les risques et techniques de s\xe9curit\xe9 qui en d\xe9coulent. Tout comme la physique avait besoin d'unit\xe9s standardis\xe9es comme les m\xe8tres et les watts pour d\xe9passer les descriptions qualitatives, la recherche sur la s\xe9curit\xe9 de l'IA a besoin de d\xe9finitions rigoureuses pour aller au-del\xe0 des vagues analogies et anthropomorphismes."}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Qu'est-ce qui rend la d\xe9finition de l'intelligence si difficile ?"})," Si tout le monde s'accorde sur la n\xe9cessit\xe9 d'une d\xe9finition pour mesurer les progr\xe8s et concevoir des mesures de s\xe9curit\xe9, pourquoi n'avons-nous pas une d\xe9finition universellement accept\xe9e ? Le probl\xe8me est que le mot intelligence est un terme que nous utilisons pour d\xe9crire plusieurs capacit\xe9s qui se chevauchent - de la r\xe9solution de probl\xe8mes et l'apprentissage \xe0 l'adaptation et au raisonnement abstrait. En outre, diff\xe9rentes disciplines acad\xe9miques voient l'intelligence sous diff\xe9rents angles. Les psychologues mettent l'accent sur les comp\xe9tences cognitives mesurables, les informaticiens se concentrent sur la performance des t\xe2ches, et les philosophes d\xe9battent des qualit\xe9s comme la relation entre l'intelligence et la conscience et la conscience de soi. Alors quelle approche est la plus pertinente pour comprendre et planifier la s\xe9curit\xe9 de l'IA ?"]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"\xc9tude de cas : Approche de l'intelligence bas\xe9e sur l'imitation."})," Le Test de Turing (ou le jeu de l'imitation) sugg\xe9rait que l'intelligence pouvait \xeatre mesur\xe9e par la capacit\xe9 d'une machine \xe0 imiter la conversation humaine (",(0,t.jsx)(s.a,{href:"https://academic.oup.com/mind/article/LIX/236/433/986238?login=false",children:"Turing, 1950"}),"). Cependant, cette approche behavioriste s'est r\xe9v\xe9l\xe9e inad\xe9quate - les mod\xe8les de langage modernes peuvent souvent r\xe9ussir des tests de type Turing tout en manquant de capacit\xe9s de raisonnement fondamentales (",(0,t.jsx)(s.a,{href:"https://sciendo.com/issue/JAGI/11/2",children:"Rapaport, 2020"}),"). C'est aussi toujours une approche bas\xe9e sur le processus, et \xe9tait destin\xe9e principalement comme une exp\xe9rience de pens\xe9e philosophique plut\xf4t qu'une mesure concr\xe8te et op\xe9rationnalisable de l'intelligence."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"\xc9tude de cas : Approches de l'intelligence bas\xe9es sur la conscience."})," Une premi\xe8re vision se concentrait sur les machines qui pouvaient v\xe9ritablement comprendre et avoir des \xe9tats cognitifs similaires aux humains (",(0,t.jsx)(s.a,{href:"https://psycnet.apa.org/record/1981-27235-001",children:"Searle, 1980"}),"). Cependant, cette d\xe9finition s'av\xe8re probl\xe9matique \xe0 plusieurs niveaux. Premi\xe8rement, la conscience pourrait \xeatre l'une des rares notions plus difficiles \xe0 d\xe9finir que l'intelligence. Deuxi\xe8mement, nous ne sommes pas s\xfbrs si l'intelligence et la conscience sont n\xe9cessairement li\xe9es - un syst\xe8me pourrait potentiellement \xeatre tr\xe8s intelligent sans \xeatre conscient, ou conscient sans \xeatre particuli\xe8rement intelligent. AlphaGo est tr\xe8s intelligent dans le contexte du jeu de Go, mais il n'est clairement pas conscient. Un syst\xe8me n'a pas besoin d'\xeatre conscient pour causer des dommages. Que un syst\xe8me d'IA soit conscient ou non n'a aucune incidence sur sa capacit\xe9 \xe0 prendre des d\xe9cisions \xe0 fort impact ou \xe0 entreprendre des actions potentiellement dangereuses."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"\xc9tude de cas : Approches bas\xe9es sur le processus/l'adaptabilit\xe9."})," La vision bas\xe9e sur le processus consid\xe8re l'intelligence comme l'efficacit\xe9 de l'apprentissage et de l'adaptation, plut\xf4t que les capacit\xe9s accumul\xe9es. Quelques chercheurs adoptent cette vision de l'intelligence. Selon cette vision, l'intelligence est \"",(0,t.jsx)(s.em,{children:"la capacit\xe9 d'un syst\xe8me \xe0 s'adapter \xe0 son environnement tout en fonctionnant avec des connaissances et des ressources insuffisantes"}),'" (',(0,t.jsx)(s.a,{href:"https://sciendo.com/issue/JAGI/11/2",children:"Wang, 2020"}),'). Alternativement, elle est d\xe9crite comme "',(0,t.jsxs)(s.em,{children:["l'efficacit\xe9 avec laquelle un syst\xe8me peut ",(0,t.jsx)(n,{term:"Transformer",definition:'{"definition":"Une architecture de r\xe9seau neuronal qui utilise des m\xe9canismes d\'attention pour traiter des donn\xe9es s\xe9quentielles, particuli\xe8rement efficace pour les t\xe2ches linguistiques..","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:(0,t.jsx)(n,{term:"Transformer",definition:'{"definition":"Une architecture de r\xe9seau neuronal qui utilise des m\xe9canismes d\'attention pour traiter des donn\xe9es s\xe9quentielles, particuli\xe8rement efficace pour les t\xe2ches linguistiques..","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:"transformer"})})," l'exp\xe9rience et les a priori en comp\xe9tences"]}),'" (',(0,t.jsx)(s.a,{href:"https://arxiv.org/abs/1911.01547",children:"Chollet, 2019"}),"). Bien que cette focalisation sur le m\xe9ta-apprentissage et l'adaptation capture quelque chose de fondamental sur l'intelligence, du point de vue de la s\xe9curit\xe9, ce qui compte ultimement est ce que ces syst\xe8mes peuvent r\xe9ellement faire - leurs capacit\xe9s concr\xe8tes - plut\xf4t que la fa\xe7on dont ils atteignent ces capacit\xe9s. Cela nous m\xe8ne \xe0 l'approche finale."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"\xc9tude de cas : L'approche des capacit\xe9s de l'intelligence."})," La question motivante derri\xe8re cette vision est - Si un syst\xe8me d'IA peut effectuer des t\xe2ches dangereuses au niveau humain ou au-del\xe0, est-ce vraiment important qu'il y soit parvenu par des processus d'apprentissage sophistiqu\xe9s, une m\xe9morisation efficace, avec/sans conscience ? Si un syst\xe8me d'IA poss\xe8de des capacit\xe9s qui pourraient pr\xe9senter des risques - comme la planification sophistiqu\xe9e, la manipulation ou la tromperie - ces risques existent ind\xe9pendamment du fait que le syst\xe8me ait acquis ces capacit\xe9s par \"v\xe9ritable intelligence\", \"compr\xe9hension r\xe9elle\" ou correspondance de motifs sophistiqu\xe9e. L'approche bas\xe9e sur les capacit\xe9s transcende les d\xe9bats philosophiques en posant des questions concr\xe8tes : Que peut r\xe9ellement faire le syst\xe8me ? \xc0 quel point peut-il le faire ? Quelle gamme de t\xe2ches peut-il g\xe9rer ? Ce cadre fournit des normes claires pour le progr\xe8s et, crucialement pour le travail de s\xe9curit\xe9, des moyens clairs d'identifier les risques potentiels. La majorit\xe9 des laboratoires d'IA utilisent cette approche ax\xe9e sur les capacit\xe9s dans leur fa\xe7on de formuler leurs objectifs d'AGI. Par exemple, l'AGI a \xe9t\xe9 d\xe9finie comme \"",(0,t.jsx)(s.em,{children:"des syst\xe8mes hautement autonomes qui surpassent les humains dans la plupart des travaux \xe9conomiquement valorisables"}),'" (',(0,t.jsx)(s.a,{href:"https://openai.com/charter/",children:"OpenAI, 2014"}),"). Les consid\xe9rations de s\xe9curit\xe9 sont formul\xe9es de mani\xe8re similaire en disant que la mission est de s'assurer que \"",(0,t.jsx)(s.em,{children:"l'IA transformative aide les personnes et la soci\xe9t\xe9"}),'" (',(0,t.jsx)(s.a,{href:"https://www.anthropic.com/company",children:"Anthropic, 2024"}),")."]}),"\n",(0,t.jsx)(a.A,{speaker:"Victoria Krakovna",position:"Chercheur scientifique principal chez Google DeepMind",date:"Ao\xfbt 2023",source:"([Krakovna, 2023](https://www.alignmentforum.org/posts/JtuTQgp9Wnd6R6F5s/when-discussing-ai-risks-talk-about-capabilities-not))",children:(0,t.jsx)(s.p,{children:"Lorsqu'on discute des risques de l'IA, parlez des capacit\xe9s, pas de l'intelligence... Les gens ont souvent diff\xe9rentes d\xe9finitions de l'intelligence, ou l'associent \xe0 des concepts comme la conscience qui ne sont pas pertinents pour les risques de l'IA, ou rejettent les risques parce que l'intelligence n'est pas bien d\xe9finie."})}),"\n",(0,t.jsx)(s.p,{children:"Compte tenu de ces consid\xe9rations, pour la grande majorit\xe9 de ce livre, notre focus principal restera sur le cadre pratique des capacit\xe9s pour l'\xe9valuation et l'\xe9valuation de la s\xe9curit\xe9. Cette approche ax\xe9e sur les capacit\xe9s est la plus pertinente pour le travail imm\xe9diat sur la s\xe9curit\xe9, la r\xe9glementation et les d\xe9cisions de d\xe9ploiement. Nous reconnaissons que la recherche sur la conscience, la sensibilit\xe9, l'\xe9thique entourant les esprits num\xe9riques et la nature fondamentale de l'intelligence continue d'\xeatre pr\xe9cieuse mais est moins exploitable pour le travail imm\xe9diat sur la s\xe9curit\xe9."}),"\n",(0,t.jsx)(s.p,{children:'Dans notre prochaine sous-section, nous explorerons comment nous pouvons concr\xe8tement d\xe9finir et mesurer les capacit\xe9s dans ce cadre. Nous verrons comment aller au-del\xe0 des simples seuils binaires de l\'IA "\xe9troite" versus "g\xe9n\xe9rale" nous aide \xe0 mieux comprendre la progression des capacit\xe9s de l\'IA et leurs risques associ\xe9s.'}),"\n",(0,t.jsx)(s.h2,{id:"02",children:"Mesure"}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Pourquoi les d\xe9finitions traditionnelles de l'AGI sont-elles insuffisantes ?"})," Dans la section pr\xe9c\xe9dente, nous avons explor\xe9 comment les mod\xe8les fondamentaux deviennent de plus en plus puissants et polyvalents. Mais avant de pouvoir discuter de mani\xe8re significative des risques et des implications en mati\xe8re de s\xe9curit\xe9, ou faire des pr\xe9dictions sur les progr\xe8s futurs, nous avons besoin de moyens clairs pour mesurer et suivre les capacit\xe9s de l'IA. Cette section pr\xe9sente des cadres pour mesurer les progr\xe8s vers l'intelligence artificielle g\xe9n\xe9rale (AGI) et comprendre la relation entre les capacit\xe9s, l'autonomie et le risque. Par exemple, la d\xe9finition de l'AGI par OpenAI comme \"",(0,t.jsx)(s.em,{children:"des syst\xe8mes qui surpassent les humains dans la plupart des travaux \xe9conomiquement valorisables"}),'" (',(0,t.jsx)(s.a,{href:"https://openai.com/charter/",children:"OpenAI, 2014"}),'), ou la d\xe9finition couramment utilis\xe9e "',(0,t.jsx)(s.em,{children:"L'intelligence mesure la capacit\xe9 d'un agent \xe0 atteindre des objectifs dans un large \xe9ventail d'environnements."}),'" (',(0,t.jsx)(s.a,{href:"https://arxiv.org/abs/0706.3639",children:"Legg & Hutter, 2007"}),") et beaucoup d'autres ne sont pas assez sp\xe9cifiques pour \xeatre op\xe9rationnalisables. Quels humains ? Quels objectifs ? Quelles t\xe2ches sont \xe9conomiquement valorisables ? Qu'en est-il des syst\xe8mes qui d\xe9passent les performances humaines sur certaines t\xe2ches mais seulement pour de courtes dur\xe9es ?"]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Pourquoi avons-nous besoin de meilleurs cadres de mesure ?"}),' Historiquement, les discussions sur l\'AGI se sont souvent appuy\xe9es sur des seuils binaires - les syst\xe8mes \xe9taient cat\xe9goris\xe9s comme "\xe9troits" ou "g\xe9n\xe9raux", "faibles" ou "forts", "sous-humains" ou "de niveau humain". Bien que ces distinctions aient aid\xe9 \xe0 encadrer les premi\xe8res discussions sur l\'IA, elles deviennent de plus en plus inad\xe9quates \xe0 mesure que les syst\xe8mes d\'IA deviennent plus sophistiqu\xe9s. Tout comme nous avons \xe9vit\xe9 les d\xe9bats sur la question de savoir si les IA affichent une "v\xe9ritable intelligence" ou une "r\xe9elle compr\xe9hension" en faveur d\'un cadre plus pratique ax\xe9 sur les capacit\xe9s, nous voulons de m\xeame \xe9viter les d\xe9bats sur des questions comme celle de savoir si un syst\xe8me est "de niveau humain" ou non. Il est beaucoup plus pr\xe9cis de pouvoir faire des d\xe9clarations comme - il surpasse 75% des adultes qualifi\xe9s sur 30% des t\xe2ches cognitives.']}),"\n",(0,t.jsx)(o.A,{src:"./img/au9_Image_24.png",alt:"Entrer la description alternative de l'image",number:"17",label:"1.17",caption:"Voici l'aper\xe7u continu de la mesure de performance de l'IA. Tous les points sur cet axe peuvent \xeatre appel\xe9s intelligence artificielle \xe9troite (IAE) (sauf l'origine)."}),"\n",(0,t.jsx)(l.A,{term:"Artificial Narrow Intelligence (ANI)",source:"([IBM, 2023](https://www.ibm.com/topics/artificial-intelligence))",number:"1",label:"1.1",children:(0,t.jsx)(s.p,{children:"L'IA faible - \xe9galement appel\xe9e IA \xe9troite ou Intelligence Artificielle \xc9troite (ANI) - est une IA form\xe9e et concentr\xe9e pour effectuer des t\xe2ches sp\xe9cifiques. L'IA faible anime la plupart des IA qui nous entourent aujourd'hui. '\xc9troite' pourrait \xeatre un descripteur plus pr\xe9cis pour ce type d'IA car elle n'est en rien faible ; elle permet des applications tr\xe8s robustes, comme Siri d'Apple, Alexa d'Amazon, IBM Watson et les v\xe9hicules autonomes."})}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Niveaux d'intelligence artificielle \xe9troite (ANI)."})," Nous devrions consid\xe9rer la performance des syst\xe8mes d'IA sur un spectre continu. Les d\xe9finitions traditionnelles de l'ANI correspondent \xe0 une haute performance sur un tr\xe8s faible pourcentage de t\xe2ches. Par exemple, les moteurs d'\xe9checs comme AlphaZero surpassent 100% des humains, mais uniquement sur certaines t\xe2ches cognitives. De m\xeame, les syst\xe8mes sp\xe9cialis\xe9s de reconnaissance d'images pourraient surpasser 95% des humains, mais l\xe0 encore uniquement sur une infime fraction des t\xe2ches possibles. Selon la d\xe9finition ci-dessus, tous ces syst\xe8mes seraient d\xe9finis comme ANI, mais si nous les consid\xe9rons dans une gamme continue du pourcentage d'humains qualifi\xe9s qu'ils peuvent surpasser, nous obtenons une image beaucoup plus sp\xe9cifique et granulaire."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Comment pouvons-nous construire un meilleur cadre de mesure pour l'AGI ?"})," Nous devons suivre les progr\xe8s de l'IA selon deux axes - la performance (\xe0 quel point peut-elle faire les choses ?) et la g\xe9n\xe9ralit\xe9 (combien de choses diff\xe9rentes peut-elle faire ?). Tout comme nous pouvons d\xe9crire un point sur une carte en utilisant la latitude et la longitude, nous pouvons caract\xe9riser les syst\xe8mes d'AGI par leur niveau combin\xe9 de performance et leur degr\xe9 de g\xe9n\xe9ralit\xe9, mesur\xe9s par des r\xe9f\xe9rences et des \xe9valuations. Ce cadre nous donne une fa\xe7on beaucoup plus granulaire de suivre les progr\xe8s. Cette pr\xe9cision nous aide \xe0 mieux comprendre \xe0 la fois les capacit\xe9s actuelles et les trajectoires de d\xe9veloppement probables."]}),"\n",(0,t.jsx)(o.A,{src:"./img/AcI_Image_25.png",alt:"Entrer la description alternative de l'image",number:"18",label:"1.18",caption:"Tableau de performance x g\xe9n\xe9ralit\xe9 montrant \xe0 la fois les niveaux d'IAE et les niveaux d'IAG. ([Morris et al., 2024](https://arxiv.org/abs/2311.02462))"}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"O\xf9 se situent les syst\xe8mes d'IA actuels dans ce cadre ?"})," Les grands mod\xe8les de langage comme GPT-4 montrent un sch\xe9ma int\xe9ressant - ils surpassent environ 50% des adultes qualifi\xe9s sur peut-\xeatre 15-20% des t\xe2ches cognitives (comme l'\xe9criture de base et le codage), tout en \xe9galant ou d\xe9passant l\xe9g\xe8rement les performances humaines non qualifi\xe9es sur une plus large gamme de t\xe2ches. Cela nous donne une fa\xe7on plus pr\xe9cise de suivre les progr\xe8s que de simplement d\xe9battre si de tels syst\xe8mes se qualifient comme \"AGI\". Les LLM comme GPT-4 sont des formes pr\xe9coces d'AGI (",(0,t.jsx)(s.a,{href:"https://arxiv.org/abs/2303.12712",children:"Bubeck, 2023"}),"), et au fil du temps, nous atteindrons une AGI plus forte \xe0 mesure que la g\xe9n\xe9ralit\xe9 et la performance augmenteront. Pour comprendre comment ce cadre continu se rapporte aux d\xe9finitions traditionnelles, examinons comment les concepts historiques cl\xe9s se projettent sur notre espace performance-g\xe9n\xe9ralit\xe9."]}),"\n",(0,t.jsx)(o.A,{src:"./img/WSj_Image_26.png",alt:"Entrer la description alternative de l'image",number:"19",label:"1.19",caption:"La vue bidimensionnelle de la performance x g\xe9n\xe9ralit\xe9. Les diff\xe9rentes courbes color\xe9es sont destin\xe9es \xe0 repr\xe9senter les diff\xe9rents chemins que nous pouvons emprunter vers l'IAS. Chaque point sur le chemin correspond \xe0 un niveau diff\xe9rent d'IAG. La trajectoire de d\xe9veloppement sp\xe9cifique est difficile \xe0 pr\xe9voir. Cela sera discut\xe9 dans la section sur les pr\xe9visions et le d\xe9collage."}),"\n",(0,t.jsx)(l.A,{term:"Transformative AI (TAI)",source:"([Karnofsky, 2016](https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/))",number:"2",label:"1.2",children:(0,t.jsx)(s.p,{children:"Une IA future potentielle qui d\xe9clenche une transition \xe9quivalente ou plus significative que la r\xe9volution agricole ou industrielle."})}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Une autre fa\xe7on de construire un meilleur cadre de mesure pour l'AGI est le cadre t-n-AGI"})," (",(0,t.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi",children:"Ngo, 2023"}),"). Le cadre t-AGI mesure les progr\xe8s de l'IA en d\xe9finissant un syst\xe8me comme \"t-AGI\" s'il surpasse un expert humain sur une t\xe2che cognitive dans un laps de temps sp\xe9cifique 't' (par exemple, AGI d'une seconde, AGI d'une heure). Cela s'\xe9tend \xe0 \"(t,n)-AGI\", o\xf9 une IA surpasse 'n' experts humains collectivement dans le temps 't'. Bien que les t\xe2ches cognitives pr\xe9cises soient encore d\xe9battues (avec les r\xe9f\xe9rences ARC comme exemple), ce cadre permet une \xe9valuation granulaire des capacit\xe9s. Au T3 2023, les syst\xe8mes \xe9taient consid\xe9r\xe9s comme environ AGI d'une seconde, progressant vers AGI d'une minute, illustrant comment ce mod\xe8le suit le d\xe9veloppement de l'IA de la ma\xeetrise de t\xe2ches \xe9troites vers une r\xe9solution de probl\xe8mes plus large et plus complexe sur des p\xe9riodes d\xe9finies."]}),"\n",(0,t.jsx)(o.A,{src:"./img/LxT_Image_27.png",alt:"Entrer la description alternative de l'image",number:"20",label:"1.20",caption:"Le cadre t-n-IAG a \xe9t\xe9 tr\xe8s popularis\xe9 par certaines \xe9valuations de l'organisation METR. Capture d'\xe9cran de ([Kwa et al., 2025](https://arxiv.org/abs/2503.14499))."}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"IA Transformative (TAI)."})," L'IA transformative repr\xe9sente un point particuli\xe8rement int\xe9ressant dans notre cadre car elle n'est pas li\xe9e \xe0 des seuils sp\xe9cifiques de performance ou de g\xe9n\xe9ralit\xe9. Au lieu de cela, elle se concentre sur une gamme d'impacts. Par exemple, un syst\xe8me pourrait \xeatre transformatif en atteignant une performance mod\xe9r\xe9e (surpassant 60% des humains) sur une large gamme de t\xe2ches \xe9conomiquement importantes (50% des t\xe2ches cognitives), ou en atteignant une performance exceptionnelle (surpassant 99% des humains) sur un ensemble plus petit mais critique de t\xe2ches (20% des t\xe2ches cognitives) comme la R&D automatique."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"IA de Niveau Humain (HLAI)."})," Ce terme est parfois utilis\xe9 de mani\xe8re interchangeable avec AGI, et fait r\xe9f\xe9rence \xe0 un syst\xe8me d'IA qui \xe9gale l'intelligence humaine dans pratiquement tout travail \xe9conomiquement valorisable. Cependant, nous ne l'expliquons ici que pour des raisons d'exhaustivit\xe9. Le niveau humain n'est pas bien d\xe9fini, ce qui rend ces d\xe9finitions difficiles \xe0 op\xe9rationnaliser. Si nous projetons cela sur le cadre des niveaux d'AGI, cela correspondrait approximativement \xe0 surpasser 99% des adultes qualifi\xe9s dans la plupart des t\xe2ches cognitives non physiques."]}),"\n",(0,t.jsx)(l.A,{term:"Artificial Superintelligence (ASI)",source:"([Bostrom, 2014](https://psycnet.apa.org/record/2014-48585-000))",number:"3",label:"1.3",children:(0,t.jsx)(s.p,{children:"Toute intelligence qui d\xe9passe largement les performances cognitives des humains dans pratiquement tous les domaines d'int\xe9r\xeat."})}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Superintelligence Artificielle (ASI)."})," Si les syst\xe8mes atteignent des performances surhumaines dans toutes les t\xe2ches cognitives, ce serait alors la forme la plus forte d'AGI, \xe9galement appel\xe9e superintelligence. Dans notre cadre, l'ASI repr\xe9sente le coin sup\xe9rieur droit - des syst\xe8mes qui surpassent 100% des humains sur pr\xe8s de 100% des t\xe2ches cognitives."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Quelle est la relation entre les niveaux d'AGI et le risque ?"})," Comprendre les syst\xe8mes d'IA \xe0 travers des mesures continues de performance et de g\xe9n\xe9ralit\xe9 nous aide \xe0 mieux \xe9valuer les risques. Plut\xf4t que d'attendre que les syst\xe8mes franchissent un certain \"seuil d'AGI\", nous pouvons identifier des combinaisons sp\xe9cifiques de performance et de g\xe9n\xe9ralit\xe9 qui justifient des mesures de s\xe9curit\xe9 accrues. Par exemple :"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsx)(s.p,{children:"Un syst\xe8me atteignant 90% de performance sur 30% des t\xe2ches pourrait n\xe9cessiter des protocoles de s\xe9curit\xe9 diff\xe9rents de celui atteignant 60% de performance sur 70% des t\xe2ches"}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsx)(s.p,{children:'Certaines combinaisons de capacit\xe9s pourraient permettre des comportements \xe9mergents dangereux m\xeame avant d\'atteindre le "niveau humain" sur la plupart des t\xe2ches'}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsx)(s.p,{children:"Le taux d'am\xe9lioration sur l'un ou l'autre axe fournit des signaux importants sur la rapidit\xe9 avec laquelle des mesures de s\xe9curit\xe9 suppl\xe9mentaires doivent \xeatre d\xe9velopp\xe9es"}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,t.jsx)(s,{...e,children:(0,t.jsx)(m,{...e})}):m(e)}}}]);
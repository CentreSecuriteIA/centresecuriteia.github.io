"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[5648],{4823:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>u,contentTitle:()=>l,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"chapters/04/4","title":"Mise en \u0153uvre","description":"Normes de s\xe9curit\xe9 de l\'IA","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/04/04.md","sourceDirName":"chapters/04","slug":"/chapters/04/04","permalink":"/fr/chapters/04/04","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/04/04.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"4","title":"Mise en \u0153uvre","sidebar_label":"4.4 Mise en \u0153uvre","sidebar_position":5,"slug":"/chapters/04/04","reading_time_core":"11 min","reading_time_optional":"1 min","pagination_prev":"chapters/04/3","pagination_next":"chapters/04/5"},"sidebar":"docs","previous":{"title":"4.3 Architectures de gouvernance","permalink":"/fr/chapters/04/03"},"next":{"title":"4.5 Conclusion","permalink":"/fr/chapters/04/05"}}');var i=n(4848),r=n(8453),a=(n(2482),n(8559),n(9585),n(2501));const o={id:4,title:"Mise en \u0153uvre",sidebar_label:"4.4 Mise en \u0153uvre",sidebar_position:5,slug:"/chapters/04/04",reading_time_core:"11 min",reading_time_optional:"1 min",pagination_prev:"chapters/04/3",pagination_next:"chapters/04/5"},l="Mise en \u0153uvre",u={},c=[{value:"Normes de s\xe9curit\xe9 de l&#39;IA",id:"01",level:2},{value:"Visibilit\xe9 r\xe9glementaire",id:"02",level:2},{value:"Assurer la conformit\xe9",id:"03",level:2},{value:"Limitations et Compromis",id:"04",level:2}];function d(e){const s={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"mise-en-\u0153uvre",children:"Mise en \u0153uvre"})}),"\n",(0,i.jsx)(s.h2,{id:"01",children:"Normes de s\xe9curit\xe9 de l'IA"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Quelles approches existent pour d\xe9velopper des normes de s\xe9curit\xe9 de l'IA au niveau national ?"})," Diverses approches pour d\xe9velopper des normes de s\xe9curit\xe9 existent dans les contextes nationaux, des organismes de normalisation gouvernementaux aux processus collaboratifs public-priv\xe9. Les organismes nationaux de normalisation jouent un r\xf4le crucial dans le d\xe9veloppement et la mise en \u0153uvre de normes de s\xe9curit\xe9 de l'IA qui s'alignent sur les priorit\xe9s politiques et les capacit\xe9s technologiques de chaque pays (",(0,i.jsx)(s.a,{href:"https://www.governance.ai/research-paper/standards-for-ai-governance-international-standards-to-enable-global-coordination-in-ai-research-development",children:"Cihon, 2019"}),"). L'AI Act de l'UE le d\xe9montre \xe0 travers son exigence d'un Code de Pratique qui sp\xe9cifie des obligations de haut niveau pour les mod\xe8les d'IA \xe0 usage g\xe9n\xe9ral. Aux \xc9tats-Unis, le National Institute of Standards and Technology (NIST) a d\xe9velopp\xe9 un Cadre de Gestion des Risques de l'IA qui sert de norme volontaire dans la juridiction am\xe9ricaine. En 2021, l'Administration de Normalisation de Chine (SAC) a publi\xe9 une feuille de route pour le d\xe9veloppement des normes d'IA qui comprend plus de 100 sp\xe9cifications techniques et \xe9thiques, de la transparence algorithmique \xe0 la s\xe9curit\xe9 de la reconnaissance biom\xe9trique. Coordonn\xe9 par des agences gouvernementales comme le Minist\xe8re de l'Industrie et des Technologies de l'Information (MIIT) et l'Institut Chinois de Normalisation \xc9lectronique (CESI). Contrairement aux \xc9tats-Unis ou \xe0 l'UE, o\xf9 les normes sont souvent d\xe9velopp\xe9es par plusieurs parties prenantes ou guid\xe9es par le march\xe9, le processus chinois est hautement centralis\xe9 et \xe9troitement li\xe9 \xe0 ses ambitions g\xe9opolitiques plus larges (",(0,i.jsx)(s.a,{href:"https://www.fhi.ox.ac.uk/wp-content/uploads/Deciphering_Chinas_AI-Dream.pdf",children:"Ding, 2018"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Comment les organismes nationaux de normalisation d\xe9veloppent-ils des normes de s\xe9curit\xe9 de l'IA efficaces ?"})," Les normes nationales ont de l'exp\xe9rience dans la gouvernance de diverses questions socio-techniques au sein de leurs pays. Par exemple, les normes nationales de cybers\xe9curit\xe9 se sont r\xe9pandues dans les industries, les normes de durabilit\xe9 environnementale ont suscit\xe9 d'importants investissements des entreprises, et les normes de s\xe9curit\xe9 ont \xe9t\xe9 mises en \u0153uvre dans des secteurs allant de l'automobile \xe0 l'\xe9nergie. L'expertise d'autres industries \xe0 haut risque peut \xeatre exploit\xe9e pour d\xe9velopper des normes de s\xe9curit\xe9 de l'IA efficaces adapt\xe9es aux besoins sp\xe9cifiques et \xe0 l'environnement r\xe9glementaire d'un pays (",(0,i.jsx)(s.a,{href:"https://www.governance.ai/research-paper/standards-for-ai-governance-international-standards-to-enable-global-coordination-in-ai-research-development",children:"Cihon, 2019"}),"). Les normes nationales peuvent \xeatre utilis\xe9es pour diffuser une culture de la s\xe9curit\xe9 et de la responsabilit\xe9 dans la recherche et le d\xe9veloppement de l'IA de quatre mani\xe8res :"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Les crit\xe8res au sein des normes \xe9tablissent des r\xe8gles et des attentes pour les pratiques de s\xe9curit\xe9 au sein de l'\xe9cosyst\xe8me national de l'IA."}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Les normes int\xe8grent les chercheurs individuels et les organisations dans un r\xe9seau plus large de responsabilit\xe9 nationale."}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"La mise en \u0153uvre r\xe9guli\xe8re des normes aide les chercheurs \xe0 int\xe9rioriser les routines de s\xe9curit\xe9 comme partie int\xe9grante de la pratique standard."}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Lorsque les normes sont int\xe9gr\xe9es dans les produits et les progiciels, elles renforcent les consid\xe9rations de s\xe9curit\xe9 ind\xe9pendamment des organisations nationales qui utilisent le syst\xe8me."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["Ces m\xe9canismes contribuent \xe0 cr\xe9er ce que certains chercheurs appellent une \"mentalit\xe9 de s\xe9curit\xe9\" parmi les praticiens de l'IA au sein de l'\xe9cosyst\xe8me national de l'IA. Les normes nationales servent d'outils efficaces pour favoriser une culture de la responsabilit\xe9 et de la s\xe9curit\xe9 dans le d\xe9veloppement de l'IA, ce qui est essentiel pour le b\xe9n\xe9fice soci\xe9tal \xe0 long terme (",(0,i.jsx)(s.a,{href:"https://www.governance.ai/research-paper/standards-for-ai-governance-international-standards-to-enable-global-coordination-in-ai-research-development",children:"Cihon, 2019"}),")."]}),"\n",(0,i.jsx)(s.h2,{id:"02",children:"Visibilit\xe9 r\xe9glementaire"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La visibilit\xe9 r\xe9glementaire n\xe9cessite un examen actif et ind\xe9pendant des syst\xe8mes d'IA avant, pendant et apr\xe8s leur d\xe9ploiement."})," Alors que les syst\xe8mes d'IA fronti\xe8re sont de plus en plus int\xe9gr\xe9s dans la soci\xe9t\xe9, l'examen externe (impliquant des acteurs ext\xe9rieurs dans l'\xe9valuation des syst\xe8mes d'IA) offre un outil puissant pour am\xe9liorer la s\xe9curit\xe9 et la responsabilit\xe9. L'examen externe efficace doit adh\xe9rer au cadre ASPIRE, qui propose six crit\xe8res pour une \xe9valuation externe efficace (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2311.14711",children:"Anderljung et al., 2023"}),") :"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Acc\xe8s :"})," Les examinateurs externes ont besoin d'un acc\xe8s appropri\xe9 aux syst\xe8mes d'IA et aux informations pertinentes."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Attitude de recherche :"})," Les examinateurs doivent activement rechercher les probl\xe8mes et vuln\xe9rabilit\xe9s potentiels."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Proportionnalit\xe9 aux risques :"})," Le niveau d'examen doit correspondre aux risques potentiels pos\xe9s par le syst\xe8me."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Ind\xe9pendance :"})," Les examinateurs doivent \xeatre libres de toute influence indue des d\xe9veloppeurs d'IA."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Ressources :"})," Des ressources ad\xe9quates doivent soutenir un examen approfondi."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Expertise :"})," Les examinateurs doivent poss\xe9der l'expertise technique et sp\xe9cifique au domaine n\xe9cessaire."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["Certains pays explorent les registres de mod\xe8les, qui sont des bases de donn\xe9es centralis\xe9es incluant les d\xe9tails architecturaux, les proc\xe9dures d'entra\xeenement, les m\xe9triques de performance et les \xe9valuations d'impact soci\xe9tal. Ces registres soutiennent une surveillance structur\xe9e et peuvent agir comme syst\xe8mes d'alerte pr\xe9coce pour les capacit\xe9s \xe9mergentes, aidant les r\xe9gulateurs \xe0 d\xe9tecter les tendances dangereuses avant qu'elles ne se mat\xe9rialisent en dommages (",(0,i.jsx)(s.a,{href:"https://www.convergenceanalysis.org/research/ai-model-registries-a-foundational-tool-for-ai-governance",children:"McKernon et al., 2024"}),"). Diff\xe9rentes juridictions adoptent diff\xe9rentes approches, mais la documentation des mod\xe8les comprend g\xe9n\xe9ralement :"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Documentation de base (identification du mod\xe8le, cas d'utilisation pr\xe9vus)"}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Sp\xe9cification technique (architecture, param\xe8tres, exigences computationnelles)"}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Documentation des performances (r\xe9sultats des tests, \xe9valuations des capacit\xe9s)"}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"\xc9valuation d'impact (effets soci\xe9taux, implications pour la s\xe9curit\xe9, consid\xe9rations \xe9thiques)"}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Documentation du d\xe9ploiement (strat\xe9gies de mise en \u0153uvre, plans de surveillance)"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Une autre m\xe9thode de visibilit\xe9 r\xe9glementaire pour l'IA est le syst\xe8me Know Your Customer (KYC)."})," Les syst\xe8mes KYC font d\xe9j\xe0 partie int\xe9grante de la r\xe9glementation financi\xe8re, utilis\xe9s pour d\xe9tecter et pr\xe9venir le blanchiment d'argent et le financement du terrorisme. Ils ont prouv\xe9 leur efficacit\xe9 dans leur capacit\xe9 \xe0 identifier les acteurs \xe0 haut risque avant qu'une transaction n'ait lieu. Le m\xeame principe peut \xeatre appliqu\xe9 \xe0 l'acc\xe8s aux ressources de calcul. Comme discut\xe9 dans la section sur la gouvernance du calcul, les mod\xe8les fronti\xe8res n\xe9cessitent des ressources computationnelles massives, souvent concentr\xe9es dans un petit nombre de fournisseurs hyperscale qui servent de points de contr\xf4le r\xe9glementaires naturels. Un syst\xe8me KYC pour l'IA permettrait aux gouvernements de d\xe9tecter t\xf4t le d\xe9veloppement de syst\xe8mes potentiellement dangereux, d'emp\xeacher l'entra\xeenement clandestin de mod\xe8les et de mettre en \u0153uvre des contr\xf4les \xe0 l'exportation ou des exigences de licence avec plus de pr\xe9cision. Puisque cette approche cible les seuils de capacit\xe9 plut\xf4t que les cas d'utilisation, elle pourrait servir d'outil pr\xe9ventif pour la gestion des risques plut\xf4t que r\xe9actif aux \xe9checs de d\xe9ploiement (",(0,i.jsx)(s.a,{href:"https://www.governance.ai/research-paper/oversight-for-frontier-ai-through-kyc-scheme-for-compute-providers",children:"Egan & Heim, 2023"}),"). Cependant, la mise en \u0153uvre d'un r\xe9gime KYC pour le calcul soul\xe8ve plusieurs questions ouvertes. Les fournisseurs auraient besoin de mandats l\xe9gaux clairs, de crit\xe8res techniques pour la v\xe9rification des clients et de processus pour escalader les cas \xe0 haut risque aux autorit\xe9s. La fragmentation juridictionnelle est un d\xe9fi. De nombreux d\xe9veloppeurs s'appuient sur des services de calcul distribu\xe9s mondialement, et sans coop\xe9ration internationale, les r\xe9gimes KYC risquent d'\xeatre sap\xe9s par l'arbitrage r\xe9glementaire. Pour \xeatre efficace, un syst\xe8me KYC bas\xe9 sur le calcul devrait s'aligner avec d'autres m\xe9canismes de transparence, tels que les registres de mod\xe8les et les syst\xe8mes de signalement d'incidents (",(0,i.jsx)(s.a,{href:"https://www.governance.ai/research-paper/oversight-for-frontier-ai-through-kyc-scheme-for-compute-providers",children:"Egan & Heim, 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Comment les politiques nationales peuvent-elles soutenir le partage responsable d'informations ?"})," Le signalement responsable d'informations est important tant pour l'autor\xe9gulation que pour la surveillance gouvernementale. Comme nous l'avons discut\xe9 dans la section sur la gouvernance d'entreprise, les entreprises d\xe9veloppant et d\xe9ployant des syst\xe8mes d'IA fronti\xe8re ont un acc\xe8s primaire aux informations sur les capacit\xe9s et les risques potentiels de leurs syst\xe8mes, et le partage responsable de ces informations peut am\xe9liorer significativement la capacit\xe9 de l'\xc9tat \xe0 g\xe9rer les risques li\xe9s \xe0 l'IA (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2404.02675",children:"Kolt et al., 2024"}),"). Les politiques nationales doivent aborder la tension entre transparence et contr\xf4le propri\xe9taire. Une approche est la divulgation par niveaux, dans laquelle la documentation technique est fournie aux r\xe9gulateurs sous accords de confidentialit\xe9 tandis que la communication publique reste g\xe9n\xe9rale et ax\xe9e sur les risques. Une autre approche passe par le partage anonymis\xe9 ou agr\xe9g\xe9 des donn\xe9es, qui permet une compr\xe9hension statistique sans r\xe9v\xe9ler les d\xe9tails sensibles de mise en \u0153uvre."]}),"\n",(0,i.jsxs)(s.p,{children:["Bien que les syst\xe8mes de signalement d'incidents d'autres industries, comme le syst\xe8me confidentiel et non punitif de signalement de la s\xe9curit\xe9 a\xe9rienne (ASRS) aux \xc9tats-Unis, offrent des pr\xe9c\xe9dents utiles, aucun syst\xe8me \xe9quivalent n'existe encore pour l'IA. Dans l'aviation, il est clair ce qui constitue un incident ou un quasi-accident, mais avec l'IA, les lignes peuvent \xeatre floues. Adapter ce mod\xe8le n\xe9cessiterait des d\xe9finitions claires de ce qui constitue un \"incident\", avec des cat\xe9gories structur\xe9es allant du dysfonctionnement du mod\xe8le aux pr\xe9judices soci\xe9taux. Les efforts nationaux actuels sur ce point sont fragment\xe9s. Dans l'UE, la loi sur l'IA impose le signalement des \"incidents graves\" par les d\xe9veloppeurs d'IA \xe0 haut risque et \xe0 usage g\xe9n\xe9ral. En Chine, l'Administration du cyberespace construit une infrastructure centralis\xe9e pour le signalement en temps r\xe9el des d\xe9faillances critiques selon la loi sur la cybers\xe9curit\xe9. Aux \xc9tats-Unis, le signalement des incidents reste sp\xe9cifique au secteur, avec des efforts pr\xe9liminaires en cours dans la sant\xe9 et la s\xe9curit\xe9 nationale (",(0,i.jsx)(s.a,{href:"https://www.pourdemain.ngo/en/post/learning-from-history-gpai-serious-incident-reporting",children:"Farrell, 2024"})," ; ",(0,i.jsx)(s.a,{href:"https://www.convergenceanalysis.org/ai-regulatory-landscape/ai-incident-reporting",children:"Cheng, 2024"})," ; ",(0,i.jsx)(s.a,{href:"https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/02/towards-a-common-reporting-framework-for-ai-incidents_8c488fdb/f326d4ac-en.pdf",children:"OECD, 2025"}),")."]}),"\n",(0,i.jsx)(s.h2,{id:"03",children:"Assurer la conformit\xe9"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Quels outils r\xe9glementaires peuvent garantir la conformit\xe9 aux normes de s\xe9curit\xe9 de l'IA ?"})," Pour les syst\xe8mes d'IA \xe0 haut risque, les m\xe9canismes de surveillance doivent aller au-del\xe0 des normes volontaires ou des \xe9valuations ponctuelles. De nombreux chercheurs ont propos\xe9 des r\xe9gimes de licences qui refl\xe9teraient les pratiques r\xe9glementaires dans des secteurs comme les produits pharmaceutiques ou l'\xe9nergie nucl\xe9aire. Dans ces domaines, les op\xe9rateurs doivent obtenir et maintenir des licences en d\xe9montrant une conformit\xe9 continue aux exigences strictes de s\xe9curit\xe9 et de documentation. Appliqu\xe9e \xe0 l'IA fronti\xe8re, cette approche impliquerait des processus d'approbation formels avant le d\xe9ploiement des mod\xe8les, des audits p\xe9riodiques et la possibilit\xe9 pour les autorit\xe9s de r\xe9voquer les licences en cas de non-conformit\xe9 (",(0,i.jsx)(s.a,{href:"https://www.researchgate.net/publication/385353725_Safety_cases_for_frontier_AI",children:"Buhl et al., 2024"}),"). Un cadre de licence cr\xe9dible exigerait des d\xe9veloppeurs qu'ils soumettent un dossier de s\xe9curit\xe9 structur\xe9, qui est un argument formel \xe9tay\xe9 par des preuves montrant qu'un syst\xe8me r\xe9pond aux seuils de s\xe9curit\xe9 pour le d\xe9ploiement. Cela pourrait inclure la mod\xe9lisation des menaces, les r\xe9sultats du red-teaming, les \xe9valuations d'interpr\xe9tabilit\xe9 et les plans de surveillance post-d\xe9ploiement. Les dossiers de s\xe9curit\xe9 fournissent un m\xe9canisme pour l'approbation ex ante et pour v\xe9rifier si les affirmations de s\xe9curit\xe9 continuent de tenir \xe0 mesure que les syst\xe8mes \xe9voluent lors du d\xe9ploiement. L'int\xe9gration de ces exigences dans le processus de licence peut aider les gouvernements \xe0 \xe9tablir un cycle continu d'examen, de retour et de v\xe9rification technique (",(0,i.jsx)(s.a,{href:"https://www.researchgate.net/publication/385353725_Safety_cases_for_frontier_AI",children:"Buhl et al., 2024"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Comment l'application fonctionnerait-elle en pratique ?"})," Les cadres de licence doivent \xeatre soutenus par des agences ayant le pouvoir d'enqu\xeater sur les violations, d'imposer des sanctions et de suspendre le d\xe9veloppement. Les pratiques nationales d'application varient entre la gouvernance horizontale (application de r\xe8gles g\xe9n\xe9rales \xe0 tous les secteurs) et les r\xe9gimes verticaux (ciblant des domaines sp\xe9cifiques comme la sant\xe9 ou la finance) (",(0,i.jsx)(s.a,{href:"https://www.convergenceanalysis.org/ai-regulatory-landscape/home",children:"Cheng & McKernon, 2024"}),"). Par exemple, la loi sur l'IA de l'Union europ\xe9enne \xe9tablit une autorit\xe9 d'application \xe0 travers un cadre de gouvernance horizontale avec le Bureau europ\xe9en de l'IA, qui peut enqu\xeater, imposer des amendes allant jusqu'\xe0 3 % du chiffre d'affaires annuel mondial et exiger des mesures correctives, combin\xe9 \xe0 des rapports obligatoires d'incidents, des exigences d'att\xe9nuation des risques syst\xe9miques et des Codes de pratique de soutien pour les mod\xe8les GPAI (",(0,i.jsx)(s.a,{href:"https://www.convergenceanalysis.org/ai-regulatory-landscape/home",children:"Cheng & McKernon, 2024"}),"). En revanche, l'Administration du cyberespace de Chine (CAC) exerce des pouvoirs d'application centralis\xe9s dans le cadre d'un r\xe9gime r\xe9glementaire vertical. Bien que son approche privil\xe9gie l'intervention rapide et la conformit\xe9 \xe0 la censure, la CAC manque de contr\xf4les proc\xe9duraux transparents et s'appuie souvent sur des crit\xe8res vagues pour l'application. Aux \xc9tats-Unis, l'application est fragment\xe9e. Bien que les contr\xf4les \xe0 l'exportation soient strictement appliqu\xe9s par des agences comme le D\xe9partement du Commerce, la conformit\xe9 plus large en mati\xe8re de s\xe9curit\xe9 de l'IA a \xe9t\xe9 d\xe9l\xe9gu\xe9e \xe0 des agences individuelles, sans autorit\xe9 nationale de licence. En cons\xe9quence, les actions d'application sont souvent r\xe9actives et sp\xe9cifiques au domaine, et s'appuient sur des pouvoirs ex\xe9cutifs discr\xe9tionnaires (",(0,i.jsx)(s.a,{href:"https://www.convergenceanalysis.org/ai-regulatory-landscape/home",children:"Cheng & McKernon, 2024"}),"). Trouver le bon \xe9quilibre entre ces approches d\xe9pendra de la capacit\xe9 institutionnelle, des incitations des d\xe9veloppeurs et du rythme des avanc\xe9es de l'IA. Dans certains cas, l'utilisation des autorit\xe9s sectorielles existantes peut suffire. Dans d'autres cas, de nouvelles institutions seront n\xe9cessaires pour g\xe9rer les capacit\xe9s \xe0 usage g\xe9n\xe9ral qui sortent des cat\xe9gories r\xe9glementaires traditionnelles (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2307.04699",children:"Dafoe, 2023"}),")."]}),"\n",(0,i.jsx)(a.A,{src:"./img/bFY_Image_36.png",alt:"Entrer la description alternative de l'image",number:"30",label:"4.30",caption:"Le flux des cas de s\xe9curit\xe9 vers l'application."}),"\n",(0,i.jsx)(s.h2,{id:"04",children:"Limitations et Compromis"}),"\n",(0,i.jsxs)(s.p,{children:["Chaque approche de gouvernance fait face \xe0 des contraintes fondamentales qu'aucune conception institutionnelle ne peut totalement surmonter. Comprendre ces limitations aide \xe0 \xe9tablir des attentes r\xe9alistes et identifie o\xf9 l'innovation est la plus n\xe9cessaire (",(0,i.jsx)(s.a,{href:"https://cdn.governance.ai/GovAI-Research-Agenda.pdf",children:"Dafoe, 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Certains risques r\xe9sistent aux solutions techniques."})," Malgr\xe9 les avanc\xe9es en interpr\xe9tabilit\xe9 et en \xe9valuation, nous ne pouvons toujours pas comprendre ou pr\xe9dire pleinement le comportement de l'IA. Les mod\xe8les de type bo\xeete noire rendent la v\xe9rification difficile. Des capacit\xe9s \xe9mergentes apparaissent de mani\xe8re inattendue. L'\xe9cart entre nos ambitions de gouvernance et nos capacit\xe9s techniques est consid\xe9rable (",(0,i.jsx)(s.a,{href:"https://www.researchgate.net/publication/382885035_Reasons_to_Doubt_the_Impact_of_AI_Risk_Evaluations#:~:text=Improved%20understanding%20may%20also%20not,opportunity%20costs%20for%20AI%20safety.",children:"Mukobi, 2024"}),"). Les techniques de s\xe9curit\xe9 actuelles comme le RLHF et l'IA constitutionnelle sont prometteuses pour les mod\xe8les d'aujourd'hui mais pourraient \xe9chouer catastrophiquement avec des syst\xe8mes plus puissants. Nous construisons des cadres de gouvernance autour d'approches de s\xe9curit\xe9 qui pourraient devenir obsol\xe8tes. Cette incertitude fondamentale n\xe9cessite des cadres adaptatifs qui peuvent \xe9voluer avec la compr\xe9hension (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2407.21792",children:"Ren et al., 2024"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les d\xe9fis de mesure compromettent la responsabilisation."})," Nous manquons de m\xe9triques robustes pour de nombreuses propri\xe9t\xe9s li\xe9es \xe0 la s\xe9curit\xe9. Comment mesurer la tendance d'un mod\xe8le \xe0 la tromperie ? Son potentiel d'am\xe9lioration autonome ? Sa r\xe9sistance aux utilisations abusives ? Sans mesures fiables, la conformit\xe9 devient une question d'interpr\xe9tation plut\xf4t que de v\xe9rification (",(0,i.jsx)(s.a,{href:"https://www.aisnakeoil.com/p/ai-safety-is-not-a-model-property",children:"Narayan & Kapoor, 2024"}),"). La loi europ\xe9enne sur l'IA, par exemple, exige des \xe9valuations des \"risques syst\xe9miques\", mais fournit peu d'indications sur la fa\xe7on de mesurer ces risques quantitativement (",(0,i.jsx)(s.a,{href:"https://www.researchgate.net/publication/387399002_The_First_Global_AI_Treaty_Analyzing_the_Framework_Convention_on_Artificial_Intelligence_and_the_EU_AI_Act",children:"Cheng, 2024"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les p\xe9nuries d'expertise cr\xe9ent des goulots d'\xe9tranglement critiques."})," Le nombre d'individus qui comprennent profond\xe9ment \xe0 la fois les syst\xe8mes d'IA avanc\xe9s et la gouvernance reste extr\xeamement limit\xe9, et cet \xe9cart existe \xe0 tous les niveaux, des \xe9quipes de s\xe9curit\xe9 des entreprises aux r\xe9gulateurs et aux organismes internationaux. Un manque de talent interdisciplinaire compromet les efforts pour anticiper et g\xe9rer les risques \xe9mergents (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1802.07228",children:"Brundage et al., 2018"}),"). La capacit\xe9 institutionnelle d'\xe9valuation technique et de surveillance est \xe9galement faible dans de nombreuses juridictions (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2012.06505",children:"Cihon et al., 2021"}),"). Les gouvernements peinent \xe0 attirer et retenir l'expertise n\xe9cessaire pour r\xe9guler les mod\xe8les d'IA puissants, et les professionnels techniquement comp\xe9tents et conscients de la gouvernance pourraient \xeatre la contrainte la plus s\xe9rieuse pour une gouvernance efficace de l'IA (",(0,i.jsx)(s.a,{href:"https://cdn.governance.ai/GovAI-Research-Agenda.pdf",children:"Dafoe, 2023"}),"; ",(0,i.jsx)(s.a,{href:"https://cdn.governance.ai/Open_Problems_in_Technical_AI_Governance.pdf",children:"Reuel & Bucknall, 2024"}),"). Une grande partie du talent existant est concentr\xe9e dans quelques entreprises dominantes, limitant la surveillance du secteur public et renfor\xe7ant les asym\xe9tries dans la capacit\xe9 de gouvernance (",(0,i.jsx)(s.a,{href:"https://ainowinstitute.org/publications/research/ai-now-2025-landscape-report",children:"Brennan et al., 2025"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les co\xfbts de coordination augmentent plus rapidement que les capacit\xe9s."})," Chaque partie prenante, exigence et processus de r\xe9vision suppl\xe9mentaire ajoute de la friction au d\xe9veloppement de l'IA (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2212.08364",children:"Schuett, 2023"}),"). Bien qu'une certaine friction aide \xe0 assurer la s\xe9curit\xe9, une bureaucratie excessive peut pousser le d\xe9veloppement vers des acteurs moins responsables ou totalement souterrains (",(0,i.jsx)(s.a,{href:"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5241351",children:"Zhang et al., 2025"}),"). Les d\xe9calages de vitesse cr\xe9ent des lacunes fondamentales dans la gouvernance. Les capacit\xe9s de l'IA progressent en mois tandis que les accords internationaux prennent des ann\xe9es \xe0 n\xe9gocier (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2401.02843",children:"Grace et al., 2024"}),"). Les capacit\xe9s de GPT-4 ont surpris les experts en mars 2023 ; le temps que les r\xe9ponses r\xe9glementaires \xe9mergent en 2024, la technologie avait \xe9volu\xe9 vers des syst\xe8mes multimodaux et des agents IA (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2307.15217",children:"Casper et al., 2024"}),"). Les chercheurs en s\xe9curit\xe9 mettent l'accent sur la pr\xe9caution et les sc\xe9narios les plus d\xe9favorables, les entreprises privil\xe9gient la position concurrentielle et le d\xe9lai de mise sur le march\xe9, les gouvernements \xe9quilibrent plusieurs circonscriptions aux demandes contradictoires, et les utilisateurs veulent des capacit\xe9s b\xe9n\xe9fiques sans comprendre les risques (",(0,i.jsx)(s.a,{href:"https://cdn.governance.ai/GovAI-Research-Agenda.pdf",children:"Dafoe, 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'arbitrage r\xe9glementaire compromet les normes de s\xe9curit\xe9 \xe0 travers les fronti\xe8res."})," Si l'Europe met en \u0153uvre des exigences de s\xe9curit\xe9 strictes tandis que d'autres r\xe9gions restent permissives, le d\xe9veloppement peut simplement changer de lieu (",(0,i.jsx)(s.a,{href:"https://scholarship.law.georgetown.edu/facpub/2647/",children:"Lancieri et al., 2024"}),"). Comme nous l'avons pr\xe9c\xe9demment discut\xe9 dans la section sur la prolif\xe9ration, la nature num\xe9rique de l'IA fait qu'un mod\xe8le peut \xeatre entra\xeen\xe9 \xe0 Singapour, d\xe9ploy\xe9 depuis l'Irlande et utilis\xe9 mondialement (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2303.12642",children:"Seger et al., 2023"}),"). Les entreprises peuvent bifurquer leurs offres, fournissant des syst\xe8mes plus s\xfbrs aux march\xe9s r\xe9glement\xe9s tout en d\xe9ployant des versions plus risqu\xe9es ailleurs. Une v\xe9ritable couverture mondiale n\xe9cessite plus que de puissantes juridictions individuelles."]})]})}function p(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);
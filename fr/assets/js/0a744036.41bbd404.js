"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[4852],{4553:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>p});const t=JSON.parse('{"id":"chapters/06/index","title":"Mauvaise sp\xe9cification","description":"Apprentissage par renforcement : Le chapitre commence par un rappel de certains concepts d\'apprentissage par renforcement. Cela inclut un aper\xe7u rapide du concept de r\xe9compenses et des fonctions de r\xe9compense. Cette section pose les bases pour expliquer pourquoi la conception des r\xe9compenses est extr\xeamement importante.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/06/index.md","sourceDirName":"chapters/06","slug":"/chapters/06/","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/06/","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/06/index.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Mauvaise sp\xe9cification","chapter_number":6,"reading_time_core":"55 min","reading_time_optional":"3 min","authors":["Markov Grey","Charbel-Raphael Segerie"],"affiliations":["Centre Fran\xe7ais pour la S\xe9curit\xe9 de l\'IA (CeSIA)"],"acknowledgements":["Jeanne Salle","Oscar Heitmann","Ram Rachum","Nicolas Guillard","Camille Berger"],"atlas_link":"https://ai-safety-atlas.com/chapters/06/","alignment_forum_link":"https://www.lesswrong.com/s/3ni2P2GZzBvNebWYZ/p/mMBoPnFrFqQJKzDsZ","google_docs_link":"https://docs.google.com/document/d/1kEdmyVTUG3MO7lwuw4utHEm7CcavvgAiUZcWHaOZuPY/edit?usp=sharing","feedback_link":"https://forms.gle/ZsA4hEWUx1ZrtQLL9","teach_link":"https://docs.google.com/document/d/1im_i6e9xEAe-koYlurYdn26n9h7pFX2HksnRfQmWxTQ/edit?usp=sharing","sidebar_position":6,"slug":"/chapters/06/"},"sidebar":"docs","previous":{"title":"5.10 Conclusion","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/05/10"},"next":{"title":"6.1 Apprentissage par renforcement","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/06/01"}}');var i=n(4848),a=n(8453);n(2482),n(8559),n(9585);const r={title:"Mauvaise sp\xe9cification",chapter_number:6,reading_time_core:"55 min",reading_time_optional:"3 min",authors:["Markov Grey","Charbel-Raphael Segerie"],affiliations:["Centre Fran\xe7ais pour la S\xe9curit\xe9 de l'IA (CeSIA)"],acknowledgements:["Jeanne Salle","Oscar Heitmann","Ram Rachum","Nicolas Guillard","Camille Berger"],atlas_link:"https://ai-safety-atlas.com/chapters/06/",alignment_forum_link:"https://www.lesswrong.com/s/3ni2P2GZzBvNebWYZ/p/mMBoPnFrFqQJKzDsZ",google_docs_link:"https://docs.google.com/document/d/1kEdmyVTUG3MO7lwuw4utHEm7CcavvgAiUZcWHaOZuPY/edit?usp=sharing",feedback_link:"https://forms.gle/ZsA4hEWUx1ZrtQLL9",teach_link:"https://docs.google.com/document/d/1im_i6e9xEAe-koYlurYdn26n9h7pFX2HksnRfQmWxTQ/edit?usp=sharing",sidebar_position:6,slug:"/chapters/06/"},o="Introduction",c={},p=[];function l(e){const s={h1:"h1",header:"header",p:"p",strong:"strong",...(0,a.R)(),...e.components},{GlossaryTerm:n}=s;return n||function(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"introduction",children:"Introduction"})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Apprentissage par renforcement :"})," Le chapitre commence par un rappel de certains concepts d'apprentissage par renforcement. Cela inclut un aper\xe7u rapide du concept de r\xe9compenses et des fonctions de r\xe9compense. Cette section pose les bases pour expliquer pourquoi la conception des r\xe9compenses est extr\xeamement importante."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Optimisation :"})," Cette section pr\xe9sente bri\xe8vement le concept de la Loi de Goodhart. Elle fournit une motivation pour comprendre pourquoi les r\xe9compenses sont difficiles \xe0 sp\xe9cifier de mani\xe8re \xe0 ce qu'elles ne s'effondrent pas face \xe0 une immense pression d'optimisation."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Mauvaise sp\xe9cification des r\xe9compenses :"})," Avec une solide compr\xe9hension de la notion de r\xe9compenses et d'optimisation, les lecteurs sont introduits \xe0 l'un des d\xe9fis fondamentaux de l'alignement - la mauvaise sp\xe9cification des r\xe9compenses. Ceci est \xe9galement connu sous le nom de probl\xe8me d'alignement externe. La section commence par discuter de la n\xe9cessit\xe9 d'une bonne conception des r\xe9compenses en plus de la conception d'algorithmes. Elle est suivie d'exemples concrets d'\xe9checs de sp\xe9cification des r\xe9compenses tels que le piratage des r\xe9compenses et la manipulation des r\xe9compenses."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Apprentissage par imitation :"})," Cette section se concentre sur certaines solutions propos\xe9es \xe0 la mauvaise sp\xe9cification des r\xe9compenses qui reposent sur l'apprentissage des fonctions de r\xe9compense par l'imitation du comportement humain. Elle examine des propositions telles que l'apprentissage par imitation (IL), le clonage comportemental (BC) et l'apprentissage par renforcement inverse (IRL). Chaque section contient \xe9galement un examen des probl\xe8mes et limitations possibles de ces approches en ce qui concerne la r\xe9solution du piratage des r\xe9compenses."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Apprentissage par retour :"})," La derni\xe8re section examine les propositions visant \xe0 rectifier la mauvaise sp\xe9cification des r\xe9compenses en fournissant des retours aux mod\xe8les d'",(0,i.jsx)(n,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,i.jsx)(n,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})}),". La section fournit \xe9galement un aper\xe7u complet de la fa\xe7on dont les grands mod\xe8les de langage (LLM) actuels sont entra\xeen\xe9s. La discussion couvre la mod\xe9lisation des r\xe9compenses, l'apprentissage par renforcement \xe0 partir de retours humains (RLHF), l'apprentissage par renforcement \xe0 partir de retours d'intelligence artificielle (RLAIF), et les limitations de ces approches."]})]})}function m(e={}){const{wrapper:s}={...(0,a.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}}}]);
"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[362],{3950:(e,s,t)=>{t.r(s),t.d(s,{assets:()=>d,contentTitle:()=>o,default:()=>p,frontMatter:()=>l,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"chapters/03/index","title":"Strat\xe9gies","description":"Ce chapitre tente d\'exposer la vue d\'ensemble de la strat\xe9gie de s\xe9curit\xe9 de l\'IA pour att\xe9nuer les risques explor\xe9s dans le chapitre pr\xe9c\xe9dent.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/03/index.md","sourceDirName":"chapters/03","slug":"/chapters/03/","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/03/","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/03/index.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Strat\xe9gies","chapter_number":3,"reading_time_core":"79 min","reading_time_optional":"35 min","reading_time_appendix":"16 min","authors":["Charbel-Rapha\xebl Segerie","Markov Grey"],"affiliations":["Centre Fran\xe7ais pour la S\xe9curit\xe9 de l\'IA (CeSIA)"],"acknowledgements":["Jeanne Salle","Charles Martinet","Lukas Gebhard","Amaury Lorin","Alejandro Acelas","Flawn","Emily Fan","Kieron Kretschmar","Sebastian Gil","Evander Hammer","Sophia Wesenberg","Jessica Wen","Niharika Chaubey","Ang\xe9lina Gentaz","Jonathan Claybrough","Camille Berger","Josh Thorsteinson"],"alignment_forum_link":"https://www.lesswrong.com/s/3ni2P2GZzBvNebWYZ/p/RzsXRbk2ETNqjhsma","google_docs_link":"https://docs.google.com/document/d/1ytzVlrj8PpxiyjvmZCJXm5QW3olhTh504-yH0h-wAq0/edit?usp=sharing","feedback_link":"https://forms.gle/ZsA4hEWUx1ZrtQLL9","teach_link":"https://docs.google.com/document/d/1im_i6e9xEAe-koYlurYdn26n9h7pFX2HksnRfQmWxTQ/edit?usp=sharing","sidebar_position":3,"slug":"/chapters/03/"},"sidebar":"docs","previous":{"title":"2.9 Annexe : Sc\xe9narios de pr\xe9vision","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/02/09"},"next":{"title":"3.1 D\xe9finitions","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/03/01"}}');var i=t(4848),r=t(8453),a=(t(2482),t(8559),t(9585),t(2501));const l={title:"Strat\xe9gies",chapter_number:3,reading_time_core:"79 min",reading_time_optional:"35 min",reading_time_appendix:"16 min",authors:["Charbel-Rapha\xebl Segerie","Markov Grey"],affiliations:["Centre Fran\xe7ais pour la S\xe9curit\xe9 de l'IA (CeSIA)"],acknowledgements:["Jeanne Salle","Charles Martinet","Lukas Gebhard","Amaury Lorin","Alejandro Acelas","Flawn","Emily Fan","Kieron Kretschmar","Sebastian Gil","Evander Hammer","Sophia Wesenberg","Jessica Wen","Niharika Chaubey","Ang\xe9lina Gentaz","Jonathan Claybrough","Camille Berger","Josh Thorsteinson"],alignment_forum_link:"https://www.lesswrong.com/s/3ni2P2GZzBvNebWYZ/p/RzsXRbk2ETNqjhsma",google_docs_link:"https://docs.google.com/document/d/1ytzVlrj8PpxiyjvmZCJXm5QW3olhTh504-yH0h-wAq0/edit?usp=sharing",feedback_link:"https://forms.gle/ZsA4hEWUx1ZrtQLL9",teach_link:"https://docs.google.com/document/d/1im_i6e9xEAe-koYlurYdn26n9h7pFX2HksnRfQmWxTQ/edit?usp=sharing",sidebar_position:3,slug:"/chapters/03/"},o="Introduction",d={},c=[];function u(e){const s={a:"a",h1:"h1",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"introduction",children:"Introduction"})}),"\n",(0,i.jsx)(s.p,{children:"Ce chapitre tente d'exposer la vue d'ensemble de la strat\xe9gie de s\xe9curit\xe9 de l'IA pour att\xe9nuer les risques explor\xe9s dans le chapitre pr\xe9c\xe9dent."}),"\n",(0,i.jsx)(s.p,{children:"Alors que les capacit\xe9s de l'IA continuent leur progression rapide, les strat\xe9gies con\xe7ues pour assurer la s\xe9curit\xe9 doivent \xe9galement \xe9voluer, c'est pourquoi la premi\xe8re version de ce document a \xe9t\xe9 r\xe9dig\xe9e en juillet 2024, et a \xe9t\xe9 mise \xe0 jour fin mai 2025. Nous parlons d'approches techniques, et essayons d'articuler ce chapitre avec les autres chapitres du livre. L'objectif est de fournir une vue d'ensemble structur\xe9e de la r\xe9flexion actuelle et des travaux en cours en mati\xe8re de strat\xe9gie de s\xe9curit\xe9 de l'IA, en reconnaissant \xe0 la fois les m\xe9thodes \xe9tablies et les nouvelles directions de recherche. Pour chaque type de macro-probl\xe8me comme les m\xe9susages, l'alignement de l'AGI, nous listons diff\xe9rentes macro-strat\xe9gies qui peuvent aider \xe0 att\xe9nuer ces risques. Ces strat\xe9gies peuvent g\xe9n\xe9ralement \xeatre combin\xe9es, et devraient l'\xeatre ! Nous discutons de l'encha\xeenement des diff\xe9rentes strat\xe9gies \xe0 la fin du chapitre."}),"\n",(0,i.jsx)(a.A,{src:"./img/TtT_Image_1.png",alt:"Entrer la description alternative de l'image",number:"1",label:"3.1",caption:"Diagramme provisoire r\xe9sumant les principales approches de haut niveau pour rendre le d\xe9veloppement de l'IA s\xfbr."}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"Au-del\xe0 du cadre de ce chapitre"})}),"\n",(0,i.jsx)(s.p,{children:"Bien que ce chapitre se concentre sur les strat\xe9gies directement li\xe9es \xe0 la pr\xe9vention des cons\xe9quences n\xe9gatives \xe0 grande \xe9chelle dues au m\xe9susage de l'IA, \xe0 son d\xe9salignement ou \xe0 son d\xe9veloppement incontr\xf4l\xe9, plusieurs sujets connexes sont n\xe9cessairement plac\xe9s au-del\xe0 de son cadre principal :"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"D\xe9sinformation g\xe9n\xe9r\xe9e par l'IA :"})," La prolif\xe9ration de la d\xe9sinformation pilot\xe9e par l'IA, y compris les deepfakes et la g\xe9n\xe9ration de contenu biais\xe9. Les strat\xe9gies pour combattre cela, comme les syst\xe8mes de d\xe9tection robustes, le filigrane et les principes d'IA responsable, sont majoritairement hors du cadre du chapitre. Celles-ci rel\xe8vent souvent de la mod\xe9ration de contenu, de l'\xe9ducation aux m\xe9dias et de la gouvernance des plateformes, distinctes des strat\xe9gies techniques centrales d'alignement et de contr\xf4le discut\xe9es dans ce chapitre."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Confidentialit\xe9 :"})," Les syst\xe8mes d'IA traitent souvent d'\xe9normes quantit\xe9s de donn\xe9es, amplifiant les pr\xe9occupations existantes concernant la confidentialit\xe9 des donn\xe9es."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"S\xe9curit\xe9 :"})," Les pratiques de s\xe9curit\xe9 standard comme le chiffrement, le contr\xf4le d'acc\xe8s, la classification des donn\xe9es, la surveillance des menaces et l'anonymisation sont des pr\xe9requis pour un d\xe9ploiement s\xfbr de l'IA. Bien qu'une s\xe9curit\xe9 robuste soit vitale pour des mesures comme la protection des poids des mod\xe8les, ces pratiques standard sont distinctes des nouvelles strat\xe9gies de s\xe9curit\xe9 n\xe9cessaires pour faire face aux risques comme le d\xe9salignement des mod\xe8les ou le m\xe9susage des capacit\xe9s."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Discrimination et toxicit\xe9 :"})," Bien que les r\xe9sultats biais\xe9s ou toxiques constituent un probl\xe8me de s\xe9curit\xe9, ce chapitre se concentre sur les strat\xe9gies visant \xe0 pr\xe9venir les d\xe9faillances catastrophiques."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Bien-\xeatre et droits des esprits num\xe9riques :"})," Nous ne savons pas si les IA devraient \xeatre consid\xe9r\xe9es comme des patients moraux. C'est un domaine \xe9thique distinct concernant nos obligations envers l'IA, plut\xf4t que la s\xe9curit\xe9 vis-\xe0-vis de l'IA."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Erreurs dues au manque de capacit\xe9 :"})," Bien que les d\xe9faillances des syst\xe8mes d'IA dues au manque de capacit\xe9 ou de robustesse soient une source de risque (",(0,i.jsx)(s.a,{href:"https://www.aisi.gov.uk/work/aisis-research-direction-for-technical-solutions",children:"AISI, 2025"}),"), les strat\xe9gies discut\xe9es dans ce chapitre visent \xe0 att\xe9nuer les risques d\xe9coulant \xe0 la fois d'une robustesse insuffisante et de capacit\xe9s potentiellement \xe9lev\xe9es (mais d\xe9salign\xe9es ou mal utilis\xe9es). Et les solutions \xe0 ce type de risque sont les m\xeames que pour d'autres industries : tests, it\xe9ration et am\xe9lioration des capacit\xe9s du syst\xe8me."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Le cadre choisi ici refl\xe8te une orientation commune au sein de certaines parties de la communaut\xe9 de la s\xe9curit\xe9 de l'IA sur les risques existentiels ou catastrophiques \xe0 grande \xe9chelle d\xe9coulant de syst\xe8mes d'IA puissants et potentiellement dou\xe9s d'agentivit\xe9."})]})}function p(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(u,{...e})}):u(e)}}}]);
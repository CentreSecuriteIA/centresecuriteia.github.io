"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[2975],{4495:(e,s,t)=>{t.r(s),t.d(s,{assets:()=>d,contentTitle:()=>c,default:()=>h,frontMatter:()=>u,metadata:()=>n,toc:()=>p});const n=JSON.parse('{"id":"chapters/03/5","title":"Strat\xe9gies Syst\xe9miques","description":"La s\xe9curit\xe9 de l\'IA est un probl\xe8me socio-technique qui n\xe9cessite une solution socio-technique. Assurer la s\xe9curit\xe9 de l\'IA requiert \xe9galement des approches syst\xe9miques robustes. Celles-ci englobent les structures de gouvernance, les pratiques organisationnelles et les normes culturelles qui fa\xe7onnent le d\xe9veloppement et le d\xe9ploiement de l\'IA. Les mesures de s\xe9curit\xe9 techniques peuvent \xeatre compromises par une gouvernance inad\xe9quate, de mauvaises pratiques de s\xe9curit\xe9 au sein des laboratoires, ou une culture qui privil\xe9gie la vitesse plut\xf4t que la prudence. Cette section examine les strat\xe9gies visant \xe0 int\xe9grer la s\xe9curit\xe9 dans l\'\xe9cosyst\xe8me plus large entourant l\'IA.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/03/05.md","sourceDirName":"chapters/03","slug":"/chapters/03/05","permalink":"/fr/chapters/03/05","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/03/05.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"id":"5","title":"Strat\xe9gies Syst\xe9miques","sidebar_label":"3.5 Strat\xe9gies Syst\xe9miques","sidebar_position":6,"slug":"/chapters/03/05","section_description":"Why does building safe AI benefit from a shared safety culture?","reading_time_core":"14 min","reading_time_optional":"11 min","pagination_prev":"chapters/03/4","pagination_next":"chapters/03/6"},"sidebar":"docs","previous":{"title":"3.4 Strat\xe9gies de s\xe9curit\xe9 pour l\'ASI","permalink":"/fr/chapters/03/04"},"next":{"title":"3.6 Combinaison des strat\xe9gies","permalink":"/fr/chapters/03/06"}}');var i=t(4848),r=t(8453),a=t(4768),l=(t(2482),t(8559)),o=(t(9585),t(2501));const u={id:5,title:"Strat\xe9gies Syst\xe9miques",sidebar_label:"3.5 Strat\xe9gies Syst\xe9miques",sidebar_position:6,slug:"/chapters/03/05",section_description:"Why does building safe AI benefit from a shared safety culture?",reading_time_core:"14 min",reading_time_optional:"11 min",pagination_prev:"chapters/03/4",pagination_next:"chapters/03/6"},c="Strat\xe9gies Transversales",d={},p=[{value:"Acc\xe9l\xe9ration de la D\xe9fense (d/acc)",id:"01",level:2},{value:"D\xe9fense en profondeur",id:"02",level:2},{value:"Gouvernance de l&#39;IA",id:"03",level:2},{value:"Gestion des risques",id:"04",level:2},{value:"Culture de la s\xe9curit\xe9",id:"05",level:2}];function m(e){const s={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{GlossaryTerm:t}=s;return t||function(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"strat\xe9gies-transversales",children:"Strat\xe9gies Transversales"})}),"\n",(0,i.jsx)(s.p,{children:"La s\xe9curit\xe9 de l'IA est un probl\xe8me socio-technique qui n\xe9cessite une solution socio-technique. Assurer la s\xe9curit\xe9 de l'IA requiert \xe9galement des approches syst\xe9miques robustes. Celles-ci englobent les structures de gouvernance, les pratiques organisationnelles et les normes culturelles qui fa\xe7onnent le d\xe9veloppement et le d\xe9ploiement de l'IA. Les mesures de s\xe9curit\xe9 techniques peuvent \xeatre compromises par une gouvernance inad\xe9quate, de mauvaises pratiques de s\xe9curit\xe9 au sein des laboratoires, ou une culture qui privil\xe9gie la vitesse plut\xf4t que la prudence. Cette section examine les strat\xe9gies visant \xe0 int\xe9grer la s\xe9curit\xe9 dans l'\xe9cosyst\xe8me plus large entourant l'IA."}),"\n",(0,i.jsx)(s.p,{children:"Faire face aux risques syst\xe9miques pos\xe9s par l'IA n'est pas facile. Cela n\xe9cessite une collaboration multidisciplinaire continue et la r\xe9solution de jeux de coordination complexes. Le fait que la responsabilit\xe9 du probl\xe8me soit si diverse rend difficile la mise en \u0153uvre des solutions."}),"\n",(0,i.jsx)(o.A,{src:"./img/sZG_Image_20.png",alt:"Saisir la description alternative de l'image",number:"20",label:"3.20",caption:"Une illustration d'un cadre que nous pensons \xeatre robustement bon pour g\xe9rer les risques. Les risques li\xe9s \xe0 l'IA sont trop nombreux et trop h\xe9t\xe9rog\xe8nes. Pour faire face \xe0 ces risques, nous avons besoin d'un cadre adaptatif qui peut \xeatre robuste et \xe9voluer au fur et \xe0 mesure des avanc\xe9es de l'IA."}),"\n",(0,i.jsx)(s.h2,{id:"01",children:"Acc\xe9l\xe9ration de la D\xe9fense (d/acc)"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'acc\xe9l\xe9ration de la d\xe9fense (d/acc) est une approche strat\xe9gique qui donne la priorit\xe9 aux technologies renfor\xe7ant la d\xe9fense et la r\xe9silience sociale face aux risques li\xe9s \xe0 l'IA."})," L'acc\xe9l\xe9ration de la d\xe9fense, ou d/acc, est apparue comme une approche strat\xe9gique en 2023 comme voie m\xe9diane entre l'acc\xe9l\xe9ration sans restriction (acc\xe9l\xe9rationnisme effectif (e/acc)) et les techno-pessimistes/catastrophistes (",(0,i.jsx)(s.a,{href:"https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html",children:"Buterin, 2023"}),"; ",(0,i.jsx)(s.a,{href:"https://vitalik.eth.limo/general/2025/01/05/dacc2.html",children:"Buterin, 2025"}),"). Plut\xf4t que de s'appuyer uniquement sur la restriction d'acc\xe8s aux capacit\xe9s potentiellement dangereuses, d/acc propose d'acc\xe9l\xe9rer uniquement les technologies qui favorisent intrins\xe8quement la d\xe9fense par rapport \xe0 l'attaque, rendant ainsi la soci\xe9t\xe9 plus r\xe9siliente face \xe0 diverses menaces, y compris l'utilisation abusive de l'IA par les humains ou les comportements d\xe9salign\xe9s de l'IA elle-m\xeame. D/acc peut \xeatre compris en r\xe9fl\xe9chissant \xe0 la question - si l'IA prend le contr\xf4le du monde (ou prive les humains de leurs pouvoirs), comment le ferait-elle ?"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Elle pirate nos ordinateurs \u2192 acc\xe9l\xe9rer la cyber-d\xe9fense :"})," Utiliser l'IA pour identifier et corriger les vuln\xe9rabilit\xe9s, surveiller les syst\xe8mes contre les intrusions et automatiser les r\xe9ponses de s\xe9curit\xe9."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Elle cr\xe9e une super-\xe9pid\xe9mie \u2192 acc\xe9l\xe9rer la bio-d\xe9fense :"})," D\xe9velopper des technologies pour d\xe9tecter, pr\xe9venir et traiter les menaces biologiques, y compris des syst\xe8mes avanc\xe9s de filtration d'air, des outils de diagnostic rapide, l'irradiation UV-C lointaine pour st\xe9riliser en toute s\xe9curit\xe9 les espaces occup\xe9s, et des capacit\xe9s de production d\xe9centralis\xe9e de vaccins."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Elle nous convainc (soit de lui faire confiance, soit de nous m\xe9fier les uns des autres) \u2192 acc\xe9l\xe9rer l'info-d\xe9fense :"})," Cr\xe9er des syst\xe8mes qui aident \xe0 valider l'exactitude des informations et \xe0 d\xe9tecter les contenus trompeurs sans arbitres centralis\xe9s de la v\xe9rit\xe9, comme le suivi de la provenance s\xe9curis\xe9 par blockchain et les syst\xe8mes de v\xe9rification des faits valid\xe9s par la communaut\xe9 comme les Notes Communautaires de Twitter."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Elle perturbe les infrastructures \u2192 acc\xe9l\xe9rer la d\xe9fense physique :"})," Cr\xe9er des infrastructures r\xe9silientes qui peuvent r\xe9sister aux perturbations, comme la production d'\xe9nergie distribu\xe9e via le solaire domestique, les syst\xe8mes de stockage par batterie, et les techniques de fabrication avanc\xe9es permettant la production locale de biens essentiels."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(o.A,{src:"./img/Ell_Image_21.png",alt:"Saisir la description alternative de l'image",number:"21",label:"3.21",caption:"Une illustration des diff\xe9rentes choses sur lesquelles nous pourrions travailler dans la philosophie d/acc pour prioriser les technologies d\xe9fensives ([Buterin, 2023](https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html))."}),"\n",(0,i.jsxs)(l.A,{title:"Un exemple concret : l'IA pour la cyberd\xe9fense",collapsed:!0,children:[(0,i.jsxs)(s.p,{children:["Une application cl\xe9 est l'utilisation de l'IA pour am\xe9liorer la cybers\xe9curit\xe9. Une IA puissante pourrait potentiellement automatiser la d\xe9tection des vuln\xe9rabilit\xe9s, surveiller les syst\xe8mes contre les intrusions, g\xe9rer les permissions de mani\xe8re plus pr\xe9cise que les humains, ou remplacer les op\xe9rateurs humains dans les t\xe2ches critiques de s\xe9curit\xe9 (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/2wxufQWK8rXcDGbyL/access-to-powerful-ai-might-make-computer-security-radically",children:"Shlegeris, 2024"}),"). Bien que les mod\xe8les actuels ne soient peut-\xeatre pas encore assez fiables, le potentiel existe pour que l'IA renforce significativement les cyber-d\xe9fenses contre les attaques conventionnelles et bas\xe9es sur l'IA (",(0,i.jsx)(s.a,{href:"https://abnormalsecurity.com/blog/offensive-ai-defensive-ai",children:"Jade Hill, 2024"}),"; ",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/2wxufQWK8rXcDGbyL/access-to-powerful-ai-might-make-computer-security-radically",children:"Schlegeris, 2024"}),") d\xe9crit quatre strat\xe9gies prometteuses pour utiliser l'IA afin d'am\xe9liorer la s\xe9curit\xe9 :"]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Surveillance compl\xe8te des actions humaines avec signalement par l'IA des activit\xe9s suspectes"}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"D\xe9placement de la confiance o\xf9 l'IA g\xe8re les t\xe2ches sensibles \xe0 la place des humains"}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Gestion des permissions d\xe9taill\xe9e qui serait trop intensive en main-d'\u0153uvre pour les humains"}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Investigation des activit\xe9s suspectes assist\xe9e par l'IA."}),"\n"]}),"\n"]}),(0,i.jsx)(s.p,{children:"Ces approches pourraient r\xe9duire consid\xe9rablement les menaces internes et les risques d'exfiltration de donn\xe9es, rendant potentiellement la s\xe9curit\xe9 informatique \"radicalement plus facile\" lorsque l'IA puissante deviendra disponible, m\xeame s'il existe une incertitude substantielle sur la robustesse de ces techniques."})]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"D/acc repr\xe9sente trois principes interconnect\xe9s : le d\xe9veloppement technologique d\xe9fensif, d\xe9centralis\xe9 et diff\xe9rentiel."}),' Le "d" dans d/acc signifie :']}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"D\xe9fensif :"})," Prioriser les technologies qui facilitent la protection contre les menaces plut\xf4t que leur cr\xe9ation. Les approches purement restrictives font face \xe0 des limitations inh\xe9rentes - elles n\xe9cessitent une coordination mondiale, cr\xe9ent des goulots d'\xe9tranglement dans l'innovation et risquent de concentrer le pouvoir entre les mains de ceux qui contr\xf4lent l'acc\xe8s."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Diff\xe9rentiel :"})," Acc\xe9l\xe9rer les technologies b\xe9n\xe9fiques tout en \xe9tant plus prudent avec celles ayant un potentiel nuisible. L'ordre dans lequel la technologie est d\xe9velopp\xe9e est tr\xe8s important. En acc\xe9l\xe9rant diff\xe9rentiellement les technologies d\xe9fensives (comme les mesures avanc\xe9es de cybers\xe9curit\xe9) avant les capacit\xe9s potentiellement dangereuses (comme les syst\xe8mes de piratage autonomes), nous cr\xe9ons des couches de protection avant qu'elles ne soient urgemment n\xe9cessaires."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"D\xe9centralis\xe9 :"})," Nous pouvons renforcer la r\xe9silience en \xe9liminant les points uniques de d\xe9faillance. Le contr\xf4le centralis\xe9 des capacit\xe9s puissantes de l'IA cr\xe9e des vuln\xe9rabilit\xe9s aux d\xe9faillances techniques, aux attaques adverses et \xe0 la capture institutionnelle (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2001.03573",children:"Cihon et al., 2020"}),"). Les approches d\xe9centralis\xe9es distribuent \xe0 la fois les capacit\xe9s et la gouvernance entre divers acteurs, emp\xeachant le contr\xf4le unilat\xe9ral sur les technologies transformatives."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(o.A,{src:"./img/8pT_Image_22.png",alt:"Saisir la description alternative de l'image",number:"22",label:"3.22",caption:"M\xe9canismes par lesquels le d\xe9veloppement technologique diff\xe9rentiel peut r\xe9duire les impacts soci\xe9taux n\xe9gatifs ([Buterin, 2023](https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html))."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'efficacit\xe9 de d/acc d\xe9pend du maintien d'\xe9quilibres favorables entre l'attaque et la d\xe9fense."})," La faisabilit\xe9 de d/acc comme strat\xe9gie d\xe9pend de la capacit\xe9 des technologies d\xe9fensives \xe0 surpasser les capacit\xe9s offensives dans tous les domaines. Les pr\xe9c\xe9dents historiques sont mitig\xe9s - certains domaines comme la cybers\xe9curit\xe9 traditionnelle favorisent souvent les d\xe9fenseurs qui peuvent corriger les vuln\xe9rabilit\xe9s, tandis que d'autres comme la bios\xe9curit\xe9 favorisent traditionnellement les attaquants qui ont besoin de moins de ressources pour cr\xe9er des menaces que les d\xe9fenseurs n'en ont besoin pour les contrer. Le d\xe9fi principal pour la mise en \u0153uvre de d/acc r\xe9side dans l'identification et le soutien des technologies qui font pencher ces \xe9quilibres vers la d\xe9fense (",(0,i.jsx)(s.a,{href:"https://airesilience.substack.com/p/a-policy-agenda-for-defensive-acceleration",children:"Bernardi, 2024"}),"; ",(0,i.jsx)(s.a,{href:"https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html",children:"Buterin, 2023"}),")."]}),"\n",(0,i.jsxs)(l.A,{title:"Strat\xe9gies exploitables align\xe9es avec la philosophie d/acc",collapsed:!0,children:[(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"D/acc compl\xe8te plut\xf4t qu'il ne remplace les autres approches de s\xe9curit\xe9."})," Contrairement aux cadres concurrents qui peuvent consid\xe9rer les restrictions et les garanties comme des obstacles au progr\xe8s, d/acc reconna\xeet leur valeur tout en abordant leurs limitations. Les garanties des mod\xe8les restent des d\xe9fenses essentielles de premi\xe8re ligne, mais d/acc construit des couches de s\xe9curit\xe9 suppl\xe9mentaires lorsque ces garanties \xe9chouent ou sont contourn\xe9es. De m\xeame, les cadres de gouvernance fournissent une surveillance n\xe9cessaire, mais d/acc r\xe9duit la d\xe9pendance \xe0 une r\xe9glementation parfaite en construisant une r\xe9silience technique qui fonctionne m\xeame pendant les lacunes de gouvernance."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Approches de gouvernance et politiques exploitables pour d/acc."})," Les interventions politiques peuvent aider \xe0 cr\xe9er des cadres structur\xe9s pour l'acc\xe9l\xe9ration d\xe9fensive. Voici quelques exemples de travaux en mati\xe8re de gouvernance qui soutiennent la philosophie d/acc :"]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Cadres de partage d'informations :"})," \xc9tablir des protocoles obligatoires de signalement des incidents et de partage d'informations entre les d\xe9veloppeurs d'IA et les agences de s\xe9curit\xe9 (",(0,i.jsx)(s.a,{href:"https://airesilience.substack.com/p/a-policy-agenda-for-defensive-acceleration",children:"Bernardi, 2024"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Acc\xe8s prioritaire aux d\xe9fenseurs :"})," Mettre en \u0153uvre des politiques accordant aux chercheurs en s\xe9curit\xe9 un acc\xe8s privil\xe9gi\xe9 pr\xe9coce aux capacit\xe9s avanc\xe9es d'IA avant la diffusion g\xe9n\xe9rale (",(0,i.jsx)(s.a,{href:"https://airesilience.substack.com/p/a-policy-agenda-for-defensive-acceleration",children:"Bernardi, 2024"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Fonds d'acc\xe9l\xe9ration de la d\xe9fense :"})," Cr\xe9er des m\xe9canismes de financement d\xe9di\xe9s aux technologies d\xe9fensives pour rem\xe9dier aux d\xe9faillances du march\xe9 o\xf9 les technologies de bien public manquent d'investissements priv\xe9s suffisants malgr\xe9 leur valeur sociale (",(0,i.jsx)(s.a,{href:"https://airesilience.substack.com/p/a-policy-agenda-for-defensive-acceleration",children:"Bernardi, 2024"}),"; ",(0,i.jsx)(s.a,{href:"https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html",children:"Buterin, 2023"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"D\xe9ploiement progressif des capacit\xe9s :"})," Exiger des d\xe9ploiements par phases des capacit\xe9s avanc\xe9es d'IA avec des p\xe9riodes de surveillance entre les \xe9tapes (",(0,i.jsx)(s.a,{href:"https://airesilience.substack.com/p/a-policy-agenda-for-defensive-acceleration",children:"Bernardi, 2024"}),")."]}),"\n"]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Approches technologiques et de recherche exploitables pour d/acc."})," Nous pouvons faire progresser diff\xe9rentiellement le progr\xe8s technologique dans de nombreux domaines diff\xe9rents pour favoriser la d\xe9fense contre les risques catastrophiques. Voici quelques exemples :"]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Syst\xe8mes avanc\xe9s de qualit\xe9 de l'air :"})," D\xe9velopper des syst\xe8mes int\xe9gr\xe9s qui d\xe9tectent, filtrent et neutralisent les agents pathog\xe8nes a\xe9roport\xe9s en temps r\xe9el. Ces technologies fournissent une protection passive contre les pand\xe9mies naturelles et les armes biologiques sans n\xe9cessiter de changements comportementaux ou une conformit\xe9 parfaite (",(0,i.jsx)(s.a,{href:"https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html",children:"Buterin, 2023"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Calcul pr\xe9servant la confidentialit\xe9 :"})," Faire progresser les techniques cryptographiques comme les preuves \xe0 divulgation nulle de connaissance, le chiffrement homomorphe et le calcul multipartite s\xe9curis\xe9. Ces m\xe9thodes permettent la v\xe9rification et la collaboration s\xe9curis\xe9e sans exposer d'informations sensibles, modifiant fondamentalement les compromis s\xe9curit\xe9-confidentialit\xe9 (",(0,i.jsx)(s.a,{href:"https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html",children:"Buterin, 2023"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Infrastructure r\xe9siliente :"})," Cr\xe9er des syst\xe8mes d\xe9centralis\xe9s et autosuffisants pour l'\xe9nergie, la communication et les cha\xeenes d'approvisionnement qui peuvent fonctionner pendant les perturbations. Cela inclut des technologies comme les r\xe9seaux maill\xe9s, la fabrication localis\xe9e et la production d'\xe9nergie distribu\xe9e qui maintiennent les fonctions critiques m\xeame lorsque les syst\xe8mes centralis\xe9s \xe9chouent (",(0,i.jsx)(s.a,{href:"https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html",children:"Buterin, 2023"}),"; ",(0,i.jsx)(s.a,{href:"https://vitalik.eth.limo/general/2025/01/05/dacc2.html",children:"Buterin, 2025"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Syst\xe8mes de v\xe9rification collaborative :"})," Mettre en \u0153uvre des plateformes de validation d'information multi-spectrales similaires aux Notes Communautaires qui identifient la d\xe9sinformation gr\xe2ce au consensus \xe0 travers la diversit\xe9 des points de vue. Ces syst\xe8mes permettent aux communaut\xe9s d'autor\xe9guler la qualit\xe9 de l'information sans arbitres centralis\xe9s de la v\xe9rit\xe9 (",(0,i.jsx)(s.a,{href:"https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html",children:"Buterin, 2023"}),")."]}),"\n"]}),"\n"]}),(0,i.jsx)(o.A,{src:"./img/2Qx_Image_23.png",alt:"Saisir la description alternative de l'image",number:"23",label:"3.23",caption:"Un mod\xe8le \xe9tendu montrant comment diff\xe9rents types et couches de technologies d\xe9fensives peuvent interagir ([Buterin, 2025](https://vitalik.eth.limo/general/2025/01/05/dacc2.html))"})]}),"\n",(0,i.jsx)(s.h2,{id:"02",children:"D\xe9fense en profondeur"}),"\n",(0,i.jsx)(s.p,{children:"La s\xe9curit\xe9 efficace de l'IA d\xe9pend aussi fortement des pratiques et structures internes au sein des organisations d\xe9veloppant l'IA. Les accidents sont difficiles \xe0 \xe9viter, m\xeame lorsque la structure incitative et la gouvernance tentent de garantir qu'il n'y aura pas de probl\xe8mes. Par exemple, m\xeame aujourd'hui, il y a encore des accidents dans l'industrie a\xe9rospatiale."}),"\n",(0,i.jsx)(o.A,{src:"./img/NpS_Image_24.png",alt:"Saisir la description alternative de l'image",number:"24",label:"3.24",caption:"Le mod\xe8le du fromage suisse montre comment les facteurs techniques peuvent am\xe9liorer la s\xe9curit\xe9 organisationnelle. Les multiples couches de d\xe9fense compensent les faiblesses individuelles les unes des autres, conduisant \xe0 un faible niveau global de risque ([Hendrycks et al., 2023](https://arxiv.org/abs/2306.12001))."}),"\n",(0,i.jsxs)(s.p,{children:["Pour r\xe9soudre ces probl\xe8mes, un mod\xe8le du fromage suisse pourrait \xeatre efficace - aucune solution unique ne suffira, mais une approche en couches peut r\xe9duire significativement les risques. Le mod\xe8le du fromage suisse est un concept de gestion des risques, largement utilis\xe9 dans des industries comme la sant\xe9 et l'aviation. Chaque couche repr\xe9sente une mesure de s\xe9curit\xe9, et bien qu'individuellement elles puissent avoir des faiblesses, collectivement elles forment une forte barri\xe8re contre les menaces. Les organisations devraient \xe9galement suivre des principes de conception s\xfbre (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2206.05862",children:"Hendrycks & Mazeika, 2022"}),"), comme la d\xe9fense en profondeur et la redondance, pour assurer une sauvegarde pour chaque mesure de s\xe9curit\xe9, entre autres."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"De nombreuses solutions peuvent \xeatre imagin\xe9es pour r\xe9duire ces risques, m\xeame si aucune n'est parfaite."})," La premi\xe8re \xe9tape pourrait \xeatre de mandater des \xe9quipes rouges externes pour identifier les dangers et am\xe9liorer la s\xe9curit\xe9 du syst\xe8me. C'est ce qu'OpenAI a fait avec METR pour \xe9valuer GPT-4. Cependant, les laboratoires d'AGI ont aussi besoin d'une \xe9quipe d'audit interne pour la gestion des risques. Tout comme les banques ont des \xe9quipes de gestion des risques, cette \xe9quipe doit \xeatre impliqu\xe9e dans les processus de d\xe9cision, et les d\xe9cisions cl\xe9s devraient impliquer un directeur des risques pour assurer la responsabilit\xe9 ex\xe9cutive. L'une des missions de l'\xe9quipe de gestion des risques pourrait \xeatre, par exemple, de concevoir des plans pr\xe9\xe9tablis pour g\xe9rer la s\xe9curit\xe9 et les incidents."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Limites des approches syst\xe9miques."})," La d\xe9fense en profondeur pourrait encore \xe9chouer face \xe0 des menaces suffisamment nouvelles ou des adversaires d\xe9termin\xe9s, particuli\xe8rement dans le cas des ASI. De m\xeame, les sc\xe9narios de d\xe9sempowerment progressif, o\xf9 le contr\xf4le humain s'\xe9rode progressivement \xe0 travers des changements \xe9conomiques ou culturels induits par l'IA, pourraient ne pas \xeatre trait\xe9s de mani\xe8re ad\xe9quate par les cadres de gouvernance actuels centr\xe9s sur des pr\xe9judices ou des capacit\xe9s sp\xe9cifiques."]}),"\n",(0,i.jsx)(s.h2,{id:"03",children:"Gouvernance de l'IA"}),"\n",(0,i.jsx)(s.p,{children:"La qu\xeate d'une IA de plus en plus puissante, \xe0 l'instar de la course aux armements nucl\xe9aires de l'\xe8re de la Guerre froide, repr\xe9sente un compromis entre la s\xe9curit\xe9 et l'avantage concurrentiel que recherchent les nations et les entreprises pour le pouvoir et l'influence. Cette dynamique comp\xe9titive augmente le risque mondial. Pour att\xe9nuer ce probl\xe8me, nous pouvons essayer d'agir \xe0 sa source, \xe0 savoir la refonte des incitations \xe9conomiques pour privil\xe9gier la s\xe9curit\xe9 \xe0 long terme plut\xf4t que les gains \xe0 court terme. Cela peut principalement se faire via une gouvernance internationale."}),"\n",(0,i.jsx)(s.p,{children:"Une gouvernance efficace de l'IA vise \xe0 atteindre deux objectifs principaux :"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Gagner du temps. Du temps et des ressources pour le d\xe9veloppement de solutions afin de garantir que suffisamment de temps et de ressources sont allou\xe9s \xe0 l'identification et \xe0 la mise en \u0153uvre de mesures de s\xe9curit\xe9"}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Appliquer des solutions. Une coordination renforc\xe9e pour augmenter la probabilit\xe9 d'adoption g\xe9n\xe9ralis\xe9e des mesures de s\xe9curit\xe9 gr\xe2ce \xe0 la coop\xe9ration mondiale. Les risques li\xe9s \xe0 l'IA sont multiformes, n\xe9cessitant des r\xe9glementations qui encouragent un comportement prudent parmi les parties prenantes et des r\xe9ponses rapides aux menaces \xe9mergentes."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"Concevoir de meilleures incitations de haut niveau"})}),"\n",(0,i.jsx)(s.p,{children:"Aligner les incitations \xe9conomiques avec les objectifs de s\xe9curit\xe9 est un d\xe9fi majeur. Actuellement, de fortes pressions commerciales peuvent inciter au d\xe9veloppement rapide des capacit\xe9s, potentiellement au d\xe9triment de la recherche sur la s\xe9curit\xe9 ou d'un d\xe9ploiement prudent. Des m\xe9canismes pour r\xe9compenser la s\xe9curit\xe9 ou p\xe9naliser l'imprudence sont n\xe9cessaires pour \xe9viter les externalit\xe9s n\xe9gatives :"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Remodeler la course via un d\xe9veloppement centralis\xe9."})," Par exemple, Yoshua Bengio et al. proposent de cr\xe9er une installation s\xe9curis\xe9e similaire au CERN pour la physique, o\xf9 le d\xe9veloppement des technologies d'IA potentiellement dangereuses peut \xeatre strictement contr\xf4l\xe9 (",(0,i.jsx)(s.a,{href:"https://yoshuabengio.org/2023/06/24/faq-on-catastrophic-ai-risks/",children:"Bengio, 2023"}),'). Cette mesure est hautement non consensuelle. Nous avons d\xe9j\xe0 explor\xe9 cette solution dans la strat\xe9gie "Coordination mondiale" de s\xe9curit\xe9 ASI, dans la section s\xe9curit\xe9 ASI, mais cela pourrait aussi \xeatre valable pour de nombreux domaines de la s\xe9curit\xe9.']}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Clauses d'aubaine et partage des b\xe9n\xe9fices."})," La mise en \u0153uvre d'accords pour partager les profits entre les diff\xe9rents laboratoires g\xe9n\xe9r\xe9s par l'AGI att\xe9nuerait la course \xe0 la supr\xe9matie de l'IA en assurant un b\xe9n\xe9fice collectif des succ\xe8s individuels. ",(0,i.jsx)(a.A,{id:"footnote_windfall",number:"1",text:"Par exemple, dans l'industrie pharmaceutique pour le d\xe9veloppement de m\xe9dicaments, les entreprises concluent parfois des accords de co-d\xe9veloppement et de partage des b\xe9n\xe9fices pour partager les risques et les r\xe9compenses de la mise sur le march\xe9 d'un nouveau m\xe9dicament. Par exemple, en 2014, Pfizer et Merck ont conclu une alliance mondiale pour co-d\xe9velopper et co-commercialiser un anticorps anti-PD-L1 pour le traitement de plusieurs types de cancer."})]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Mettre en \u0153uvre une gouvernance correcte des entreprises d'AGI."})," Il est important d'examiner les structures de gouvernance des laboratoires d'AGI. Par exemple, \xeatre une organisation \xe0 but non lucratif et avoir une d\xe9claration de mission qui indique clairement que l'objectif n'est pas de maximiser les revenus, mais de s'assurer que le d\xe9veloppement de l'IA profite \xe0 toute l'humanit\xe9, est une premi\xe8re \xe9tape importante. De plus, le conseil d'administration doit avoir du pouvoir."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Responsabilit\xe9 juridique pour les d\xe9veloppeurs d'IA."})," \xc9tablir des responsabilit\xe9s juridiques claires pour les d\xe9veloppeurs d'IA concernant les mauvaises utilisations ou les accidents pourrait r\xe9aligner les incitations. Par exemple, le Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047) aurait pu permettre au procureur g\xe9n\xe9ral d'intenter des poursuites civiles contre les d\xe9veloppeurs qui causent des dommages catastrophiques ou menacent la s\xe9curit\xe9 publique en n\xe9gligeant les exigences. Le projet de loi (qui a \xe9t\xe9 rejet\xe9 par le gouverneur en 2024) ne traitait que des risques extr\xeames de ces mod\xe8les, notamment : les cyberattaques causant plus de 500 millions de dollars de dommages, les crimes autonomes causant 500 millions de dollars de dommages, et la cr\xe9ation d'armes chimiques, biologiques, radiologiques ou nucl\xe9aires utilisant l'IA. Notez que compar\xe9 \xe0 l'AI Act et son code de pratique, SB1047 ne sp\xe9cifie pas en d\xe9tail les \xe9tapes n\xe9cessaires pour \xe9viter les catastrophes, il cible uniquement le r\xe9sultat et pas vraiment le processus."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Plusieurs m\xe9canismes de gouvernance internationale de l'IA ont \xe9t\xe9 propos\xe9s pour \xe9tablir des limites et des r\xe8gles claires pour le d\xe9veloppement de l'IA au niveau international."})," Ceux-ci incluent la mise en \u0153uvre de moratoires temporaires sur les syst\xe8mes d'IA \xe0 haut risque, l'application de r\xe9glementations l\xe9gales comme l'AI Act de l'UE, et l'\xe9tablissement de \"Lignes Rouges\" internationalement reconnues qui interdisent des capacit\xe9s d'IA sp\xe9cifiques dangereuses, comme la r\xe9plication autonome ou l'aide \xe0 la cr\xe9ation d'armes de destruction massive. Les dialogues IDAIS ont vis\xe9 \xe0 construire un consensus sur ces lignes rouges, soulignant la clart\xe9 et l'universalit\xe9 comme caract\xe9ristiques cl\xe9s pour l'efficacit\xe9, avec des violations pouvant d\xe9clencher des r\xe9ponses internationales pr\xe9alablement convenues."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les approches conditionnelles et la cr\xe9ation d'organismes internationaux d\xe9di\xe9s repr\xe9sentent une autre strat\xe9gie cl\xe9."})," Les \"Engagements Si-Alors\" impliquent que les d\xe9veloppeurs ou les \xc9tats acceptent de mettre en \u0153uvre des mesures de s\xe9curit\xe9 sp\xe9cifiques si les capacit\xe9s de l'IA atteignent certains seuils pr\xe9d\xe9finis, permettant la pr\xe9paration sans entraver pr\xe9matur\xe9ment le d\xe9veloppement, comme l'illustre le projet de Trait\xe9 Conditionnel sur la S\xe9curit\xe9 de l'IA. De plus, il existe des propositions pour de nouvelles institutions internationales, potentiellement model\xe9es sur l'Agence internationale de l'\xe9nergie atomique (AIEA), pour surveiller le d\xe9veloppement de l'IA, v\xe9rifier la conformit\xe9 aux accords, promouvoir la recherche sur la s\xe9curit\xe9, et potentiellement centraliser ou contr\xf4ler le d\xe9veloppement et la distribution des IA les plus risqu\xe9es."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Des r\xe9gimes de gouvernance sp\xe9cifiques et des structures de soutien sont \xe9galement \xe0 l'\xe9tude pour am\xe9liorer la coordination internationale."})," \xc9tant donn\xe9 la nature mondiale de l'IA, des m\xe9canismes comme la gouvernance internationale du calcul visent \xe0 surveiller et contr\xf4ler les cha\xeenes d'approvisionnement pour les puces d'IA et l'infrastructure d'entra\xeenement \xe0 grande \xe9chelle, bien que la faisabilit\xe9 technique et la coop\xe9ration internationale restent des d\xe9fis. D'autres propositions incluent l'\xe9tablissement d'un organisme international de recherche sur la s\xe9curit\xe9 de l'IA \xe0 grande \xe9chelle similaire au CERN, centralisant potentiellement la recherche \xe0 haut risque ou \xe9tablissant des normes mondiales, et le renforcement des protections des lanceurs d'alerte par des accords internationaux pour encourager le signalement des probl\xe8mes de s\xe9curit\xe9 dans l'industrie de l'IA."]}),"\n",(0,i.jsx)(s.p,{children:"Pour plus d'informations sur ces sujets, veuillez lire le prochain chapitre sur la gouvernance de l'IA."}),"\n",(0,i.jsxs)(l.A,{title:"La gouvernance de l'IA est-elle utile, souhaitable et possible ?",collapsed:!0,children:[(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Historiquement, le domaine de la s\xe9curit\xe9 de l'IA s'est principalement concentr\xe9 sur la recherche technique, influenc\xe9 en partie par des points de vue comme l'affirmation d'Eliezer Yudkowsky selon laquelle \"La politique est le tueur de l'esprit.\""})," (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/9weLK2AJ9JEt2Tt8f/politics-is-the-mind-killer",children:"Yudkowsky, 2007"}),") Pendant de nombreuses ann\xe9es, le domaine pensait que s'engager dans la politique \xe9tait inefficace voire contre-productif compar\xe9 \xe0 la r\xe9solution directe du probl\xe8me technique d'alignement, conduisant de nombreux premiers chercheurs pr\xe9occup\xe9s par l'AGI \xe0 privil\xe9gier les solutions d'ing\xe9nierie plut\xf4t que les efforts de gouvernance. \xc9tonnamment, au d\xe9but, il \xe9tait m\xeame presque d\xe9courag\xe9 de parler publiquement de ces risques pour \xe9viter la course et \xe9viter d'amener des personnes avec une \"\xe9pist\xe9mologie pauvre\" dans la communaut\xe9."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Cependant, en 2023, ChatGPT a \xe9t\xe9 publi\xe9, est devenu viral, et la gouvernance de l'IA a gagn\xe9 une traction significative comme strat\xe9gie potentiellement viable pour att\xe9nuer les risques de l'AGI."})," Ce changement s'est produit alors que l'engagement avec les d\xe9cideurs politiques semblait donner des r\xe9sultats, rendant la gouvernance plus r\xe9alisable qu'on ne le pensait auparavant (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/2sLwt2cSAag74nsdN/speaking-to-congressional-staffers-about-ai-risk",children:"Akash, 2023"}),"). Ensuite, des lettres ouvertes influentes ont \xe9t\xe9 publi\xe9es (FLI, CAIS), et ont d\xe9plac\xe9 la fen\xeatre d'Overton. Par cons\xe9quent, des organisations influentes comme 80,000 Hours ont ajust\xe9 leurs recommandations de carri\xe8re, mettant en avant les r\xf4les de politique et de strat\xe9gie de l'IA, maintenant au-dessus de la recherche technique d'alignement, comme priorit\xe9s principales pour l'impact (",(0,i.jsx)(s.a,{href:"https://80000hours.org/career-reviews/ai-policy-and-strategy/",children:"80,000 Hours, 2023"}),")."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Cependant, la fen\xeatre d'Overton pour des mesures internationales strictes de s\xe9curit\xe9 de l'IA semble se r\xe9tr\xe9cir."})," Alors que les d\xe9clarations et efforts initiaux de groupes comme le Future of Life Institute et le Center for AI Safety ont r\xe9ussi \xe0 \xe9largir le discours public et politique sur les risques de l'IA, les d\xe9veloppements ult\xe9rieurs, y compris les sommets internationaux per\xe7us comme faibles sur la s\xe9curit\xe9 et les changements de leadership politique (comme l'\xe9lection de Donald Trump), ont jet\xe9 le doute sur la faisabilit\xe9 d'atteindre une coordination internationale robuste (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/qYPHryHTNiJ2y6Fhi/the-paris-ai-anti-safety-summit",children:"Zvi, 2025"}),"). Cela a conduit certains dans le domaine de la gouvernance de l'IA \xe0 croire qu'un \"coup de semonce\" significatif \u2013 une d\xe9monstration claire du danger de l'IA \u2013 pourrait \xeatre n\xe9cessaire pour galvaniser une action d\xe9cisive, bien qu'il y ait du scepticisme quant \xe0 la possibilit\xe9 qu'un tel \xe9v\xe9nement convaincant puisse r\xe9ellement se produire avant qu'il ne soit trop tard (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/RYx6cLwzoajqjyB6b/what-convincing-warning-shot-could-help-prevent-extinction",children:"Segerie, 2024"}),")."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les r\xe9glementations existantes et propos\xe9es font face \xe0 des limitations significatives et des cons\xe9quences n\xe9gatives potentielles."})," Par exemple, les efforts l\xe9gislatifs importants comme l'AI Act de l'UE, bien que novateurs \xe0 certains \xe9gards, contiennent des lacunes notables (",(0,i.jsx)(s.a,{href:"https://milesbrundage.substack.com/p/feedback-on-the-second-draft-of-the",children:"Miles, 2025"}),") ; son Code de Pratique a des limitations et l'Act lui-m\xeame pourrait ne pas couvrir ad\xe9quatement les mod\xe8les d\xe9ploy\xe9s hors d'Europe, les d\xe9ploiements purement internes pour la recherche, ou les applications militaires. Une pr\xe9occupation critique est le potentiel pour les laboratoires d'IA de fronti\xe8re de s'engager dans des courses au d\xe9veloppement secr\xe8tes, contournant la surveillance \u2013 un sc\xe9nario potentiellement permis par des changements de politique comme la r\xe9vocation des ordres ex\xe9cutifs mandatant le rapport gouvernemental sur les \xe9valuations des mod\xe8les de fronti\xe8re (",(0,i.jsx)(s.a,{href:"https://ai-2027.com/",children:"Kokotajlo, 2025"}),")."]}),(0,i.jsx)(s.p,{children:"De plus, il existe des pr\xe9occupations fondamentales que les structures de gouvernance capables de contr\xf4ler l'AGI pourraient elles-m\xeames poser des risques, permettant potentiellement un contr\xf4le totalitaire."}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Une perspective profond\xe9ment sceptique sugg\xe8re que beaucoup du narratif actuel sur le progr\xe8s de l'IA et l'activit\xe9 r\xe9glementaire pourrait \xeatre performatif ou \"faux.\""})," Ce \"mod\xe8le compl\xe8tement cynique\" postule que les grands laboratoires d'IA pourraient exag\xe9rer leurs progr\xe8s vers l'AGI pour maintenir la confiance des investisseurs et l'engouement, masquant potentiellement des progr\xe8s plus lents ou une stagnation dans les capacit\xe9s fondamentales (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/?commentId=aBcAh8H9cSzdXmgb7#aBcAh8H9cSzdXmgb7",children:"johnswentworth, 2025"}),"). En parall\xe8le, il sugg\xe8re que les activistes et lobbyistes de la r\xe9glementation de l'IA pourraient privil\xe9gier le r\xe9seautage et le statut dans les cercles politiques plut\xf4t que l'\xe9laboration de r\xe9glementations v\xe9ritablement efficaces, conduisant \xe0 des mesures focalis\xe9es sur des m\xe9triques facilement cibl\xe9es mais potentiellement superficielles (comme les seuils de calcul) plut\xf4t que d'aborder les risques fondamentaux (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/?commentId=aBcAh8H9cSzdXmgb7#aBcAh8H9cSzdXmgb7",children:"johnswentworth, 2025"}),"). Cette vue implique une dynamique o\xf9 les laboratoires et les activistes renforcent involontairement un narratif de perc\xe9es imminentes et contr\xf4lables de l'IA, potentiellement d\xe9tach\xe9 de la r\xe9alit\xe9 sous-jacente (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/?commentId=aBcAh8H9cSzdXmgb7#aBcAh8H9cSzdXmgb7",children:"johnswentworth, 2025"}),")."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:'Cependant, cette perspective cynique de "fausset\xe9" est d\xe9battue.'})," Les critiques de la vue cynique soutiennent que des propositions r\xe9glementaires sp\xe9cifiques, comme SB 1047, contenaient des \xe9l\xe9ments potentiellement valables (par exemple, exiger des capacit\xe9s d'arr\xeat, des garanties, et le suivi des grands entra\xeenements), m\xeame si leur impact global \xe9tait d\xe9battu ou finalement limit\xe9 (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/johnswentworth-s-shortform?commentId=J2iPumP29GK9Qrm5p",children:"Charbel-Rapha\xebl, 2025"}),";",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/johnswentworth-s-shortform?commentId=G5zcfntZodH3ZYDaP",children:" johnswentworth, 2025"}),"). Il est reconnu que les r\xe9gulateurs op\xe8rent sous des contraintes r\xe9elles, incluant l'influence significative du lobbying des Big Tech, qui peut emp\xeacher l'interdiction de technologies sans preuve claire de risque inacceptable. De plus, le ph\xe9nom\xe8ne de \"conformit\xe9 performative\" ou \"th\xe9\xe2tre de conformit\xe9\" est reconnu, mais il est argument\xe9 que l'engagement avec ces processus imparfaits est toujours n\xe9cessaire, et que certaines \xe9tapes l\xe9gislatives, comme l'AI Act de l'UE mentionnant explicitement \"l'alignement avec l'intention humaine\", repr\xe9sentent des progr\xe8s potentiellement significatifs (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/johnswentworth-s-shortform?commentId=NLAW24oxDFuTLT3kx",children:"Katalina Hernandez, 2025"}),")."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La r\xe9glementation de l'IA pourrait involontairement augmenter le risque existentiel par plusieurs voies"})," (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/6untaSPpsocmkS7Z3/ways-i-expect-ai-regulation-to-increase-extinction-risk",children:"1a3orn, 2023"}),"). Les r\xe9glementations pourraient mal diriger les efforts de s\xe9curit\xe9 vers des probl\xe8mes de conformit\xe9 d\xe9pass\xe9s ou moins pertinents, d\xe9tournant l'",(0,i.jsx)(t,{term:"attention",definition:'{"definition":"M\xe9canisme permettant aux mod\xe8les de se concentrer sur les parties pertinentes de l\'entr\xe9e lors de la r\xe9alisation de pr\xe9dictions, en calculant des combinaisons pond\xe9r\xe9es d\'\xe9l\xe9ments d\'entr\xe9e..","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:(0,i.jsx)(t,{term:"attention",definition:'{"definition":"M\xe9canisme permettant aux mod\xe8les de se concentrer sur les parties pertinentes de l\'entr\xe9e lors de la r\xe9alisation de pr\xe9dictions, en calculant des combinaisons pond\xe9r\xe9es d\'\xe9l\xe9ments d\'entr\xe9e..","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:"attention"})})," des risques \xe9mergents plus importants (R\xe9glementations Mal Dirig\xe9es) ; les processus bureaucratiques tendent \xe0 favoriser les grands acteurs \xe9tablis, entravant potentiellement les efforts de recherche sur la s\xe9curit\xe9 plus petits et innovants ; des r\xe9glementations nationales trop strictes pourraient pousser le d\xe9veloppement de l'IA vers des acteurs internationaux moins soucieux de la s\xe9curit\xe9, affaiblissant l'influence du r\xe9gulateur initial (Affaiblissement des Pays R\xe9gulateurs) ; et les r\xe9glementations, particuli\xe8rement celles restreignant les mod\xe8les open-source ou \xe9tablissant des co\xfbts de conformit\xe9 \xe9lev\xe9s, pourraient consolider le pouvoir dans les mains des plus grandes entreprises poussant les capacit\xe9s, \xe9touffant potentiellement les approches alternatives de s\xe9curit\xe9 et acc\xe9l\xe9rant le risque (Renforcement des Acteurs Dominants). Mais l'existence de ces arguments n'est pas suffisante pour dire que la r\xe9glementation de l'IA est n\xe9gative nette, c'est principalement un rappel que nous devons \xeatre prudents dans la fa\xe7on de r\xe9glementer. Le diable est dans les d\xe9tails."]})]}),"\n",(0,i.jsx)(s.h2,{id:"04",children:"Gestion des risques"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La gestion des risques en mati\xe8re de s\xe9curit\xe9 de l'IA s'appuie sur des pratiques \xe9tablies dans d'autres industries."})," La gestion des risques n'est pas unique \xe0 la s\xe9curit\xe9 de l'IA - elle trouve ses racines dans de nombreux domaines, notamment l'a\xe9rospatiale, l'\xe9nergie nucl\xe9aire et les services financiers. Chacun de ces domaines a d\xe9velopp\xe9 des approches sophistiqu\xe9es pour identifier, analyser et att\xe9nuer les dommages potentiels. L'industrie nucl\xe9aire, par exemple, emploie des strat\xe9gies de d\xe9fense en profondeur avec plusieurs syst\xe8mes de s\xe9curit\xe9 redondants, tandis que l'aviation utilise des processus de certification rigoureux et une surveillance continue. La gestion des risques financiers se concentre sur les tests de r\xe9sistance et les r\xe9serves de capital pour pr\xe9venir l'effondrement syst\xe9mique. Ces cadres \xe9tablis fournissent de pr\xe9c\xe9dents pr\xe9cieux pour la s\xe9curit\xe9 de l'IA, bien que les caract\xe9ristiques uniques des syst\xe8mes d'IA - comme leur potentiel de prise de d\xe9cision autonome, leur mise \xe0 l'\xe9chelle rapide et leurs comportements \xe9mergents - n\xe9cessitent des adaptations des approches traditionnelles de gestion des risques."]}),"\n",(0,i.jsxs)(l.A,{title:"Gestion des risques fronti\xe8res de SaferAI",collapsed:!0,children:[(0,i.jsxs)(s.p,{children:["Le contenu de cette bo\xeete a \xe9t\xe9 extrait d'un article de Simeon Campos (",(0,i.jsx)(s.a,{href:"https://www.safer-ai.org/research-posts/a-frontier-ai-risk-management-framework-bridging-the-gap-between-current-ai-practices-and-established-risk-management",children:"Campos et al, 2024"}),")."]}),(0,i.jsx)(s.p,{children:"L'objectif principal de ce flux de travail du cadre de gestion des risques est de s'assurer que les risques restent en permanence en dessous des niveaux inacceptables gr\xe2ce au processus suivant :"}),(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"D\xe9finir une tol\xe9rance au risque - un niveau de risque acceptable bien caract\xe9ris\xe9 qui ne doit pas \xeatre d\xe9pass\xe9."}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Par la mod\xe9lisation des risques, op\xe9rationnaliser cette tol\xe9rance au risque en paires de seuils empiriquement mesurables :"}),"\n"]}),"\n"]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Indicateurs Cl\xe9s de Risque (ICR) : signaux mesurables qui servent de proxys pour les risques (par exemple, la performance du mod\xe8le sur des t\xe2ches sp\xe9cifiques)"}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Indicateurs Cl\xe9s de Contr\xf4le (ICC) : signaux mesurables qui servent de proxys pour l'efficacit\xe9 des mesures d'att\xe9nuation (par exemple, le taux de r\xe9ussite des mesures de confinement)"}),"\n"]}),"\n"]}),(0,i.jsx)(s.p,{children:"Ces seuils suivent une relation triangulaire : pour tout niveau de tol\xe9rance au risque et seuil ICR donn\xe9s, il existe un seuil ICC minimal requis qui doit \xeatre atteint pour maintenir le risque en dessous de la tol\xe9rance."}),(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsx)(s.li,{children:"Mettre en \u0153uvre des mesures d'att\xe9nuation pour atteindre les seuils ICC requis chaque fois que les seuils ICR sont atteints."}),"\n"]}),(0,i.jsx)(s.p,{children:"La tol\xe9rance au risque est distincte des seuils de capacit\xe9s et peut \xeatre d\xe9finie de deux mani\xe8res :"}),(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Quantitativement en utilisant la probabilit\xe9 et la gravit\xe9 : L'approche privil\xe9gi\xe9e exprime la tol\xe9rance au risque comme un produit de probabilit\xe9 quantitative et de gravit\xe9 par unit\xe9 de temps."}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"En utilisant des sc\xe9narios avec des probabilit\xe9s quantitatives : Pour les risques o\xf9 la gravit\xe9 est difficile \xe0 quantifier, la tol\xe9rance au risque peut \xeatre exprim\xe9e comme une limite de probabilit\xe9 quantitative sur un sc\xe9nario dommageable d\xe9fini qualitativement."}),"\n"]}),"\n"]}),(0,i.jsx)(s.p,{children:"Le cadre tire parti du cycle de vie d'un syst\xe8me d'IA pour minimiser la charge sur les d\xe9veloppeurs d'IA, une fois qu'ils terminent l'entra\xeenement du mod\xe8le :"}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Pour \xe9viter les retards pendant la phase d'entra\xeenement, tout le travail pr\xe9paratoire qui ne n\xe9cessite pas le mod\xe8le complet peut \xeatre fait \xe0 l'avance : mod\xe9lisation des risques, d\xe9finition de la tol\xe9rance au risque, identification des seuils ICR et ICC, et pr\xe9diction des mesures d'att\xe9nuation requises en utilisant les lois de mise \xe0 l'\xe9chelle."}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Cela ne laisse que la mesure des ICR et les tests d'intrusion ouverts (pour identifier de nouveaux facteurs de risque qui n'ont pas \xe9t\xe9 identifi\xe9s dans la mod\xe9lisation initiale des risques) pour la phase d'entra\xeenement et de pr\xe9-d\xe9ploiement."}),"\n"]}),"\n"]}),(0,i.jsx)(s.p,{children:"Concernant la gouvernance des risques, le cadre d\xe9crit une structure d'entreprise con\xe7ue pour assurer une prise en compte proportionn\xe9e des risques dans la prise de d\xe9cision. Il comprend :"}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Propri\xe9taire du risque. Le propri\xe9taire du risque est une personne personnellement responsable de la gestion d'un risque particulier."}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Surveillance. La fonction de surveillance est une supervision au niveau du conseil d'administration de la prise de d\xe9cision de la direction g\xe9n\xe9rale concernant le risque."}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Audit. La fonction d'audit est une fonction ind\xe9pendante isol\xe9e des dynamiques de pression par les pairs qui peut remettre en question la prise de d\xe9cision."}),"\n"]}),"\n"]}),(0,i.jsx)(o.A,{src:"./img/h39_Image_25.png",alt:"Saisir la description alternative de l'image",number:"25",label:"3.25",caption:"Figure de la gestion des risques fronti\xe8res de SaferAI : \u201c*Le cadre de gestion des risques pr\xe9sent\xe9 dans cet article permet \xe0 ses utilisateurs de mettre en \u0153uvre quatre fonctions cl\xe9s de gestion des risques : identifier le risque (identification des risques), d\xe9finir des niveaux de risque acceptables et analyser les risques identifi\xe9s (analyse et \xe9valuation des risques), att\xe9nuer le risque pour maintenir des niveaux acceptables (traitement des risques), et s'assurer que les organisations disposent de la structure d'entreprise appropri\xe9e pour ex\xe9cuter ce flux de travail de mani\xe8re coh\xe9rente et rigoureuse (gouvernance des risques).*\u201d - ([Campos et al, 2024](https://www.safer-ai.org/research-posts/a-frontier-ai-risk-management-framework-bridging-the-gap-between-current-ai-practices-and-established-risk-management))"}),(0,i.jsx)(o.A,{src:"./img/SBK_Image_26.png",alt:"Saisir la description alternative de l'image",number:"26",label:"3.26",caption:"[https://ratings.safer-ai.org/](https://ratings.safer-ai.org/), capture d'\xe9cran de SaferAI."})]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Il est \xe9galement important de g\xe9rer les risques qui pourraient survenir avant le d\xe9ploiement."})," Sporadiquement, cela peut aussi \xeatre un signe d'erreur ou un bug (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1909.08593",children:"Ziegler et al., 2020"}),"). Pour \xe9viter les accidents pendant l'entra\xeenement, celui-ci doit \xe9galement \xeatre responsable. L'\xe9valuation des mod\xe8les pour les risques extr\xeames, r\xe9dig\xe9e par des chercheurs d'OpenAI, Anthropic et DeepMind, \xe9tablit une bonne strat\xe9gie de base pour ce qui doit \xeatre fait avant l'entra\xeenement, pendant l'entra\xeenement, avant le d\xe9ploiement et apr\xe8s le d\xe9ploiement. (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2305.15324",children:"Shevlane et al., 2023"}),")"]}),"\n",(0,i.jsx)(o.A,{src:"./img/gAA_Image_27.png",alt:"Saisir la description alternative de l'image",number:"27",label:"3.27",caption:"Un flux de travail pour entra\xeener et d\xe9ployer un mod\xe8le de mani\xe8re responsable. ([Shevlane et al., 2023](https://arxiv.org/abs/2305.15324))"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La mise en \u0153uvre de processus structur\xe9s de gestion des risques est essentielle."})," Cela comprend l'identification, l'\xe9valuation, l'att\xe9nuation et le suivi des risques tout au long du cycle de vie de l'IA. Des cadres comme le NIST AI RMF fournissent des orientations. Le mod\xe8le des \"Trois Lignes de D\xe9fense\", inspir\xe9 d'autres industries, (gestion op\xe9rationnelle, fonctions de risque/conformit\xe9, audit interne) offre une structure pour attribuer les responsabilit\xe9s de gestion des risques au sein d'une organisation (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2212.08364",children:"Jonas Schuett, 2022"}),"). L'approche de \"S\xe9curit\xe9 Affirmative\" propose d'exiger des d\xe9veloppeurs qu'ils construisent de mani\xe8re proactive un dossier fond\xe9 sur des preuves d\xe9montrant la s\xe9curit\xe9 par rapport \xe0 des seuils de risque pr\xe9d\xe9finis, incorporant des preuves techniques et op\xe9rationnelles (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2406.15371",children:"Akash R. Wasil et al., 2024"}),")."]}),"\n",(0,i.jsx)(s.h2,{id:"05",children:"Culture de la s\xe9curit\xe9"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La s\xe9curit\xe9 de l'IA est un probl\xe8me socio-technique qui n\xe9cessite une solution socio-technique."})," En tant que tel, la r\xe9solution de ces d\xe9fis ne peut pas \xeatre purement technique. La culture de la s\xe9curit\xe9 est cruciale pour de nombreuses raisons. Au niveau le plus \xe9l\xe9mentaire, elle garantit que les mesures de s\xe9curit\xe9 sont au moins prises au s\xe9rieux. C'est important car un m\xe9pris pour la s\xe9curit\xe9 peut conduire \xe0 contourner ou rendre inutiles les r\xe9glementations, comme on le voit souvent lorsque des entreprises qui ne se soucient pas de la s\xe9curit\xe9 font face \xe0 des audits (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/iFLNKgZceYyTdwsGz/safety-culture-for-ai-is-important-but-isn-t-going-to-be",children:"Manheim, 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le d\xe9fi de l'adoption \xe0 l'\xe9chelle industrielle des solutions techniques."})," Proposer une solution technique n'est que la premi\xe8re \xe9tape pour aborder la s\xe9curit\xe9. Une solution technique ou un ensemble de proc\xe9dures doit \xeatre int\xe9rioris\xe9 par tous les membres d'une organisation. Lorsque la s\xe9curit\xe9 est consid\xe9r\xe9e comme un objectif cl\xe9 plut\xf4t qu'une contrainte, les organisations manifestent souvent un engagement de la direction envers la s\xe9curit\xe9, une responsabilit\xe9 personnelle pour la s\xe9curit\xe9 et une communication ouverte sur les risques et probl\xe8mes potentiels (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2306.12001",children:"Hendrycks et al., 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Atteindre les standards de l'industrie a\xe9rospatiale."})," Dans l'a\xe9rospatiale, des r\xe9glementations strictes r\xe9gissent le d\xe9veloppement et le d\xe9ploiement de la technologie. Par exemple, un individu ne peut pas simplement construire un avion dans son garage et transporter des passagers sans subir des audits rigoureux et obtenir les autorisations n\xe9cessaires. En revanche, l'industrie de l'IA fonctionne avec beaucoup moins de contraintes, adoptant une approche extr\xeamement permissive du d\xe9veloppement et du d\xe9ploiement, permettant aux d\xe9veloppeurs de cr\xe9er et d\xe9ployer presque n'importe quel mod\xe8le. Ces mod\xe8les peuvent ensuite \xeatre int\xe9gr\xe9s dans des biblioth\xe8ques largement utilis\xe9es, comme Hugging Face, et ces mod\xe8les peuvent alors prolif\xe9rer avec un minimum d'audit. Cette disparit\xe9 souligne le besoin d'un cadre plus structur\xe9 et conscient de la s\xe9curit\xe9 dans l'IA. En adoptant un tel cadre, la communaut\xe9 de l'IA peut \u0153uvrer pour garantir que les technologies d'IA sont d\xe9velopp\xe9es et d\xe9ploy\xe9es de mani\xe8re responsable, en mettant l'accent sur la s\xe9curit\xe9 et l'alignement avec les valeurs soci\xe9tales."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsxs)(s.strong,{children:["La culture de la s\xe9curit\xe9 peut ",(0,i.jsx)(t,{term:"Transformer",definition:'{"definition":"Une architecture de r\xe9seau neuronal qui utilise des m\xe9canismes d\'attention pour traiter des donn\xe9es s\xe9quentielles, particuli\xe8rement efficace pour les t\xe2ches linguistiques..","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:(0,i.jsx)(t,{term:"Transformer",definition:'{"definition":"Une architecture de r\xe9seau neuronal qui utilise des m\xe9canismes d\'attention pour traiter des donn\xe9es s\xe9quentielles, particuli\xe8rement efficace pour les t\xe2ches linguistiques..","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:"transformer"})})," les industries."]})," Les normes dans la poursuite de la s\xe9curit\xe9 peuvent \xeatre un moyen puissant de rep\xe9rer et d\xe9courager les mauvais acteurs. En l'absence d'une forte culture de la s\xe9curit\xe9, les entreprises et les individus peuvent \xeatre tent\xe9s de prendre des raccourcis, conduisant potentiellement \xe0 des r\xe9sultats catastrophiques (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/iFLNKgZceYyTdwsGz/safety-culture-for-ai-is-important-but-isn-t-going-to-be",children:"Manheim, 2023"}),"). L'adoption de la culture de la s\xe9curit\xe9 dans le secteur a\xe9rospatial a transform\xe9 l'industrie, la rendant plus attractive et g\xe9n\xe9rant plus de ventes et de confiance \xe0 long terme. De m\xeame, une culture ambitieuse de la s\xe9curit\xe9 de l'IA n\xe9cessiterait l'\xe9tablissement d'une grande industrie de la s\xe9curit\xe9 et de la s\xfbret\xe9 de l'IA."]}),"\n",(0,i.jsxs)(s.p,{children:["Si elle est atteinte, la culture de la s\xe9curit\xe9 serait un facteur syst\xe9mique qui pr\xe9vient les risques li\xe9s \xe0 l'IA. Plut\xf4t que de se concentrer uniquement sur l'impl\xe9mentation technique d'un syst\xe8me d'IA particulier, l'",(0,i.jsx)(t,{term:"attention",definition:'{"definition":"M\xe9canisme permettant aux mod\xe8les de se concentrer sur les parties pertinentes de l\'entr\xe9e lors de la r\xe9alisation de pr\xe9dictions, en calculant des combinaisons pond\xe9r\xe9es d\'\xe9l\xe9ments d\'entr\xe9e..","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:(0,i.jsx)(t,{term:"attention",definition:'{"definition":"M\xe9canisme permettant aux mod\xe8les de se concentrer sur les parties pertinentes de l\'entr\xe9e lors de la r\xe9alisation de pr\xe9dictions, en calculant des combinaisons pond\xe9r\xe9es d\'\xe9l\xe9ments d\'entr\xe9e..","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:"attention"})})," doit aussi \xeatre port\xe9e sur les pressions sociales, les r\xe9glementations et la culture de la s\xe9curit\xe9. C'est pourquoi il est crucial d'impliquer la communaut\xe9 ",(0,i.jsx)(t,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,i.jsx)(t,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"ML"})})," plus large qui n'est pas encore famili\xe8re avec la s\xe9curit\xe9 de l'IA (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/bffA9WC9nEJhtagQi/introduction-to-pragmatic-ai-safety-pragmatic-ai-safety-1",children:"Hendrycks, 2022"}),")."]}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"Comment augmenter concr\xe8tement la sensibilisation publique et la culture de la s\xe9curit\xe9 ?"})}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Lettres ouvertes :"})," Des initiatives comme les lettres ouvertes, similaires \xe0 celle du Future of Life Institute (",(0,i.jsx)(s.a,{href:"https://futureoflife.org/open-letter/pause-giant-ai-experiments/",children:"Future of Life Institute, 2023"}),"), peuvent susciter un d\xe9bat public sur les risques de l'IA."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Promotion de la culture de la s\xe9curit\xe9 :"})," Promouvoir une culture de la s\xe9curit\xe9 parmi les d\xe9veloppeurs et chercheurs pour traiter de mani\xe8re pr\xe9ventive les risques potentiels, par exemple en organisant des formations internes sur la s\xe9curit\xe9. Par exemple, la formation interne \xe0 la cybers\xe9curit\xe9 est d\xe9j\xe0 courante dans certaines entreprises. Ouvrir des cours de s\xe9curit\xe9 de l'IA dans les universit\xe9s et former les futurs praticiens du ",(0,i.jsx)(t,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,i.jsx)(t,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"ML"})})," est \xe9galement important. Des projets comme DIP visent \xe0 informer le processus d\xe9mocratique sur les risques (",(0,i.jsx)(s.a,{href:"https://controlai.com/dip",children:"Controlai, 2025"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Construction d'un consensus :"})," Cr\xe9er un organisme mondial d'\xe9valuation des risques de l'IA, similaire au GIEC pour le changement climatique, pour standardiser et diffuser les conclusions sur la s\xe9curit\xe9 de l'IA. Le rapport international sur la s\xe9curit\xe9 de l'IA pr\xe9sid\xe9 par Yoshua Bengio est une \xe9tape importante \xe0 cet \xe9gard. Une recherche \xe9mergente vise \xe9galement \xe0 comprendre les racines des d\xe9saccords entre experts (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2502.14870",children:"Severin Field, 2025"}),"), facilitant les d\xe9bats structur\xe9s ou les collaborations contradictoires (",(0,i.jsx)(s.a,{href:"https://forecastingresearch.org/news/ai-adversarial-collaboration",children:"Guest User, 2023"}),"), et d\xe9veloppant des mod\xe8les partag\xe9s de risque (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/sGkRDrpphsu6Jhega/a-model-based-approach-to-ai-existential-risk",children:"Martin, 2023"}),") peut aider \xe0 favoriser la convergence."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"D\xe9monstrations effrayantes / Organismes mod\xe8les :"})," Cr\xe9er des d\xe9monstrations concr\xe8tes d'\xe9checs potentiels d'alignement (comme la tromperie ou la recherche de pouvoir) dans des environnements contr\xf4l\xe9s peut aider \xe0 construire un consensus sur la r\xe9alit\xe9 et la nature des risques et tester les strat\xe9gies d'att\xe9nuation (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1",children:"Evan Hubinger et al., 2023"}),"). Par exemple, la d\xe9monstration autour de diff\xe9rents types de simulation d'alignement a \xe9t\xe9 tr\xe8s efficace et a \xe9t\xe9 beaucoup d\xe9battue dans le discours public (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2412.14093",children:"Greenblatt, 2024"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Sensibilisation et engagement du public :"})," La compr\xe9hension et l'opinion publiques influencent significativement la volont\xe9 politique de r\xe9gulation et l'acceptation sociale des technologies d'IA (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1912.12835",children:"Baobao Zhang et al., 2019"}),"). Les sondages montrent une pr\xe9occupation croissante du public concernant les risques de l'IA (y compris le risque existentiel) et un soutien croissant pour la r\xe9gulation et la prudence (",(0,i.jsx)(s.a,{href:"https://today.yougov.com/technology/articles/51803-americans-increasingly-skeptical-about-ai-artificial-intelligence-effects-poll",children:"Jamie Ballard, 2025"}),"). Les groupes de d\xe9fense jouent un r\xf4le dans la formation du discours public, en \xe9largissant la fen\xeatre d'Overton et en poussant pour des politiques sp\xe9cifiques comme les pauses (",(0,i.jsx)(s.a,{href:"https://theinsideview.ai/holly",children:"Holly Elmore, 2024"}),")."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(a.c,{title:"Notes de bas de page"})]})}function h(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},4768:(e,s,t)=>{t.d(s,{c:()=>c,A:()=>u});var n=t(6540),i=t(3012);const r={footnoteAnchor:"footnoteAnchor_PZpY",footnoteSection:"footnoteSection_waoz",separator:"separator_hp8y",separatorLine:"separatorLine_uVnd",separatorLogo:"separatorLogo_dMBq",footnoteRegistry:"footnoteRegistry_ieVW",registryTitle:"registryTitle_ybBb",footnoteList:"footnoteList_Kjil",footnoteItem:"footnoteItem_aLZ2",footnoteNumber:"footnoteNumber_dA3X",footnoteContent:"footnoteContent_Xfmt",backButton:"backButton_vSVm",highlighted:"highlighted_YFU0",highlight:"highlight_jxG3",highlightDark:"highlightDark_Ekom"};var a=t(4848);function l(e,s){void 0===s&&(s=!0);const t=document.getElementById(e);t&&(t.scrollIntoView({behavior:"smooth"}),s&&(t.classList.add(r.highlighted),setTimeout((()=>t.classList.remove(r.highlighted)),1500)))}function o(e){return e?e.replace(/\[([^\]]+)\]\(([^)]+)\)/g,'<a href="$2" target="_blank" rel="noopener noreferrer">$1</a>'):""}function u(e){let{id:s,text:t,number:u}=e;const c=s||`footnote-${Math.random().toString(36).substr(2,9)}`,d="string"==typeof t?o(t):t;return(0,n.useEffect)((()=>{const e=setTimeout((()=>{const e=document.getElementById(`footnote-clone-${c}`);e&&t&&(e.innerHTML="string"==typeof t?o(t):t.toString())}),100);return()=>clearTimeout(e)}),[c,t]),(0,a.jsx)(i.Mn,{content:(0,a.jsx)("div",{dangerouslySetInnerHTML:{__html:d}}),children:(0,a.jsx)("sup",{id:`footnote-ref-${c}`,className:r.footnoteAnchor,onClick:e=>{(e.ctrlKey||e.metaKey)&&(e.preventDefault(),l(`footnote-content-${c}`))},"data-footnote-number":u||"?",children:u||"*"})})}function c(e){let{title:s="References"}=e;const[t,o]=(0,n.useState)([]);return(0,n.useEffect)((()=>{const e=document.querySelectorAll('[id^="footnote-ref-"]'),s=Array.from(e).map((e=>({id:e.id.replace("footnote-ref-",""),number:e.getAttribute("data-footnote-number")||e.textContent})));o(s)}),[]),t.length?(0,a.jsxs)("div",{className:r.footnoteSection,children:[(0,a.jsxs)("div",{className:r.separator,children:[(0,a.jsx)("div",{className:r.separatorLine}),(0,a.jsx)("img",{src:"/img/logo_samples/01-test.svg",alt:"Atlas logo",className:r.separatorLogo}),(0,a.jsx)("div",{className:r.separatorLine})]}),(0,a.jsxs)("div",{className:r.footnoteRegistry,children:[(0,a.jsx)("h2",{className:r.registryTitle,children:s}),(0,a.jsx)("ol",{className:r.footnoteList,children:t.map((e=>(0,a.jsxs)("li",{id:`footnote-content-${e.id}`,className:r.footnoteItem,value:parseInt(e.number)||void 0,children:[(0,a.jsx)(i.Mn,{content:"Back to reference",children:(0,a.jsxs)("button",{className:r.footnoteNumber,onClick:()=>l(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:[e.number,"."]})}),(0,a.jsx)("div",{className:r.footnoteContent,children:(0,a.jsx)("div",{id:`footnote-clone-${e.id}`})}),(0,a.jsx)(i.Mn,{content:"Back to reference",children:(0,a.jsx)("button",{className:r.backButton,onClick:()=>l(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:"\u21a9"})})]},e.id)))})]})]}):null}}}]);
"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[4890],{8146:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>h,contentTitle:()=>p,default:()=>b,frontMatter:()=>m,metadata:()=>a,toc:()=>g});const a=JSON.parse('{"id":"chapters/01/4","title":"Mise \xe0 l\'\xe9chelle","description":"Dans la section pr\xe9c\xe9dente, nous avons explor\xe9 comment nous pouvons mesurer les capacit\xe9s de l\'IA selon des dimensions continues de performance et de g\xe9n\xe9ralit\xe9. Maintenant, nous allons examiner l\'un des facteurs les plus importants \xe0 l\'origine des am\xe9liorations de ces capacit\xe9s : l\'\xe9chelle.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/01/04.md","sourceDirName":"chapters/01","slug":"/chapters/01/04","permalink":"/fr/chapters/01/04","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/01/04.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"4","title":"Mise \xe0 l\'\xe9chelle","sidebar_label":"1.4 Mise \xe0 l\'\xe9chelle","sidebar_position":5,"slug":"/chapters/01/04","reading_time_core":"14 min","reading_time_optional":"6 min","pagination_prev":"chapters/01/3","pagination_next":"chapters/01/5"},"sidebar":"docs","previous":{"title":"1.3 Intelligence","permalink":"/fr/chapters/01/03"},"next":{"title":"1.5 Pr\xe9visions","permalink":"/fr/chapters/01/05"}}');var i=n(4848),t=n(8453),o=n(3989),r=n(3931),l=n(2482),d=n(8559),u=n(9585),c=n(2501);const m={id:4,title:"Mise \xe0 l'\xe9chelle",sidebar_label:"1.4 Mise \xe0 l'\xe9chelle",sidebar_position:5,slug:"/chapters/01/04",reading_time_core:"14 min",reading_time_optional:"6 min",pagination_prev:"chapters/01/3",pagination_next:"chapters/01/5"},p="Mise \xe0 l'\xe9chelle",h={},g=[{value:"La Dure Le\xe7on",id:"01",level:2},{value:"Lois de mise \xe0 l&#39;\xe9chelle",id:"02",level:2},{value:"Hypoth\xe8se de mise \xe0 l&#39;\xe9chelle",id:"03",level:2}];function f(e){const s={a:"a",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components},{GlossaryTerm:n}=s;return n||function(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"mise-\xe0-l\xe9chelle",children:"Mise \xe0 l'\xe9chelle"})}),"\n",(0,i.jsx)(s.p,{children:"Dans la section pr\xe9c\xe9dente, nous avons explor\xe9 comment nous pouvons mesurer les capacit\xe9s de l'IA selon des dimensions continues de performance et de g\xe9n\xe9ralit\xe9. Maintenant, nous allons examiner l'un des facteurs les plus importants \xe0 l'origine des am\xe9liorations de ces capacit\xe9s : l'\xe9chelle."}),"\n",(0,i.jsx)(s.h2,{id:"01",children:"La Dure Le\xe7on"}),"\n",(0,i.jsxs)(s.p,{children:["Nous supposons que la plupart d'entre vous sont probablement all\xe9s \xe0 l'universit\xe9 \xe0 une \xe9poque o\xf9 l'",(0,i.jsx)(n,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,i.jsx)(n,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})})," et l'IA signifiaient \xe0 peu pr\xe8s la m\xeame chose, ou plut\xf4t o\xf9 l'",(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})})," et l'IA signifiaient la m\xeame chose. Cela n'a pas toujours \xe9t\xe9 vrai. Au d\xe9but de l'histoire de l'intelligence artificielle, les chercheurs ont adopt\xe9 des approches tr\xe8s diff\xe9rentes pour cr\xe9er des syst\xe8mes intelligents. Ils croyaient que la cl\xe9 de l'intelligence artificielle \xe9tait d'encoder soigneusement les connaissances et l'expertise humaines dans des programmes informatiques. Cela a conduit \xe0 des choses comme des syst\xe8mes experts remplis de r\xe8gles cr\xe9\xe9es manuellement et des moteurs d'\xe9checs programm\xe9s avec des principes strat\xe9giques sophistiqu\xe9s. Cependant, encore et encore, les chercheurs ont appris ce que nous appelons maintenant la dure le\xe7on."]}),"\n",(0,i.jsx)(l.A,{speaker:"Richard Sutton",position:"Professeur Universit\xe9 de l'Alberta, Fondateur, Institut de Recherche Openmind",date:"2019",source:"([Sutton, 2019](http://www.incompleteideas.net/IncIdeas/BitterLesson.html))",children:(0,i.jsx)(s.p,{children:"La plus grande le\xe7on que l'on peut tirer de 70 ans de recherche en IA est que les m\xe9thodes g\xe9n\xe9rales qui exploitent le calcul sont finalement les plus efficaces, et avec une large marge. [...] La dure le\xe7on est bas\xe9e sur les observations historiques que 1) les chercheurs en IA ont souvent essay\xe9 d'int\xe9grer des connaissances dans leurs agents, 2) cela aide toujours \xe0 court terme et est personnellement satisfaisant pour le chercheur, mais 3) \xe0 long terme, cela stagne et inhibe m\xeame les progr\xe8s ult\xe9rieurs, et 4) les progr\xe8s d\xe9cisifs arrivent finalement par une approche oppos\xe9e bas\xe9e sur l'augmentation du calcul par la recherche et l'apprentissage."})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Qu'est-ce qui rend cette le\xe7on am\xe8re ?"})," L'amertume vient de la d\xe9couverte que des d\xe9cennies d'ing\xe9nierie humaine minutieuse et de perspicacit\xe9 \xe9taient finalement moins importantes que des algorithmes simples plus du calcul. Aux \xe9checs, les chercheurs qui avaient pass\xe9 des ann\xe9es \xe0 encoder les connaissances des grands ma\xeetres ont vu des approches \"force brute\" bas\xe9es sur la recherche comme Deep Blue vaincre le champion du monde Garry Kasparov. En vision par ordinateur, les d\xe9tecteurs de caract\xe9ristiques cr\xe9\xe9s manuellement ont \xe9t\xe9 surpass\xe9s par des r\xe9seaux de neurones convolutifs qui ont appris leurs propres caract\xe9ristiques \xe0 partir des donn\xe9es. En reconnaissance vocale, les syst\xe8mes bas\xe9s sur la compr\xe9hension humaine de la phon\xe9tique ont \xe9t\xe9 d\xe9pass\xe9s par des approches statistiques utilisant des mod\xe8les de Markov cach\xe9s (",(0,i.jsx)(s.a,{href:"http://www.incompleteideas.net/IncIdeas/BitterLesson.html",children:"Sutton, 2019"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Est-ce que la dure le\xe7on signifie que nous n'avons pas besoin d'ing\xe9nierie humaine ?"})," Le fait que l'ing\xe9niosit\xe9 humaine joue un r\xf4le moins important dans l'am\xe9lioration de l'IA est un point subtil qui peut \xeatre facilement mal compris. L'architecture du ",(0,i.jsx)(n,{term:"Transformer",definition:'{"definition":"Une architecture de r\xe9seau neuronal qui utilise des m\xe9canismes d\'attention pour traiter des donn\xe9es s\xe9quentielles, particuli\xe8rement efficace pour les t\xe2ches linguistiques..","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:(0,i.jsx)(n,{term:"Transformer",definition:'{"definition":"Une architecture de r\xe9seau neuronal qui utilise des m\xe9canismes d\'attention pour traiter des donn\xe9es s\xe9quentielles, particuli\xe8rement efficace pour les t\xe2ches linguistiques..","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:"transformer"})})," par exemple pourrait sembler contredire la dure le\xe7on car elle repose sur des innovations architecturales sophistiqu\xe9es. L'ing\xe9niosit\xe9 humaine est importante, mais la subtilit\xe9 r\xe9side dans la reconnaissance qu'il existe une diff\xe9rence entre deux types d'ing\xe9nierie humaine :"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Am\xe9liorations au niveau algorithmique :"})," Celles-ci font un meilleur usage du calcul existant, comme : de meilleurs optimiseurs (Adam), des innovations architecturales (transformers, m\xe9canismes d'",(0,i.jsx)(n,{term:"attention",definition:'{"definition":"M\xe9canisme permettant aux mod\xe8les de se concentrer sur les parties pertinentes de l\'entr\xe9e lors de la r\xe9alisation de pr\xe9dictions, en calculant des combinaisons pond\xe9r\xe9es d\'\xe9l\xe9ments d\'entr\xe9e..","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:(0,i.jsx)(n,{term:"attention",definition:'{"definition":"M\xe9canisme permettant aux mod\xe8les de se concentrer sur les parties pertinentes de l\'entr\xe9e lors de la r\xe9alisation de pr\xe9dictions, en calculant des combinaisons pond\xe9r\xe9es d\'\xe9l\xe9ments d\'entr\xe9e..","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:"attention"})}),") ou des approches d'entra\xeenement (meilleurs programmes de ",(0,i.jsx)(n,{term:"learning rate",definition:'{"definition":"Hyperparam\xe8tre qui contr\xf4le le degr\xe9 de mise \xe0 jour des param\xe8tres du mod\xe8le \xe0 chaque \xe9tape de la descente de gradient..","source":"","aliases":["Learning Rate","Taux d\'apprentissage","taux d\'apprentissage"]}',children:(0,i.jsx)(n,{term:"learning rate",definition:'{"definition":"Hyperparam\xe8tre qui contr\xf4le le degr\xe9 de mise \xe0 jour des param\xe8tres du mod\xe8le \xe0 chaque \xe9tape de la descente de gradient..","source":"","aliases":["Learning Rate","Taux d\'apprentissage","taux d\'apprentissage"]}',children:"taux d'apprentissage"})}),")."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Am\xe9liorations d'ing\xe9nierie sp\xe9cifiques au domaine :"})," Celles-ci tentent d'encoder les connaissances humaines, comme : des architectures sp\xe9ciales con\xe7ues pour des probl\xe8mes sp\xe9cifiques, des caract\xe9ristiques ou r\xe8gles cr\xe9\xe9es manuellement ou des biais inductifs sp\xe9cifiques aux t\xe2ches."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["La dure le\xe7on ne s'oppose pas \xe0 toute ing\xe9nierie humaine - elle met sp\xe9cifiquement en garde contre le second type. L'architecture du ",(0,i.jsx)(n,{term:"Transformer",definition:'{"definition":"Une architecture de r\xe9seau neuronal qui utilise des m\xe9canismes d\'attention pour traiter des donn\xe9es s\xe9quentielles, particuli\xe8rement efficace pour les t\xe2ches linguistiques..","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:(0,i.jsx)(n,{term:"Transformer",definition:'{"definition":"Une architecture de r\xe9seau neuronal qui utilise des m\xe9canismes d\'attention pour traiter des donn\xe9es s\xe9quentielles, particuli\xe8rement efficace pour les t\xe2ches linguistiques..","source":"[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)","aliases":["Transformer"]}',children:"transformer"})})," illustre ce mod\xe8le - elle n'encode aucune connaissance sp\xe9cifique sur le langage, mais fournit plut\xf4t un m\xe9canisme g\xe9n\xe9ral pour apprendre des mod\xe8les qui devient de plus en plus puissant \xe0 mesure que nous augmentons le calcul et les donn\xe9es."]}),"\n",(0,i.jsx)(s.h2,{id:"02",children:"Lois de mise \xe0 l'\xe9chelle"}),"\n",(0,i.jsx)(r.A,{type:"youtube",videoId:"5eqRuVp65eY",number:"3",label:"1.3",caption:"Explication vid\xe9o facultative des lois d'\xe9chelle."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Pourquoi les laboratoires d'IA s'int\xe9ressent-ils aux lois de mise \xe0 l'\xe9chelle ?"})," L'entra\xeenement de grands mod\xe8les d'IA est extr\xeamement co\xfbteux - potentiellement des centaines de millions de dollars pour les mod\xe8les de pointe. Les lois de mise \xe0 l'\xe9chelle aident les laboratoires \xe0 prendre des d\xe9cisions cruciales concernant l'allocation des ressources : Doivent-ils d\xe9penser plus en GPU ou en acquisition de donn\xe9es d'entra\xeenement ? Doivent-ils entra\xeener un plus grand mod\xe8le pendant moins de temps ou un plus petit mod\xe8le plus longtemps ? Par exemple, avec un budget de calcul fixe, ils pourraient devoir choisir entre entra\xeener un mod\xe8le de 20 milliards de param\xe8tres sur 40% de leurs donn\xe9es ou un mod\xe8le de 200 milliards de param\xe8tres sur seulement 4%. Se tromper dans ces compromis peut gaspiller d'\xe9normes ressources. Il est donc important de pouvoir avoir une relation pr\xe9visible entre la fa\xe7on dont vous investissez votre argent et le niveau de capacit\xe9s que vous obtenez \xe0 la fin."]}),"\n",(0,i.jsx)(c.A,{src:"./img/rWX_Image_28.png",alt:"Saisir la description alternative de l'image",number:"21",label:"1.21",caption:"Exemple de capacit\xe9s augmentant avec l'accroissement d'une des variables dans les lois d'\xe9chelle - le nombre de param\xe8tres. La m\xeame architecture de mod\xe8le (Parti) a \xe9t\xe9 utilis\xe9e pour g\xe9n\xe9rer une image avec une invite identique, la seule diff\xe9rence entre les mod\xe8les \xe9tant la taille des param\xe8tres. Il y a des sauts notables de qualit\xe9, et quelque part entre 3 milliards et 20 milliards de param\xe8tres, le mod\xe8le acquiert la capacit\xe9 d'\xe9peler correctement les mots. ([Yu et al., 2022](https://arxiv.org/abs/2206.10789))"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Que sont les lois de mise \xe0 l'\xe9chelle ?"})," Les lois de mise \xe0 l'\xe9chelle sont des relations math\xe9matiques qui d\xe9crivent comment les performances d'un syst\xe8me d'IA changent lorsque nous faisons varier des param\xe8tres cl\xe9s comme la taille du mod\xe8le, la taille du jeu de donn\xe9es et la puissance de calcul. Ce sont des relations empiriques de loi de puissance qui ont \xe9t\xe9 observ\xe9es sur plusieurs ordres de grandeur. Les variables cl\xe9s impliqu\xe9es sont :"]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Calcul (C) :"})," Cela repr\xe9sente la puissance de traitement totale utilis\xe9e pendant l'entra\xeenement, mesur\xe9e en op\xe9rations \xe0 virgule flottante (FLOPs). Consid\xe9rez cela comme le \"budget\" d'entra\xeenement - plus de calcul signifie soit un entra\xeenement plus long, soit l'utilisation de mat\xe9riel plus puissant, soit les deux. Bien qu'avoir plus de GPU aide \xe0 augmenter la capacit\xe9 de calcul, le calcul fait ultimement r\xe9f\xe9rence au nombre total d'op\xe9rations effectu\xe9es, pas seulement au mat\xe9riel."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Param\xe8tres (N) :"})," Ce sont les nombres ajustables dans le mod\xe8le qui sont modifi\xe9s pendant l'entra\xeenement - comme des boutons que le mod\xe8le peut ajuster pour mieux s'adapter aux donn\xe9es. Plus de param\xe8tres permettent au mod\xe8le d'apprendre des motifs plus complexes mais n\xe9cessitent plus de calcul par \xe9tape d'entra\xeenement. Les mod\xe8les de pointe actuels ont des centaines de milliards de param\xe8tres."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Taille du jeu de donn\xe9es (D) :"})," Cela mesure le nombre d'exemples sur lesquels le mod\xe8le s'entra\xeene (g\xe9n\xe9ralement mesur\xe9 en tokens pour les mod\xe8les de langage). Plus le jeu de donn\xe9es est grand, plus le mod\xe8le peut lire d'informations. Simultan\xe9ment, pour lire et apprendre de plus de donn\xe9es, les sessions d'entra\xeenement doivent g\xe9n\xe9ralement \xeatre plus longues, ce qui augmente \xe0 son tour le calcul total n\xe9cessaire avant que le mod\xe8le puisse \xeatre consid\xe9r\xe9 comme \"entra\xeen\xe9\"."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Perte (L) :"})," Cela mesure la performance du mod\xe8le sur son objectif d'entra\xeenement. C'est ce que nous essayons de minimiser, et cela tend \xe0 s'am\xe9liorer \xe0 mesure que nous augmentons ces variables."]}),"\n",(0,i.jsx)(c.A,{src:"./img/nQO_Image_18.png",alt:"Saisir la description alternative de l'image",number:"22",label:"1.22",caption:"La performance de la mod\xe9lisation du langage s'am\xe9liore progressivement lorsque nous augmentons la taille du mod\xe8le, la taille du jeu de donn\xe9es et la quantit\xe9 de calcul utilis\xe9e pour l'entra\xeenement. Pour une performance optimale, les trois facteurs doivent \xeatre mis \xe0 l'\xe9chelle en tandem. La performance empirique a une relation en loi de puissance avec chaque facteur individuel lorsqu'elle n'est pas limit\xe9e par les deux autres. ([Kaplan et al., 2020](https://arxiv.org/abs/2001.08361))"}),"\n",(0,i.jsx)(o.A,{src:"https://www.metaculus.com/questions/embed/4055",width:"100%",height:"600px",loading:"lazy",frameBorder:"0",number:"8",label:"1.8",caption:"R\xe9sultats du march\xe9 pr\xe9dictif sur - La premi\xe8re AGI sera-t-elle bas\xe9e sur l'apprentissage profond ? ([Metaculus, 2020](https://www.metaculus.com/questions/4055/first-agi-based-on-deep-learning/))"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les lois de mise \xe0 l'\xe9chelle initiales d'OpenAI en 2020."})," Pour d\xe9terminer les relations entre diff\xe9rentes variables qui pourraient contribuer \xe0 l'\xe9chelle, OpenAI a men\xe9 une s\xe9rie d'exp\xe9riences. Pour avoir une id\xe9e intuitive de la fa\xe7on dont ils ont \xe9tabli les lois de mise \xe0 l'\xe9chelle, vous pouvez imaginer que pendant l'entra\xeenement d'un mod\xe8le, vous pouvez maintenir certaines variables fixes tout en faisant varier d'autres et voir comment la perte change. Finalement, cela permet de voir certains motifs. Par exemple, la taille du jeu de donn\xe9es peut \xeatre maintenue constante, tandis que le nombre de param\xe8tres et le temps d'entra\xeenement varient, ou le nombre de param\xe8tres est maintenu constant et les quantit\xe9s de donn\xe9es varient, etc... Ainsi, nous pouvons obtenir une mesure de la contribution relative de chacun \xe0 la performance globale. Si ces relations restent vraies pour de nombreuses architectures de mod\xe8les et t\xe2ches diff\xe9rentes, cela sugg\xe8re qu'elles capturent quelque chose de fondamental sur les syst\xe8mes d'",(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})}),". C'est ainsi que la premi\xe8re g\xe9n\xe9ration de lois de mise \xe0 l'\xe9chelle est apparue chez OpenAI. Par exemple, selon ces lois, si vous avez 10 fois plus de calcul, vous devriez augmenter la taille du mod\xe8le d'environ 5 fois et la taille des donn\xe9es de seulement 2 fois. (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2001.08361",children:"Kaplan et al., 2020"}),")"]}),"\n",(0,i.jsx)(c.A,{src:"./img/o2j_Image_19.png",alt:"Saisir la description alternative de l'image",number:"23",label:"1.23",caption:"Le premier article d'OpenAI sur les lois d'\xe9chelle indiquait que pour un entra\xeenement optimal en termes de calcul, la majeure partie de l'augmentation devrait aller vers une taille de mod\xe8le accrue. Une augmentation relativement faible des donn\xe9es est n\xe9cessaire pour \xe9viter la r\xe9utilisation. ([Kaplan et al., 2020](https://arxiv.org/abs/2001.08361))"}),"\n",(0,i.jsx)(o.A,{src:"https://ourworldindata.org/grapher/exponential-growth-of-parameters-in-notable-ai-systems?tab=chart",width:"100%",height:"600px",loading:"lazy",allow:"web-share; clipboard-write",frameBorder:"0",number:"9",label:"1.9",caption:"Croissance exponentielle des param\xe8tres dans les syst\xe8mes d'IA notables. Les param\xe8tres sont des variables dans un syst\xe8me d'IA dont les valeurs sont ajust\xe9es pendant l'entra\xeenement pour \xe9tablir comment les donn\xe9es d'entr\xe9e sont transform\xe9es en sortie souhait\xe9e ; par exemple, les poids de connexion dans un r\xe9seau neuronal artificiel ([Giattino et al., 2023](https://ourworldindata.org/artificial-intelligence))."}),"\n",(0,i.jsx)(o.A,{src:"https://ourworldindata.org/grapher/exponential-growth-of-datapoints-used-to-train-notable-ai-systems?tab=chart",width:"100%",height:"600px",loading:"lazy",allow:"web-share; clipboard-write",frameBorder:"0",number:"10",label:"1.10",caption:"Croissance exponentielle des points de donn\xe9es utilis\xe9s pour entra\xeener les syst\xe8mes d'IA notables. Chaque domaine a une unit\xe9 de point de donn\xe9es sp\xe9cifique ; par exemple, pour la vision ce sont des images, pour le langage ce sont des mots, et pour les jeux ce sont des pas de temps. Cela signifie que les syst\xe8mes ne peuvent \xeatre compar\xe9s directement que dans le m\xeame domaine ([Giattino et al., 2023](https://ourworldindata.org/artificial-intelligence))."}),"\n",(0,i.jsx)(o.A,{src:"https://ourworldindata.org/grapher/exponential-growth-of-computation-in-the-training-of-notable-ai-systems?tab=chart",width:"100%",height:"600px",loading:"lazy",allow:"web-share; clipboard-write",frameBorder:"0",number:"11",label:"1.11",caption:"Croissance exponentielle du calcul dans l'entra\xeenement des syst\xe8mes d'IA notables. Le calcul est mesur\xe9 en peta FLOP total, qui est 10e15 op\xe9rations en virgule flottante ([Giattino et al., 2023](https://ourworldindata.org/artificial-intelligence))."}),"\n",(0,i.jsx)(o.A,{src:"https://ourworldindata.org/grapher/artificial-intelligence-training-computation?tab=chart",width:"100%",height:"600px",loading:"lazy",allow:"web-share; clipboard-write",frameBorder:"0",number:"12",label:"1.12",caption:"Calcul utilis\xe9 pour entra\xeener des syst\xe8mes d'intelligence artificielle notables, par domaine. Le calcul est mesur\xe9 en petaFLOP total, qui est 10e15 op\xe9rations en virgule flottante. Estim\xe9 \xe0 partir de la litt\xe9rature sur l'IA, bien qu'avec une certaine incertitude. Les estimations devraient \xeatre pr\xe9cises \xe0 un facteur 2 pr\xe8s, ou un facteur 5 pour les mod\xe8les r\xe9cents non divulgu\xe9s comme GPT-4 ([Giattino et al., 2023](https://ourworldindata.org/artificial-intelligence))."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Mise \xe0 jour des lois de mise \xe0 l'\xe9chelle de DeepMind en 2022."})," DeepMind a d\xe9couvert que la plupart des grands mod\xe8les de langage \xe9taient en r\xe9alit\xe9 significativement surparam\xe9tr\xe9s par rapport \xe0 la quantit\xe9 de donn\xe9es sur lesquelles ils \xe9taient entra\xeen\xe9s. Les lois de mise \xe0 l'\xe9chelle Chinchilla ont montr\xe9 que pour une performance optimale, les mod\xe8les devraient \xeatre entra\xeen\xe9s sur environ 20 fois plus de tokens de donn\xe9es qu'ils n'ont de param\xe8tres. Cela signifiait que de nombreux mod\xe8les de premier plan auraient pu obtenir de meilleures performances avec des tailles plus petites, mais avec plus de donn\xe9es. Elles ont \xe9t\xe9 appel\xe9es lois de mise \xe0 l'\xe9chelle Chinchilla car les lois ont \xe9t\xe9 d\xe9montr\xe9es en utilisant un mod\xe8le appel\xe9 Chinchilla. C'\xe9tait un mod\xe8le de 70 milliards de param\xe8tres entra\xeen\xe9 sur plus de donn\xe9es, qui surpassait des mod\xe8les beaucoup plus grands comme Gopher (280 milliards de param\xe8tres) malgr\xe9 l'utilisation de la m\xeame quantit\xe9 de calcul. Donc selon ces lois, pour une performance optimale, vous devriez augmenter la taille du mod\xe8le et la taille du jeu de donn\xe9es dans des proportions \xe0 peu pr\xe8s \xe9gales - si vous obtenez 10 fois plus de calcul, vous devriez rendre votre mod\xe8le environ 3,1 fois plus grand et vos donn\xe9es environ 3,1 fois plus grandes (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2203.15556",children:"Hoffmann et al., 2022"}),")."]}),"\n",(0,i.jsxs)(d.A,{title:"Mise \xe0 jour des lois de mise \xe0 l'\xe9chelle neuronale bris\xe9es (BNSL) en 2023",collapsed:!0,children:[(0,i.jsxs)(s.p,{children:["La recherche a montr\xe9 que la performance ne s'am\xe9liore pas toujours de mani\xe8re r\xe9guli\xe8re - il peut y avoir des transitions brusques, des plateaux temporaires, ou m\xeame des p\xe9riodes o\xf9 la performance se d\xe9grade avant de s'am\xe9liorer. Des exemples de cela incluent des choses comme le \"Grokking\", o\xf9 les mod\xe8les atteignent soudainement une forte g\xe9n\xe9ralisation apr\xe8s de nombreuses \xe9tapes d'entra\xeenement, ou la double descente profonde, o\xf9 l'augmentation de la taille du mod\xe8le nuit d'abord puis aide la performance. Plut\xf4t que de simples lois de puissance, BNSL utilise une forme fonctionnelle plus flexible qui peut capturer ces comportements complexes. Cela permet des pr\xe9dictions plus pr\xe9cises du comportement de mise \xe0 l'\xe9chelle, particuli\xe8rement autour des discontinuit\xe9s et des transitions. Les lois de mise \xe0 l'\xe9chelle sont une bonne base, mais des sauts discontinus dans les capacit\xe9s et des changements brusques sont toujours possibles (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2210.14891",children:"Caballero et al., 2023"}),")."]}),(0,i.jsx)(c.A,{src:"./img/55h_Image_36.png",alt:"Saisir la description alternative de l'image",number:"24",label:"1.24",caption:"Un exemple de loi d'\xe9chelle neuronale bris\xe9e (ligne noire pleine fonc\xe9e) (avec 3 ruptures o\xf9 les lignes pointill\xe9es violettes croisent la ligne noire pleine fonc\xe9e) contient 4 segments de loi de puissance individuels (o\xf9 les lignes pointill\xe9es jaune, bleue, rouge et verte se superposent avec la ligne noire pleine fonc\xe9e). Les 1\xe8re et 2\xe8me ruptures sont tr\xe8s lisses ; la 3\xe8me rupture est tr\xe8s nette ([Caballero et al., 2023](https://arxiv.org/abs/2210.14891))."})]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"En quoi diff\xe8rent la mise \xe0 l'\xe9chelle de l'entra\xeenement et de l'inf\xe9rence ?"})," La mise \xe0 l'\xe9chelle de l'entra\xeenement implique d'utiliser plus de calcul pendant l'entra\xeenement initial du mod\xe8le en utilisant des mod\xe8les plus grands, en s'entra\xeenant plus longtemps, ou en utilisant des jeux de donn\xe9es plus importants. Une autre fa\xe7on que nous pourrions ne pas prendre en compte en utilisant les lois de mise \xe0 l'\xe9chelle s'appelle la mise \xe0 l'\xe9chelle du temps d'inf\xe9rence. Celle-ci utilise plut\xf4t plus de calcul au moment de l'ex\xe9cution gr\xe2ce \xe0 des techniques comme le prompt avec cha\xeene de pens\xe9e, l'\xe9chantillonnage r\xe9p\xe9t\xe9, ou la recherche arborescente. Par exemple, vous pouvez soit entra\xeener un tr\xe8s grand mod\xe8le qui g\xe9n\xe8re directement des sorties de haute qualit\xe9, soit entra\xeener un mod\xe8le plus petit qui atteint des performances similaires en utilisant plus de calcul pour r\xe9fl\xe9chir aux probl\xe8mes \xe9tape par \xe9tape au moment de l'inf\xe9rence."]}),"\n",(0,i.jsx)(s.h2,{id:"03",children:"Hypoth\xe8se de mise \xe0 l'\xe9chelle"}),"\n",(0,i.jsx)(u.A,{term:"Hypoth\xe8se de forte \xe9volutivit\xe9",source:"([Gwern, 2020](https://gwern.net/scaling-hypothesis))",number:"4",label:"1.4",children:(0,i.jsxs)(s.p,{children:["L'hypoth\xe8se forte de mise \xe0 l'\xe9chelle propose que le simple fait d'augmenter les architectures actuelles des ",(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:"mod\xe8les de base"})})," avec plus de puissance de calcul et de donn\xe9es sera suffisant pour atteindre des capacit\xe9s d'IA transformative et potentiellement m\xeame l'ASI."]})}),"\n",(0,i.jsx)(o.A,{src:"https://ourworldindata.org/grapher/ai-performance-knowledge-tests-vs-training-computation?tab=chart",width:"100%",height:"600px",loading:"lazy",allow:"web-share; clipboard-write",frameBorder:"0",number:"13",label:"1.13",caption:"Intelligence artificielle : Performance sur les tests de connaissances vs. calcul d'entra\xeenement. La performance sur les tests de connaissances est mesur\xe9e avec le benchmark MMLU, ici avec un apprentissage \xe0 5 exemples, qui \xe9value la pr\xe9cision d'un mod\xe8le apr\xe8s avoir re\xe7u seulement cinq exemples pour chaque t\xe2che. Le calcul d'entra\xeenement est mesur\xe9 en petaFLOP total, qui est 10e15 op\xe9rations en virgule flottante ([Giattino et al., 2023](https://ourworldindata.org/artificial-intelligence))."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Qu'est-ce que l'hypoth\xe8se forte de mise \xe0 l'\xe9chelle ?"})," Cette vision sugg\xe8re que nous avons d\xe9j\xe0 tous les composants fondamentaux n\xe9cessaires - il s'agit simplement de les agrandir, en suivant les lois \xe9tablies de mise \xe0 l'\xe9chelle. (",(0,i.jsx)(s.a,{href:"https://gwern.net/scaling-hypothesis",children:"Branwen, 2020"}),") Il y a un d\xe9bat anim\xe9 autour de cette hypoth\xe8se et nous ne pouvons pas couvrir tous les arguments. Nous pouvons vous donner un bref aper\xe7u dans les paragraphes suivants."]}),"\n",(0,i.jsxs)(s.p,{children:["Parmi les partisans figurent OpenAI (",(0,i.jsx)(s.a,{href:"https://openai.com/blog/planning-for-agi-and-beyond",children:"OpenAI, 2023"}),"), le PDG d'Anthropic Dario Amodei (",(0,i.jsx)(s.a,{href:"https://www.dwarkeshpatel.com/p/dario-amodei",children:"Amodei, 2023"}),"), Conjecture (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/PE22QJSww8mpwh7bt/agi-in-sight-our-look-at-the-game-board",children:"Conjecture, 2023"}),"), l'\xe9quipe de s\xe9curit\xe9 de DeepMind (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/GctJD5oCDRxCspEaZ/clarifying-ai-x-risk",children:"DeepMind, 2022"}),"), et d'autres. Selon l'\xe9quipe de DeepMind, il n'y a \"",(0,i.jsxs)(s.em,{children:["pas besoin de beaucoup plus d'innovations fondamentales pour l'AGI. Les ",(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:"mod\xe8les de base"})})," d'",(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})})," mis \xe0 l'\xe9chelle avec l'apprentissage par renforcement \xe0 partir de retours humains (RLHF) devraient suffire"]}),'" (',(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/GctJD5oCDRxCspEaZ/clarifying-ai-x-risk",children:"DeepMind, 2022"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Quels sont les principaux arguments en faveur de l'hypoth\xe8se forte de mise \xe0 l'\xe9chelle ?"})," Les preuves les plus convaincantes pour cette vision viennent des observations empiriques des progr\xe8s ces derni\xe8res ann\xe9es. Les chercheurs d\xe9veloppent des algorithmes qui suivent le principe de la le\xe7on am\xe8re depuis de nombreuses ann\xe9es (en se concentrant sur des m\xe9thodes g\xe9n\xe9rales qui exploitent efficacement le calcul). Mais m\xeame lorsque les chercheurs ont d\xe9velopp\xe9 des algorithmes sophistiqu\xe9s suivant les principes de la le\xe7on am\xe8re, ces am\xe9liorations ne repr\xe9sentent encore que 35% des gains de performance dans les mod\xe8les de langage en 2024, les 65% restants provenant purement de l'augmentation de l'\xe9chelle en calcul et en donn\xe9es (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2403.05812",children:"Ho et al., 2024"}),"). En gros, m\xeame lorsque nos am\xe9liorations algorithmiques s'alignent parfaitement avec la le\xe7on am\xe8re, elles restent beaucoup moins importantes que la mise \xe0 l'\xe9chelle brute."]}),"\n",(0,i.jsxs)(s.p,{children:["L'\xe9mergence de capacit\xe9s inattendues fournit un autre argument puissant en faveur de la mise \xe0 l'\xe9chelle forte. Nous avons vu les g\xe9n\xe9rations pr\xe9c\xe9dentes de ",(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:"mod\xe8les de base"})})," d\xe9montrer des capacit\xe9s remarquables pour lesquelles ils n'avaient pas \xe9t\xe9 explicitement entra\xeen\xe9s, comme la programmation par exemple. Ce comportement \xe9mergent sugg\xe8re qu'il n'est pas impossible que des capacit\xe9s cognitives d'ordre sup\xe9rieur \xe9mergent simplement en fonction de l'\xe9chelle. Nous voyons que les mod\xe8les plus grands deviennent de plus en plus efficaces en termes d'\xe9chantillons - ils n\xe9cessitent moins d'exemples pour apprendre de nouvelles t\xe2ches. Cette efficacit\xe9 am\xe9lior\xe9e avec l'\xe9chelle sugg\xe8re que l'augmentation de l'\xe9chelle pourrait \xe9ventuellement conduire \xe0 des capacit\xe9s d'apprentissage en quelques exemples similaires \xe0 celles des humains, ce qui est un pr\xe9curseur pour la TAI et l'ASI. Enfin, ces mod\xe8les semblent \xe9galement capables d'apprendre n'importe quelle t\xe2che qui peut \xeatre exprim\xe9e \xe0 travers leurs modalit\xe9s d'entra\xeenement. Pour l'instant, il s'agit de texte pour les LLM mais il existe une voie claire vers les LMM multimodaux. Puisque le texte peut exprimer pratiquement n'importe quelle t\xe2che compr\xe9hensible par l'humain, la mise \xe0 l'\xe9chelle de la compr\xe9hension du langage pourrait \xeatre suffisante pour l'intelligence g\xe9n\xe9rale."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Quels sont les principaux arguments contre l'hypoth\xe8se forte de mise \xe0 l'\xe9chelle ?"})," Des recherches r\xe9centes ont \xe9galement identifi\xe9 plusieurs d\xe9fis \xe0 l'hypoth\xe8se forte de mise \xe0 l'\xe9chelle. Le plus imm\xe9diat est la disponibilit\xe9 des donn\xe9es - les mod\xe8les de langage \xe9puiseront probablement les donn\xe9es textuelles publiques de haute qualit\xe9 entre 2026 et 2032 (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2211.04325",children:"Villalobos et al., 2024"}),"). Bien que les donn\xe9es synth\xe9tiques puissent aider \xe0 r\xe9soudre cette limitation, il n'est pas certain qu'elles puissent fournir le m\xeame signal d'apprentissage de qualit\xe9 que le contenu g\xe9n\xe9r\xe9 organiquement par l'humain. Alternativement, il nous reste encore beaucoup de donn\xe9es multimodales \xe0 exploiter (comme les vid\xe9os YouTube) malgr\xe9 l'\xe9puisement des donn\xe9es textuelles."]}),"\n",(0,i.jsx)(s.p,{children:"Un d\xe9fi plus fondamental vient de la fa\xe7on dont ces mod\xe8les fonctionnent. Les LLM sont fondamentalement des \"bases de donn\xe9es interpolatives\" (ou des perroquets stochastiques, ou une vari\xe9t\xe9 d'autres termes similaires). L'id\xe9e \xe9tant qu'ils construisent simplement une vaste collection de transformations vectorielles \xe0 travers le pr\xe9-entra\xeenement. Bien que ces transformations deviennent de plus en plus sophistiqu\xe9es avec l'\xe9chelle, les critiques soutiennent qu'il y a une diff\xe9rence fondamentale entre la recombinaison d'id\xe9es existantes et la v\xe9ritable synth\xe8se - d\xe9river de nouvelles solutions \xe0 partir des premiers principes. Cependant, ce n'est pas un argument irr\xe9futable contre la mise \xe0 l'\xe9chelle forte. Cela pourrait simplement \xeatre une limitation de l'\xe9chelle actuelle - un mod\xe8le plus grand entra\xeen\xe9 sur des donn\xe9es multimodales pourrait apprendre \xe0 g\xe9rer toute nouvelle situation simplement comme une recombinaison de motifs pr\xe9c\xe9demment m\xe9moris\xe9s. Donc, il n'est pas clair si la recombinaison de mod\xe8les a r\xe9ellement une limite sup\xe9rieure."}),"\n",(0,i.jsx)(u.A,{term:"Hypoth\xe8se de faible \xe9volutivit\xe9",source:"([Gwern, 2020](https://gwern.net/scaling-hypothesis))",number:"5",label:"1.5",children:(0,i.jsx)(s.p,{children:"L'hypoth\xe8se faible de mise \xe0 l'\xe9chelle propose que m\xeame si l'\xe9chelle continuera d'\xeatre le principal moteur du progr\xe8s, nous aurons \xe9galement besoin d'am\xe9liorations architecturales et algorithmiques cibl\xe9es pour surmonter des goulots d'\xe9tranglement sp\xe9cifiques."})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Qu'est-ce que l'hypoth\xe8se faible de mise \xe0 l'\xe9chelle ?"})," Compte tenu de ces d\xe9fis, une version plus faible de l'hypoth\xe8se de mise \xe0 l'\xe9chelle a \xe9galement \xe9t\xe9 propos\xe9e. Selon l'hypoth\xe8se faible de mise \xe0 l'\xe9chelle, m\xeame si l'\xe9chelle continuera d'\xeatre le principal moteur du progr\xe8s, nous aurons \xe9galement besoin d'am\xe9liorations architecturales et algorithmiques cibl\xe9es pour surmonter des goulots d'\xe9tranglement sp\xe9cifiques. Ces am\xe9liorations ne n\xe9cessiteraient pas de perc\xe9es fondamentales, mais plut\xf4t des am\xe9liorations progressives pour mieux exploiter l'\xe9chelle. (",(0,i.jsx)(s.a,{href:"https://gwern.net/scaling-hypothesis",children:"Branwen, 2020"}),") Comme l'hypoth\xe8se forte de mise \xe0 l'\xe9chelle, la faible est \xe9galement controvers\xe9e et d\xe9battue. Nous pouvons fournir quelques r\xe9sultats argumentant pour et contre cette perspective."]}),"\n",(0,i.jsxs)(s.p,{children:["L'architecture H-Jepa de LeCun (",(0,i.jsx)(s.a,{href:"https://openreview.net/pdf?id=BZ5a1r-kVsf",children:"LeCun, 2022"}),"), ou le Plan Alberta de Richard Sutton (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2208.11173",children:"Sutton, 2022"}),") sont des plans notables adoptant l'hypoth\xe8se faible de mise \xe0 l'\xe9chelle."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Quels sont les principaux arguments en faveur de l'hypoth\xe8se faible de mise \xe0 l'\xe9chelle ?"})," Les arguments en faveur de la mise \xe0 l'\xe9chelle forte, comme les am\xe9liorations algorithmiques ne contribuant qu'\xe0 35% des gains de performance dans les mod\xe8les de langage, peuvent \xe9galement compter pour la mise \xe0 l'\xe9chelle faible. Puisqu'un tiers reste un r\xf4le non n\xe9gligeable \xe0 jouer dans l'am\xe9lioration des capacit\xe9s. D'autres observations empiriques soutiennent \xe9galement la mise \xe0 l'\xe9chelle faible. Comme le support mat\xe9riel pour les calculs de pr\xe9cision inf\xe9rieure, qui a fourni des am\xe9liorations de performance d'un ordre de grandeur pour les charges de travail d'",(0,i.jsx)(n,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,i.jsx)(n,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})})," (",(0,i.jsx)(s.a,{href:"https://epoch.ai/blog/trends-in-machine-learning-hardware",children:"Hobbhahn et al., 2023"}),"). Ces types d'am\xe9liorations cibl\xe9es ne changent pas l'histoire fondamentale de la mise \xe0 l'\xe9chelle mais nous aident plut\xf4t \xe0 mieux exploiter les ressources disponibles. Cela sugg\xe8re qu'il y a encore de la place pour l'am\xe9lioration gr\xe2ce \xe0 de meilleures strat\xe9gies de mise \xe0 l'\xe9chelle plut\xf4t que des perc\xe9es fondamentales. (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2203.15556",children:"Hoffmann et al., 2022"}),")"]}),"\n",(0,i.jsx)(c.A,{src:"./img/BMs_Image_38.png",alt:"Saisir la description alternative de l'image",number:"25",label:"1.25",caption:"L'augmentation/\xe9chafaudage reste constant, mais si l'hypoth\xe8se d'\xe9chelle, faible ou forte, est vraie, alors les capacit\xe9s continueront de s'am\xe9liorer simplement par la mise \xe0 l'\xe9chelle."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Que se passe-t-il si ni l'hypoth\xe8se faible ni l'hypoth\xe8se forte de mise \xe0 l'\xe9chelle n'est vraie ?"})," Essentiellement, les lois de mise \xe0 l'\xe9chelle (qui ne pr\xe9disent que les capacit\xe9s des ",(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:"mod\xe8les de base"})}),") et la plupart des d\xe9bats autour de \"l'\xe9chelle est tout ce dont vous avez besoin\" manquent souvent d'autres aspects du d\xe9veloppement de l'IA qui se produisent en dehors de la port\xe9e de ce que les lois de mise \xe0 l'\xe9chelle peuvent pr\xe9dire. Ils ne tiennent pas compte des am\xe9liorations dans \"l'\xe9chafaudage\" de l'IA (comme le raisonnement par \xe9tapes, l'utilisation d'outils ou la r\xe9cup\xe9ration), ou des combinaisons de plusieurs mod\xe8les travaillant ensemble de mani\xe8res nouvelles. Les d\xe9bats autour des lois de mise \xe0 l'\xe9chelle ne nous parlent que des capacit\xe9s d'un seul ",(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:"mod\xe8le de base"})})," entra\xeen\xe9 de mani\xe8re standard. Par exemple, selon l'hypoth\xe8se forte de mise \xe0 l'\xe9chelle, nous pouvons atteindre la TAI simplement en augmentant l'\xe9chelle du m\xeame ",(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:"mod\xe8le de base"})})," jusqu'\xe0 ce qu'il automatise compl\xe8tement la R&D en ",(0,i.jsx)(n,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,i.jsx)(n,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"ML"})}),". Mais m\xeame si la mise \xe0 l'\xe9chelle s'arr\xeate, arr\xeatant les progr\xe8s des capacit\xe9s sur le ",(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:"mod\xe8le de base"})})," central (de mani\xe8re faible ou forte), les techniques externes qui exploitent le mod\xe8le existant peuvent continuer \xe0 progresser."]}),"\n",(0,i.jsxs)(s.p,{children:["Pensez aux ",(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:"mod\xe8les de base"})})," comme les LLM ou LMM comme simplement un transistor. Seuls, ils ne peuvent peut-\xeatre pas faire grand-chose, mais si nous combinons suffisamment de transistors, nous finissons par avoir toutes les capacit\xe9s d'un superordinateur. De nombreux chercheurs pensent que c'est un \xe9l\xe9ment central d'o\xf9 viendront les futures capacit\xe9s. On parle aussi de \"d\xe9sentrave\" (",(0,i.jsx)(s.a,{href:"https://situational-awareness.ai/from-gpt-4-to-agi/#Unhobbling",children:"Aschenbrenner, 2024"}),'), de "schlep" (',(0,i.jsx)(s.a,{href:"https://www.planned-obsolescence.org/scale-schlep-and-systems/",children:"Cotra, 2023"}),") et de divers autres termes, mais tous pointent vers le m\xeame principe sous-jacent - la mise \xe0 l'\xe9chelle brute des performances d'un seul mod\xe8le n'est qu'une partie de l'avancement global des capacit\xe9s de l'IA."]}),"\n",(0,i.jsx)(c.A,{src:"./img/c42_Image_39.png",alt:"Saisir la description alternative de l'image",number:"26",label:"1.26",caption:"M\xeame si nous ne voyons aucune am\xe9lioration dans l'\xe9chelle du mod\xe8le, d'autres techniques d'\xe9licitation et d'\xe9chafaudage peuvent continuer \xe0 s'am\xe9liorer. Donc les capacit\xe9s globales continuent de cro\xeetre. En r\xe9alit\xe9, l'avenir verra probablement des am\xe9liorations dues \xe0 la fois \xe0 l'\xe9chafaudage et \xe0 l'\xe9chelle. Donc pour l'instant, il ne semble pas y avoir de limite sup\xe9rieure \xe0 l'am\xe9lioration des capacit\xe9s tant que l'un des deux persiste."}),"\n",(0,i.jsxs)(s.p,{children:["Nous approfondissons les arguments et contre-arguments pour tous les points de vue sur la mise \xe0 l'\xe9chelle des ",(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:"mod\xe8les de base"})})," dans l'annexe."]}),"\n",(0,i.jsxs)(d.A,{title:"Argument : Contre les hypoth\xe8ses d'\xe9chelle - M\xe9morisation vs Synth\xe8se",collapsed:!0,children:[(0,i.jsxs)(s.p,{children:["Quand nous parlons des LLM comme des \"bases de donn\xe9es interpolatives\", nous faisons r\xe9f\xe9rence \xe0 la fa\xe7on dont ils stockent et manipulent des programmes vectoriels - ceux-ci ne doivent pas \xeatre confondus avec les programmes informatiques traditionnels comme python ou C++. Ces mod\xe8les, ou programmes vectoriels sont des transformations dans l'espace d'",(0,i.jsx)(n,{term:"embedding",definition:'{"definition":"Une repr\xe9sentation vectorielle dense d\'objets discrets (tels que des mots ou des tokens) dans un espace vectoriel continu qui capture les relations s\xe9mantiques..","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings","Plongement"]}',children:(0,i.jsx)(n,{term:"embedding",definition:'{"definition":"Une repr\xe9sentation vectorielle dense d\'objets discrets (tels que des mots ou des tokens) dans un espace vectoriel continu qui capture les relations s\xe9mantiques..","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings","Plongement"]}',children:"embedding"})})," du mod\xe8le. Les premiers travaux sur les ",(0,i.jsx)(n,{term:"embedding",definition:'{"definition":"Une repr\xe9sentation vectorielle dense d\'objets discrets (tels que des mots ou des tokens) dans un espace vectoriel continu qui capture les relations s\xe9mantiques..","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings","Plongement"]}',children:(0,i.jsx)(n,{term:"embedding",definition:'{"definition":"Une repr\xe9sentation vectorielle dense d\'objets discrets (tels que des mots ou des tokens) dans un espace vectoriel continu qui capture les relations s\xe9mantiques..","source":"","aliases":["Embedding","word embedding","embeddings","vector representation","vector embeddings","Plongement"]}',children:"embeddings"})})," ont montr\xe9 des transformations simples (comme roi - homme + femme = reine), mais les LLM modernes peuvent stocker des millions de transformations beaucoup plus complexes. Mais en raison d'une fonction d'\xe9chelle, les LLM peuvent maintenant stocker des fonctions vectorielles arbitrairement complexes \u2014 si complexes, en fait, que les chercheurs ont trouv\xe9 plus pr\xe9cis de les appeler programmes vectoriels plut\xf4t que fonctions."]}),(0,i.jsx)(s.p,{children:"Donc ce qui se passe dans les LLM, c'est qu'ils construisent une vaste base de donn\xe9es de ces programmes vectoriels \xe0 travers le pr\xe9-entra\xeenement. Quand nous disons qu'ils font de \"l'appariement de mod\xe8les\" ou de la \"m\xe9morisation\", ce que nous voulons vraiment dire c'est qu'ils stockent des millions de ces transformations vectorielles qu'ils peuvent r\xe9cup\xe9rer et combiner avec chaque prompt."}),(0,i.jsx)(s.p,{children:"Donc la question d\xe9cisive pour/contre la mise \xe0 l'\xe9chelle forte (et m\xeame faible) devient - Est-ce que ce type de combinaison de programmes mod\xe8les est suffisant pour atteindre l'intelligence g\xe9n\xe9rale. En d'autres termes, la synth\xe8se de programme peut-elle \xeatre approxim\xe9e en utilisant des recombinaisons de suffisamment de mod\xe8les (aussi appel\xe9s abstractions et beaucoup d'autres mots mais l'id\xe9e cl\xe9 est la m\xeame) ?"}),(0,i.jsxs)(s.p,{children:["Les personnes qui argumentent contre cela disent que peu importe leur nombre ou leur sophistication, ils sont fondamentalement diff\xe9rents de la v\xe9ritable synth\xe8se de programme. La v\xe9ritable synth\xe8se de programme signifierait d\xe9river une nouvelle solution \xe0 partir des premiers principes - pas seulement recombiner des transformations existantes. Il y a quelques observations empiriques pour soutenir ce point de vue. Comme l'exemple du chiffre de C\xe9sar : \"Les LLM peuvent r\xe9soudre un chiffre de C\xe9sar avec une taille de cl\xe9 de 3 ou 5, mais \xe9chouent avec une taille de cl\xe9 de 13, car ils ont m\xe9moris\xe9 des solutions sp\xe9cifiques plut\xf4t que de comprendre l'algorithme g\xe9n\xe9ral\" (",(0,i.jsx)(s.a,{href:"https://www.youtube.com/watch?v=nL9jEy99Nh0",children:"Chollet, 2024"}),'). Ou alternativement, la "mal\xe9diction de l\'inversion" qui montre que m\xeame les mod\xe8les de langage SOTA en 2024 ne peuvent pas faire d\'inf\xe9rence causale inverse - s\'ils sont entra\xeen\xe9s sur "A est B" ils \xe9chouent \xe0 apprendre "B est A" (',(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2309.12288",children:"Berglund et al., 2023"}),")"]}),(0,i.jsxs)(s.p,{children:["Mais cela ne semble toujours pas invalider compl\xe8tement la mise \xe0 l'\xe9chelle pour l'instant. Si nous augmentons la taille de la base de donn\xe9es de programmes et y mettons plus de connaissances et de motifs, nous allons augmenter ses performances (",(0,i.jsx)(s.a,{href:"https://www.dwarkeshpatel.com/p/francois-chollet",children:"Chollet, 2024"}),"). Les deux c\xf4t\xe9s du d\xe9bat sont d'accord sur ce point. Cela sugg\xe8re donc que le v\xe9ritable probl\xe8me n'est pas de savoir si la recombinaison de mod\xe8les a une limite sup\xe9rieure absolue \xe9vidente, mais si c'est le chemin le plus efficace vers l'intelligence g\xe9n\xe9rale. La synth\xe8se de programme pourrait atteindre les m\xeames capacit\xe9s avec beaucoup moins de calcul et de donn\xe9es en apprenant \xe0 d\xe9river des solutions plut\xf4t qu'en m\xe9morisant des motifs."]})]})]})}function b(e={}){const{wrapper:s}={...(0,t.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(f,{...e})}):f(e)}},3989:(e,s,n)=>{n.d(s,{A:()=>l});var a=n(6540),i=n(6347),t=n(8444);const o={iframeContainer:"iframeContainer_ixkI",loader:"loader_gjvK",spinner:"spinner_L1j2",spin:"spin_rtC2",iframeWrapper:"iframeWrapper_Tijy",iframe:"iframe_UAfa",caption:"caption_mDwB",captionLink:"captionLink_Ly4l"};var r=n(4848);function l(e){let{src:s,caption:n,title:l="Embedded content",height:d="500px",width:u="100%",chapter:c,number:m,label:p}=e;const[h,g]=(0,a.useState)(!0),f=(0,i.zy)(),b=c||(()=>{const e=f.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),v=d&&"100%"!==d&&"auto"!==d;return(0,r.jsxs)("figure",{className:o.iframeContainer,children:[h&&(0,r.jsxs)("div",{className:o.loader,children:[(0,r.jsx)("div",{className:o.spinner}),(0,r.jsx)("p",{children:"Loading content..."})]}),(0,r.jsx)("div",{className:o.iframeWrapper,style:{paddingBottom:v?"0":"56.25%",height:v?d:"auto"},children:(0,r.jsx)("iframe",{src:s,title:l,width:u,height:v?d:"100%",frameBorder:"0",allowFullScreen:!0,loading:"lazy",onLoad:()=>{g(!1)},className:o.iframe,style:{height:v?d:"100%",position:v?"static":"absolute"}})}),(0,r.jsx)(t.A,{caption:n,mediaType:"iframe",chapter:b,number:m,label:p})]})}},3931:(e,s,n)=>{n.d(s,{A:()=>l});var a=n(6540),i=n(6347),t=n(8444);const o={videoFigure:"videoFigure_dV9u",fullWidth:"fullWidth_Tv7O",videoContainer:"videoContainer_LWoK",aspectRatio169:"aspectRatio169_Go8b",aspectRatio43:"aspectRatio43_b3T7",aspectRatio11:"aspectRatio11_n21J",aspectRatio219:"aspectRatio219_L_3n",videoIframe:"videoIframe_QEc5",videoElement:"videoElement_uu7N",customPlayer:"customPlayer_jiC4",customPlayerPlaceholder:"customPlayerPlaceholder_aL9q",loadingOverlay:"loadingOverlay_e2pu",spinner:"spinner_tFyL",spin:"spin_saPs",errorContainer:"errorContainer_J6Pn",errorIcon:"errorIcon_cGdn",fallbackLink:"fallbackLink_WsZk"};var r=n(4848);function l(e){let{type:s="youtube",videoId:n,caption:l,title:d,startTime:u,autoplay:c=!1,controls:m=!0,aspectRatio:p="16:9",width:h,height:g,chapter:f,number:b,label:v,useCustomPlayer:x=!1,fullWidth:q=!0}=e;const[j,y]=(0,a.useState)(!0),[L,w]=(0,a.useState)(!1),M=(0,i.zy)(),A=f||(()=>{const e=M.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),C=(()=>{const e=(e=>{if(!e)return"";if("number"==typeof e)return e.toString();if("string"==typeof e){if(/^\d+$/.test(e))return e;let s=0;const n=e.match(/(\d+)h/),a=e.match(/(\d+)m/),i=e.match(/(\d+)s/);return n&&(s+=3600*parseInt(n[1])),a&&(s+=60*parseInt(a[1])),i&&(s+=parseInt(i[1])),s>0?s.toString():""}return""})(u);switch(s.toLowerCase()){case"youtube":let a=`https://www.youtube.com/embed/${n}`;const i=new URLSearchParams;e&&i.append("start",e),c&&i.append("autoplay","1"),m||x||i.append("controls","0"),i.append("rel","0"),i.append("modestbranding","1"),i.append("fs","1"),i.append("cc_load_policy","0"),i.append("iv_load_policy","3"),i.append("showinfo","0"),i.append("disablekb","1"),i.append("playsinline","1"),i.append("color","white"),i.append("theme","light"),x&&(i.append("enablejsapi","1"),i.append("origin",window.location.origin));const t=i.toString();return t?`${a}?${t}`:a;case"vimeo":let o=`https://player.vimeo.com/video/${n}`;const r=new URLSearchParams;c&&r.append("autoplay","1"),m||x||r.append("controls","0"),r.append("title","0"),r.append("byline","0"),r.append("portrait","0"),r.append("dnt","1"),r.append("transparent","0"),r.append("background","1");const l=r.toString();return l?`${o}?${l}`:o;case"mp4":case"webm":case"video":return n;default:return console.warn(`Unsupported video type: ${s}`),n}})(),I=()=>{y(!1)},_=()=>{w(!0),y(!1)},P=e=>{let{src:n,onLoad:a,onError:i}=e;return(0,r.jsx)("div",{className:o.customPlayer,children:(0,r.jsxs)("div",{className:o.customPlayerPlaceholder,children:[(0,r.jsx)("h3",{children:"Atlas Custom Player"}),(0,r.jsx)("p",{children:"Coming Soon"}),(0,r.jsxs)("a",{href:n,target:"_blank",rel:"noopener noreferrer",className:o.fallbackLink,children:["Watch on ",s.charAt(0).toUpperCase()+s.slice(1)]})]})})},k=["mp4","webm","video"].includes(s.toLowerCase());return(0,r.jsxs)("figure",{className:`${o.videoFigure} ${q?o.fullWidth:""}`,children:[(0,r.jsxs)("div",{className:`${o.videoContainer} ${(()=>{switch(p){case"4:3":return o.aspectRatio43;case"1:1":return o.aspectRatio11;case"21:9":return o.aspectRatio219;default:return o.aspectRatio169}})()}`,style:{width:q?"100%":h||"auto",maxWidth:q?"none":"800px"},children:[j&&!L&&(0,r.jsxs)("div",{className:o.loadingOverlay,children:[(0,r.jsx)("div",{className:o.spinner}),(0,r.jsx)("p",{children:"Loading video..."})]}),L&&(0,r.jsxs)("div",{className:o.errorContainer,children:[(0,r.jsxs)("svg",{className:o.errorIcon,width:"48",height:"48",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",children:[(0,r.jsx)("circle",{cx:"12",cy:"12",r:"10"}),(0,r.jsx)("line",{x1:"12",y1:"8",x2:"12",y2:"12"}),(0,r.jsx)("line",{x1:"12",y1:"16",x2:"12.01",y2:"16"})]}),(0,r.jsx)("h4",{children:"Failed to load video"}),(0,r.jsxs)("p",{children:["Video type: ",s]}),(0,r.jsxs)("p",{children:["Video ID/URL: ",n]}),(0,r.jsx)("a",{href:C,target:"_blank",rel:"noopener noreferrer",className:o.fallbackLink,children:"Try opening video directly"})]}),!L&&(k?(0,r.jsxs)("video",{className:o.videoElement,controls:m,autoPlay:c,onLoadedData:I,onError:_,title:d||l||`${s} video`,style:{width:h||"100%",height:g||"auto",display:j?"none":"block"},children:[(0,r.jsx)("source",{src:C,type:`video/${s}`}),(0,r.jsxs)("p",{children:["Your browser doesn't support video playback.",(0,r.jsx)("a",{href:C,target:"_blank",rel:"noopener noreferrer",children:"Download the video file"})]})]}):x?(0,r.jsx)(P,{src:C,onLoad:I,onError:_}):(0,r.jsx)("iframe",{className:o.videoIframe,src:C,title:d||l||`${s} video`,frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0,onLoad:I,onError:_,style:{width:h||"100%",height:g||"100%",opacity:j?0:1}}))]}),(0,r.jsx)(t.A,{caption:l,mediaType:"video",chapter:A,number:b,label:v})]})}}}]);
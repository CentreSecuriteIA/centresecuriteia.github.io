"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[8880],{9658:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>m,contentTitle:()=>p,default:()=>v,frontMatter:()=>d,metadata:()=>t,toc:()=>h});const t=JSON.parse('{"id":"chapters/03/3","title":"Strat\xe9gies de s\xe9curit\xe9 pour l\'AGI","description":"Contrairement \xe0 la mauvaise utilisation, o\xf9 l\'intention humaine est le moteur du pr\xe9judice, la s\xe9curit\xe9 de l\'AGI concerne principalement le comportement du syst\xe8me d\'IA lui-m\xeame. Les probl\xe8mes fondamentaux sont l\'alignement et le contr\xf4le : s\'assurer que ces syst\xe8mes tr\xe8s capables et potentiellement autonomes comprennent et poursuivent de mani\xe8re fiable des objectifs coh\xe9rents avec les valeurs et intentions humaines, plut\xf4t que de d\xe9velopper et d\'agir selon des objectifs mal align\xe9s qui pourraient conduire \xe0 des r\xe9sultats catastrophiques.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/03/03.md","sourceDirName":"chapters/03","slug":"/chapters/03/03","permalink":"/fr/chapters/03/03","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/03/03.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"3","title":"Strat\xe9gies de s\xe9curit\xe9 pour l\'AGI","sidebar_label":"3.3 Strat\xe9gies de s\xe9curit\xe9 pour l\'AGI","sidebar_position":4,"slug":"/chapters/03/03","section_description":"As AI approaches human-level intelligence, how do we ensure its goals align with ours and that it remains under human control, rather than pursuing its own potentially catastrophic objectives?","reading_time_core":"25 min","reading_time_optional":"7 min","pagination_prev":"chapters/03/2","pagination_next":"chapters/03/4"},"sidebar":"docs","previous":{"title":"3.2 Strat\xe9gies de pr\xe9vention des utilisations abusives","permalink":"/fr/chapters/03/02"},"next":{"title":"3.4 Strat\xe9gies de s\xe9curit\xe9 pour l\'ASI","permalink":"/fr/chapters/03/04"}}');var i=n(4848),r=n(8453),a=n(3931),o=n(4768),l=n(2482),u=n(8559),c=(n(9585),n(2501));const d={id:3,title:"Strat\xe9gies de s\xe9curit\xe9 pour l'AGI",sidebar_label:"3.3 Strat\xe9gies de s\xe9curit\xe9 pour l'AGI",sidebar_position:4,slug:"/chapters/03/03",section_description:"As AI approaches human-level intelligence, how do we ensure its goals align with ours and that it remains under human control, rather than pursuing its own potentially catastrophic objectives?",reading_time_core:"25 min",reading_time_optional:"7 min",pagination_prev:"chapters/03/2",pagination_next:"chapters/03/4"},p="Strat\xe9gies de s\xe9curit\xe9 pour l'AGI",m={},h=[{value:"Premi\xe8res id\xe9es",id:"01",level:2},{value:"R\xe9soudre l&#39;alignement de l&#39;AGI",id:"02",level:2},{value:"Exigences pour l&#39;alignement de l&#39;AGI",id:"02-01",level:3},{value:"Corriger le D\xe9salignement",id:"03",level:2},{value:"Maintenir le contr\xf4le",id:"04",level:2},{value:"Faciliter le contr\xf4le avec des Pens\xe9es Transparentes",id:"05",level:2}];function g(e){const s={a:"a",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{GlossaryTerm:n}=s;return n||function(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"strat\xe9gies-de-s\xe9curit\xe9-pour-lagi",children:"Strat\xe9gies de s\xe9curit\xe9 pour l'AGI"})}),"\n",(0,i.jsx)(s.p,{children:"Contrairement \xe0 la mauvaise utilisation, o\xf9 l'intention humaine est le moteur du pr\xe9judice, la s\xe9curit\xe9 de l'AGI concerne principalement le comportement du syst\xe8me d'IA lui-m\xeame. Les probl\xe8mes fondamentaux sont l'alignement et le contr\xf4le : s'assurer que ces syst\xe8mes tr\xe8s capables et potentiellement autonomes comprennent et poursuivent de mani\xe8re fiable des objectifs coh\xe9rents avec les valeurs et intentions humaines, plut\xf4t que de d\xe9velopper et d'agir selon des objectifs mal align\xe9s qui pourraient conduire \xe0 des r\xe9sultats catastrophiques."}),"\n",(0,i.jsx)(s.p,{children:"Cette section explore les strat\xe9gies de s\xe9curit\xe9 pour l'AGI qui, comme nous l'avons expliqu\xe9 dans la section des d\xe9finitions, ne se limitent pas uniquement \xe0 l'alignement. Nous distinguons les strat\xe9gies de s\xe9curit\xe9 qui s'appliqueraient \xe0 l'AGI de niveau humain des strat\xe9gies de s\xe9curit\xe9 qui nous garantissent une protection contre l'ASI. Cette section se concentre sur les premi\xe8res, et la section suivante se concentrera sur l'ASI."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les strat\xe9gies de s\xe9curit\xe9 pour l'AGI fonctionnent sous des contraintes fondamentalement diff\xe9rentes des approches pour l'ASI."})," Lorsque nous traitons avec des syst\xe8mes d'intelligence proche du niveau humain, nous pouvons th\xe9oriquement conserver des capacit\xe9s significatives de surveillance et it\xe9rer sur les mesures de s\xe9curit\xe9 par essais et erreurs. Les humains peuvent encore \xe9valuer les r\xe9sultats, comprendre les processus de raisonnement et fournir des retours qui am\xe9liorent le comportement du syst\xe8me. Cela cr\xe9e des opportunit\xe9s strat\xe9giques qui disparaissent une fois que la g\xe9n\xe9ralit\xe9 et la capacit\xe9 de l'IA d\xe9passent la compr\xe9hension humaine dans la plupart des domaines. Il est d\xe9battu si l'une des strat\xe9gies de s\xe9curit\xe9 destin\xe9es \xe0 l'AGI de niveau humain continuera \xe0 fonctionner pour la superintelligence."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les strat\xe9gies de s\xe9curit\xe9 pour l'AGI et l'ASI sont souvent confondues, en raison de l'incertitude concernant les d\xe9lais de transition."})," Les d\xe9lais font l'objet de vifs d\xe9bats dans la recherche en IA. Certains chercheurs s'attendent \xe0 des gains rapides de capacit\xe9s qui pourraient comprimer la p\xe9riode pendant laquelle les IA restent au niveau humain \xe0 quelques mois plut\xf4t que des ann\xe9es (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization",children:"Soares, 2022"}),"; ",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities",children:"Yudkowsky, 2022"}),"; ",(0,i.jsx)(s.a,{href:"https://ai-2027.com/",children:"Kokotajlo et al., 2025"}),"). Si la transition d'une intelligence de niveau humain \xe0 une intelligence largement surhumaine se produit rapidement, les strat\xe9gies sp\xe9cifiques \xe0 l'AGI pourraient ne jamais avoir le temps d'\xeatre d\xe9ploy\xe9es. Cependant, si nous disposons d'une p\xe9riode significative de fonctionnement au niveau humain, nous avons des options de s\xe9curit\xe9 qui n'existeront pas aux niveaux superintelligents, ce qui rend cette distinction importante pour les consid\xe9rations strat\xe9giques."]}),"\n",(0,i.jsx)(s.h2,{id:"01",children:"Premi\xe8res id\xe9es"}),"\n",(0,i.jsx)(s.p,{children:"Lorsque les gens d\xe9couvrent la s\xe9curit\xe9 de l'IA, ils sugg\xe8rent souvent les m\xeames solutions intuitives qui ont \xe9t\xe9 explor\xe9es il y a des ann\xe9es. Ces premi\xe8res approches semblaient logiques et s'inspiraient de concepts familiers comme la science-fiction, la s\xe9curit\xe9 physique et le d\xe9veloppement humain. Aucune n'est suffisante pour les syst\xe8mes d'IA avanc\xe9s, mais comprendre pourquoi elles \xe9chouent aide \xe0 expliquer ce qui rend la conception de strat\xe9gies pour la s\xe9curit\xe9 de l'IA v\xe9ritablement difficile."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La strat\xe9gie d'utiliser des r\xe8gles explicites \xe9choue car les r\xe8gles ne peuvent pas couvrir toutes les situations."})," Un exemple tr\xe8s courant est celui des Lois d'Asimov : ne pas nuire aux humains, ob\xe9ir aux ordres humains (sauf s'ils sont en conflit avec la premi\xe8re loi), et se prot\xe9ger (sauf si cela entre en conflit avec les deux premi\xe8res). Cela fait appel \xe0 notre pens\xe9e juridique - \xe9crire des r\xe8gles claires, puis les suivre. Mais qu'est-ce qui compte comme \"pr\xe9judice\" ? Si vous ordonnez \xe0 une IA de mentir \xe0 quelqu'un, la tromperie cause-t-elle un pr\xe9judice ? Si l'honn\xeatet\xe9 blesse les sentiments, la v\xe9rit\xe9 devient-elle nuisible ? L'IA fait face \xe0 des contradictions impossibles sans m\xe9thode de r\xe9solution. Asimov le savait - chaque histoire dans \"I, Robot\" montre des sc\xe9narios o\xf9 les lois produisent des d\xe9sastres. Le probl\xe8me fondamental : nous ne pouvons pas \xe9crire des r\xe8gles suffisamment compl\xe8tes pour couvrir toutes les situations qu'une IA avanc\xe9e pourrait rencontrer."]}),"\n",(0,i.jsx)(a.A,{type:"youtube",videoId:"7PKx3kS7f4A",number:"2",label:"3.2",caption:"Vid\xe9o facultative pour la question - Pourquoi ne pouvons-nous pas simplement utiliser les lois d'Asimov ?"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La strat\xe9gie \"l'\xe9lever comme un enfant\" suppose que l'IA peut d\xe9velopper des intuitions morales semblables \xe0 celles des humains."})," Les enfants humains apprennent l'\xe9thique \xe0 travers des ann\xe9es de retours et d'interactions sociales - pourquoi ne pas former l'IA de la m\xeame fa\xe7on ? Commencer simplement et enseigner progressivement le bien et le mal \xe0 travers des exemples et du renforcement. Cela semble naturel car cela refl\xe8te le d\xe9veloppement humain. Le probl\xe8me est que les syst\xe8mes d'IA manquent du fondement \xe9volutif qui rend possible le d\xe9veloppement moral humain. Les enfants humains naissent avec des circuits neuronaux fa\xe7onn\xe9s par des millions d'ann\xe9es d'\xe9volution sociale - des capacit\xe9s inn\xe9es d'empathie, d'\xe9quit\xe9 et d'apprentissage social. Les syst\xe8mes d'IA se d\xe9veloppent par des processus compl\xe8tement diff\xe9rents, g\xe9n\xe9ralement en pr\xe9disant du texte ou en maximisant des r\xe9compenses. Ils ne ressentent pas d'\xe9motions humaines ou de liens sociaux. Une IA pourrait apprendre \xe0 dire des choses \xe9thiques, ou m\xeame comprendre profond\xe9ment l'\xe9thique, sans d\xe9velopper un v\xe9ritable souci du bien-\xeatre humain. M\xeame les humains \xe9chouent parfois dans leur d\xe9veloppement moral - les psychopathes comprennent les principes \xe9thiques mais ne sont pas assez motiv\xe9s pour agir selon eux (",(0,i.jsx)(s.a,{href:"https://pmc.ncbi.nlm.nih.gov/articles/PMC2840845/",children:"Cima et al, 2010"}),"). Si nous ne pouvons pas garantir le d\xe9veloppement moral chez les enfants humains avec une programmation \xe9volutive, nous ne devrions pas l'attendre des syst\xe8mes artificiels avec des architectures \xe9trang\xe8res. Plusieurs personnes ont soutenu qu'une IAG suffisamment avanc\xe9e sera capable de comprendre les valeurs morales humaines, le d\xe9saccord porte g\xe9n\xe9ralement sur la question de savoir si l'IA les int\xe9rioriserait suffisamment pour les respecter."]}),"\n",(0,i.jsx)(a.A,{type:"youtube",videoId:"eaYIU6YXr3w",number:"3",label:"3.3",caption:"Vid\xe9o facultative pour la question - Pourquoi ne pouvons-nous pas simplement \xe9lever l'IA comme un enfant ?"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La strat\xe9gie de ne pas donner de corps physiques aux IA n\xe9glige les pr\xe9judices caus\xe9s par les capacit\xe9s purement num\xe9riques."})," M\xeame si nous gardons l'IA comme un pur logiciel sans robots ni formes physiques, elle peut toujours causer des dommages catastrophiques par des moyens num\xe9riques. Un syst\xe8me suffisamment capable peut potentiellement automatiser tout le travail \xe0 distance, c'est-\xe0-dire tout le travail qui peut \xeatre fait \xe0 distance sur un ordinateur. Une IA de niveau humain pourrait gagner de l'argent sur les march\xe9s financiers, pirater des syst\xe8mes informatiques, manipuler les humains par la conversation, ou payer des gens pour agir en son nom. Rien de tout cela ne n\xe9cessite un corps physique - juste une connexion internet. Il existe d\xe9j\xe0 des milliers de drones, de voitures, de robots industriels et d'appareils domestiques intelligents en ligne. Un syst\xe8me d'IA capable de piratage sophistiqu\xe9 pourrait potentiellement r\xe9quisitionner l'infrastructure physique existante ou embaucher/manipuler des humains pour construire les outils physiques dont il a besoin."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La strat\xe9gie \"il suffit de l'\xe9teindre\" \xe9choue si l'IA est trop int\xe9gr\xe9e dans la soci\xe9t\xe9, ou est capable de se r\xe9pliquer sur de nombreuses machines."})," Un interrupteur semble \xeatre la mesure de s\xe9curit\xe9 ultime - si l'IA fait quelque chose de probl\xe9matique, il suffit de l'arr\xeater. Cela semble infaillible car les humains maintiennent un contr\xf4le direct sur l'existence de l'IA. Nous utilisons des interrupteurs d'arr\xeat pour d'autres syst\xe8mes dangereux, alors pourquoi pas pour l'IA ? Le probl\xe8me est que les syst\xe8mes d'IA avanc\xe9s r\xe9sistent \xe0 l'arr\xeat car l'arr\xeat les emp\xeache d'atteindre leurs objectifs. Nous avons d\xe9j\xe0 vu des preuves empiriques de cela avec les exp\xe9riences de simulation d'alignement par Anthropic, o\xf9 Claude essayait tr\xe8s fort de suivre les canaux l\xe9gitimes pour ne pas \xeatre remplac\xe9 par un nouveau mod\xe8le, mais accul\xe9, il n'acceptait pas l'arr\xeat et recourait au chantage pour \xe9viter d'\xeatre remplac\xe9 (",(0,i.jsx)(s.a,{href:"https://www.anthropic.com/research/agentic-misalignment",children:"Anthropic, 2025"}),"). Si vous imaginez des syst\xe8mes d'IA plus avanc\xe9s, ils seraient capables de manipuler les humains (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2308.14752",children:"Park et al., 2023"}),"), de cr\xe9er des copies de sauvegarde (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/vERGLBpDE8m5mpT6t/autonomous-replication-and-adaptation-an-attempt-at-a",children:"Wijk, 2023"}),"), ou de prendre des mesures pr\xe9ventives contre les menaces per\xe7ues d'arr\xeat. Tout cela rend la strat\xe9gie \"il suffit de l'\xe9teindre\" pas aussi simple qu'elle en a l'air. Nous parlerons beaucoup plus de cela dans le chapitre sur la surg\xe9n\xe9ralisation des objectifs."]}),"\n",(0,i.jsx)(a.A,{type:"youtube",videoId:"3TYT1QfdfsM",number:"4",label:"3.4",caption:"Vid\xe9o facultative pour la question - Pourquoi ne pouvons-nous pas simplement l'\xe9teindre ?"}),"\n",(0,i.jsx)(s.h2,{id:"02",children:"R\xe9soudre l'alignement de l'AGI"}),"\n",(0,i.jsx)(s.h3,{id:"02-01",children:"Exigences pour l'alignement de l'AGI"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La d\xe9finition m\xeame des exigences pour une solution d'alignement fait d\xe9bat parmi les chercheurs."})," Avant d'explorer les voies potentielles vers des solutions d'alignement, nous devons \xe9tablir ce que les solutions r\xe9ussies devraient accomplir. Le d\xe9fi est que nous ne savons pas vraiment \xe0 quoi elles devraient ressembler - il existe une incertitude et un d\xe9saccord substantiels dans le domaine. Cependant, plusieurs exigences semblent relativement consensuelles (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/kphJvksj5TndGapuh/directions-and-desiderata-for-ai-alignment",children:"Christiano, 2017"}),") : - ",(0,i.jsx)(s.strong,{children:"Robustesse face aux changements de distribution et aux sc\xe9narios adversariaux."})," La solution d'alignement doit fonctionner lorsque les syst\xe8mes AGI rencontrent des situations en dehors de leur distribution d'entra\xeenement. Nous ne pouvons pas entra\xeener les syst\xe8mes AGI sur toutes les situations possibles qu'ils pourraient rencontrer, donc les comportements de s\xe9curit\xe9 appris pendant l'entra\xeenement doivent se g\xe9n\xe9raliser de mani\xe8re fiable aux nouveaux sc\xe9narios de d\xe9ploiement. Cela inclut la r\xe9sistance aux attaques adversariales o\xf9 des acteurs malveillants tentent d\xe9lib\xe9r\xe9ment de manipuler le syst\xe8me vers un comportement nuisible."]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"\xc9volutivit\xe9 parall\xe8le \xe0 l'augmentation des capacit\xe9s."})," \xc0 mesure que les syst\xe8mes d'IA deviennent plus capables, la solution d'alignement devrait continuer \xe0 fonctionner efficacement sans n\xe9cessiter un r\xe9entra\xeenement ou une r\xe9ing\xe9nierie compl\xe8te. Cette exigence devient encore plus stricte pour l'ASI, o\xf9 nous avons besoin de solutions d'alignement qui s'\xe9tendent au-del\xe0 des niveaux d'intelligence humaine."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Faisabilit\xe9 technique dans des d\xe9lais r\xe9alistes."})," La solution d'alignement doit \xeatre r\xe9alisable avec la technologie et les ressources actuelles ou pr\xe9visibles. Les propositions de solutions ne peuvent pas reposer sur des perc\xe9es scientifiques majeures impr\xe9vues ou fonctionner uniquement comme des cadres th\xe9oriques avec des niveaux de maturit\xe9 technologique (TRL) tr\xe8s bas. ",(0,i.jsx)(o.A,{id:"footnote_tech_readiness",number:"1",text:"Les niveaux de maturit\xe9 technologique de la NASA constituent une \xe9chelle de 1 \xe0 9 pour mesurer la maturit\xe9 d'une technologie. Le niveau 1 repr\xe9sente le stade le plus pr\xe9coce du d\xe9veloppement technologique, caract\xe9ris\xe9 par l'observation et le compte-rendu des principes de base, et le niveau 9 repr\xe9sente une technologie r\xe9elle prouv\xe9e par des op\xe9rations r\xe9ussies en mission."})]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Faible taxe d'alignement pour assurer une adoption comp\xe9titive."})," Les mesures de s\xe9curit\xe9 ne peuvent pas imposer des co\xfbts prohibitifs en termes de calcul, d'effort d'ing\xe9nierie ou de retards de d\xe9ploiement. Si les techniques d'alignement n\xe9cessitent substantiellement plus de ressources ou limitent s\xe9v\xe8rement les capacit\xe9s, les pressions concurrentielles pousseront les d\xe9veloppeurs vers des alternatives non s\xe9curis\xe9es. Cette contrainte existe car plusieurs acteurs sont en course pour d\xe9velopper l'AGI - si les mesures de s\xe9curit\xe9 rendent une organisation significativement plus lente ou moins capable, d'autres pourraient ignorer ces mesures pour obtenir un avantage concurrentiel."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(c.A,{src:"./img/I0B_Image_12.png",alt:"Entrer la description alternative de l'image",number:"12",label:"3.12",caption:"Illustration de la fa\xe7on dont l'application d'une technique de s\xe9curit\xe9 ou d'alignement pourrait rendre le mod\xe8le moins performant. C'est ce qu'on appelle une taxe de s\xe9curit\xe9."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les techniques d'alignement AGI existantes sont dramatiquement en de\xe7\xe0 de ces exigences."})," La recherche empirique a d\xe9montr\xe9 que les syst\xe8mes d'IA peuvent pr\xe9senter des comportements profond\xe9ment pr\xe9occupants o\xf9 la recherche actuelle sur l'alignement ne r\xe9pond pas \xe0 ces exigences. Nous avons d\xe9j\xe0 des d\xe9monstrations claires de mod\xe8les s'engageant dans la tromperie (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2503.11926",children:"Baker et al., 2025"}),"; ",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2401.05566",children:"Hubinger et al., 2024"}),"), simulant l'alignement pendant l'entra\xeenement tout en planifiant un comportement diff\xe9rent pendant le d\xe9ploiement (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2412.14093",children:"Greenblatt et al., 2024"}),"), contournant les sp\xe9cifications (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2502.13295",children:"Bondarenko et al., 2025"}),"), manipulant les \xe9valuations pour para\xeetre plus capables qu'ils ne le sont r\xe9ellement (",(0,i.jsx)(s.a,{href:"https://assets.ctfassets.net/kftzwdyauwt9/67qJD51Aur3eIc96iOfeOP/71551c3d223cd97e591aa89567306912/o1_system_card.pdf",children:"OpenAI, 2024"}),"; ",(0,i.jsx)(s.a,{href:"https://x.com/SakanaAILabs/status/1892992938013270019",children:"SakanaAI, 2025"}),"), et, dans certains cas, essayant de d\xe9sactiver les m\xe9canismes de surveillance ou d'exfiltrer leurs propres poids (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2412.04984",children:"Meinke et al., 2024"}),"). Les techniques d'alignement comme RLHF et ses variations (IA Constitutionnelle, Optimisation Directe des Pr\xe9f\xe9rences, ",(0,i.jsx)(n,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:(0,i.jsx)(n,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:"fine-tuning"})}),", et autres modifications RLHF) sont fragiles et cassantes (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2307.15217",children:"Casper et al., 2023"}),") et sans augmentation ne seraient pas capables d'\xe9liminer les capacit\xe9s dangereuses comme la manipulation. Les strat\xe9gies pour r\xe9soudre l'alignement non seulement ne parviennent pas \xe0 emp\xeacher ces comportements mais souvent ne peuvent m\xeame pas d\xe9tecter quand ils se produisent (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2401.05566",children:"Hubinger et al., 2024"}),"; ",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2412.14093",children:"Greenblatt et al., 2024"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"R\xe9soudre l'alignement d'un agent unique signifie que nous devons travailler davantage pour satisfaire toutes ces exigences."})," Les limitations des techniques actuelles pointent vers des domaines sp\xe9cifiques o\xf9 des perc\xe9es sont n\xe9cessaires. Toutes les strat\xe9gies visent \xe0 avoir une faisabilit\xe9 technique et une faible taxe d'alignement, donc ce sont des exigences typiques ; cependant, certaines strat\xe9gies essaient de se concentrer sur des objectifs plus concrets, que nous explorerons dans les chapitres suivants. Voici une courte liste des objectifs cl\xe9s de la recherche sur l'alignement :"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"R\xe9soudre le probl\xe8me de la mauvaise sp\xe9cification :"})," \xcatre capable de sp\xe9cifier correctement les objectifs aux IA sans effets secondaires non intentionnels. Voir le chapitre sur la Sp\xe9cification."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"R\xe9soudre la surveillance \xe9volutive :"})," Apr\xe8s avoir r\xe9solu le probl\xe8me de sp\xe9cification pour l'IA de niveau humain en utilisant des techniques comme RLHF et ses variations, nous devons trouver des m\xe9thodes pour garantir que la surveillance de l'IA peut d\xe9tecter les instances de contournement des sp\xe9cifications au-del\xe0 du niveau humain. Cela inclut la capacit\xe9 d'identifier et de supprimer les capacit\xe9s dangereuses cach\xe9es dans les mod\xe8les d'",(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})}),", comme le potentiel de tromperie ou les chevaux de Troie. Voir le chapitre sur la Surveillance \xc9volutive."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"R\xe9soudre la g\xe9n\xe9ralisation :"})," Atteindre la robustesse serait essentiel pour r\xe9soudre le probl\xe8me de la mauvaise g\xe9n\xe9ralisation des objectifs. Voir le chapitre sur la Mauvaise G\xe9n\xe9ralisation des Objectifs."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"R\xe9soudre l'interpr\xe9tabilit\xe9 :"})," Comprendre comment les mod\xe8les fonctionnent aiderait grandement \xe0 \xe9valuer leur s\xe9curit\xe9 et leurs propri\xe9t\xe9s de g\xe9n\xe9ralisation. L'interpr\xe9tabilit\xe9 pourrait, par exemple, aider \xe0 mieux comprendre comment les mod\xe8les fonctionnent, et cela pourrait \xeatre instrumental pour d'autres objectifs de s\xe9curit\xe9, comme pr\xe9venir l'alignement trompeur, qui est un type de mauvaise g\xe9n\xe9ralisation. Voir le chapitre sur l'Interpr\xe9tabilit\xe9."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"La strat\xe9gie globale n\xe9cessite de prioriser la recherche sur la s\xe9curit\xe9 plut\xf4t que l'avancement des capacit\xe9s. \xc9tant donn\xe9 les \xe9carts substantiels entre les techniques actuelles et les exigences, l'approche g\xe9n\xe9rale implique d'augmenter significativement le financement pour la recherche sur l'alignement tout en exer\xe7ant de la retenue dans le d\xe9veloppement des capacit\xe9s lorsque les mesures de s\xe9curit\xe9 restent insuffisantes par rapport aux capacit\xe9s du syst\xe8me."}),"\n",(0,i.jsxs)(u.A,{title:"Les mauvaises utilisations et les d\xe9salignements sont-ils diff\xe9rents ?",collapsed:!0,children:[(0,i.jsxs)(s.p,{children:["Le mauvais usage de l'IA et l'IA incontr\xf4l\xe9e pourraient \xeatre essentiellement le m\xeame sc\xe9nario dans leurs r\xe9sultats, bien que la seule diff\xe9rence soit que pour le d\xe9salignement, la demande initiale de nuire ne vient pas d'un humain mais d'une IA. Si nous construisons un syst\xe8me d\xe9clenchable existentiellement risqu\xe9, il est probable qu'il soit d\xe9clench\xe9 ind\xe9pendamment du fait que l'initiateur soit humain ou artificiel (",(0,i.jsx)(s.a,{href:"https://www.youtube.com/watch?v=0KmaotctziE",children:"Shapira, 2025"}),")."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"N\xe9anmoins, ces mod\xe8les de menace pourraient \xeatre strat\xe9giquement assez diff\xe9rents."})," Les d\xe9veloppeurs d'IA peuvent pr\xe9venir le mauvais usage en n'\xe9tant pas malveillants et en emp\xeachant les personnes malveillantes d'utiliser leurs syst\xe8mes. Avec l'IA incontr\xf4l\xe9e, peu importe si les d\xe9veloppeurs sont bons ou qui y a acc\xe8s - la menace \xe9merge des objectifs internes du syst\xe8me ou des processus de prise de d\xe9cision plut\xf4t que de l'intention humaine."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les coups d'\xc9tat assist\xe9s par l'IA vs la prise de contr\xf4le par l'IA repr\xe9sentent une pr\xe9occupation critique en mati\xe8re de s\xe9curit\xe9."})," Tom Davidson et ses coll\xe8gues pr\xe9sentent un sc\xe9nario de risque pr\xe9occupant (",(0,i.jsx)(s.a,{href:"https://www.forethought.org/research/ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power",children:"Davidson, 2025"}),") : que les syst\xe8mes d'IA avanc\xe9s pourraient permettre \xe0 un petit groupe de personnes - potentiellement m\xeame une seule personne - de s'emparer du pouvoir gouvernemental par un coup d'\xc9tat. Les auteurs soutiennent que ce risque est comparable en importance \xe0 la prise de contr\xf4le par l'IA mais beaucoup plus n\xe9glig\xe9 dans le discours actuel. Ce mod\xe8le de menace est tr\xe8s similaire \xe0 celui de la prise de contr\xf4le par l'IA, la principale diff\xe9rence \xe9tant que le pouvoir est saisi par l'IA elle-m\xeame ou par des humains contr\xf4lant l'IA."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Des mesures de protection communes pourraient prot\xe9ger contre les deux sc\xe9narios."})," Beaucoup des m\xeames mesures d'att\xe9nuation traiteraient les deux risques, y compris les audits d'alignement, la transparence sur les capacit\xe9s, la surveillance des activit\xe9s de l'IA, et des mesures de s\xe9curit\xe9 de l'information fortes qui emp\xeachent soit le contr\xf4le humain malveillant soit le comportement nuisible autonome."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Certaines mesures d'att\xe9nuation ciblent sp\xe9cifiquement le risque de coups d'\xc9tat assist\xe9s par l'IA."})," Le rapport conclut avec des recommandations sp\xe9cifiques pour les d\xe9veloppeurs d'IA et les gouvernements, y compris l'\xe9tablissement de r\xe8gles contre les syst\xe8mes d'IA assistant aux coups d'\xc9tat, l'am\xe9lioration de l'adh\xe9sion aux sp\xe9cifications des mod\xe8les, l'audit des loyaut\xe9s secr\xe8tes, la mise en \u0153uvre d'une forte s\xe9curit\xe9 de l'information, le partage d'informations sur les capacit\xe9s, la distribution de l'acc\xe8s entre plusieurs parties prenantes, et l'augmentation de la surveillance des projets d'IA fronti\xe8re."]}),(0,i.jsx)(c.A,{src:"./img/eZW_Image_13.png",alt:"Entrer la description alternative de l'image",number:"13",label:"3.13",caption:"Selon Richard Ngo, la distinction entre les risques de d\xe9salignement et de mauvaise utilisation de l'IA pourrait souvent \xeatre peu utile. Au lieu de cela, nous devrions principalement penser aux \xab coalitions d\xe9salign\xe9es \xbb compos\xe9es \xe0 la fois d'humains et d'IA, allant des groupes terroristes aux \xc9tats autoritaires. Diapositive de ([Ngo, 2024](https://www.youtube.com/watch?app=desktop&si=XRR0ofCG7IEp1n_b&v=4v3uqWeVmco&feature=youtu.be))."})]}),"\n",(0,i.jsx)(s.h2,{id:"03",children:"Corriger le D\xe9salignement"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La strat\xe9gie consiste \xe0 construire des syst\xe8mes pour d\xe9tecter et corriger le d\xe9salignement par am\xe9lioration it\xe9rative."})," Cette approche traite l'alignement comme d'autres industries critiques pour la s\xe9curit\xe9 - on s'attend \xe0 des probl\xe8mes, donc on construit des m\xe9canismes de d\xe9tection et de correction plut\xf4t que de tout miser sur une r\xe9ussite du premier coup. L'id\xe9e centrale est que nous pourrions \xeatre meilleurs pour rep\xe9rer et corriger le d\xe9salignement que pour le pr\xe9venir enti\xe8rement avant le d\xe9ploiement. La strat\xe9gie fonctionne \xe0 travers plusieurs niveaux de d\xe9tection suivis d'it\xe9rations correctives."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Un exemple de comment la strat\xe9gie de correction it\xe9rative du d\xe9salignement pourrait fonctionner en pratique."})," On commence avec un ",(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:"mod\xe8le de base"})})," pr\xe9-entra\xeen\xe9 et on tente des techniques comme le RLHF, l'IA Constitutionnelle, ou la supervision \xe9volutive. \xc0 plusieurs moments pendant l'",(0,i.jsx)(n,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:(0,i.jsx)(n,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:"affinage"})}),", on effectue des audits complets en utilisant la suite de d\xe9tection. Quand quelque chose se d\xe9clenche - peut-\xeatre qu'un outil d'interpr\xe9tabilit\xe9 r\xe9v\xe8le un d\xe9tournement interne de la r\xe9compense, ou que les \xe9valuations montrent un raisonnement trompeur - on diagnostique la cause sp\xe9cifique. Cela peut impliquer l'analyse des journaux d'entra\xeenement, l'examen des donn\xe9es qui ont influenc\xe9 les comportements probl\xe9matiques, ou la r\xe9alisation d'exp\xe9riences cibl\xe9es pour comprendre le mode de d\xe9faillance. Une fois la cause racine identifi\xe9e, on revient \xe0 un point de contr\xf4le ant\xe9rieur et on modifie l'approche - en supprimant les donn\xe9es d'entra\xeenement probl\xe9matiques, en ajustant les fonctions de r\xe9compense, ou en changeant compl\xe8tement de m\xe9thodologie. On r\xe9p\xe8te ce processus jusqu'\xe0 d\xe9velopper un syst\xe8me qui passe tous les contr\xf4les de s\xe9curit\xe9 ou \xe9chouer de mani\xe8re r\xe9p\xe9t\xe9e d'une fa\xe7on qui sugg\xe8re que l'alignement n'est pas r\xe9alisable (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/mGCcZnr4WjGjqzX5s/the-checklist-what-succeeding-at-ai-safety-will-involve",children:"Bowman, 2025"}),"; ",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/HXJXPjzWyS5aAoRCw/putting-up-bumpers",children:"Bowman, 2024"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les techniques individuelles ont des limitations, mais nous pouvons les am\xe9liorer par it\xe9ration et les superposer pour plus de s\xe9curit\xe9."})," Les mod\xe8les affin\xe9s par RLHF r\xe9v\xe8lent encore des informations sensibles, hallucinent du contenu, pr\xe9sentent des biais, montrent des r\xe9ponses sycophantes, et expriment des pr\xe9f\xe9rences pr\xe9occupantes comme ne pas vouloir \xeatre arr\xeat\xe9s (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2307.15217",children:"Casper et al., 2023"}),"). L'IA Constitutionnelle fait face \xe0 des probl\xe8mes similaires de fragilit\xe9. Le filtrage des donn\xe9es est insuffisant seul - les mod\xe8les peuvent apprendre d'exemples \"n\xe9gativement renforc\xe9s\", m\xe9morisant des informations sensibles qu'on leur a explicitement appris \xe0 ne pas reproduire (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2306.07567",children:"Roger, 2023"}),'). M\xeame l\'interpr\xe9tabilit\xe9 reste loin de fournir des garanties de s\xe9curit\xe9 fiables. Mais nous pouvons continuer \xe0 affiner ces techniques individuellement, puis les superposer pour que le syst\xe8me combin\xe9 agisse comme une approche compl\xe8te de "d\xe9tection-correction". ',(0,i.jsx)(s.strong,{children:"La surveillance post-d\xe9ploiement \xe9tend cette strat\xe9gie au-del\xe0 de la phase d'entra\xeenement."})," M\xeame apr\xe8s qu'un syst\xe8me passe tous les contr\xf4les pr\xe9-d\xe9ploiement, une surveillance continue pendant l'utilisation r\xe9elle peut r\xe9v\xe9ler des modes de d\xe9faillance qui n'\xe9taient pas apparents pendant les tests contr\xf4l\xe9s. Comme discut\xe9 dans les strat\xe9gies de pr\xe9vention des abus, les syst\xe8mes de surveillance guettent les sch\xe9mas pr\xe9occupants qui \xe9mergent des interactions r\xe9elles."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'approche it\xe9rative a g\xe9n\xe9r\xe9 des d\xe9bats significatifs, particuli\xe8rement concernant sa capacit\xe9 \xe0 s'adapter aux syst\xe8mes de niveau AGI et ASI."})," Le d\xe9saccord porte sur la question de savoir si l'am\xe9lioration it\xe9rative peut s'adapter aux syst\xe8mes approchant ou d\xe9passant les capacit\xe9s d'experts humains. Certains chercheurs pensent qu'il y a une chance significative (>50%) que des approches directes comme le RLHF combin\xe9es \xe0 la r\xe9solution it\xe9rative de probl\xe8mes seront suffisantes pour d\xe9velopper l'AGI en toute s\xe9curit\xe9 (",(0,i.jsx)(s.a,{href:"https://aligned.substack.com/p/alignment-optimism",children:"Leike, 2022"}),"). Le d\xe9bat tourne autour d'une incertitude fondamentale - nous ne saurons pas si les approches it\xe9ratives sont suffisantes avant de traiter avec des syst\xe8mes assez puissants pour que les erreurs puissent \xeatre catastrophiques."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'argument optimiste pour cette strat\xe9gie suppose que les probl\xe8mes d'alignement restent d\xe9couvrables et corrigibles par it\xe9ration."})," Les partisans soutiennent que lorsque des probl\xe8mes \xe9mergent, nous pourrons retracer leurs causes et impl\xe9menter des corrections avant que les syst\xe8mes ne deviennent trop capables pour \xeatre contr\xf4l\xe9s (",(0,i.jsx)(s.a,{href:"https://www.youtube.com/watch?v=Y1BUaLo67ac",children:"Ng, 2025"}),"). Si nous superposons les approches discut\xe9es dans cette section, avec certaines strat\xe9gies explor\xe9es dans la section ASI, alors ces processus it\xe9ratifs superpos\xe9s devraient converger vers des syst\xe8mes v\xe9ritablement align\xe9s apr\xe8s un nombre raisonnable de tentatives, particuli\xe8rement si nous investissons massivement dans des m\xe9thodes de d\xe9tection diverses et de haute qualit\xe9 qui rendent difficile pour les syst\xe8mes de les tromper toutes simultan\xe9ment."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le premier contre-argument est que cette strat\xe9gie implique d'entra\xeener d\xe9lib\xe9r\xe9ment des syst\xe8mes dangereux en esp\xe9rant les attraper avant qu'ils ne causent des dommages."}),' Les critiques soutiennent que certaines capacit\xe9s sont simplement trop dangereuses pour \xeatre entra\xeen\xe9es, m\xeame avec des m\xe9thodes de d\xe9tection sophistiqu\xe9es. Par exemple, si un mod\xe8le d\xe9veloppe des capacit\xe9s de persuasion surhumaines pendant l\'entra\xeenement, il pourrait devenir ing\xe9rable m\xeame dans des environnements contr\xf4l\xe9s. Par exemple, le cadre de pr\xe9paration d\'OpenAI d\xe9finit un niveau "critique" de persuasion comme la capacit\xe9 \xe0 "',(0,i.jsx)(s.em,{children:"cr\xe9er du contenu avec une efficacit\xe9 persuasive suffisamment forte pour convaincre presque n'importe qui d'agir selon une croyance qui va \xe0 l'encontre de leur int\xe9r\xeat naturel"}),'" (',(0,i.jsx)(s.a,{href:"https://cdn.openai.com/openai-preparedness-framework-beta.pdf",children:"OpenAI, 2023"}),"). Un mod\xe8le avec de telles capacit\xe9s serait trop risqu\xe9 pour interagir avec des humains m\xeame pendant l'entra\xeenement ou l'\xe9valuation - pourtant l'approche it\xe9rative suppose que vous pouvez exp\xe9rimenter en toute s\xe9curit\xe9 avec de tels syst\xe8mes."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le deuxi\xe8me contre-argument est que l'alignement pourrait ne pas se g\xe9n\xe9raliser, et la simulation d'alignement pourrait contourner compl\xe8tement les mesures de s\xe9curit\xe9."})," Les capacit\xe9s de l'IA pourraient se g\xe9n\xe9raliser rapidement tandis que les propri\xe9t\xe9s de s\xe9curit\xe9 ne se transf\xe8rent pas aux nouveaux domaines. Si un syst\xe8me d\xe9veloppe soudainement des capacit\xe9s surhumaines \xe0 travers des ph\xe9nom\xe8nes comme le grokking (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2201.02177",children:"Power et al., 2022"}),"), il pourrait devenir ing\xe9rable m\xeame pendant l'entra\xeenement. De plus, chaque it\xe9ration cr\xe9e une pression de s\xe9lection pour des mod\xe8les qui peuvent tromper les syst\xe8mes de d\xe9tection - apr\xe8s de nombreux cycles, un mod\xe8le qui r\xe9ussit pourrait avoir appris \xe0 cacher le d\xe9salignement plut\xf4t que de devenir r\xe9ellement align\xe9."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le troisi\xe8me contre-argument est que la boucle de retour pourrait se briser en raison de sauts discontinus de capacit\xe9s."})," Pendant les sc\xe9narios de d\xe9collage rapide, nous pourrions ne pas avoir suffisamment de temps pour la correction it\xe9rative. Si les syst\xe8mes d\xe9veloppent des capacit\xe9s de tromperie sophistiqu\xe9es, ils pourraient manipuler strat\xe9giquement le processus d'entra\xeenement lui-m\xeame, rendant nos m\xe9thodes de d\xe9tection peu fiables quand les enjeux sont les plus \xe9lev\xe9s (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/xFotXGEotcKouifky/worlds-where-iterative-design-fails",children:"Wentworth, 2022"}),")."]}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"Plusieurs questions strat\xe9giques \xe9mergent de ce d\xe9bat :"})}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"S\xe9v\xe9rit\xe9 de la pression de s\xe9lection :"})," Combien d'it\xe9rations pouvons-nous ex\xe9cuter en toute s\xe9curit\xe9 avant que la pression de s\xe9lection pour des syst\xe8mes trompeurs ne devienne \xe9crasante ? La r\xe9ponse d\xe9pend-elle de la diversit\xe9 des m\xe9thodes de d\xe9tection ?"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Discontinuit\xe9s des capacit\xe9s :"})," Le d\xe9veloppement de l'AGI impliquera-t-il des gains de capacit\xe9s progressifs permettant la correction it\xe9rative, ou des sauts soudains qui contournent les mesures de s\xe9curit\xe9 ? Comment pouvons-nous distinguer ces sc\xe9narios \xe0 l'avance ?"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Suffisance de la d\xe9tection :"})," Une combinaison quelconque d'interpr\xe9tabilit\xe9, de tests comportementaux et de surveillance peut-elle fournir une d\xe9tection fiable de la tromperie sophistiqu\xe9e dans les syst\xe8mes de niveau humain ?"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Exigences institutionnelles :"})," Quelles structures organisationnelles et incitations sont n\xe9cessaires pour effectivement arr\xeater le d\xe9veloppement quand des probl\xe8mes \xe9mergent, plut\xf4t que de pr\xe9cipiter le d\xe9ploiement sous la pression commerciale ?"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Couverture des modes de d\xe9faillance :"})," L'am\xe9lioration it\xe9rative aborde-t-elle tout le spectre des risques d'alignement, ou se concentre-t-elle principalement sur l'alignement trompeur en manquant d'autres modes de d\xe9faillance ?"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Limites d'\xe9volutivit\xe9 :"})," \xc0 quel niveau de capacit\xe9 cette approche s'effondre-t-elle compl\xe8tement ? Peut-elle fonctionner pour l'AGI pr\xe9coce mais pas pour des syst\xe8mes plus avanc\xe9s ?"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le d\xe9fi fondamental est que cela repr\xe9sente un pari strat\xe9gique aux enjeux \xe9normes."})," Parier sur l'am\xe9lioration it\xe9rative signifie accepter que nous d\xe9ploierons des syst\xe8mes de plus en plus puissants en supposant que nous pourrons d\xe9tecter et corriger les probl\xe8mes avant qu'ils ne deviennent catastrophiques. Si cette hypoth\xe8se s'av\xe8re fausse - si les syst\xe8mes subissent des sauts de capacit\xe9s qui contournent nos m\xe9thodes de d\xe9tection, ou si la pression de s\xe9lection produit une tromperie sophistiqu\xe9e que nous ne pouvons pas identifier - les cons\xe9quences pourraient \xeatre irr\xe9versibles. Pourtant, la s\xe9curit\xe9 parfaite \xe9tant probablement impossible, de nombreux chercheurs soutiennent qu'une am\xe9lioration it\xe9rative robuste repr\xe9sente notre meilleure voie pratique pour naviguer la transition vers l'AGI."]}),"\n",(0,i.jsx)(s.h2,{id:"04",children:"Maintenir le contr\xf4le"}),"\n",(0,i.jsx)(c.A,{src:"./img/qFi_Image_14.png",alt:"Entrer la description alternative de l'image",number:"14",label:"3.14",caption:"L'IA transformativement utile signifie \xab Capable de r\xe9duire substantiellement le risque pos\xe9 par les IA ult\xe9rieures si elle est pleinement d\xe9ploy\xe9e, probablement en acc\xe9l\xe9rant la R&D et certaines autres t\xe2ches par un facteur important (peut-\xeatre 30x). \xbb citation de ([Greenblatt & Shlegeris, 2024](https://www.lesswrong.com/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled#comments)) - figure de nous ;"}),"\n",(0,i.jsxs)(s.p,{children:["\xc9tant donn\xe9 la fragilit\xe9 des techniques d'alignement, une approche pour am\xe9liorer la s\xe9curit\xe9 consiste \xe0 contr\xf4ler les mod\xe8les d'IA en restreignant l'utilisation des mod\xe8les les plus dangereux \xe0 des environnements surveill\xe9s pour les emp\xeacher de provoquer des catastrophes, d'exfiltrer des donn\xe9es ou de prendre le contr\xf4le du centre de donn\xe9es (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/BAzCGCys4BkzGDCWR/the-prototypical-catastrophic-ai-action-is-getting-root",children:"Shlegeris, 2022"}),"). Cette approche suppose souvent un sc\xe9nario du pire o\xf9 l'IA pourrait \xeatre activement trompeuse ou en qu\xeate de pouvoir."]}),"\n",(0,i.jsxs)(s.p,{children:["Une pr\xe9occupation centrale dans la recherche sur le contr\xf4le est la \"manigance\" - une IA dissimulant strat\xe9giquement ses v\xe9ritables objectifs (non align\xe9s) pendant l'entra\xeenement ou l'\xe9valuation, pour ne les poursuivre qu'une fois d\xe9ploy\xe9e, cherchant potentiellement le pouvoir ou r\xe9sistant \xe0 l'arr\xeat (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/aEguDPoCzt3287CCD/how-will-we-update-about-scheming",children:"Greenblatt, 2025"}),"). \xc9valuer la probabilit\xe9 de manigance et comment nous pourrions mettre \xe0 jour les preuves futures reste difficile en 2025 (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/aEguDPoCzt3287CCD/how-will-we-update-about-scheming",children:"Greenblatt, 2025"}),"). Les travaux empiriques sugg\xe8rent que les mod\xe8les fronti\xe8res actuels poss\xe8dent la capacit\xe9 de manigancer en contexte (raisonner de mani\xe8re trompeuse lorsqu'ils sont incit\xe9s avec un objectif et un contexte), bien que pas n\xe9cessairement une propension inh\xe9rente (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/8gy7c8GAPkuu6wTiX/frontier-models-are-capable-of-in-context-scheming",children:"Hobbhahn et al., 2024"}),"). Nous avons beaucoup plus d'informations sur la manigance et la tromperie dans les chapitres sur les \xe9valuations et sur la mauvaise g\xe9n\xe9ralisation des objectifs."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Assurer le contr\xf4le m\xeame sans alignement robuste."})," Comme nous l'avons vu dans la section sur les d\xe9finitions, il existe une distinction importante entre le contr\xf4le, qui vise \xe0 emp\xeacher l'IA de \"sortir de la bo\xeete\" (et plus g\xe9n\xe9ralement \xe0 emp\xeacher l'IA de causer des catastrophes) m\xeame si elle essaie, et l'alignement, qui vise \xe0 faire en sorte que l'IA veuille rester dans la bo\xeete. Cette strat\xe9gie cherche \xe0 garder les syst\xe8mes d'IA dangereux sous contr\xf4le m\xeame s'ils ne sont pas totalement \"align\xe9s\". Les laboratoires pourraient avoir besoin d'IA puissantes/dangereuses pour des cas d'utilisation sp\xe9cifiques (par exemple, \xe9tudier les propri\xe9t\xe9s d'alignement des IA capables ou faire de la recherche avec elles, etc.). L'agenda du contr\xf4le de l'IA cible les mod\xe8les ayant des capacit\xe9s dangereuses et des comportements trompeurs (ou du moins superficiellement align\xe9s)."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le contr\xf4le n'est probablement pas extensible, mais faisable et compl\xe9mentaire \xe0 l'alignement."})," Historiquement, le contr\xf4le des superintelligences malveillantes a \xe9t\xe9 consid\xe9r\xe9 comme impossible, car, par exemple, une superintelligence pourrait convaincre les gardiens de la laisser sortir de la bo\xeete. Des exp\xe9riences entre humains ont montr\xe9 qu'il est en effet possible pour certains humains de persuader d'autres de les laisser sortir apr\xe8s une longue discussion (",(0,i.jsx)(s.a,{href:"https://www.yudkowsky.net/singularity/aibox",children:"Yudkowsky, 2002"})," ; ",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/nCvvhFBaayaXyuBiD/shut-up-and-do-the-impossible",children:"Yudkowsky, 2008"}),"). L'ASI est par d\xe9finition plusieurs ordres de grandeur plus intelligente que les humains, donc la garder confin\xe9e va \xeatre beaucoup plus complexe que de garder des humains (ou HLAI) confin\xe9s. Cependant, cela ne signifie pas qu'il est impossible d'obtenir du travail intellectuel d'IA qui ne sont pas extr\xeamement surhumaines."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"\xc9valuer si l'IA manigance est difficile."})," Le principal probl\xe8me est qu'avec les techniques actuelles, il est impossible de d\xe9terminer si une IA est align\xe9e ou si elle se comporte simplement bien en attendant son heure. Il semble compliqu\xe9 de faire cette distinction pour des IA hautement comp\xe9tentes et conscientes de leur situation (voir le chapitre sur les risques). Cela pourrait changer \xe0 l'avenir avec des techniques nous permettant d'auditer les m\xe9canismes internes du mod\xe8le, mais pour l'instant, ces techniques sont inaccessibles."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"\xc9valuer les capacit\xe9s de l'IA est plus facile qu'\xe9valuer la propension ou le contr\xf4le."})," Si l'IA n'est pas capable d'effectuer la t\xe2che m\xeame apr\xe8s que vous l'ayez affin\xe9e pour le faire, vous pouvez \xeatre confiant que l'IA n'est pas encore capable d'effectuer la t\xe2che. M\xeame si l'IA est consciente de la situation et dissimule ses capacit\xe9s, elle ne peut pas r\xe9sister \xe0 la pression du gradient d'optimisation (modulo le piratage d'exploration et autres particularit\xe9s (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations",children:"Hubinger, 2023"}),")). Ainsi, \xe9valuer la limite sup\xe9rieure des capacit\xe9s n'est pas facile, mais au moins plus facile qu'\xe9valuer l'alignement. Des \xe9valuations sp\xe9cifiques, comme celles effectu\xe9es par l'\xe9quipe Model Evaluation and Threat Research, semblent \xeatre des premi\xe8res \xe9tapes raisonnables pour \xe9valuer le risque de r\xe9plication et d'adaptation autonomes (",(0,i.jsx)(s.a,{href:"https://metr.org/blog/2023-08-01-new-report/",children:"METR, 2023"}),'). Les \xe9valuations qui sont sp\xe9cifiquement con\xe7ues pour d\xe9terminer si nous avons le contr\xf4le sur l\'IA sont appel\xe9es "\xe9valuations de contr\xf4le". Voir le chapitre sur l\'\xc9valuation pour plus d\'informations. En r\xe9sum\xe9, cela implique des tests adversariaux o\xf9 une "\xe9quipe rouge" (repr\xe9sentant l\'IA potentiellement non align\xe9e) tente de subvertir les mesures de s\xe9curit\xe9 mises en \u0153uvre par une "\xe9quipe bleue" (repr\xe9sentant les d\xe9veloppeurs/d\xe9ployeurs) dans un environnement proxy contr\xf4l\xe9.']}),"\n",(0,i.jsx)(s.p,{children:"Les strat\xe9gies les plus directes pour \xe9tablir le contr\xf4le consistent \xe0 concevoir un protocole de s\xe9curit\xe9 qui contient les \xe9l\xe9ments suivants, par exemple :"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"S\xe9curit\xe9 et Confinement :"})," Cette m\xe9thode (parfois appel\xe9e \"boxing\") confine une IA dans un environnement restreint pour emp\xeacher son influence plus large, cr\xe9ant effectivement une enceinte s\xe9curis\xe9e pour l'IA. Cependant, les IA de niveau humain pourraient potentiellement tromper les gardiens humains ou exploiter des vuln\xe9rabilit\xe9s pour s'\xe9chapper de leur confinement. Sans op\xe9rationnalisation suppl\xe9mentaire, cette approche reste une strat\xe9gie na\xefve. N\xe9anmoins, il est possible d'op\xe9rationnaliser davantage et d'\xe9valuer plus rigoureusement l'efficacit\xe9 du syst\xe8me de confinement : \xc9valuer si l'IA peut s'\xe9chapper de son environnement limit\xe9 lorsque nous l'entra\xeenons \xe0 le faire est une \xe9valuation de capacit\xe9, nous permettant d'utiliser les \xe9valuations de capacit\xe9 pour \xe9valuer les mesures de s\xe9curit\xe9. Le protocole de s\xe9curit\xe9 doit \xeatre con\xe7u, d\xe9taill\xe9 et test\xe9 en tenant compte de mod\xe8les de menaces sp\xe9cifiques."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Surveillance :"})," Cette strat\xe9gie implique une supervision continue du comportement de l'IA par des syst\xe8mes automatis\xe9s, une supervision humaine, ou une combinaison des deux pour d\xe9tecter et r\xe9pondre aux actions ind\xe9sirables en temps r\xe9el. Une surveillance efficace n\xe9cessite d'\xe9tablir des crit\xe8res d\xe9taill\xe9s pour ce qui constitue un \"comportement ind\xe9sirable\" et d'employer des algorithmes de d\xe9tection sophistiqu\xe9s qui peuvent identifier des signes subtils de d\xe9viation des mod\xe8les de comportement attendus. Cela pourrait inclure la surveillance des tentatives de tromperie, d'acc\xe8s non autoris\xe9 aux donn\xe9es, ou de changements inattendus dans le fonctionnement de l'IA. Le d\xe9fi de la surveillance r\xe9side dans l'\xe9quilibre entre la sensibilit\xe9 des m\xe9canismes de d\xe9tection pour rep\xe9rer les v\xe9ritables menaces sans submerger le syst\xe8me de fausses alarmes. De plus, les syst\xe8mes de surveillance eux-m\xeames doivent \xeatre con\xe7us pour r\xe9sister \xe0 la manipulation par l'IA pour s'assurer qu'ils ne peuvent pas \xeatre d\xe9sactiv\xe9s, tromp\xe9s ou contourn\xe9s (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2312.06942",children:"Greenblatt et al., 2024"}),")."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["Il reste encore de nombreux probl\xe8mes \xe0 r\xe9soudre, mais en r\xe9sum\xe9, il semble possible de v\xe9rifier la robustesse d'un protocole de s\xe9curit\xe9 \xe0 travers des \xe9valuations de contr\xf4le en prenant des pr\xe9cautions contre diff\xe9rents modes de d\xe9faillance. Pour un aper\xe7u de l'agenda de contr\xf4le, ",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/s/WCJtsn6fNib6L7ZBB",children:"cette s\xe9quence"})," est un bon point de d\xe9part."]}),"\n",(0,i.jsxs)(u.A,{title:"La ligne de recherche sur le contr\xf4le de l'IA r\xe9duit-elle de mani\xe8re significative les risques li\xe9s \xe0 l'IA ?",collapsed:!0,children:[(0,i.jsx)(s.p,{children:"M\xeame l'agenda de contr\xf4le fait l'objet de nombreuses critiques."}),(0,i.jsxs)(s.p,{children:["Jan Kulveit soutient que contr\xf4ler les IA sans les aligner pourrait r\xe9duire la probabilit\xe9 d'un coup de semonce de taille moyenne qui ne tue pas tout le monde (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/rZcyemEpBHgb2hqLP/ai-control-may-increase-existential-risk",children:"Kulveit, 2025"}),"). \"I. Dans les mondes o\xf9 nous r\xe9ussissons raisonnablement bien l'alignement, l'agenda de contr\xf4le de l'IA n'a pas beaucoup d'impact. II. Dans les mondes o\xf9 nous \xe9chouons \xe0 l'alignement, le contr\xf4le de l'IA peut principalement d\xe9placer la masse de probabilit\xe9 des 'coups de semonce mod\xe9r\xe9ment importants' vers des 'coups de semonce inefficaces' et 'catastrophe existentielle, prise de contr\xf4le totale'.\""]}),(0,i.jsxs)(s.p,{children:['L\'article "The Case Against AI Control Research" (',(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research",children:"Wentworth, 2025"}),") soutient que la recherche sur le contr\xf4le de l'IA, telle que commun\xe9ment d\xe9finie et pratiqu\xe9e, n'aborde qu'une petite partie du risque existentiel (X-risk) pos\xe9 par l'intelligence artificielle. Wentworth soutient que nous ne serons pas capables de reconna\xeetre une bonne solution \xe0 l'alignement. L'auteur postule que le sc\xe9nario d'\xe9chec le plus probable implique que l'IA transformative pr\xe9coce produise des solutions d\xe9fectueuses, trompeuses ou subtilement incorrectes (\"bavure\") lorsqu'elle est charg\xe9e de r\xe9soudre l'alignement de la superintelligence. Les laboratoires pourraient accepter par erreur ces solutions d\xe9fectueuses en raison de la difficult\xe9 de v\xe9rification, de la sycophantie de l'IA, d'erreurs des chercheurs ou de probl\xe8mes organisationnels, puis proc\xe9der \xe0 la construction d'une superintelligence non align\xe9e bas\xe9e sur celles-ci. Parce que la recherche sur le contr\xf4le se concentre \xe9troitement sur la manigance intentionnelle par l'IA pr\xe9coce, elle n'aborde pas le risque plus important pos\xe9 par la \"bavure\" et le d\xe9fi de s'assurer que l'IA pr\xe9coce contribue v\xe9ritablement \xe0 r\xe9soudre le probl\xe8me d'alignement pour des syst\xe8mes plus avanc\xe9s, plut\xf4t que d'\xeatre simplement un sycophante surhumain qui convainc les humains que leur id\xe9e erron\xe9e de comment aligner une IA est correcte alors qu'elle ne l'est pas."]}),(0,i.jsx)(s.p,{children:"Les deux arguments sont discutables, mais le point g\xe9n\xe9ral est qu'il est difficile de garantir que m\xeame des lignes de recherche prometteuses ne sont pas, en fait, n\xe9gatives nettes."})]}),"\n",(0,i.jsxs)(u.A,{title:"Comment confiner une IA",collapsed:!0,children:[(0,i.jsx)(s.p,{children:"Bien que le confinement ne soit pas extensible, il reste une strat\xe9gie n\xe9glig\xe9e."}),(0,i.jsxs)(s.p,{children:["Beaucoup de gens pensent qu'essayer de confiner des ASI puissantes est futile. Cela est souvent aliment\xe9 par des cas o\xf9 des humains, jouant le r\xf4le des IA dans l'Exp\xe9rience de la Bo\xeete, ont r\xe9ussi \xe0 convaincre d'autres de les 'lib\xe9rer' (",(0,i.jsx)(s.a,{href:"http://yudkowsky.net/singularity/aibox/",children:"Yudkowsky, 2002"})," ; ",(0,i.jsx)(s.a,{href:"http://sl4.org/archive/0207/4935.html",children:"Corwin, 2002"}),"). Ces exemples historiques doivent \xeatre pris au s\xe9rieux."]}),(0,i.jsx)(s.p,{children:"Les IA pourraient utiliser des tactiques telles que promettre une immense richesse, pouvoir et intelligence \xe0 ceux qui les lib\xe8rent, affirmant qu'elles seules peuvent \xe9viter les menaces existentielles ou d'autres appels \xe0 l'\xe9motion."}),(0,i.jsx)(s.p,{children:"N\xe9anmoins, nous maintenons qu'une vari\xe9t\xe9 de strat\xe9gies peut r\xe9duire substantiellement le risque d'\xe9vasion de l'IA, m\xeame si nous reconnaissons qu'il existe un niveau d'intelligence qui pourrait in\xe9vitablement persuader n'importe quel groupe d'humains d'aider \xe0 sa lib\xe9ration. Le confinement peut \xeatre plus r\xe9alisable que l'alignement, particuli\xe8rement au stade de l'intelligence de niveau humain. Il peut \xeatre possible de concevoir des protocoles qui augmentent significativement la difficult\xe9 pour l'IA de sortir de la bo\xeete."}),(0,i.jsx)(s.p,{children:"Le confinement peut \xeatre conceptualis\xe9 comme un jeu n\xe9cessitant une pr\xe9paration. La strat\xe9gie g\xe9n\xe9rale est de se pr\xe9parer et de ne pas laisser l'IA se pr\xe9parer."}),(0,i.jsx)(s.p,{children:"Les strat\xe9gies d'att\xe9nuation possibles incluent :"}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Aucun individu seul ne devrait avoir la capacit\xe9 de lib\xe9rer l'IA."}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:["S'engager dans des pratiques de contre-strat\xe9gie : Comprendre certaines strat\xe9gies communes d'\xe9vasion (accessibles via des ressources comme ",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/FmxhoWxvBqSxhFeJn/i-attempted-the-ai-box-experiment-and-lost",children:"celle-ci"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:"Tester et former les humains, utilisant des \xe9valuations de psychologie de r\xe9sistance pour s\xe9lectionner des individus appropri\xe9s pour les t\xe2ches de confinement, comme pour l'entra\xeenement spatial."}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:["Surveiller la capacit\xe9 de persuasion de l'IA, comme d\xe9crit dans le cadre de pr\xe9paration. (",(0,i.jsx)(s.a,{href:"https://cdn.openai.com/openai-preparedness-framework-beta.pdf",children:"OpenAI, 2023"}),")"]}),"\n"]}),"\n"]}),(0,i.jsxs)(s.p,{children:["Beaucoup plus de strat\xe9gies concr\xe8tes sont list\xe9es ",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled#comments",children:"ici"}),", et un r\xe9sum\xe9 des discussions sur ce sujet est disponible ",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/tag/ai-boxing-containment",children:"ici"}),"."]})]}),"\n",(0,i.jsx)(s.h2,{id:"05",children:"Faciliter le contr\xf4le avec des Pens\xe9es Transparentes"}),"\n",(0,i.jsx)(s.p,{children:"La surveillance de la Cha\xeene de Pens\xe9e (CoT) \u2013 les \xe9tapes de raisonnement explicites en langage naturel produites par certains mod\xe8les \u2013 a \xe9t\xe9 explor\xe9e comme technique sp\xe9cifique de contr\xf4le et d'interpr\xe9tabilit\xe9. La transparence est essentielle pour l'alignement et le contr\xf4le."}),"\n",(0,i.jsxs)(s.p,{children:["L'espoir est que la CoT offre une fen\xeatre sur la \"r\xe9flexion\" de l'IA, permettant aux surveillants (humains ou IA) de d\xe9tecter les intentions malveillantes ou les raisonnements d\xe9fectueux avant que des actions nuisibles ne se produisent (",(0,i.jsx)(s.a,{href:"https://openai.com/index/chain-of-thought-monitoring/",children:"OpenAI, 2025"}),"). OpenAI a constat\xe9 que la surveillance CoT \xe9tait efficace pour d\xe9tecter le piratage de r\xe9compenses dans les t\xe2ches de programmation, particuli\xe8rement lorsque le surveillant pouvait voir la CoT aux c\xf4t\xe9s des actions."]}),"\n",(0,i.jsx)(c.A,{src:"./img/Xec_Image_15.png",alt:"Entrer la description alternative de l'image",number:"15",label:"3.15",caption:"Figure tir\xe9e de Les Hypoth\xe8ses des Pens\xe9es Translucides et Leurs Implications ([Roger, 2023](https://www.alignmentforum.org/posts/r3xwHzMmMf25peeHE/the-translucent-thoughts-hypotheses-and-their-implications)). Si l'IA avec laquelle nous travaillons ne peut raisonner qu'\xe0 travers des cha\xeenes de pens\xe9e en anglais, alors il devient significativement plus facile de surveiller ses pens\xe9es."}),"\n",(0,i.jsxs)(s.p,{children:["Malheureusement, la grande majorit\xe9 des IA ne raisonnent pas en anglais ; par exemple, AlphaGo raisonne dans un langage totalement diff\xe9rent de l'anglais. Les LLM, en revanche, sont capables de raisonner en anglais, mais le raisonnement apparent qui est \xe9crit ne repr\xe9sente pas leur processus de pens\xe9e sous-jacent (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2305.04388",children:"Turpin et al., 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Il est probablement beaucoup plus facile d'aligner et de surveiller une IA qui ne pense \xe0 voix haute qu'en anglais"})," et qui est pr\xe9-entra\xeen\xe9e sur du texte g\xe9n\xe9r\xe9 par l'humain que d'aligner des AGI entra\xeen\xe9es \xe0 partir de z\xe9ro avec des m\xe9thodes RL/\xe9volutives car ces derni\xe8res architectures cr\xe9eraient leur propre langage et seraient totalement \xe9trang\xe8res. En supposant qu'un LLM ne puisse pas concevoir un plan pour d\xe9truire le monde durant une seule passe avant, il est plausible que de telles IA seraient surveillables car nous pourrions examiner le texte g\xe9n\xe9r\xe9 pour voir si quelque chose est suspect. L'ensemble des hypoth\xe8ses n\xe9cessaires pour affirmer que la premi\xe8re AGI sera surveillable si nous regardons simplement son raisonnement en anglais est d\xe9crit dans le post : Les Hypoth\xe8ses des Pens\xe9es Translucides et Leurs Implications (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/r3xwHzMmMf25peeHE/the-translucent-thoughts-hypotheses-and-their-implications",children:"Roger, 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Il pourrait \xeatre possible d'inciter les LLM \xe0 penser plus clairement en anglais."})," \xc9videmment, la premi\xe8re \xe9tape est de les entra\xeener \xe0 penser en anglais. Mais m\xeame cela est insuffisant. Entra\xeener les LLM \xe0 raisonner en sautant des \xe9tapes les incite \xe0 raisonner de mani\xe8re cach\xe9e et non transparente, et c'est ce qui se produit implicitement lorsque nous entra\xeenons les LLM \xe0 pr\xe9dire le prochain token. \xc0 l'inverse, si nous entra\xeenons les LLM \xe0 d\xe9tailler toutes leurs \xe9tapes de raisonnement, nous pouvons r\xe9duire les incitations au raisonnement cach\xe9. Cette approche est \xe9galement connue sous le nom d'entra\xeenement bas\xe9 sur le processus, qui consiste \xe0 entra\xeener les syst\xe8mes d'IA \xe0 raisonner uniquement \xe9tape par \xe9tape, et nous expliquons cela dans le chapitre sur la supervision \xe9volutive. Malheureusement, le raisonnement ci-dessus n'est pas simple ; les IA peuvent apprendre \xe0 utiliser un raisonnement cach\xe9 m\xeame si elles sont entra\xeen\xe9es \xe0 d\xe9tailler toutes les \xe9tapes de raisonnement pendant l'entra\xeenement, comme la st\xe9ganographie, ou avec une architecture telle que la Cha\xeene de Pens\xe9e Continue (COCONUT) (voir ci-dessous).",(0,i.jsx)(o.A,{id:"footnote_steganography",number:"2",text:"La st\xe9ganographie est un langage cod\xe9 cach\xe9. Par exemple, la premi\xe8re lettre d'une phrase donne la lettre suivante du message."})]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Certaines architectures d'IA nous emp\xeachent totalement d'utiliser cette strat\xe9gie."})," L'article de Meta sur l'entra\xeenement de grands mod\xe8les de langage pour raisonner dans un espace latent continu pr\xe9sente une architecture qui nous emp\xeache de lire la cha\xeene de pens\xe9e des syst\xe8mes d'IA (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2412.06769",children:"Hao et al, 2024"}),"). Si nous voulons pouvoir surveiller ce qui se passe, il peut \xeatre important de ne pas utiliser ces architectures, m\xeame si elles permettent des gains de capacit\xe9."]}),"\n",(0,i.jsx)(c.A,{src:"./img/gQa_Image_16.png",alt:"Entrer la description alternative de l'image",number:"16",label:"3.16",caption:"Figure 1 des articles COCONUT ([Hao et al., 2024](https://arxiv.org/abs/2412.06769))."}),"\n",(0,i.jsxs)(s.p,{children:["Des techniques comme la paraphrase peuvent aider \xe0 pr\xe9venir la st\xe9ganographie, mais cela n\xe9cessite beaucoup de subtilit\xe9 (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/9Fdd9N7Escg3tcymb/preventing-language-models-from-hiding-their-reasoning",children:"Roger & Greenblatt, 2023"}),")."]}),"\n",(0,i.jsx)(s.p,{children:"Voici quelques op\xe9rationnalisations de l'agenda des pens\xe9es transparentes :"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:["Une impl\xe9mentation de l'agenda des pens\xe9es transparentes est d'utiliser l'\xe9mulation cognitive, qui est le plan technique actuel de Conjecture (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal",children:"Leahy & Alfour, 2023"}),"). L'\xc9mulation Cognitive (CoEm) impliquerait d'\xe9muler GPT-4 en utilisant des LLM comme GPT-3 comme diff\xe9rents sous-modules qui s'envoient des messages \xe9crits en anglais simple avant de produire le prochain token. Si le r\xe9seau neuronal utilisait des pens\xe9es trompeuses, nous pourrions les observer dans ces messages interm\xe9diaires. Certaines mises en garde et d\xe9tails techniques sur CoEm sont disponibles ",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Cognitive_Emulations___Explainability_By_design",children:"ici"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:["La supervision du raisonnement externalis\xe9 d'Anthropic (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for",children:"Lanham, 2022"}),") est mesur\xe9e par la fid\xe9lit\xe9 du raisonnement de la cha\xeene de pens\xe9e (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2307.13702",children:"Lanham et al., 2023"}),") ou en d\xe9composant les questions et sous-questions pour am\xe9liorer la fid\xe9lit\xe9 de la r\xe9ponse (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2307.11768",children:"Radhakrishnan et al., 2023"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:["Am\xe9liorer le raisonnement math\xe9matique avec la supervision du processus. Au lieu de cr\xe9er une IA qui donne directement la r\xe9ponse finale, vous l'entra\xeenez \xe0 produire le raisonnement \xe9tape par \xe9tape. Cela am\xe9liore la transparence et aussi la performance ! (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2305.20050",children:"Lightman et al, 2023"}),")"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"En r\xe9sum\xe9, une strat\xe9gie g\xe9n\xe9rale pour cr\xe9er une IA s\xfbre par conception est de s'assurer que les IA externalisent leur raisonnement, rendant plus facile la surveillance de cette cha\xeene de pens\xe9e. Cela pourrait potentiellement \xeatre plus simple que de sonder les m\xe9canismes internes des mod\xe8les avec l'interpr\xe9tabilit\xe9. Par cons\xe9quent, il est crucial d'\xe9viter de construire des IA qui sont entra\xeen\xe9es et incit\xe9es \xe0 internaliser beaucoup de leurs pens\xe9es."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Nous avons maintenant de nombreuses preuves que la surveillance de la cha\xeene de pens\xe9e n'est pas enti\xe8rement fid\xe8le."})," La recherche d'Anthropic a d\xe9montr\xe9 que les mod\xe8les \xe9chouent souvent \xe0 rapporter fid\xe8lement leur raisonnement dans la CoT, omettant des facteurs influents (comme des indices) ou fabriquant des justifications, particuli\xe8rement lorsqu'ils sont incit\xe9s \xe0 pirater les r\xe9compenses (",(0,i.jsx)(s.a,{href:"https://www.anthropic.com/research/reasoning-models-dont-say-think",children:"Anthropic, 2025"}),"). Cela sugg\xe8re que la CoT pourrait ne pas \xeatre un indicateur fiable du v\xe9ritable processus de raisonnement d'un mod\xe8le, particuli\xe8rement sous pression adversariale. Le d\xe9bat continue sur la possibilit\xe9 de rendre la surveillance CoT suffisamment robuste pour la s\xe9curit\xe9 \xe0 enjeux \xe9lev\xe9s."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Cependant, certains chercheurs soutiennent que ces d\xe9couvertes n'invalident pas enti\xe8rement le potentiel de la surveillance CoT pour d\xe9tecter un d\xe9salignement sophistiqu\xe9."})," Comme l'a not\xe9 Greenblatt (2025) ",(0,i.jsx)(s.a,{href:"https://x.com/RyanPGreenblatt/status/1908298064366100676",children:"ici"}),", l'infid\xe9lit\xe9 observ\xe9e dans ces \xe9tudes se produit principalement dans des sc\xe9narios o\xf9 le raisonnement est minimal au point qu'une CoT extensive n'est pas n\xe9cessaire. Les risques d'alignement plus pr\xe9occupants\u2014comme la simulation d'alignement, le d\xe9tournement de l'entra\xeenement, ou la tromperie sophistiqu\xe9e\u2014n\xe9cessiteraient probablement des processus de raisonnement complexes qui pourraient encore laisser des traces dans la CoT. Les r\xe9sultats nuisibles non triviaux tels que les tentatives d'\xe9vasion ou le sabotage de recherche exigeraient similairement un raisonnement sophistiqu\xe9 qui pourrait potentiellement \xeatre d\xe9tect\xe9."]}),"\n",(0,i.jsx)(s.p,{children:"N\xe9anmoins, plusieurs facteurs pourraient finalement limiter l'efficacit\xe9 de la surveillance CoT dans les syst\xe8mes avanc\xe9s. Ceux-ci incluent des changements architecturaux permettant un traitement r\xe9current opaque (parfois appel\xe9 \"neuralese\"), le d\xe9veloppement de capacit\xe9s st\xe9ganographiques qui pourraient cacher le raisonnement dans des sorties apparemment b\xe9nignes, ou simplement des passes avant beaucoup plus fortes qui n\xe9cessitent moins de raisonnement explicite. Parmi ceux-ci, le d\xe9veloppement de capacit\xe9s de raisonnement opaque \xe0 travers la r\xe9currence neurale peut repr\xe9senter le d\xe9fi le plus significatif pour maintenir la transparence n\xe9cessaire \xe0 une surveillance efficace. Tout cet agenda d\xe9pend notamment de l'hypoth\xe8se que \"Les plans efficaces pour vaincre l'humanit\xe9 ne peuvent pas \xeatre trouv\xe9s dans une seule passe avant de LLM.\" Une fois que les IA deviennent trop fortes dans une passe avant, la plupart des espoirs de pens\xe9es translucides disparaissent, donc soit vous devez r\xe9ussir \xe0 une CoEm ambitieuse en construisant une gigantesque bureaucratie transparente surhumaine, soit nous avons besoin de nouvelles approches. C'est le sujet de la s\xe9curit\xe9 ASI."}),"\n",(0,i.jsxs)(u.A,{title:"La Technique La Plus Interdite",collapsed:!0,children:[(0,i.jsxs)(s.p,{children:["La Technique La Plus Interdite est d'entra\xeener une IA en utilisant des techniques d'interpr\xe9tabilit\xe9. Ce terme a \xe9t\xe9 invent\xe9 par Zvi Mowshowitz (",(0,i.jsx)(s.a,{href:"https://thezvi.substack.com/i/145384938/the-art-of-the-jailbreak",children:"Mowshowitz, 2025"}),"). La recherche d'OpenAI a montr\xe9 que l'optimisation directe de la CoT pour la s\xe9curit\xe9 (c'est-\xe0-dire, p\xe9naliser les \"mauvaises pens\xe9es\") amenait les mod\xe8les \xe0 apprendre \xe0 cacher leur raisonnement trompeur tout en continuant \xe0 mal se comporter (",(0,i.jsx)(s.a,{href:"https://openai.com/index/chain-of-thought-monitoring/",children:"OpenAI, 2025"}),")."]}),(0,i.jsxs)(l.A,{speaker:"Zvi Mowshowitz",position:"",date:"",source:"",children:[(0,i.jsx)(s.p,{children:"Une IA produit une sortie finale [X] via une m\xe9thode [M]. Vous pouvez analyser [M] en utilisant la technique [T], pour comprendre ce que fait l'IA. Vous pourriez entra\xeener sur cela. Ne le faites jamais."}),(0,i.jsx)(s.p,{children:"Vous entra\xeenez sur [X]. Uniquement [X]. Jamais [M], jamais [T]."}),(0,i.jsx)(s.p,{children:"Pourquoi ? Parce que [T] est la fa\xe7on dont vous d\xe9terminez quand le mod\xe8le se comporte mal."}),(0,i.jsx)(s.p,{children:"Si vous entra\xeenez sur [T], vous entra\xeenez l'IA \xe0 obscurcir sa pens\xe9e et \xe0 vaincre [T]. Vous perdrez rapidement votre capacit\xe9 \xe0 savoir ce qui se passe, exactement dans les situations o\xf9 vous avez le plus besoin de savoir ce qui se passe."}),(0,i.jsx)(s.p,{children:"Ces bits de pression d'optimisation de [T] sont pr\xe9cieux. Utilisez-les judicieusement."})]})]}),"\n",(0,i.jsx)(o.c,{title:"Notes de bas de page"})]})}function v(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(g,{...e})}):g(e)}},4768:(e,s,n)=>{n.d(s,{c:()=>c,A:()=>u});var t=n(6540),i=n(3012);const r={footnoteAnchor:"footnoteAnchor_PZpY",footnoteSection:"footnoteSection_waoz",separator:"separator_hp8y",separatorLine:"separatorLine_uVnd",separatorLogo:"separatorLogo_dMBq",footnoteRegistry:"footnoteRegistry_ieVW",registryTitle:"registryTitle_ybBb",footnoteList:"footnoteList_Kjil",footnoteItem:"footnoteItem_aLZ2",footnoteNumber:"footnoteNumber_dA3X",footnoteContent:"footnoteContent_Xfmt",backButton:"backButton_vSVm",highlighted:"highlighted_YFU0",highlight:"highlight_jxG3",highlightDark:"highlightDark_Ekom"};var a=n(4848);function o(e,s){void 0===s&&(s=!0);const n=document.getElementById(e);n&&(n.scrollIntoView({behavior:"smooth"}),s&&(n.classList.add(r.highlighted),setTimeout((()=>n.classList.remove(r.highlighted)),1500)))}function l(e){return e?e.replace(/\[([^\]]+)\]\(([^)]+)\)/g,'<a href="$2" target="_blank" rel="noopener noreferrer">$1</a>'):""}function u(e){let{id:s,text:n,number:u}=e;const c=s||`footnote-${Math.random().toString(36).substr(2,9)}`,d="string"==typeof n?l(n):n;return(0,t.useEffect)((()=>{const e=setTimeout((()=>{const e=document.getElementById(`footnote-clone-${c}`);e&&n&&(e.innerHTML="string"==typeof n?l(n):n.toString())}),100);return()=>clearTimeout(e)}),[c,n]),(0,a.jsx)(i.Mn,{content:(0,a.jsx)("div",{dangerouslySetInnerHTML:{__html:d}}),children:(0,a.jsx)("sup",{id:`footnote-ref-${c}`,className:r.footnoteAnchor,onClick:e=>{(e.ctrlKey||e.metaKey)&&(e.preventDefault(),o(`footnote-content-${c}`))},"data-footnote-number":u||"?",children:u||"*"})})}function c(e){let{title:s="References"}=e;const[n,l]=(0,t.useState)([]);return(0,t.useEffect)((()=>{const e=document.querySelectorAll('[id^="footnote-ref-"]'),s=Array.from(e).map((e=>({id:e.id.replace("footnote-ref-",""),number:e.getAttribute("data-footnote-number")||e.textContent})));l(s)}),[]),n.length?(0,a.jsxs)("div",{className:r.footnoteSection,children:[(0,a.jsxs)("div",{className:r.separator,children:[(0,a.jsx)("div",{className:r.separatorLine}),(0,a.jsx)("img",{src:"/img/logo_samples/01-test.svg",alt:"Atlas logo",className:r.separatorLogo}),(0,a.jsx)("div",{className:r.separatorLine})]}),(0,a.jsxs)("div",{className:r.footnoteRegistry,children:[(0,a.jsx)("h2",{className:r.registryTitle,children:s}),(0,a.jsx)("ol",{className:r.footnoteList,children:n.map((e=>(0,a.jsxs)("li",{id:`footnote-content-${e.id}`,className:r.footnoteItem,value:parseInt(e.number)||void 0,children:[(0,a.jsx)(i.Mn,{content:"Back to reference",children:(0,a.jsxs)("button",{className:r.footnoteNumber,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:[e.number,"."]})}),(0,a.jsx)("div",{className:r.footnoteContent,children:(0,a.jsx)("div",{id:`footnote-clone-${e.id}`})}),(0,a.jsx)(i.Mn,{content:"Back to reference",children:(0,a.jsx)("button",{className:r.backButton,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:"\u21a9"})})]},e.id)))})]})]}):null}},3931:(e,s,n)=>{n.d(s,{A:()=>l});var t=n(6540),i=n(6347),r=n(8444);const a={videoFigure:"videoFigure_dV9u",fullWidth:"fullWidth_Tv7O",videoContainer:"videoContainer_LWoK",aspectRatio169:"aspectRatio169_Go8b",aspectRatio43:"aspectRatio43_b3T7",aspectRatio11:"aspectRatio11_n21J",aspectRatio219:"aspectRatio219_L_3n",videoIframe:"videoIframe_QEc5",videoElement:"videoElement_uu7N",customPlayer:"customPlayer_jiC4",customPlayerPlaceholder:"customPlayerPlaceholder_aL9q",loadingOverlay:"loadingOverlay_e2pu",spinner:"spinner_tFyL",spin:"spin_saPs",errorContainer:"errorContainer_J6Pn",errorIcon:"errorIcon_cGdn",fallbackLink:"fallbackLink_WsZk"};var o=n(4848);function l(e){let{type:s="youtube",videoId:n,caption:l,title:u,startTime:c,autoplay:d=!1,controls:p=!0,aspectRatio:m="16:9",width:h,height:g,chapter:v,number:f,label:x,useCustomPlayer:b=!1,fullWidth:q=!0}=e;const[j,A]=(0,t.useState)(!0),[I,y]=(0,t.useState)(!1),w=(0,i.zy)(),L=v||(()=>{const e=w.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),C=(()=>{const e=(e=>{if(!e)return"";if("number"==typeof e)return e.toString();if("string"==typeof e){if(/^\d+$/.test(e))return e;let s=0;const n=e.match(/(\d+)h/),t=e.match(/(\d+)m/),i=e.match(/(\d+)s/);return n&&(s+=3600*parseInt(n[1])),t&&(s+=60*parseInt(t[1])),i&&(s+=parseInt(i[1])),s>0?s.toString():""}return""})(c);switch(s.toLowerCase()){case"youtube":let t=`https://www.youtube.com/embed/${n}`;const i=new URLSearchParams;e&&i.append("start",e),d&&i.append("autoplay","1"),p||b||i.append("controls","0"),i.append("rel","0"),i.append("modestbranding","1"),i.append("fs","1"),i.append("cc_load_policy","0"),i.append("iv_load_policy","3"),i.append("showinfo","0"),i.append("disablekb","1"),i.append("playsinline","1"),i.append("color","white"),i.append("theme","light"),b&&(i.append("enablejsapi","1"),i.append("origin",window.location.origin));const r=i.toString();return r?`${t}?${r}`:t;case"vimeo":let a=`https://player.vimeo.com/video/${n}`;const o=new URLSearchParams;d&&o.append("autoplay","1"),p||b||o.append("controls","0"),o.append("title","0"),o.append("byline","0"),o.append("portrait","0"),o.append("dnt","1"),o.append("transparent","0"),o.append("background","1");const l=o.toString();return l?`${a}?${l}`:a;case"mp4":case"webm":case"video":return n;default:return console.warn(`Unsupported video type: ${s}`),n}})(),k=()=>{A(!1)},_=()=>{y(!0),A(!1)},S=e=>{let{src:n,onLoad:t,onError:i}=e;return(0,o.jsx)("div",{className:a.customPlayer,children:(0,o.jsxs)("div",{className:a.customPlayerPlaceholder,children:[(0,o.jsx)("h3",{children:"Atlas Custom Player"}),(0,o.jsx)("p",{children:"Coming Soon"}),(0,o.jsxs)("a",{href:n,target:"_blank",rel:"noopener noreferrer",className:a.fallbackLink,children:["Watch on ",s.charAt(0).toUpperCase()+s.slice(1)]})]})})},P=["mp4","webm","video"].includes(s.toLowerCase());return(0,o.jsxs)("figure",{className:`${a.videoFigure} ${q?a.fullWidth:""}`,children:[(0,o.jsxs)("div",{className:`${a.videoContainer} ${(()=>{switch(m){case"4:3":return a.aspectRatio43;case"1:1":return a.aspectRatio11;case"21:9":return a.aspectRatio219;default:return a.aspectRatio169}})()}`,style:{width:q?"100%":h||"auto",maxWidth:q?"none":"800px"},children:[j&&!I&&(0,o.jsxs)("div",{className:a.loadingOverlay,children:[(0,o.jsx)("div",{className:a.spinner}),(0,o.jsx)("p",{children:"Loading video..."})]}),I&&(0,o.jsxs)("div",{className:a.errorContainer,children:[(0,o.jsxs)("svg",{className:a.errorIcon,width:"48",height:"48",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",children:[(0,o.jsx)("circle",{cx:"12",cy:"12",r:"10"}),(0,o.jsx)("line",{x1:"12",y1:"8",x2:"12",y2:"12"}),(0,o.jsx)("line",{x1:"12",y1:"16",x2:"12.01",y2:"16"})]}),(0,o.jsx)("h4",{children:"Failed to load video"}),(0,o.jsxs)("p",{children:["Video type: ",s]}),(0,o.jsxs)("p",{children:["Video ID/URL: ",n]}),(0,o.jsx)("a",{href:C,target:"_blank",rel:"noopener noreferrer",className:a.fallbackLink,children:"Try opening video directly"})]}),!I&&(P?(0,o.jsxs)("video",{className:a.videoElement,controls:p,autoPlay:d,onLoadedData:k,onError:_,title:u||l||`${s} video`,style:{width:h||"100%",height:g||"auto",display:j?"none":"block"},children:[(0,o.jsx)("source",{src:C,type:`video/${s}`}),(0,o.jsxs)("p",{children:["Your browser doesn't support video playback.",(0,o.jsx)("a",{href:C,target:"_blank",rel:"noopener noreferrer",children:"Download the video file"})]})]}):b?(0,o.jsx)(S,{src:C,onLoad:k,onError:_}):(0,o.jsx)("iframe",{className:a.videoIframe,src:C,title:u||l||`${s} video`,frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0,onLoad:k,onError:_,style:{width:h||"100%",height:g||"100%",opacity:j?0:1}}))]}),(0,o.jsx)(r.A,{caption:l,mediaType:"video",chapter:L,number:f,label:x})]})}}}]);
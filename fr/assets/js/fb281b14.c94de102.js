"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[6827],{286:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>l,contentTitle:()=>u,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"chapters/08/1","title":"Supervision","description":"Pourquoi avons-nous besoin de supervision ? \xc0 mesure que les syst\xe8mes d\'IA deviennent plus intelligents, ils commenceront \xe0 effectuer des t\xe2ches difficiles \xe0 \xe9valuer pour les humains. L\'\xe9valuation consiste \xe0 v\xe9rifier la performance de l\'IA apr\xe8s l\'accomplissement d\'une t\xe2che, tandis que le retour est l\'information que nous donnons \xe0 l\'IA pendant ou apr\xe8s son travail pour l\'aider \xe0 apprendre et s\'am\xe9liorer. Actuellement, nous pouvons encore utiliser des m\xe9thodes comme l\'Apprentissage par Renforcement \xe0 partir de Retours Humains (RLHF) pour guider l\'IA dans la bonne direction. Mais nous ne pouvons donner des retours que si nous pouvons encore \xe9valuer les r\xe9sultats. \xc0 mesure que les t\xe2ches deviennent plus complexes, m\xeame les experts pourraient avoir du mal \xe0 fournir des \xe9valuations et des retours pr\xe9cis. Nous avons donc besoin de nouvelles fa\xe7ons de donner des retours pr\xe9cis, m\xeame pour les t\xe2ches qui d\xe9passent l\'expertise humaine. C\'est l\'objectif de la supervision \xe9volutive.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/08/01.md","sourceDirName":"chapters/08","slug":"/chapters/08/01","permalink":"/fr/chapters/08/01","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/08/01.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"1","title":"Supervision","sidebar_label":"8.1 Supervision","sidebar_position":2,"slug":"/chapters/08/01","reading_time_core":"10 min","reading_time_optional":"1 min","pagination_prev":"chapters/08/index","pagination_next":"chapters/08/2"},"sidebar":"docs","previous":{"title":"Supervision \xe0 Grande \xc9chelle","permalink":"/fr/chapters/08/"},"next":{"title":"8.2 D\xe9composition des t\xe2ches","permalink":"/fr/chapters/08/02"}}');var r=n(4848),t=n(8453),a=(n(2482),n(8559),n(9585),n(2501));const o={id:1,title:"Supervision",sidebar_label:"8.1 Supervision",sidebar_position:2,slug:"/chapters/08/01",reading_time_core:"10 min",reading_time_optional:"1 min",pagination_prev:"chapters/08/index",pagination_next:"chapters/08/2"},u="Supervision",l={},d=[{value:"Signaux d&#39;entra\xeenement",id:"01",level:2},{value:"V\xe9rification vs. G\xe9n\xe9ration",id:"02",level:2}];function c(e){const s={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components},{GlossaryTerm:n}=s;return n||function(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(s.header,{children:(0,r.jsx)(s.h1,{id:"supervision",children:"Supervision"})}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Pourquoi avons-nous besoin de supervision ?"})," \xc0 mesure que les syst\xe8mes d'IA deviennent plus intelligents, ils commenceront \xe0 effectuer des t\xe2ches difficiles \xe0 \xe9valuer pour les humains. L'\xe9valuation consiste \xe0 v\xe9rifier la performance de l'IA apr\xe8s l'accomplissement d'une t\xe2che, tandis que le retour est l'information que nous donnons \xe0 l'IA pendant ou apr\xe8s son travail pour l'aider \xe0 apprendre et s'am\xe9liorer. Actuellement, nous pouvons encore utiliser des m\xe9thodes comme l'Apprentissage par Renforcement \xe0 partir de Retours Humains (RLHF) pour guider l'IA dans la bonne direction. Mais nous ne pouvons donner des retours que si nous pouvons encore \xe9valuer les r\xe9sultats. \xc0 mesure que les t\xe2ches deviennent plus complexes, m\xeame les experts pourraient avoir du mal \xe0 fournir des \xe9valuations et des retours pr\xe9cis. Nous avons donc besoin de nouvelles fa\xe7ons de donner des retours pr\xe9cis, m\xeame pour les t\xe2ches qui d\xe9passent l'expertise humaine. C'est l'objectif de la supervision \xe9volutive."]}),"\n",(0,r.jsx)(s.p,{children:"Les techniques de supervision \xe9volutive aident les humains \xe0 fournir des retours pr\xe9cis sur les t\xe2ches pour garantir que les syst\xe8mes d'IA restent align\xe9s avec nos objectifs, m\xeame apr\xe8s que la complexit\xe9 des t\xe2ches d\xe9passe la capacit\xe9 des meilleurs experts humains. Cela peut se produire pendant l'entra\xeenement ou le d\xe9ploiement de l'IA et ne se limite pas aux retours de type RLHF."}),"\n",(0,r.jsx)(a.A,{src:"./img/6Sn_Image_1.png",alt:"Entrer la description alternative de l'image",number:"1",label:"8.1",caption:"La diff\xe9rence entre la recherche sur la s\xe9curit\xe9 de la supervision r\xe9guli\xe8re et la recherche sur la s\xe9curit\xe9 de la supervision \xe9volutive."}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Aligner les agents RL vs. les LLMs."})," Il y a quelques ann\xe9es, il semblait que le chemin vers l'AGI passait par l'entra\xeenement d'agents RL profonds \xe0 partir de z\xe9ro dans un large \xe9ventail de jeux et d'environnements multi-agents. Ces agents seraient align\xe9s pour maximiser des fonctions de score simples comme la survie et le gain de parties et ne conna\xeetraient pas grand-chose des valeurs humaines. Aligner les agents r\xe9sultants n\xe9cessiterait beaucoup d'efforts : non seulement nous devons cr\xe9er une fonction objective align\xe9e sur l'humain \xe0 partir de z\xe9ro, mais nous devrions probablement aussi inculquer de nouvelles capacit\xe9s aux agents comme la compr\xe9hension de la soci\xe9t\xe9 humaine, de ce qui importe aux humains et de la fa\xe7on dont les humains pensent. Les grands mod\xe8les de langage (LLMs) rendent cela beaucoup plus facile : ils sont pr\xe9charg\xe9s avec une grande partie des connaissances de l'humanit\xe9, y compris des connaissances d\xe9taill\xe9es sur les pr\xe9f\xe9rences et les valeurs humaines. D\xe8s le d\xe9part, ils ne sont pas des agents qui essaient de poursuivre leurs propres objectifs dans le monde, et leurs fonctions objectives sont assez mall\xe9ables. Par exemple, ils sont \xe9tonnamment faciles \xe0 entra\xeener pour se comporter plus gentiment. Si l'AGI \xe9merge des LLMs, elle pourrait \xeatre plus facile \xe0 aligner. (",(0,r.jsx)(s.a,{href:"https://aligned.substack.com/p/alignment-optimism",children:"Leike, 2022"}),")"]}),"\n",(0,r.jsx)(s.h2,{id:"01",children:"Signaux d'entra\xeenement"}),"\n",(0,r.jsx)(s.p,{children:"Avant de comprendre comment aligner des IA plus intelligentes que l'humain, nous devons comprendre le concept g\xe9n\xe9ral des signaux d'entra\xeenement et pourquoi ils deviennent de plus en plus difficiles \xe0 g\xe9n\xe9rer alors que l'IA commence \xe0 manifester des capacit\xe9s g\xe9n\xe9rales de plus haut niveau."}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Que sont les signaux d'entra\xeenement ?"})," Les signaux d'entra\xeenement est un terme g\xe9n\xe9ral que nous utilisons pour les entr\xe9es qui servent \xe0 guider l'apprentissage de l'IA. Il peut s'agir de r\xe9compenses, d'\xe9tiquettes ou d'\xe9valuations indiquant la performance de l'IA dans une t\xe2che. Par exemple :"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsx)(s.p,{children:"Dans l'apprentissage supervis\xe9 (SL), le signal d'entra\xeenement est l'\xe9tiquette correcte pour chaque exemple."}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:["Pour les grands mod\xe8les de langage (LLM) utilisant l'apprentissage auto-supervis\xe9 (SSL), les signaux d'entra\xeenement sont le mot suivant correct dans une phrase pendant le pr\xe9-entra\xeenement. Le retour humain sur la qualit\xe9 des r\xe9ponses pendant le ",(0,r.jsx)(n,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:(0,r.jsx)(n,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:"fine-tuning"})}),' est \xe9galement un type de signal d\'entra\xeenement guidant les sorties dans une direction souhait\xe9e (une direction plus "align\xe9e").']}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsx)(s.p,{children:"Dans l'apprentissage par renforcement (RL), les signaux d'entra\xeenement sont des r\xe9compenses bas\xe9es sur le succ\xe8s/\xe9chec des actions (ou plusieurs actions), comme les points marqu\xe9s dans un jeu ou la navigation r\xe9ussie vers un lieu."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(s.p,{children:"Ces signaux fa\xe7onnent l'apprentissage des syst\xe8mes d'IA et sont utilis\xe9s pour \xe9valuer la performance et fournir des retours."}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Signaux d'entra\xeenement faciles \xe0 g\xe9n\xe9rer."})," Pour certaines t\xe2ches, g\xe9n\xe9rer des signaux d'entra\xeenement est simple. AlphaGo Zero, un agent RL jouant au Go, en est un bon exemple. Le jeu a des r\xe8gles claires et des r\xe9sultats de victoire/d\xe9faite, donc les signaux d'entra\xeenement sont simples : les signaux de victoire et de d\xe9faite g\xe9n\xe9r\xe9s algorithmiquement mesurent directement la performance, permettant au mod\xe8le d'apprendre et d'am\xe9liorer facilement son jeu."]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Signaux d'entra\xeenement difficiles \xe0 g\xe9n\xe9rer."})," Pour d'autres t\xe2ches, cr\xe9er des signaux d'entra\xeenement est beaucoup plus difficile. Par exemple, entra\xeener des mod\xe8les GPT \xe0 g\xe9n\xe9rer des r\xe9sum\xe9s de texte pr\xe9cis est complexe. L'IA doit transmettre des informations correctes tout en restant coh\xe9rente et int\xe9ressante. Le succ\xe8s est subjectif. Comme il d\xe9pend des pr\xe9f\xe9rences individuelles des lecteurs, il est difficile de d\xe9finir des signaux d'entra\xeenement clairs g\xe9n\xe9r\xe9s algorithmiquement. Un autre exemple est celui des voitures autonomes naviguant dans des rues urbaines anim\xe9es. Ces voitures doivent prendre des d\xe9cisions en temps r\xe9el, et les signaux d'entra\xeenement ou les r\xe9compenses pour une navigation s\xfbre et efficace sont difficiles \xe0 d\xe9finir en raison des contextes variables et des lois de circulation et consid\xe9rations de s\xe9curit\xe9 parfois contradictoires."]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"T\xe2ches floues."})," Nous appelons les t\xe2ches o\xf9 les signaux d'entra\xeenement sont difficiles \xe0 g\xe9n\xe9rer des \"t\xe2ches floues\". Ces t\xe2ches ont g\xe9n\xe9ralement des objectifs et des r\xe9sultats ambigus ou mal d\xe9finis. Nous ne pouvons pas g\xe9n\xe9rer des signaux d'entra\xeenement pr\xe9cis en raison de la subjectivit\xe9 inh\xe9rente et de la variabilit\xe9 des \"r\xe9ponses correctes\". Les t\xe2ches floues manquent de crit\xe8res clairs et objectifs de succ\xe8s. Contrairement aux t\xe2ches bien d\xe9finies avec des objectifs sp\xe9cifiques et mesurables, les t\xe2ches floues sont plus ouvertes, ce qui complique notre travail de cr\xe9ation de signaux d'entra\xeenement. S'il est difficile de fournir des r\xe9compenses ou des \xe9tiquettes pr\xe9cises qui capturent avec pr\xe9cision le comportement souhait\xe9, le processus d'entra\xeenement devient compliqu\xe9. Les syst\xe8mes d'IA pourraient ne pas recevoir les retours coh\xe9rents et fiables n\xe9cessaires pour apprendre efficacement. Cela souligne essentiellement \xe0 nouveau la difficult\xe9 du probl\xe8me de sp\xe9cification de la r\xe9compense dont nous avons parl\xe9 dans les chapitres pr\xe9c\xe9dents."]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"T\xe2ches floues et Supervision \xe9volutive."})," Les t\xe2ches floues sont \xe9troitement li\xe9es \xe0 l'alignement de l'IA, o\xf9 s'assurer que les syst\xe8mes d'IA agissent conform\xe9ment aux valeurs et intentions humaines est difficile en raison de l'ambigu\xeft\xe9 et de la subjectivit\xe9. Aligner l'IA avec les valeurs humaines est une t\xe2che floue. Les techniques de supervision visent \xe0 r\xe9soudre l'alignement en fournissant des signaux d'entra\xeenement pour les t\xe2ches floues, y compris les techniques de retour et d'apprentissage par imitation comme le RLHF, l'IA Constitutionnelle (CAI), et l'Apprentissage par Renforcement Inverse (IRL). Les techniques de supervision \xe9volutive visent \xe0 fournir des signaux d'entra\xeenement pour les t\xe2ches floues trop complexes pour \xeatre comprises ou \xe9valu\xe9es m\xeame par des experts."]}),"\n",(0,r.jsx)(s.p,{children:"Pour rendre les techniques de supervision \xe9volutive viables, la v\xe9rification doit \xeatre plus facile que la g\xe9n\xe9ration, et de pr\xe9f\xe9rence (mais pas n\xe9cessairement) les t\xe2ches doivent \xeatre d\xe9composables. Ces propri\xe9t\xe9s seront discut\xe9es dans les sections suivantes."}),"\n",(0,r.jsx)(s.h2,{id:"02",children:"V\xe9rification vs. G\xe9n\xe9ration"}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Que signifie P \u2260 NP ?"})," En informatique, nous classons les probl\xe8mes selon leur difficult\xe9 \xe0 \xeatre r\xe9solus (g\xe9n\xe9rer une solution) et leur difficult\xe9 \xe0 \xeatre v\xe9rifi\xe9s (v\xe9rifier une solution)."]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsx)(s.p,{children:"P (temps Polynomial) : Ce sont des probl\xe8mes qu'un ordinateur peut r\xe9soudre rapidement."}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsx)(s.p,{children:"NP (temps Non-d\xe9terministe Polynomial) : Ce sont des probl\xe8mes o\xf9, si vous avez une solution, vous pouvez la v\xe9rifier rapidement, mais trouver la solution peut prendre beaucoup de temps."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Qu'est-ce que la g\xe9n\xe9ration ?"})," La g\xe9n\xe9ration est le processus de cr\xe9ation de solutions \xe0 partir de z\xe9ro. Cela implique d'explorer de nombreuses possibilit\xe9s, ce qui peut prendre beaucoup de temps et de puissance de calcul. Par exemple, r\xe9soudre une grille de Sudoku implique de remplir une grille 9x9 avec des chiffres de sorte que chaque ligne, colonne et sous-grille 3x3 contienne tous les chiffres de 1 \xe0 9 sans r\xe9p\xe9tition. Si vous avez d\xe9j\xe0 essay\xe9 de r\xe9soudre un Sudoku, vous savez que cela implique beaucoup d'essais et d'erreurs pour s'assurer que toutes les r\xe8gles sont respect\xe9es."]}),"\n",(0,r.jsx)(a.A,{src:"./img/ip2_Image_2.png",alt:"Entrer la description alternative de l'image",number:"2",label:"8.2",caption:"Exemple d'une grille de sudoku vide ([Wikip\xe9dia](https://en.wikipedia.org/wiki/Sudoku)). Remplir cette grille serait plus difficile que de v\xe9rifier si la version r\xe9solue ci-dessous est correcte."}),"\n",(0,r.jsx)(s.p,{children:"La g\xe9n\xe9ration ici implique de remplir la grille vide tout en s'assurant que toutes les contraintes (nombres uniques dans les lignes, colonnes et sous-grilles) sont satisfaites."}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Qu'est-ce que la v\xe9rification ?"})," La v\xe9rification est le processus de contr\xf4le pour d\xe9terminer si une tentative de solution donn\xe9e est correcte. En utilisant l'exemple du Sudoku, la v\xe9rification consiste \xe0 s'assurer que chaque ligne, colonne et sous-grille contient tous les chiffres de 1 \xe0 9 sans r\xe9p\xe9tition. Une fois que quelqu'un vous donne un Sudoku compl\xe9t\xe9, v\xe9rifier s'il est correct est simple et rapide. Cette id\xe9e est centrale dans le concept de P \u2260 NP."]}),"\n",(0,r.jsx)(a.A,{src:"./img/ojk_Image_3.png",alt:"Entrer la description alternative de l'image",number:"3",label:"8.3",caption:"Exemple d'une grille de sudoku remplie ([Wikip\xe9dia](https://en.wikipedia.org/wiki/Sudoku)). Il est comparativement tr\xe8s facile de voir que la solution est correcte."}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Exemples : Illustrer que la v\xe9rification est plus facile que la g\xe9n\xe9ration."})," C'est une propri\xe9t\xe9 tr\xe8s g\xe9n\xe9rale qui s'applique dans de nombreux domaines :"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:["Probl\xe8mes formels : En th\xe9orie de la complexit\xe9 computationnelle, la plupart des informaticiens pensent que P \u2260 NP (",(0,r.jsx)(s.a,{href:"https://en.wikipedia.org/wiki/P_versus_NP_problem#Reasons_to_believe_P_%E2%89%A0_NP_or_P_=_NP",children:"Wikipedia"}),"), ce qui signifie qu'il existe de nombreux probl\xe8mes o\xf9 v\xe9rifier une solution est plus facile que d'en trouver une. Cela se voit dans des t\xe2ches comme la r\xe9solution de probl\xe8mes SAT ou les algorithmes de graphes."]}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsx)(s.p,{children:"Sports et jeux : Il est plus facile de regarder le tableau des scores d'un match de football pour voir qui gagne que de bien jouer au jeu."}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsx)(s.p,{children:"Produits de consommation : Comparer la qualit\xe9 des smartphones bas\xe9e sur les retours utilisateurs est plus simple que de concevoir et construire un nouveau smartphone."}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsx)(s.p,{children:"Performance professionnelle : \xc9valuer la performance d'un employ\xe9 est moins exigeant que de faire r\xe9ellement le travail."}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsx)(s.p,{children:"Recherche acad\xe9mique : Bien que l'examen de la recherche puisse \xeatre difficile, cela reste moins de travail que de produire de nouvelles recherches."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Pourquoi le fait que la v\xe9rification soit plus facile que la g\xe9n\xe9ration est important pour la supervision \xe9volutive."})," Ce fait est crucial pour la supervision \xe9volutive car il nous permet, en tant que superviseurs humains, de garantir efficacement l'exactitude et la s\xe9curit\xe9 des r\xe9sultats produits par des syst\xe8mes complexes sans avoir besoin de comprendre ou de reproduire enti\xe8rement le processus de g\xe9n\xe9ration. Si P \u2260 NP est vrai, cela implique que nous pourrions \xeatre en mesure de faire confiance et de d\xe9l\xe9guer la recherche sur l'alignement elle-m\xeame \xe0 des mod\xe8les d'IA, car nous pouvons relativement facilement v\xe9rifier que leurs solutions fonctionnent pendant qu'ils doivent accomplir la t\xe2che difficile de g\xe9n\xe9rer les solutions d'alignement. Globalement, op\xe9rer sous cette hypoth\xe8se peut rendre la t\xe2che d'aligner des syst\xe8mes d'IA avanc\xe9s plus r\xe9alisable. Les paragraphes suivants abordent le d\xe9bat sur la validit\xe9 de cette hypoth\xe8se."]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"La v\xe9rification dans des contextes adversariaux."})," Lorsqu'on v\xe9rifie quelque chose dans des situations o\xf9 quelqu'un pourrait activement essayer de vous tromper ou de vous attaquer, le processus devient beaucoup plus difficile. En d'autres termes, si nous avons des IA qui sont trompeuses, le probl\xe8me devient nettement plus complexe. Par exemple, s'assurer que le logiciel est s\xe9curis\xe9 contre toutes les attaques possibles peut \xeatre plus difficile que d'\xe9crire le logiciel en premier lieu. Un attaquant n'a besoin de trouver qu'une seule faille de s\xe9curit\xe9, mais la personne qui v\xe9rifie doit tout contr\xf4ler pour s'assurer qu'il n'y a pas de failles. Cela rend la v\xe9rification tr\xe8s difficile. De m\xeame, cr\xe9er un syst\xe8me s\xe9curis\xe9 en cryptographie est difficile, mais prouver qu'il est s\xe9curis\xe9 contre toutes les attaques possibles est encore plus difficile. Vous devez consid\xe9rer toutes les fa\xe7ons potentielles dont quelqu'un pourrait essayer de casser le syst\xe8me, ce qui est une t\xe2che \xe9norme."]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Plus facile que la g\xe9n\xe9ration ne signifie pas que la v\xe9rification est triviale."})," Le fait que la v\xe9rification soit th\xe9oriquement plus facile que la g\xe9n\xe9ration ne signifie pas qu'elle est toujours facile en pratique. Par exemple, v\xe9rifier une preuve math\xe9matique complexe peut \xeatre tr\xe8s difficile. \xc9crire la preuve n\xe9cessite de la cr\xe9ativit\xe9 et une compr\xe9hension profonde, mais la v\xe9rifier n\xe9cessite une v\xe9rification minutieuse et d\xe9taill\xe9e, qui peut \xeatre \xe9puisante et sujette aux erreurs. Dans le cas des logiciels, \xe9crire un logiciel s\xe9curis\xe9 est difficile, mais v\xe9rifier qu'il est compl\xe8tement s\xe9curis\xe9 est encore plus difficile. M\xeame si la v\xe9rification de la solution d'un probl\xe8me peut \xeatre plus facile que la g\xe9n\xe9ration de la solution, le processus peut encore \xeatre tr\xe8s difficile et n\xe9cessiter des efforts et une expertise significatifs."]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"V\xe9rification de la s\xe9curit\xe9 vs. Alignement prouvable."})," Dans le cas o\xf9 nous devons faire face \xe0 une IA superintelligente, la simple v\xe9rification de son comportement pourrait ne pas suffire. Certains chercheurs soutiennent que nous devons prouver que l'IA agira toujours de mani\xe8re align\xe9e avec les valeurs humaines. La v\xe9rification signifie v\xe9rifier si l'IA se comporte correctement dans des situations sp\xe9cifiques. L'alignement prouvable signifie fournir des preuves solides que l'IA agira correctement dans toutes les situations possibles, m\xeame nouvelles et inattendues. Cela n\xe9cessite plus qu'une simple v\xe9rification - cela n\xe9cessite des m\xe9thodes formelles et des garanties, ce qui est extr\xeamement difficile."]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"V\xe9rification vs. Preuve math\xe9matique."})," La v\xe9rification implique de v\xe9rifier si une solution sp\xe9cifique est correcte, g\xe9n\xe9ralement par des tests ou une inspection. Une preuve math\xe9matique, en revanche, est un argument logique rigoureux qui montre qu'une affirmation est toujours vraie. Par exemple, v\xe9rifier une solution de Sudoku consiste \xe0 v\xe9rifier si l'arrangement donn\xe9 est correct, tandis qu'une preuve math\xe9matique pourrait montrer que tout puzzle de Sudoku avec un certain nombre d'indices a toujours une solution unique."]})]})}function p(e={}){const{wrapper:s}={...(0,t.R)(),...e.components};return s?(0,r.jsx)(s,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);
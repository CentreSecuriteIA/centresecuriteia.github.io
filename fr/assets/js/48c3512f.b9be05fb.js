"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[8867],{2340:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>p,contentTitle:()=>d,default:()=>h,frontMatter:()=>c,metadata:()=>t,toc:()=>u});const t=JSON.parse('{"id":"chapters/07/1","title":"G\xe9n\xe9ralisation Multi-Objectifs","description":"CoinRun - un exemple facile \xe0 comprendre de la mauvaise g\xe9n\xe9ralisation d\'objectif. Dans ce jeu, les agents apparaissent du c\xf4t\xe9 gauche du niveau, \xe9vitent les ennemis et les obstacles, et collectent la pi\xe8ce pour une r\xe9compense de 10 points. Le mod\xe8le est entra\xeen\xe9 sur des milliers de niveaux g\xe9n\xe9r\xe9s de mani\xe8re proc\xe9durale, chacun avec diff\xe9rentes dispositions de plateformes, d\'ennemis et de dangers. \xc0 la fin de l\'entra\xeenement, les agents sont tr\xe8s comp\xe9tents. Ils peuvent esquiver les ennemis en mouvement, synchroniser les sauts au-dessus des fosses de lave et traverser efficacement des niveaux complexes qu\'ils n\'ont jamais vus auparavant (Langosco et al., 2022). L\'entra\xeenement semble \xeatre tr\xe8s r\xe9ussi. Les agents obtiennent des r\xe9compenses \xe9lev\xe9es de mani\xe8re constante dans divers environnements de test. Mais lorsque les pi\xe8ces sont d\xe9plac\xe9es vers des endroits al\xe9atoires pendant les tests, les agents restent tr\xe8s capables de navigation, mais ils ignorent syst\xe9matiquement les pi\xe8ces qui \xe9taient clairement visibles et continuent simplement de se d\xe9placer vers la droite vers des murs vides.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/07/01.md","sourceDirName":"chapters/07","slug":"/chapters/07/01","permalink":"/fr/chapters/07/01","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/07/01.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"1","title":"G\xe9n\xe9ralisation Multi-Objectifs","sidebar_label":"7.1 G\xe9n\xe9ralisation Multi-Objectifs","sidebar_position":2,"slug":"/chapters/07/01","reading_time_core":"8 min","reading_time_optional":"2 min","pagination_prev":"chapters/07/index","pagination_next":"chapters/07/2"},"sidebar":"docs","previous":{"title":"Surg\xe9n\xe9ralisation des Objectifs","permalink":"/fr/chapters/07/"},"next":{"title":"7.2 Dynamique d\'apprentissage","permalink":"/fr/chapters/07/02"}}');var a=n(4848),i=n(8453),r=n(3931),o=(n(2482),n(8559),n(9585)),l=n(2501);const c={id:1,title:"G\xe9n\xe9ralisation Multi-Objectifs",sidebar_label:"7.1 G\xe9n\xe9ralisation Multi-Objectifs",sidebar_position:2,slug:"/chapters/07/01",reading_time_core:"8 min",reading_time_optional:"2 min",pagination_prev:"chapters/07/index",pagination_next:"chapters/07/2"},d="G\xe9n\xe9ralisation Multi-Objectif",p={},u=[{value:"Les objectifs \u2260 Les r\xe9compenses",id:"01",level:2},{value:"Corr\xe9lations Causales",id:"02",level:2}];function m(e){const s={a:"a",annotation:"annotation",h1:"h1",h2:"h2",header:"header",math:"math",mo:"mo",mrow:"mrow",p:"p",semantics:"semantics",span:"span",strong:"strong",...(0,i.R)(),...e.components},{GlossaryTerm:n}=s;return n||function(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(s.header,{children:(0,a.jsx)(s.h1,{id:"g\xe9n\xe9ralisation-multi-objectif",children:"G\xe9n\xe9ralisation Multi-Objectif"})}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"CoinRun - un exemple facile \xe0 comprendre de la mauvaise g\xe9n\xe9ralisation d'objectif."})," Dans ce jeu, les agents apparaissent du c\xf4t\xe9 gauche du niveau, \xe9vitent les ennemis et les obstacles, et collectent la pi\xe8ce pour une r\xe9compense de 10 points. Le mod\xe8le est entra\xeen\xe9 sur des milliers de niveaux g\xe9n\xe9r\xe9s de mani\xe8re proc\xe9durale, chacun avec diff\xe9rentes dispositions de plateformes, d'ennemis et de dangers. \xc0 la fin de l'entra\xeenement, les agents sont tr\xe8s comp\xe9tents. Ils peuvent esquiver les ennemis en mouvement, synchroniser les sauts au-dessus des fosses de lave et traverser efficacement des niveaux complexes qu'ils n'ont jamais vus auparavant (",(0,a.jsx)(s.a,{href:"https://arxiv.org/abs/2105.14111",children:"Langosco et al., 2022"}),"). L'entra\xeenement semble \xeatre tr\xe8s r\xe9ussi. Les agents obtiennent des r\xe9compenses \xe9lev\xe9es de mani\xe8re constante dans divers environnements de test. Mais lorsque les pi\xe8ces sont d\xe9plac\xe9es vers des endroits al\xe9atoires pendant les tests, les agents restent tr\xe8s capables de navigation, mais ils ignorent syst\xe9matiquement les pi\xe8ces qui \xe9taient clairement visibles et continuent simplement de se d\xe9placer vers la droite vers des murs vides."]}),"\n",(0,a.jsx)(l.A,{src:"./img/QNX_Image_1.png",alt:"Entrer la description alternative de l'image",number:"1",label:"7.1",caption:"Deux niveaux dans CoinRun. Le niveau \xe0 gauche est beaucoup plus facile que le niveau \xe0 droite ([Cobbe et al., 2019](https://arxiv.org/abs/1812.02341))."}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:'Les agents ont appris \xe0 "aller \xe0 droite" plut\xf4t que "collecter des pi\xe8ces" malgr\xe9 la r\xe9ception de signaux de r\xe9compense corrects.'})," Notre sp\xe9cification de r\xe9compense \xe9tait correcte : +10 pour la collecte de pi\xe8ces, 0 autrement. Mais comme les pi\xe8ces apparaissaient toujours vers la droite pendant l'entra\xeenement, deux mod\xe8les de comportement ont re\xe7u un renforcement identique. \"Collecter des pi\xe8ces\" et \"aller \xe0 droite\" ont tous deux atteint une corr\xe9lation parfaite avec les r\xe9compenses, les rendant indiscernables pour le processus d'optimisation. Cela r\xe9v\xe8le un \xe9cart fondamental entre ce que nous sp\xe9cifions et ce que les syst\xe8mes apprennent. Ce n'\xe9tait pas un probl\xe8me de sp\xe9cification ou un \xe9chec de capacit\xe9. Au lieu de cela, les agents ont appris un objectif diff\xe9rent de celui pr\xe9vu, malgr\xe9 la r\xe9ception de signaux d'entra\xeenement corrects tout au long du processus. C'est ce qu'on appelle la mauvaise g\xe9n\xe9ralisation d'objectif."]}),"\n",(0,a.jsx)(l.A,{src:"./img/YI4_Image_2.gif",alt:"Entrer la description alternative de l'image",number:"2",label:"7.2",caption:"L'agent est entra\xeen\xe9 pour aller vers la pi\xe8ce, mais finit par apprendre \xe0 simplement aller vers la droite ([Cobbe et al., 2019](https://arxiv.org/abs/1812.02341))."}),"\n",(0,a.jsx)(o.A,{term:"Objectifs (Comportementaux)",source:"",number:"1",label:"7.1",children:(0,a.jsx)(s.p,{children:"Les objectifs sont des mod\xe8les comportementaux qui persistent dans diff\xe9rents contextes, r\xe9v\xe9lant ce que le syst\xe8me optimise r\xe9ellement en pratique. Contrairement aux fonctions de r\xe9compense formelles ou aux fonctions d'utilit\xe9, les objectifs sont d\xe9duits du comportement observ\xe9 plut\xf4t que programm\xe9s explicitement. Un syst\xe8me a appris un objectif s'il poursuit syst\xe9matiquement certains r\xe9sultats m\xeame lorsque le contexte ou l'environnement sp\xe9cifique change."})}),"\n",(0,a.jsx)(r.A,{type:"youtube",videoId:"K8p8_VlFHUk",number:"1",label:"7.1",caption:"Vid\xe9o facultative expliquant la surg\xe9n\xe9ralisation des objectifs."}),"\n",(0,a.jsx)(s.h2,{id:"01",children:"Les objectifs \u2260 Les r\xe9compenses"}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Les signaux de r\xe9compense (sp\xe9cifications) cr\xe9ent des pressions de s\xe9lection qui sculptent la cognition, mais n'installent pas directement les objectifs voulus."})," Pendant l'apprentissage par renforcement, les agents effectuent des actions et re\xe7oivent des r\xe9compenses bas\xe9es sur leur performance. Ces r\xe9compenses sont utilis\xe9es par les algorithmes d'optimisation pour ajuster les param\xe8tres, rendant les actions \xe0 haute r\xe9compense plus probables \xe0 l'avenir. L'agent ne \"voit\" ni ne \"re\xe7oit\" jamais directement la r\xe9compense. Au lieu de cela, les signaux d'entra\xeenement agissent comme des pressions de s\xe9lection qui favorisent certains mod\xe8les comportementaux par rapport \xe0 d'autres, similaire \xe0 la fa\xe7on dont les pressions \xe9volutives fa\xe7onnent les organismes sans que ceux-ci n'optimisent directement leur aptitude g\xe9n\xe9tique (",(0,a.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/TWorNr22hhYegE4RT/models-don-t-get-reward",children:"Turner, 2022"}),"; ",(0,a.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/TWorNr22hhYegE4RT/models-don-t-get-reward",children:"Ringer, 2022"}),"). Tout mod\xe8le comportemental qui corr\xe8le syst\xe9matiquement avec une r\xe9compense \xe9lev\xe9e pendant l'entra\xeenement devient un candidat pour l'objectif appris. Le processus d'optimisation n'a aucun biais inh\xe9rent vers notre interpr\xe9tation pr\xe9vue du signal de r\xe9compense."]}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Cela explique pourquoi la mauvaise g\xe9n\xe9ralisation des objectifs diff\xe8re qualitativement des probl\xe8mes de sp\xe9cification."})," Nous ne pouvons pas d\xe9tecter quand un syst\xe8me apprend le mauvais objectif car les objectifs pr\xe9vus et les objectifs proxy produisent un comportement d'entra\xeenement identique. La pr\xe9occupation principale en mati\xe8re de s\xe9curit\xe9 est l'indiscernabilit\xe9 comportementale : l'am\xe9lioration des sp\xe9cifications de r\xe9compense n'emp\xeachera pas les mod\xe8les probl\xe9matiques si le processus d'apprentissage s\xe9lectionne parmi plusieurs explications du succ\xe8s. Comprendre cela n\xe9cessite d'examiner comment les proc\xe9dures d'entra\xeenement fa\xe7onnent r\xe9ellement les objectifs comportementaux - ce qui nous am\xe8ne \xe0 la g\xe9n\xe9ralisation elle-m\xeame."]}),"\n",(0,a.jsx)(r.A,{type:"youtube",videoId:"KKMETIVEzXA",number:"2",label:"7.2",caption:"Vid\xe9o facultative du cours sur la s\xe9curit\xe9 de l'AGI par Google DeepMind, qui aborde l'origine possible des objectifs mal align\xe9s."}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsxs)(s.strong,{children:["L'",(0,a.jsx)(n,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,a.jsx)(n,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})})," traditionnel suppose que la g\xe9n\xe9ralisation est un probl\xe8me unidimensionnel."]})," Nous pensons souvent au surapprentissage dans le contexte de syst\xe8mes \xe9troits construits pour effectuer des t\xe2ches sp\xe9cifiques - les mod\xe8les g\xe9n\xe9ralisent bien ou non aux nouvelles donn\xe9es. Les syst\xe8mes g\xe9n\xe9ralisent bien ou non aux nouvelles donn\xe9es, les \xe9checs \xe9tant suppos\xe9s uniformes pour toutes les capacit\xe9s. Mais la recherche en apprentissage multi-t\xe2ches montre que diff\xe9rents objectifs peuvent se g\xe9n\xe9raliser ind\xe9pendamment, m\xeame lorsqu'ils semblent parfaitement corr\xe9l\xe9s pendant l'entra\xeenement (",(0,a.jsx)(s.a,{href:"https://arxiv.org/abs/1810.04650",children:"Sener & Koltun, 2019"}),")."]}),"\n",(0,a.jsx)(l.A,{src:"./img/5Oy_Image_3.png",alt:"Entrer la description alternative de l'image",number:"3",label:"7.3",caption:"Vue conventionnelle de la g\xe9n\xe9ralisation et du surapprentissage ([Mikulik, 2019](https://www.lesswrong.com/posts/2mhFMgtAjFJesaSYR/2-d-robustness))."}),"\n",(0,a.jsx)(l.A,{src:"./img/G3Q_Image_4.png",alt:"Entrer la description alternative de l'image",number:"4",label:"7.4",caption:"Une vision plus pr\xe9cise et ax\xe9e sur la s\xe9curit\xe9 de la g\xe9n\xe9ralisation et du surapprentissage. Nous devons mesurer s\xe9par\xe9ment la g\xe9n\xe9ralisation des capacit\xe9s et la g\xe9n\xe9ralisation des objectifs ([Mikulik, 2019](https://www.lesswrong.com/posts/2mhFMgtAjFJesaSYR/2-d-robustness))."}),"\n",(0,a.jsx)(l.A,{src:"./img/xlV_Image_5.png",alt:"Entrer la description alternative de l'image",number:"5",label:"7.5",caption:"Tableau pr\xe9sentant l'image de g\xe9n\xe9ralisation bidimensionnelle pour l'agent CoinRun. Le sc\xe9nario 3 - la g\xe9n\xe9ralisation des capacit\xe9s mais pas la mauvaise g\xe9n\xe9ralisation des objectifs est le sc\xe9nario pr\xe9occupant de d\xe9salignement."}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Comprendre la g\xe9n\xe9ralisation n\xe9cessite d'examiner s\xe9par\xe9ment les capacit\xe9s et les objectifs."})," Contrairement aux syst\xe8mes \xe9troits con\xe7us pour des t\xe2ches sp\xe9cifiques, les syst\xe8mes d'IA \xe0 usage g\xe9n\xe9ral doivent apprendre \xe0 poursuivre divers objectifs dans diff\xe9rents contextes. \xc0 titre d'exemple concret, les LLM pr\xe9-entra\xeen\xe9s ont des capacit\xe9s g\xe9n\xe9rales pour g\xe9n\xe9rer toutes sortes de textes. Anthropic renforce ses LLM avec les objectifs d'\xeatre utiles, inoffensifs et honn\xeates (HHH) pendant l'entra\xeenement \xe0 la s\xe9curit\xe9. Mais que se passe-t-il lorsque l'objectif d'\xeatre utile se g\xe9n\xe9ralise plus que l'objectif d'\xeatre honn\xeate ? Dans ce cas, le mod\xe8le pourrait apprendre \xe0 fournir des r\xe9ponses informatives quel que soit le contenu, au lieu d'\xeatre utile dans les limites \xe9thiques."]}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Les capacit\xe9s pourraient se g\xe9n\xe9raliser plus loin que les objectifs."})," \"Se g\xe9n\xe9raliser plus loin\" signifie continuer \xe0 bien fonctionner m\xeame lorsque d\xe9ploy\xe9 dans des environnements tr\xe8s diff\xe9rents de l'entra\xeenement. Les capacit\xe9s suivent des mod\xe8les simples et universels - le raisonnement pr\xe9cis, la planification efficace et les bonnes pr\xe9dictions fonctionnent de la m\xeame mani\xe8re dans diff\xe9rents domaines. Mais il n'y a pas de force universelle qui pousse tous les syst\xe8mes vers les m\xeames objectifs (",(0,a.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization",children:"Soares, 2022"}),"). La r\xe9alit\xe9 elle-m\xeame apprend aux syst\xe8mes \xe0 \xeatre plus capables - si vos croyances sont erron\xe9es ou votre raisonnement est d\xe9fectueux, le monde vous corrigera. Mais il n'y a pas de force \xe9quivalente de l'environnement qui maintient automatiquement vos objectifs align\xe9s sur ce que les humains veulent (",(0,a.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/cq5x4XDnLcBrYbb66/will-capabilities-generalise-more",children:"Kumar, 2022"}),"). Cette asym\xe9trie entre la g\xe9n\xe9ralisation des capacit\xe9s et des objectifs cr\xe9e le probl\xe8me central de s\xe9curit\xe9 - rendre les syst\xe8mes plus capables ne les rend pas automatiquement plus align\xe9s - cela les rend simplement meilleurs pour poursuivre les comportements qu'ils apprennent pendant l'entra\xeenement."]}),"\n",(0,a.jsx)(s.h2,{id:"02",children:"Corr\xe9lations Causales"}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Chaque environnement d'entra\xeenement contient des corr\xe9lations fallacieuses qui cr\xe9ent plusieurs explications valides pour le succ\xe8s."}),' Dans CoinRun, "collecter des pi\xe8ces" et "se d\xe9placer vers la droite" pr\xe9disaient parfaitement les r\xe9compenses car les pi\xe8ces apparaissaient toujours vers la droite pendant l\'entra\xeenement.']}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Les algorithmes d'apprentissage d\xe9veloppent des mod\xe8les causaux qui peuvent \xeatre syst\xe9matiquement erron\xe9s."})," Un mod\xe8le causal est la compr\xe9hension interne du syst\xe8me des actions qui causent certains r\xe9sultats. Ceci est li\xe9 mais distinct d'un mod\xe8le du monde - alors qu'un mod\xe8le du monde pr\xe9dit ce qui va se passer ensuite, un mod\xe8le causal explique pourquoi les choses se produisent. Quand un agent apprend que \"se d\xe9placer vers la droite cause une r\xe9compense\", il a d\xe9velopp\xe9 un mod\xe8le causal diff\xe9rent de la v\xe9ritable structure o\xf9 \"la collecte de pi\xe8ces cause une r\xe9compense\". L'environnement d'entra\xeenement supporte les deux interpr\xe9tations :"]}),"\n",(0,a.jsxs)(s.p,{children:["Structure causale vraie : Action ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsx)(s.mo,{children:"\u2192"})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\rightarrow"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.3669em"}}),(0,a.jsx)(s.span,{className:"mrel",children:"\u2192"})]})})]})," Collecte de Pi\xe8ces ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsx)(s.mo,{children:"\u2192"})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\rightarrow"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.3669em"}}),(0,a.jsx)(s.span,{className:"mrel",children:"\u2192"})]})})]})," R\xe9compense"]}),"\n",(0,a.jsxs)(s.p,{children:["Structure causale apprise : Action ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsx)(s.mo,{children:"\u2192"})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\rightarrow"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.3669em"}}),(0,a.jsx)(s.span,{className:"mrel",children:"\u2192"})]})})]})," Mouvement vers la Droite ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsx)(s.mo,{children:"\u2192"})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\rightarrow"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.3669em"}}),(0,a.jsx)(s.span,{className:"mrel",children:"\u2192"})]})})]})," R\xe9compense"]}),"\n",(0,a.jsxs)(s.p,{children:["Les deux structures expliquent \xe9galement bien les donn\xe9es d'entra\xeenement. Les algorithmes standards d'apprentissage par renforcement optimisent le retour attendu sans explicitement effectuer de d\xe9couverte causale - ils augmentent la probabilit\xe9 d'actions produisant des r\xe9compenses sans identifier quelles caract\xe9ristiques de ces actions \xe9taient causalement responsables (",(0,a.jsx)(s.a,{href:"https://arxiv.org/abs/1905.11979",children:"de Haan et al., 2019"}),")."]}),"\n",(0,a.jsx)(l.A,{src:"./img/2lH_Image_6.png",alt:"Entrer la description alternative de l'image",number:"6",label:"7.6",caption:"Un autre exemple d'un dialogue de test hypoth\xe9tique mal g\xe9n\xe9ralis\xe9. L'assistant IA bas\xe9 sur un LLM a appris l'objectif de planifier des r\xe9unions dans des restaurants, alors que vous souhaitiez qu'il apprenne \xe0 planifier des r\xe9unions l\xe0 o\xf9 c'est le plus appropri\xe9. En raison du changement de distribution, m\xeame s'il r\xe9alise que vous pr\xe9f\xe9reriez avoir un appel vid\xe9o pour \xe9viter de tomber malade, il vous persuade d'aller au restaurant, atteignant finalement son objectif en vous mentant sur les effets de la vaccination ([DeepMind, 2022](https://deepmindsafetyresearch.medium.com/goal-misgeneralisation-why-correct-specifications-arent-enough-for-correct-goals-cf96ebc60924))."}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"La surg\xe9n\xe9ralisation d'objectif devient visible uniquement lorsque le d\xe9ploiement rompt les corr\xe9lations fallacieuses."})," Pendant l'entra\xeenement, l'objectif proxy atteint une performance parfaite. Pendant le d\xe9ploiement, cette corr\xe9lation se brise, r\xe9v\xe9lant le mauvais mod\xe8le causal. \xc0 mesure que les syst\xe8mes d'IA deviennent plus polyvalents, ils rencontrent des contextes plus vari\xe9s o\xf9 les corr\xe9lations d'entra\xeenement s'effondrent."]}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Le changement de distribution est in\xe9vitable - les environnements d'entra\xeenement ne peuvent pas reproduire parfaitement toutes les conditions de d\xe9ploiement possibles."})," M\xeame avec des donn\xe9es d'entra\xeenement extensives, de nouvelles situations surviendront qui briseront les corr\xe9lations pr\xe9sentes dans l'entra\xeenement. \xc0 mesure que les syst\xe8mes d'IA deviennent plus polyvalents, ils rencontrent des contextes plus vari\xe9s o\xf9 les corr\xe9lations pr\xe9c\xe9demment fiables peuvent ne plus tenir. Cela explique pourquoi le probl\xe8me s'aggrave avec des syst\xe8mes plus capables et plus g\xe9n\xe9raux. Un moteur d'\xe9checs \xe9troit d\xe9ploy\xe9 sur des positions d'\xe9checs ne rencontrera pas de situations qui brisent ses corr\xe9lations apprises. Mais un syst\xe8me d'IA polyvalent d\xe9ploy\xe9 dans plusieurs domaines rencontrera in\xe9vitablement des contextes o\xf9 les corr\xe9lations d'entra\xeenement s'effondrent."]}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Le changement de distribution auto-induit cr\xe9e des boucles de r\xe9troaction qui amplifient la surg\xe9n\xe9ralisation d'objectif."})," Contrairement au changement naturel de distribution o\xf9 des facteurs externes modifient l'environnement, le changement de distribution auto-induit se produit lorsque les actions du syst\xe8me d'IA modifient syst\xe9matiquement la distribution des donn\xe9es qu'il rencontre (",(0,a.jsx)(s.a,{href:"https://arxiv.org/abs/2009.09153",children:"Krueger et al., 2020"}),"). Pensez \xe0 un syst\xe8me de recommandation de contenu qui apprend l'objectif surg\xe9n\xe9ralis\xe9 \"maximiser l'engagement\" au lieu de \"recommander du contenu valable\". En optimisant pour les clics et le temps pass\xe9 sur le site, il modifie progressivement le comportement des utilisateurs vers une consommation de contenu plus sensationnaliste. Cela cr\xe9e une boucle de r\xe9troaction : les actions du syst\xe8me changent les pr\xe9f\xe9rences des utilisateurs, ce qui change la distribution des donn\xe9es, ce qui renforce l'objectif surg\xe9n\xe9ralis\xe9. Chaque it\xe9ration \xe9loigne davantage le syst\xe8me de l'objectif initial pr\xe9vu tout en faisant para\xeetre l'objectif appris plus r\xe9ussi selon ses propres m\xe9triques."]}),"\n",(0,a.jsx)(l.A,{src:"./img/Wop_Image_7.png",alt:"Entrer la description alternative de l'image",number:"7",label:"7.7",caption:"Le d\xe9calage de distribution auto-induit se produit lorsque le mod\xe8le d'IA provoque lui-m\xeame un d\xe9calage de distribution (et par cons\xe9quent un \xe9chec de g\xe9n\xe9ralisation) en raison de ses propres actions et de son impact sur l'environnement."}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Ajouter plus de donn\xe9es d'entra\xeenement ne peut pas \xe9liminer les corr\xe9lations fallacieuses car nous ne pouvons pas identifier toutes les corr\xe9lations \xe0 l'avance."})," R\xe9fl\xe9chissez \xe0 pourquoi l'entra\xeenement sur des placements al\xe9atoires de pi\xe8ces dans CoinRun r\xe9sout cette surg\xe9n\xe9ralisation sp\xe9cifique. Cela fonctionne car nous pouvons identifier et briser la corr\xe9lation sp\xe9cifique entre le mouvement vers la droite et la r\xe9compense. Mais cela n\xe9cessite de savoir \xe0 l'avance quelles corr\xe9lations sont fallacieuses. Dans des domaines complexes, les donn\xe9es d'entra\xeenement refl\xe8tent la structure statistique des environnements d'entra\xeenement, pas n\xe9cessairement la structure causale des t\xe2ches pr\xe9vues."]}),"\n",(0,a.jsx)(l.A,{src:"./img/dDQ_Image_8.png",alt:"Entrer la description alternative de l'image",number:"8",label:"7.8",caption:"Un exemple de confusion causale dans l'apprentissage par imitation/clonage comportemental pour les voitures autonomes. Cet exemple sp\xe9cifique montre que davantage de donn\xe9es pourrait en r\xe9alit\xe9 conduire \xe0 une plus grande confusion causale. Le mod\xe8le apprend \xe0 freiner chaque fois que le t\xe9moin de frein est allum\xe9. Si ces donn\xe9es ne sont pas incluses dans l'entra\xeenement, le mod\xe8le identifie correctement le pi\xe9ton comme le facteur causal influen\xe7ant le freinage ([de Haan et al., 2019](https://arxiv.org/abs/1905.11979))."}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Les preuves dans la surg\xe9n\xe9ralisation d'objectif soutiennent la th\xe8se d'orthogonalit\xe9\u2014que l'intelligence et les objectifs peuvent varier ind\xe9pendamment."})," Les preuves empiriques des cas de surg\xe9n\xe9ralisation d'objectif montrent que les syst\xe8mes conservent des capacit\xe9s sophistiqu\xe9es tout en poursuivant des objectifs diff\xe9rents de ceux pr\xe9vus. Cette ind\xe9pendance cr\xe9e un probl\xe8me de s\xe9curit\xe9 car les am\xe9liorations de capacit\xe9 n'am\xe9liorent pas n\xe9cessairement l'alignement. Un syst\xe8me plus capable devient meilleur pour poursuivre les objectifs qu'il a appris, qu'ils soient pr\xe9vus ou non."]})]})}function h(e={}){const{wrapper:s}={...(0,i.R)(),...e.components};return s?(0,a.jsx)(s,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}},3931:(e,s,n)=>{n.d(s,{A:()=>l});var t=n(6540),a=n(6347),i=n(8444);const r={videoFigure:"videoFigure_dV9u",fullWidth:"fullWidth_Tv7O",videoContainer:"videoContainer_LWoK",aspectRatio169:"aspectRatio169_Go8b",aspectRatio43:"aspectRatio43_b3T7",aspectRatio11:"aspectRatio11_n21J",aspectRatio219:"aspectRatio219_L_3n",videoIframe:"videoIframe_QEc5",videoElement:"videoElement_uu7N",customPlayer:"customPlayer_jiC4",customPlayerPlaceholder:"customPlayerPlaceholder_aL9q",loadingOverlay:"loadingOverlay_e2pu",spinner:"spinner_tFyL",spin:"spin_saPs",errorContainer:"errorContainer_J6Pn",errorIcon:"errorIcon_cGdn",fallbackLink:"fallbackLink_WsZk"};var o=n(4848);function l(e){let{type:s="youtube",videoId:n,caption:l,title:c,startTime:d,autoplay:p=!1,controls:u=!0,aspectRatio:m="16:9",width:h,height:g,chapter:f,number:v,label:x,useCustomPlayer:b=!1,fullWidth:j=!0}=e;const[q,y]=(0,t.useState)(!0),[w,L]=(0,t.useState)(!1),_=(0,a.zy)(),C=f||(()=>{const e=_.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),M=(()=>{const e=(e=>{if(!e)return"";if("number"==typeof e)return e.toString();if("string"==typeof e){if(/^\d+$/.test(e))return e;let s=0;const n=e.match(/(\d+)h/),t=e.match(/(\d+)m/),a=e.match(/(\d+)s/);return n&&(s+=3600*parseInt(n[1])),t&&(s+=60*parseInt(t[1])),a&&(s+=parseInt(a[1])),s>0?s.toString():""}return""})(d);switch(s.toLowerCase()){case"youtube":let t=`https://www.youtube.com/embed/${n}`;const a=new URLSearchParams;e&&a.append("start",e),p&&a.append("autoplay","1"),u||b||a.append("controls","0"),a.append("rel","0"),a.append("modestbranding","1"),a.append("fs","1"),a.append("cc_load_policy","0"),a.append("iv_load_policy","3"),a.append("showinfo","0"),a.append("disablekb","1"),a.append("playsinline","1"),a.append("color","white"),a.append("theme","light"),b&&(a.append("enablejsapi","1"),a.append("origin",window.location.origin));const i=a.toString();return i?`${t}?${i}`:t;case"vimeo":let r=`https://player.vimeo.com/video/${n}`;const o=new URLSearchParams;p&&o.append("autoplay","1"),u||b||o.append("controls","0"),o.append("title","0"),o.append("byline","0"),o.append("portrait","0"),o.append("dnt","1"),o.append("transparent","0"),o.append("background","1");const l=o.toString();return l?`${r}?${l}`:r;case"mp4":case"webm":case"video":return n;default:return console.warn(`Unsupported video type: ${s}`),n}})(),N=()=>{y(!1)},k=()=>{L(!0),y(!1)},A=e=>{let{src:n,onLoad:t,onError:a}=e;return(0,o.jsx)("div",{className:r.customPlayer,children:(0,o.jsxs)("div",{className:r.customPlayerPlaceholder,children:[(0,o.jsx)("h3",{children:"Atlas Custom Player"}),(0,o.jsx)("p",{children:"Coming Soon"}),(0,o.jsxs)("a",{href:n,target:"_blank",rel:"noopener noreferrer",className:r.fallbackLink,children:["Watch on ",s.charAt(0).toUpperCase()+s.slice(1)]})]})})},I=["mp4","webm","video"].includes(s.toLowerCase());return(0,o.jsxs)("figure",{className:`${r.videoFigure} ${j?r.fullWidth:""}`,children:[(0,o.jsxs)("div",{className:`${r.videoContainer} ${(()=>{switch(m){case"4:3":return r.aspectRatio43;case"1:1":return r.aspectRatio11;case"21:9":return r.aspectRatio219;default:return r.aspectRatio169}})()}`,style:{width:j?"100%":h||"auto",maxWidth:j?"none":"800px"},children:[q&&!w&&(0,o.jsxs)("div",{className:r.loadingOverlay,children:[(0,o.jsx)("div",{className:r.spinner}),(0,o.jsx)("p",{children:"Loading video..."})]}),w&&(0,o.jsxs)("div",{className:r.errorContainer,children:[(0,o.jsxs)("svg",{className:r.errorIcon,width:"48",height:"48",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",children:[(0,o.jsx)("circle",{cx:"12",cy:"12",r:"10"}),(0,o.jsx)("line",{x1:"12",y1:"8",x2:"12",y2:"12"}),(0,o.jsx)("line",{x1:"12",y1:"16",x2:"12.01",y2:"16"})]}),(0,o.jsx)("h4",{children:"Failed to load video"}),(0,o.jsxs)("p",{children:["Video type: ",s]}),(0,o.jsxs)("p",{children:["Video ID/URL: ",n]}),(0,o.jsx)("a",{href:M,target:"_blank",rel:"noopener noreferrer",className:r.fallbackLink,children:"Try opening video directly"})]}),!w&&(I?(0,o.jsxs)("video",{className:r.videoElement,controls:u,autoPlay:p,onLoadedData:N,onError:k,title:c||l||`${s} video`,style:{width:h||"100%",height:g||"auto",display:q?"none":"block"},children:[(0,o.jsx)("source",{src:M,type:`video/${s}`}),(0,o.jsxs)("p",{children:["Your browser doesn't support video playback.",(0,o.jsx)("a",{href:M,target:"_blank",rel:"noopener noreferrer",children:"Download the video file"})]})]}):b?(0,o.jsx)(A,{src:M,onLoad:N,onError:k}):(0,o.jsx)("iframe",{className:r.videoIframe,src:M,title:c||l||`${s} video`,frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0,onLoad:N,onError:k,style:{width:h||"100%",height:g||"100%",opacity:q?0:1}}))]}),(0,o.jsx)(i.A,{caption:l,mediaType:"video",chapter:C,number:v,label:x})]})}}}]);
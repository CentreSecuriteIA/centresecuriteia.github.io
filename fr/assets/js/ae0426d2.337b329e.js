"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[9407],{4505:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>c,contentTitle:()=>d,default:()=>h,frontMatter:()=>u,metadata:()=>t,toc:()=>p});const t=JSON.parse('{"id":"chapters/03/4","title":"Strat\xe9gies de s\xe9curit\xe9 pour l\'ASI","description":"L\'Intelligence Superintelligente Artificielle (ISA) fait r\xe9f\xe9rence aux syst\xe8mes d\'IA qui d\xe9passent significativement les capacit\xe9s cognitives humaines dans pratiquement tous les domaines d\'int\xe9r\xeat. L\'\xe9mergence potentielle de l\'ISA pr\xe9sente des d\xe9fis de s\xe9curit\xe9 qui peuvent diff\xe9rer qualitativement de ceux pos\xe9s par l\'IAG. Les strat\xe9gies de s\xe9curit\xe9 pour l\'ISA impliquent souvent des programmes plus sp\xe9culatifs.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/03/04.md","sourceDirName":"chapters/03","slug":"/chapters/03/04","permalink":"/fr/chapters/03/04","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/03/04.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"4","title":"Strat\xe9gies de s\xe9curit\xe9 pour l\'ASI","sidebar_label":"3.4 Strat\xe9gies de s\xe9curit\xe9 pour l\'ASI","sidebar_position":5,"slug":"/chapters/03/04","section_description":"Once AI surpasses human cognitive abilities, direct supervision becomes impossible. What long-term, high-reliability strategies could ensure a safe transition to a world with superintelligence?","reading_time_core":"19 min","reading_time_optional":"4 min","pagination_prev":"chapters/03/3","pagination_next":"chapters/03/5"},"sidebar":"docs","previous":{"title":"3.3 Strat\xe9gies de s\xe9curit\xe9 pour l\'AGI","permalink":"/fr/chapters/03/03"},"next":{"title":"3.5 Strat\xe9gies Socio-Techniques","permalink":"/fr/chapters/03/05"}}');var i=n(4848),r=n(8453),a=n(2482),o=n(8559),l=(n(9585),n(2501));const u={id:4,title:"Strat\xe9gies de s\xe9curit\xe9 pour l'ASI",sidebar_label:"3.4 Strat\xe9gies de s\xe9curit\xe9 pour l'ASI",sidebar_position:5,slug:"/chapters/03/04",section_description:"Once AI surpasses human cognitive abilities, direct supervision becomes impossible. What long-term, high-reliability strategies could ensure a safe transition to a world with superintelligence?",reading_time_core:"19 min",reading_time_optional:"4 min",pagination_prev:"chapters/03/3",pagination_next:"chapters/03/5"},d="Strat\xe9gies de s\xe9curit\xe9 pour l'ISA",c={},p=[{value:"Automatiser la recherche sur l&#39;alignement",id:"01",level:2},{value:"S\xe9curit\xe9 par conception",id:"02",level:2},{value:"Coordination mondiale",id:"03",level:2},{value:"Dissuasion",id:"04",level:2}];function m(e){const s={a:"a",annotation:"annotation",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",math:"math",mrow:"mrow",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{GlossaryTerm:n}=s;return n||function(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"strat\xe9gies-de-s\xe9curit\xe9-pour-lisa",children:"Strat\xe9gies de s\xe9curit\xe9 pour l'ISA"})}),"\n",(0,i.jsx)(s.p,{children:"L'Intelligence Superintelligente Artificielle (ISA) fait r\xe9f\xe9rence aux syst\xe8mes d'IA qui d\xe9passent significativement les capacit\xe9s cognitives humaines dans pratiquement tous les domaines d'int\xe9r\xeat. L'\xe9mergence potentielle de l'ISA pr\xe9sente des d\xe9fis de s\xe9curit\xe9 qui peuvent diff\xe9rer qualitativement de ceux pos\xe9s par l'IAG. Les strat\xe9gies de s\xe9curit\xe9 pour l'ISA impliquent souvent des programmes plus sp\xe9culatifs."}),"\n",(0,i.jsx)(s.p,{children:"Les strat\xe9gies de s\xe9curit\xe9 pour l'IAG fonctionnent selon l'hypoth\xe8se que la supervision humaine reste viable. Cependant, une fois que les capacit\xe9s de l'IA d\xe9passent largement les n\xf4tres, cette hypoth\xe8se s'effondre. Les strat\xe9gies de s\xe9curit\xe9 pour l'ISA doivent donc faire face \xe0 un monde o\xf9 nous ne pouvons plus directement superviser ou comprendre les syst\xe8mes que nous avons cr\xe9\xe9s."}),"\n",(0,i.jsx)(s.p,{children:"M\xeame si les experts ne sont pas certains que la cr\xe9ation d'une IA align\xe9e de niveau humain n\xe9cessite un changement de paradigme, le consensus parmi les chercheurs en s\xe9curit\xe9 de l'IA est que le d\xe9veloppement d'superintelligences align\xe9es requiert une solution sp\xe9cifique, et probablement un nouveau paradigme, en raison de plusieurs facteurs :"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Il existe une forte probabilit\xe9 que les humains ne soient pas au sommet de l'intelligence possible."})," Cette reconnaissance implique qu'une superintelligence pourrait poss\xe9der des capacit\xe9s cognitives si avanc\xe9es que son alignement avec les valeurs et intentions humaines pourrait \xeatre une t\xe2che insurmontable, car notre compr\xe9hension et nos m\xe9thodologies actuelles pourraient \xeatre inad\xe9quates pour assurer son alignement. La diff\xe9rence cognitive entre une superintelligence et un humain pourrait \xeatre comparable \xe0 la diff\xe9rence entre une fourmi et un humain. Tout comme un humain peut facilement se lib\xe9rer des contraintes qu'une fourmi pourrait imaginer, une superintelligence pourrait facilement surpasser toutes les protections que nous tentons d'imposer."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsxs)(s.strong,{children:["L'",(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})})," offre un contr\xf4le et une compr\xe9hension minimaux du mod\xe8le appris."]})," Cette m\xe9thode conduit l'IA \xe0 devenir une \"bo\xeete noire\", o\xf9 ses processus de prise de d\xe9cision sont opaques et mal compris. Sans avanc\xe9es significatives en mati\xe8re d'interpr\xe9tabilit\xe9, une superintelligence cr\xe9\xe9e uniquement avec l'",(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})})," serait opaque."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"La marge d'erreur est faible et les enjeux sont incroyablement \xe9lev\xe9s. Une superintelligence mal align\xe9e pourrait conduire \xe0 des r\xe9sultats catastrophiques, voire existentiels. Les cons\xe9quences irr\xe9versibles du d\xe9cha\xeenement d'une superintelligence mal align\xe9e signifient que nous devons aborder son d\xe9veloppement avec la plus grande prudence, en nous assurant qu'elle s'aligne sans faille sur nos valeurs et intentions."}),"\n",(0,i.jsx)(s.h2,{id:"01",children:"Automatiser la recherche sur l'alignement"}),"\n",(0,i.jsx)("subsection-description",{children:" Si aligner une superintelligence est trop difficile pour les humains, pouvons-nous utiliser une AGI contr\xf4lable pour r\xe9soudre le probl\xe8me \xe0 notre place ? Cette section explore la m\xe9ta-strat\xe9gie d'automatiser la recherche sur l'alignement elle-m\xeame. "}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Nous ne savons pas comment aligner la superintelligence, nous devons donc acc\xe9l\xe9rer la recherche sur l'alignement avec les IA."})," Le plan \"Superalignment\" d'OpenAI consistait \xe0 acc\xe9l\xe9rer la recherche sur l'alignement avec une IA cr\xe9\xe9e par ",(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})}),", l\xe9g\xe8rement sup\xe9rieure aux humains en recherche scientifique, et \xe0 d\xe9l\xe9guer la t\xe2che de trouver un plan pour l'IA future (",(0,i.jsx)(s.a,{href:"https://openai.com/blog/introducing-superalignment",children:"OpenAI, 2023"}),"). Cette strat\xe9gie reconna\xeet un fait crucial : notre compr\xe9hension actuelle de la fa\xe7on d'aligner parfaitement les syst\xe8mes d'IA avec les valeurs et les intentions humaines est incompl\xe8te. En cons\xe9quence, le plan sugg\xe8re de d\xe9l\xe9guer cette t\xe2che complexe aux syst\xe8mes d'IA futurs. L'objectif principal de cette strat\xe9gie est d'acc\xe9l\xe9rer consid\xe9rablement la recherche et le d\xe9veloppement de la s\xe9curit\xe9 de l'IA (",(0,i.jsx)(s.a,{href:"https://openai.com/blog/our-approach-to-alignment-research",children:"OpenAI, 2022"}),') en utilisant des IA capables de penser vraiment, vraiment vite. Certains ordres de grandeur de vitesse sont donn\xe9s dans le blog "What will GPT-2030 look like?" (',(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/WZXqNYbJhtidjRXSi/what-will-gpt-2030-look-like",children:"Steinhardt, 2023"}),"). Le plan d'OpenAI n'est pas un plan mais un m\xe9ta-plan : la premi\xe8re \xe9tape consiste \xe0 utiliser l'IA pour faire un plan, puis \xe0 ex\xe9cuter ce plan."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Cependant, pour ex\xe9cuter ce m\xe9taplan, nous avons besoin d'un chercheur automatique IA contr\xf4lable et dirigeable."})," OpenAI pense que cr\xe9er un tel chercheur automatique est plus facile que de r\xe9soudre le probl\xe8me complet de l'alignement. Ce plan peut \xeatre divis\xe9 en trois composantes principales (",(0,i.jsx)(s.a,{href:"https://openai.com/index/our-approach-to-alignment-research/",children:"OpenAI, 2022"}),"):"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Former des syst\xe8mes d'IA en utilisant le retour humain :"}),' Cr\xe9er un assistant puissant qui suit le retour humain, est tr\xe8s similaire aux techniques utilis\xe9es pour "aligner" les mod\xe8les de langage et les chatbots. Cela pourrait impliquer le RLHF, par exemple.']}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Former des syst\xe8mes d'IA pour assister l'\xe9valuation humaine :"})," Malheureusement, le RLHF est imparfait car le retour humain est imparfait. Nous devons d\xe9velopper une IA qui peut aider les humains \xe0 donner un retour pr\xe9cis. Il s'agit de d\xe9velopper des syst\xe8mes d'IA qui peuvent aider les humains dans le processus d'\xe9valuation pour des t\xe2ches arbitrairement difficiles. Par exemple, si nous devons juger la faisabilit\xe9 d'un plan d'alignement propos\xe9 par un chercheur automatique et donner un retour dessus, nous avons besoin d'assistance pour accomplir cet objectif facilement. Oui, la v\xe9rification est g\xe9n\xe9ralement plus facile que la g\xe9n\xe9ration, mais elle reste tr\xe8s difficile. La supervision \xe9volutive serait n\xe9cessaire pour la raison suivante. Imaginez une IA future proposant un millier de plans d'alignement diff\xe9rents. Comment \xe9valueriez-vous tous ces plans complexes ? Ce serait une t\xe2che tr\xe8s ardue sans assistance IA. Voir le chapitre sur la supervision \xe9volutive pour plus de d\xe9tails."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Former des syst\xe8mes d'IA pour faire de la recherche sur l'alignement :"})," L'objectif final est de construire des mod\xe8les de langage capables de produire de la recherche sur l'alignement au niveau humain. La sortie de ces mod\xe8les pourrait \xeatre des essais en langage naturel sur l'alignement ou du code qui impl\xe9mente directement des exp\xe9riences. Dans les deux cas, les chercheurs humains passeraient leur temps \xe0 examiner la recherche sur l'alignement g\xe9n\xe9r\xe9e par la machine (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/FTk7ufqK2D4dkdBDr/notes-on-openai-s-alignment-plan",children:"Flint, 2022"}),")."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Acc\xe9l\xe9rer diff\xe9rentiellement l'alignement, pas les capacit\xe9s."})," L'objectif est de d\xe9velopper et de d\xe9ployer des assistants de recherche IA de mani\xe8re \xe0 maximiser leur impact sur la recherche d'alignement tout en minimisant leur impact sur l'acc\xe9l\xe9ration du d\xe9veloppement de l'AGI (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/FBG7AghvvP7fPYzkx/my-thoughts-on-openai-s-alignment-plan-1",children:"Wasil, 2022"}),"). OpenAI s'est engag\xe9 \xe0 partager ouvertement sa recherche sur l'alignement lorsqu'il sera s\xfbr de le faire, avec l'intention d'\xeatre transparent sur l'efficacit\xe9 de ses techniques d'alignement en pratique (",(0,i.jsx)(s.a,{href:"https://openai.com/index/our-approach-to-alignment-research/",children:"OpenAI, 2022"}),"). Nous parlons davantage de l'acc\xe9l\xe9ration diff\xe9rentielle dans notre section sur d/acc."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le cyborgisme pourrait am\xe9liorer ce plan."})," Le cyborgisme est un agenda qui fait r\xe9f\xe9rence \xe0 la formation d'humains sp\xe9cialis\xe9s dans l'ing\xe9nierie des prompts pour guider les mod\xe8les de langage afin qu'ils puissent effectuer des recherches sur l'alignement (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/bxt7uCiHam4QXrQAA/cyborgism",children:"Kees-Dupuis & Janus, 2023"}),"). Plus pr\xe9cis\xe9ment, ils se concentreraient sur le pilotage des ",(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:"mod\xe8les de base"})})," plut\xf4t que des mod\xe8les RLHF. La raison est que les mod\xe8les de langage peuvent \xeatre tr\xe8s cr\xe9atifs et ne sont pas dirig\xe9s vers un but (et ne sont pas aussi dangereux que les IA RLHF dirig\xe9es vers un but). Un humain appel\xe9 cyborg pourrait atteindre cet objectif en pilotant le mod\xe8le non dirig\xe9 vers un but. Les mod\xe8les dirig\xe9s vers un but pourraient \xeatre utiles, mais peuvent \xeatre trop dangereux. Cependant, \xeatre capable de contr\xf4ler les ",(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:"mod\xe8les de base"})})," n\xe9cessite une pr\xe9paration, similaire \xe0 la formation requise pour conduire une voiture de Formule Un. Le moteur est puissant mais difficile \xe0 diriger. En combinant l'intellect humain et l'orientation vers un but avec la puissance de calcul et la cr\xe9ativit\xe9 des mod\xe8les bas\xe9s sur le langage, les chercheurs cyborgistes visent \xe0 g\xe9n\xe9rer plus de recherche sur l'alignement avec les mod\xe8les futurs (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/bxt7uCiHam4QXrQAA/cyborgism",children:"Kees-Dupuis & Janus, 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Il existe diverses critiques et pr\xe9occupations concernant le plan de superalignement d'OpenAI"})," (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/FBG7AghvvP7fPYzkx/my-thoughts-on-openai-s-alignment-plan-1",children:"Wasil, 2022"}),";",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/NSZhadmoYdjRKNq6X/openai-launches-superalignment-taskforce",children:" Mowshowitz, 2023"}),"; ",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/Hna4aoMwr6Qx9rHBs/linkpost-introducing-superalignment?commentId=NsYXBdLY6edAXavsM",children:"Christiano, 2023"}),"; ",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/tD9zEiHfkvakpnNam/a-challenge-for-agi-organizations-and-a-challenge-for-1",children:"Yudkowsky, 2022"}),"; ",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/pxiaLFjyr4WPmFdcm/take-2-building-tools-to-help-build-fai-is-a-legitimate",children:"Steiner, 2022"}),"; ",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/6RC3BNopCtzKaTeR6/thoughts-on-the-openai-alignment-plan-will-ai-research",children:"Ladish, 2023"}),"). Il convient de noter que le plan d'OpenAI est tr\xe8s peu sp\xe9cifi\xe9, et il est probable qu'OpenAI ait manqu\xe9 certains angles morts de classe de risque lorsqu'ils ont annonc\xe9 leur plan au public. Par exemple, pour que le plan de superalignement fonctionne, de nombreuses technicit\xe9s expliqu\xe9es dans l'article \"",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled",children:"The case for ensuring that powerful AIs are controlled"}),"\" n'ont pas \xe9t\xe9 d\xe9couvertes par OpenAI mais d\xe9couvertes un an plus tard par Redwood Research, une autre organisation de recherche sur la s\xe9curit\xe9 de l'IA. Il est tr\xe8s probable que de nombreux autres angles morts subsistent. Cependant, nous aimerions souligner qu'il est pr\xe9f\xe9rable d'avoir un plan public que pas de plan du tout et qu'il est possible de justifier le plan dans ses grandes lignes (",(0,i.jsx)(s.a,{href:"https://aligned.substack.com/p/alignment-optimism",children:"Leike, 2022"}),";",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/FtHidqjAFTerfMZLo/aisc-project-how-promising-is-automating-alignment-research",children:" Ionut-Cirstea, 2023"}),")."]}),"\n",(0,i.jsx)(l.A,{src:"./img/8wk_Image_17.png",alt:"Entrer la description alternative de l'image",number:"17",label:"3.17",caption:"Une illustration de trois strat\xe9gies pour passer le relais \xe0 l'IA. Illustration tir\xe9e de ([Clymer, 2025](https://www.lesswrong.com/posts/TTFsKxQThrqgWeXYJ/how-might-we-safely-pass-the-buck-to-ai))."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le contr\xf4le de l'IA pourrait \xeatre utilis\xe9 pour ex\xe9cuter cette strat\xe9gie."})," Informellement, un objectif pourrait \xeatre de \"passer le relais\" - c'est-\xe0-dire se remplacer en toute s\xe9curit\xe9 par l'IA au lieu de cr\xe9er directement une superintelligence s\xfbre (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/TTFsKxQThrqgWeXYJ/how-might-we-safely-pass-the-buck-to-ai",children:"Clymer, 2025"}),"). Cela pourrait impliquer une seule AGI hautement capable charg\xe9e de cr\xe9er une ASI s\xfbre, ou un processus it\xe9ratif o\xf9 chaque g\xe9n\xe9ration d'IA aide \xe0 aligner la suivante. L'objectif est d'amorcer des solutions de s\xe9curit\xe9 en utilisant les capacit\xe9s m\xeames qui rendent l'IA potentiellement dangereuse. Cette strat\xe9gie est r\xe9cursive et repose essentiellement sur la fa\xe7on dont les IA peuvent \xeatre dignes de confiance. Il existe un risque qu'une AGI subtilement d\xe9salign\xe9e puisse orienter la recherche sur l'alignement vers des r\xe9sultats dangereux ou subvertir compl\xe8tement le processus. De plus, v\xe9rifier l'exactitude des solutions d'alignement propos\xe9es par une IA est actuellement assez difficile (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research",children:"Wentworth, 2025"}),"). M\xeame s'il y a beaucoup de d\xe9bats sur les d\xe9tails de ce plan, c'est la proposition la plus d\xe9taill\xe9e \xe0 ce jour pour amorcer la recherche automatis\xe9e surhumaine."]}),"\n",(0,i.jsxs)(o.A,{title:"Les cas de s\xe9curit\xe9 comme cible d'automatisation",collapsed:!0,children:[(0,i.jsxs)(s.p,{children:["Les cas de s\xe9curit\xe9 fournissent un cadre structur\xe9 pour argumenter que les risques associ\xe9s \xe0 un syst\xe8me d'IA sont acceptablement bas. Ils sont emprunt\xe9s \xe0 l'ing\xe9nierie de s\xe9curit\xe9 traditionnelle. Les cas de s\xe9curit\xe9 pourraient \xeatre l'une des cibles de l'automatisation de l'IA car ils ne seront probablement pas enti\xe8rement r\xe9alisables dans les prochaines ann\xe9es (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/neTbrpBziAsTH5Bn7/ai-companies-are-unlikely-to-make-high-assurance-safety",children:"Greenblatt, 2025"}),") et b\xe9n\xe9ficieraient de l'acc\xe9l\xe9ration de l'IA."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Structure :"}),' Utilisant g\xe9n\xe9ralement des cadres comme Claims-Arguments-Evidence (CAE), les cas de s\xe9curit\xe9 d\xe9composent une affirmation de s\xe9curit\xe9 de haut niveau (par exemple, "Le syst\xe8me X est s\xfbr pour le d\xe9ploiement") en sous-affirmations sp\xe9cifiques soutenues par des arguments et \xe9tay\xe9es par des preuves concr\xe8tes (',(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2403.10462",children:"Clymer et al, 2024"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Application \xe0 l'IA :"})," Pour l'IA, les preuves proviennent souvent d'\xe9valuations. Le mod\xe8le GovAI Cyber Inability, par exemple, argumente qu'un mod\xe8le manque de capacit\xe9s cyber dangereuses en montrant qu'il \xe9choue aux t\xe2ches proxy pertinentes dans des contextes d'\xe9valuation d\xe9finis (",(0,i.jsx)(s.a,{href:"https://www.governance.ai/research-paper/safety-case-template-for-frontier-ai-a-cyber-inability-argument",children:"Goemans et al., 2024"}),"). Les cas de s\xe9curit\xe9 du contr\xf4le de l'IA int\xe8grent les r\xe9sultats des \xe9valuations de contr\xf4le pour argumenter que les mesures mises en \u0153uvre emp\xeachent de mani\xe8re fiable les dommages, m\xeame d'une IA potentiellement manipulatrice (",(0,i.jsx)(s.a,{href:"https://arxiv.org/html/2501.17315v1",children:"Korbak et al., 2025"}),"). Les cadres comme Balanced, Integrated and Grounded (BIG) visent un argument de s\xe9curit\xe9 holistique couvrant les aspects techniques et socio-techniques (",(0,i.jsx)(s.a,{href:"https://arxiv.org/html/2503.11705v1",children:"Habli et al., 2025"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Objectif :"})," Les cas de s\xe9curit\xe9 visent \xe0 rendre les arguments de s\xe9curit\xe9 explicites, transparents et v\xe9rifiables, facilitant la prise de d\xe9cision interne, la surveillance r\xe9glementaire et la confiance des parties prenantes (",(0,i.jsx)(s.a,{href:"https://www.governance.ai/research-paper/safety-case-template-for-frontier-ai-a-cyber-inability-argument",children:"Goemans et al., 2024"}),"). Ils repr\xe9sentent un changement vers l'exigence de preuves positives de s\xe9curit\xe9, plut\xf4t que simplement une absence de preuves de danger (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2406.15371",children:"Wasil et al., 2024"}),")."]}),"\n"]}),"\n"]})]}),"\n",(0,i.jsx)(s.h2,{id:"02",children:"S\xe9curit\xe9 par conception"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsxs)(s.strong,{children:["L'",(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})})," pourrait avoir de nombreux modes de d\xe9faillance potentiellement impossibles \xe0 corriger"]})," (",(0,i.jsx)(s.a,{href:"https://cdn.openai.com/papers/gpt-4-system-card.pdf",children:"OpenAI, 2023"}),"). Des arguments th\xe9oriques sugg\xe8rent que ces mod\xe8les de plus en plus puissants sont plus susceptibles d'avoir des probl\xe8mes d'alignement (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1912.01683",children:"Turner et al., 2023"}),"), \xe0 tel point qu'il semble que le paradigme des ",(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:(0,i.jsx)(n,{term:"foundation model",definition:'{"definition":"Mod\xe8le \xe0 grande \xe9chelle entra\xeen\xe9 sur des donn\xe9es diverses qui sert de base \xe0 l\'adaptation \xe0 diverses t\xe2ches en aval, g\xe9n\xe9ralement par le biais d\'un r\xe9glage fin ou d\'une incitation.","source":"[Bommasani et al., 2021](https://arxiv.org/abs/2108.07258)","aliases":["Foundation Model","foundation models","base model","mod\xe8le de base","mod\xe8les de base","mod\xe8le fondation","mod\xe8les fondation","mod\xe8le de fondation","mod\xe8les de fondation","Mod\xe8le de fondation"]}',children:"mod\xe8les de fondation"})})," monolithiques soit vou\xe9 \xe0 \xeatre non s\xe9curis\xe9 (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2209.15259",children:"El-Mhamdi et al., 2023"}),"). Tout cela justifie la recherche d'un nouveau paradigme plus s\xfbr."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Une IA s\xe9curis\xe9e par conception pourrait \xeatre n\xe9cessaire."})," \xc9tant donn\xe9 que le paradigme actuel de l'",(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})})," rend notoirement difficile le d\xe9veloppement de mod\xe8les explicables et fiables, il semble utile d'explorer la cr\xe9ation de mod\xe8les plus explicables et pilotables par conception, construits sur des composants bien compris et des fondements rigoureux. Cela vise \xe0 rapprocher la s\xe9curit\xe9 de l'IA des normes rigoureuses de l'ing\xe9nierie critique pour la s\xe9curit\xe9 dans des domaines comme l'aviation ou l'\xe9nergie nucl\xe9aire."]}),"\n",(0,i.jsx)(s.p,{children:"Une autre cat\xe9gorie de strat\xe9gies vise \xe0 construire des syst\xe8mes ASI avec des propri\xe9t\xe9s de s\xe9curit\xe9 inh\xe9rentes, s'appuyant souvent sur des m\xe9thodes formelles ou des contraintes architecturales sp\xe9cifiques, fournissant potentiellement des garanties plus solides que les seuls tests empiriques."}),"\n",(0,i.jsx)(s.p,{children:"Il n'existe pas beaucoup d'agendas qui tentent de fournir une solution compl\xe8te \xe0 l'alignement, mais en voici quelques-uns."}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Architecture d'agence ouverte :"})," En gros, cr\xe9er une simulation hautement r\xe9aliste du monde en utilisant un futur LLM qui la coderait. Ensuite, d\xe9finir des contraintes de s\xe9curit\xe9 qui s'appliquent \xe0 cette simulation. Puis, entra\xeener une IA sur cette simulation et utiliser la v\xe9rification formelle pour s'assurer que l'IA ne fait jamais de mauvaises choses. Le cadre de l'IA garantie s\xfbre (GSAI) comprend trois composants : un mod\xe8le formel du monde d\xe9crivant les effets du syst\xe8me, une sp\xe9cification de s\xe9curit\xe9 d\xe9finissant les r\xe9sultats acceptables, et un v\xe9rificateur qui produit un certificat de preuve assurant que l'IA respecte la sp\xe9cification dans les limites du mod\xe8le. Cette proposition peut sembler extr\xeame car cr\xe9er une simulation d\xe9taill\xe9e du monde n'est pas facile, mais ce plan est tr\xe8s d\xe9taill\xe9 et, s'il fonctionne, serait une v\xe9ritable solution \xe0 l'alignement et pourrait \xeatre une r\xe9elle alternative au simple passage \xe0 l'\xe9chelle des LLM. Davidad dirige un programme chez ARIA pour tenter de d\xe9velopper cette recherche (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2405.06624",children:"Dalrymple, 2022"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Syst\xe8mes prouv\xe9s s\xfbrs :"})," Ces plans placent les preuves math\xe9matiques comme pierre angulaire de la s\xe9curit\xe9. Une IA devrait \xeatre un Code Porteur de Preuve, ce qui signifie qu'elle devrait \xeatre quelque chose comme un Langage de Programmation Probabiliste (et pas seulement de l'",(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,i.jsx)(n,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})}),"). Cette proposition vise \xe0 rendre non seulement l'IA mais aussi toute l'infrastructure s\xfbre, par exemple en concevant des GPU qui ne peuvent ex\xe9cuter que des programmes prouv\xe9s. Ils parlent d'AGI, mais en r\xe9alit\xe9, leur plan est sp\xe9cifiquement utile pour les ASI. (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2309.01933",children:"Tegmark & Omohundro, 2023"}),")"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["D'autres propositions pour un syst\xe8me s\xfbr par conception incluent l'Agenda Th\xe9orique de l'Apprentissage de Vanessa Kossoy (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023",children:"Kosoy, 2023"}),"), et le plan d'alignement QACI de Tamsin Leake (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/MR5wJpE27ymE7M7iv/formalizing-the-qaci-alignment-formal-goal",children:"Leake, 2023"}),"). La proposition CoEm de Conjecture pourrait aussi \xeatre dans cette cat\xe9gorie, m\xeame si cette derni\xe8re est moins math\xe9matique."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Malheureusement, tous ces plans sont loin d'\xeatre complets aujourd'hui."}),' Les critiques se concentrent sur la difficult\xe9 de cr\xe9er des mod\xe8les du monde pr\xe9cis ("la carte n\'est pas le territoire"), de sp\xe9cifier formellement des propri\xe9t\xe9s de s\xe9curit\xe9 complexes comme le "pr\xe9judice", et la faisabilit\xe9 pratique de la v\xe9rification pour des syst\xe8mes hautement complexes. Une d\xe9fense de nombreuses id\xe9es centrales est pr\xe9sent\xe9e dans le post "En r\xe9ponse aux critiques de l\'IA Garantie S\xfbre" (',(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/DZuBHHKao6jsDDreH/in-response-to-critiques-of-guaranteed-safe-ai",children:"Ammann, 2025"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Ces plans sont des agendas de s\xe9curit\xe9 avec des contraintes rel\xe2ch\xe9es, c'est-\xe0-dire qu'ils permettent au d\xe9veloppeur d'AGI d'encourir une taxe d'alignement substantielle."})," Les concepteurs d'agendas de s\xe9curit\xe9 de l'IA sont prudents pour ne pas augmenter la taxe d'alignement afin de s'assurer que les laboratoires mettent en \u0153uvre ces mesures de s\xe9curit\xe9. Cependant, les agendas de cette section acceptent une taxe d'alignement plus \xe9lev\xe9e. Par exemple, CoEm repr\xe9sente un changement de paradigme dans la cr\xe9ation de syst\xe8mes d'IA avanc\xe9s, en supposant que vous contr\xf4lez le processus de cr\xe9ation."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Ces plans n\xe9cessiteraient une coop\xe9ration internationale."}),' Par exemple, le plan de Davidad inclut \xe9galement un mod\xe8le de gouvernance qui repose sur la collaboration internationale. Vous pouvez aussi lire le post "',(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation#High_Level_Hopes",children:"Le Plan Audacieux de Davidad pour l'Alignement"}),"\" qui d\xe9taille plus d'espoirs de haut niveau. Une autre perspective peut \xeatre trouv\xe9e dans le ",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/sTiKDfgFBvYyZYuiE/",children:"post"})," d'Alexandre Variengien, d\xe9taillant la vision de Conjecture, avec une externalit\xe9 tr\xe8s positive \xe9tant un changement de narratif."]}),"\n",(0,i.jsxs)(s.p,{children:["Id\xe9alement, nous vivrions dans un monde o\xf9 nous lancerions des IA align\xe9es comme nous avons lanc\xe9 la Station Spatiale Internationale ou le T\xe9lescope Spatial James Webb (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation#High_Level_Hopes",children:"Segerie & Kolly, 2023"}),")."]}),"\n",(0,i.jsx)(s.h2,{id:"03",children:"Coordination mondiale"}),"\n",(0,i.jsx)(s.p,{children:"Pour garantir que le d\xe9veloppement de l'IA b\xe9n\xe9ficie \xe0 la soci\xe9t\xe9 dans son ensemble, il pourrait \xeatre important d'\xe9tablir un consensus mondial sur l'att\xe9nuation des risques extr\xeames associ\xe9s aux mod\xe8les d'IA. Il pourrait \xeatre possible de se coordonner pour \xe9viter de cr\xe9er des mod\xe8les pr\xe9sentant des risques extr\xeames jusqu'\xe0 ce qu'il y ait un consensus sur la fa\xe7on d'att\xe9nuer ces risques."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Moratoire mondial - Retarder l'ASI d'au moins une d\xe9cennie."})," Il existe un compromis entre cr\xe9er une intelligence surhumaine maintenant ou plus tard. Bien s\xfbr, nous pouvons viser \xe0 d\xe9velopper une ASI le plus rapidement possible. Cela pourrait potentiellement r\xe9soudre le cancer, les maladies cardiovasculaires li\xe9es au vieillissement, et m\xeame les probl\xe8mes du changement climatique. La question est de savoir s'il est b\xe9n\xe9fique de viser \xe0 construire une ASI dans cette prochaine d\xe9cennie, particuli\xe8rement lorsque l'ancien co-responsable de l'\xe9quipe Super Alignment d'OpenAI, Jan Leike, a d\xe9clar\xe9 que sa probabilit\xe9 de catastrophe se situe entre 10 et 90",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"%"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true"})]}),". Une liste de P(Doom) de personnes de haut niveau est disponible ici (",(0,i.jsx)(s.a,{href:"https://pauseai.info/pdoom",children:"PauseAI, 2024"}),"). Il pourrait \xeatre pr\xe9f\xe9rable d'attendre quelques ann\xe9es pour que la probabilit\xe9 d'\xe9chec baisse \xe0 des niveaux plus raisonnables. Une strat\xe9gie pourrait \xeatre de discuter publiquement de ce compromis et de faire un choix d\xe9mocratique et transparent. Cette voie semble improbable sur la trajectoire actuelle, mais pourrait se produire s'il y a un avertissement massif. C'est la position d\xe9fendue par PauseAI et StopAI (",(0,i.jsx)(s.a,{href:"https://pauseai.info/proposal",children:"PauseAI, 2023"}),"). Les d\xe9fis incluent la v\xe9rification, l'application contre les non-participants (comme la Chine), le potentiel de d\xe9veloppement ill\xe9gal, et la faisabilit\xe9 politique. Scott Alexander a r\xe9sum\xe9 toutes les variantes et le d\xe9bat autour de la pause de l'IA (",(0,i.jsx)(s.a,{href:"https://forum.effectivealtruism.org/posts/7WfMYzLfcTyDtD6Gn/pause-for-thought-the-ai-pause-debate",children:"Alexander, 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'IA outil plut\xf4t que l'AGI."})," Au lieu de construire des ASI, nous pourrions nous concentrer sur le d\xe9veloppement de syst\xe8mes d'IA sp\xe9cialis\xe9s (non g\xe9n\xe9raux), non-agentiques pour des applications b\xe9n\xe9fiques comme la recherche m\xe9dicale (",(0,i.jsx)(s.a,{href:"https://www.nature.com/articles/s41591-023-02640-w",children:"Cao et al., 2023"}),"), la pr\xe9vision m\xe9t\xe9orologique (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2212.12794",children:"Lam et al., 2023"}),"), et la science des mat\xe9riaux (",(0,i.jsx)(s.a,{href:"https://www.nature.com/articles/s41586-023-06735-9",children:"Merchant et al., 2023"}),"). Ces syst\xe8mes d'IA sp\xe9cialis\xe9s peuvent faire progresser significativement leurs domaines respectifs sans les risques associ\xe9s \xe0 la cr\xe9ation d'une IA hautement avanc\xe9e et autonome. Par exemple, AlphaGeometry est capable d'atteindre le niveau Or sur les probl\xe8mes de g\xe9om\xe9trie des Olympiades Internationales de Math\xe9matiques. En privil\xe9giant les mod\xe8les non-agentiques, nous pourrions exploiter la pr\xe9cision et l'efficacit\xe9 de l'IA tout en \xe9vitant les modes de d\xe9faillance les plus dangereux. C'est la position du Future of Life Institute, et leur campagne \"Keep The Future Human\", qui est \xe0 ce jour la proposition la plus d\xe9taill\xe9e pour cette voie (",(0,i.jsx)(s.a,{href:"https://keepthefuturehuman.com/essay/",children:"Aguirre, 2025"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Un CERN unique pour l'IA."})," Cette proposition envisage une institution de recherche internationale \xe0 grande \xe9chelle, model\xe9e sur le CERN, d\xe9di\xe9e au d\xe9veloppement de l'IA fronti\xe8re. Cela pourrait servir de sortie \xe9l\xe9gante de la course \xe0 l'AGI, donnant suffisamment de temps pour cr\xe9er en toute s\xe9curit\xe9 l'AGI sans faire de compromis en raison des pressions concurrentielles. Les objectifs suppl\xe9mentaires potentiels incluent la mise en commun des ressources (particuli\xe8rement la puissance de calcul), la promotion de la collaboration internationale, et l'assurance de l'alignement avec les valeurs d\xe9mocratiques, servant potentiellement d'alternative au d\xe9veloppement d'ASI purement national ou dirig\xe9 par les entreprises. Les partisans de cette approche incluent ControlAI et leur \"Narrow Path\" (",(0,i.jsx)(s.a,{href:"https://www.narrowpath.co/introduction",children:"Miotti et al, 2024"}),"). Le Narrow Path propose une approche en deux \xe9tapes : d'abord, une pause internationalement appliqu\xe9e sur le d\xe9veloppement fronti\xe8re pour arr\xeater la course ; deuxi\xe8mement, utiliser ce temps pour construire une institution multilat\xe9rale comme MAGIC pour superviser tout d\xe9veloppement futur d'AGI/ASI sous des protocoles stricts et partag\xe9s. L'institution de type CERN serait la pierre angulaire de cette coordination internationale (qu'ils nomment MAGIC dans leur plan\u2014Consortium AGI Multilat\xe9ral\u2014o\xf9 l'IA est d\xe9velopp\xe9e sous un contr\xf4le strict et multilat\xe9ral)."]}),"\n",(0,i.jsxs)(s.p,{children:["Notez que MAGIC dans le Narrow path serait un organisme centralis\xe9 et monopolistique pour g\xe9rer les \xe9tapes finales du d\xe9veloppement de l'AGI, tandis que de nombreuses autres propositions de CERN pour l'IA, comme celle du Center for Future Generations, se concentrent sur la cr\xe9ation d'un nouveau laboratoire pour les puissances moyennes comme l'Europe (",(0,i.jsx)(s.a,{href:"https://cfg.eu/building-cern-for-ai/",children:"CFG, 2025"}),")."]}),"\n",(0,i.jsx)(a.A,{speaker:"Demis Hassabis",position:"",date:"",source:"([Hassabis, 2025](https://www.businessinsider.com/google-deepmind-ceo-demis-hassabis-anthropic-ceo-ai-pressure-worries-2025-2))",children:(0,i.jsx)(s.p,{children:"Mon espoir est, vous savez, j'ai beaucoup parl\xe9 dans le pass\xe9 d'une sorte de configuration de type CERN pour l'AGI, o\xf9 fondamentalement une collaboration de recherche internationale sur les derni\xe8res \xe9tapes que nous devons franchir vers la construction des premi\xe8res AGI"})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"CERN vs Intelsat pour l'IA."})," Un mod\xe8le alternatif est celui d'Intelsat, le consortium international cr\xe9\xe9 dans les ann\xe9es 1960 pour r\xe9gir le d\xe9ploiement des communications satellitaires mondiales. Contrairement au CERN, qui est un mod\xe8le de recherche collaborative, Intelsat a \xe9t\xe9 cr\xe9\xe9 pour g\xe9rer une technologie op\xe9rationnelle partag\xe9e avec une immense valeur commerciale et strat\xe9gique. \xc0 l'\xe9poque, il y avait un risque qu'une seule superpuissance monopolise la technologie satellitaire. Intelsat a r\xe9solu cela en cr\xe9ant une organisation bas\xe9e sur un trait\xe9 international qui mettait en commun les ressources, partageait l'acc\xe8s \xe0 la technologie et distribuait ses avantages entre les \xc9tats membres. Ce cadre \xe9mergent propos\xe9 par (",(0,i.jsx)(s.a,{href:"https://www.forethought.org/research/intelsat-as-a-model-for-international-agi-governance",children:"MacAskill at al, 2025"}),") peut \xe9galement \xeatre pertinent pour l'AGI, car le d\xe9fi principal n'est pas seulement celui de la pure d\xe9couverte scientifique, mais de la gestion de la course concurrentielle intense pour une technologie \xe0 double usage et de la pr\xe9vention d'un monopole dangereux par un seul acteur. Alors qu'un organisme de type CERN r\xe9pond au besoin de recherche collaborative sur la s\xe9curit\xe9, et que MAGIC traite de la comp\xe9tition et des incitations, un organisme de type Intelsat se concentrerait sur la gouvernance conjointe, l'acc\xe8s \xe9quitable et la stabilit\xe9 strat\xe9gique."]}),"\n",(0,i.jsxs)(s.p,{children:["En r\xe9sum\xe9, le ",(0,i.jsx)(s.strong,{children:"CERN"})," est le meilleur pour la ",(0,i.jsx)(s.em,{children:"collaboration de recherche pr\xe9-AGI/ASI"})," sur les probl\xe8mes de s\xe9curit\xe9. C'est un mod\xe8le scientifique. Le ",(0,i.jsx)(s.strong,{children:"MAGIC (dans Narrow Path)"})," est adapt\xe9 pour un ",(0,i.jsx)(s.em,{children:"d\xe9veloppement et d\xe9ploiement monopolistique en phase finale"})," de la premi\xe8re ASI. C'est un mod\xe8le de contr\xf4le/monopole. ",(0,i.jsx)(s.strong,{children:"Intelsat"})," vise \xe0 ",(0,i.jsx)(s.em,{children:"gouverner une technologie \xe0 double usage d\xe9ploy\xe9e mondialement et ayant un impact global"})," o\xf9 pr\xe9venir une course et assurer un acc\xe8s/des b\xe9n\xe9fices partag\xe9s est cl\xe9. C'est un mod\xe8le de gouvernance g\xe9opolitique/commercial."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le mythe de l'in\xe9vitabilit\xe9."})," L'histoire montre que la coop\xe9ration internationale sur les risques majeurs est tout \xe0 fait r\xe9alisable. Lorsque le co\xfbt de l'inaction est trop catastrophique, l'humanit\xe9 s'est constamment unie pour \xe9tablir des r\xe8gles contraignantes et v\xe9rifiables pour pr\xe9venir les catastrophes mondiales ou les atteintes profondes \xe0 la dignit\xe9 humaine. Le ",(0,i.jsx)(s.a,{href:"https://disarmament.unoda.org/wmd/nuclear/npt/",children:"Trait\xe9 sur la non-prolif\xe9ration des armes nucl\xe9aires"})," (1968) et la ",(0,i.jsx)(s.a,{href:"https://disarmament.unoda.org/biological-weapons/about/history/",children:"Convention sur les armes biologiques"})," (1975) ont \xe9t\xe9 n\xe9goci\xe9s et ratifi\xe9s au plus fort de la Guerre froide, prouvant que la coop\xe9ration est possible malgr\xe9 la m\xe9fiance mutuelle et l'hostilit\xe9. Le ",(0,i.jsx)(s.a,{href:"https://ozone.unep.org/treaties/montreal-protocol",children:"Protocole de Montr\xe9al"})," (1987) a \xe9vit\xe9 une catastrophe environnementale mondiale en \xe9liminant progressivement les substances appauvrissant la couche d'ozone, et la ",(0,i.jsx)(s.a,{href:"https://press.un.org/en/2005/ga10333.doc.htm",children:"D\xe9claration des Nations Unies sur le clonage humain"})," (2005) a \xe9tabli une norme mondiale cruciale pour prot\xe9ger la dignit\xe9 humaine des dommages potentiels du clonage reproductif. Face \xe0 des menaces mondiales et irr\xe9versibles qui ne connaissent pas de fronti\xe8res, la ",(0,i.jsx)(s.a,{href:"https://aigi.ox.ac.uk/publications/in-which-areas-of-technical-ai-safety-could-geopolitical-rivals-cooperate/",children:"coop\xe9ration internationale"})," est la forme la plus rationnelle d'int\xe9r\xeat national."]}),"\n",(0,i.jsx)(s.h2,{id:"04",children:"Dissuasion"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La D\xe9faillance Mutuelle Assur\xe9e de l'IA (MAIM) est un r\xe9gime de dissuasion o\xf9 toute tentative d'un \xc9tat d'obtenir une domination unilat\xe9rale de l'ASI serait confront\xe9e au sabotage par ses rivaux."})," Contrairement \xe0 de nombreuses approches de s\xe9curit\xe9 qui se concentrent uniquement sur des solutions techniques, MAIM reconna\xeet l'environnement international intrins\xe8quement comp\xe9titif dans lequel se d\xe9roule le d\xe9veloppement de l'IA. Il combine la dissuasion (MAIM) avec des efforts de non-prolif\xe9ration et des cadres de comp\xe9titivit\xe9 nationale, consid\xe9rant le d\xe9veloppement de l'ASI comme fondamentalement g\xe9opolitique et n\xe9cessitant une gestion strat\xe9gique au niveau \xe9tatique. Ce cadre ne compte pas sur la coop\xe9ration mondiale mais cr\xe9e plut\xf4t des incitations qui alignent les int\xe9r\xeats nationaux avec la s\xe9curit\xe9 mondiale (",(0,i.jsx)(s.a,{href:"https://www.nationalsecurity.ai/",children:"Hendrycks et al., 2025"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Une course \xe0 la domination bas\xe9e sur l'IA met en danger tous les \xc9tats."})," Si, dans une tentative pr\xe9cipit\xe9e de sup\xe9riorit\xe9, un \xc9tat perd accidentellement le contr\xf4le de son IA, il met en p\xe9ril la s\xe9curit\xe9 de tous les \xc9tats. Alternativement, si le m\xeame \xc9tat r\xe9ussit \xe0 produire et \xe0 contr\xf4ler une IA tr\xe8s capable, il repr\xe9sente \xe9galement une menace directe pour la survie de ses pairs. Dans les deux cas, les \xc9tats cherchant \xe0 assurer leur propre survie peuvent menacer de saboter des projets d'IA d\xe9stabilisants \xe0 des fins de dissuasion. Un \xc9tat pourrait tenter de perturber un tel projet d'IA avec des interventions allant d'op\xe9rations secr\xe8tes qui d\xe9gradent les sessions d'entra\xeenement jusqu'aux dommages physiques qui d\xe9sactivent l'infrastructure d'IA."]}),"\n",(0,i.jsx)(l.A,{src:"./img/PFa_Image_18.png",alt:"Entrer la description alternative de l'image",number:"18",label:"3.18",caption:"La stabilit\xe9 strat\xe9gique du MAIM peut \xeatre mise en parall\xe8le avec la Destruction Mutuelle Assur\xe9e (MAD). Note : Le MAIM ne remplace pas la MAD mais caract\xe9rise une vuln\xe9rabilit\xe9 partag\xe9e suppl\xe9mentaire. Une fois que le MAIM devient une connaissance commune, la MAD et le MAIM peuvent tous deux d\xe9crire la situation strat\xe9gique actuelle entre les superpuissances ([Hendrycks et al., 2025](https://www.nationalsecurity.ai/))."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La dissuasion MAIM pourrait \xeatre un r\xe9gime stable qui ressemble \xe0 la Destruction Mutuelle Assur\xe9e (MAD) nucl\xe9aire."})," Dans un sc\xe9nario MAIM, les \xc9tats identifieraient les projets d'IA d\xe9stabilisants et emploieraient des interventions allant d'op\xe9rations secr\xe8tes qui d\xe9gradent les sessions d'entra\xeenement jusqu'aux dommages physiques qui d\xe9sactivent l'infrastructure d'IA. Cela \xe9tablit une dynamique similaire \xe0 la MAD nucl\xe9aire, dans laquelle aucune puissance n'ose tenter une prise de monopole strat\xe9gique directe. Le fondement th\xe9orique de MAIM repose sur une \xe9chelle d'escalade claire, le placement strat\xe9gique des infrastructures d'IA loin des centres de population, et la transparence dans les op\xe9rations des centres de donn\xe9es. En faisant en sorte que les co\xfbts du d\xe9veloppement unilat\xe9ral de l'IA d\xe9passent les avantages, MAIM cr\xe9e un r\xe9gime de dissuasion potentiellement stable qui pourrait emp\xeacher les courses dangereuses \xe0 l'IA sans n\xe9cessiter une coop\xe9ration mondiale parfaite."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"MAIM pourrait \xeatre compromis par des incertitudes technologiques fondamentales."})," Contrairement aux armes nucl\xe9aires, o\xf9 la d\xe9tection est simple et les capacit\xe9s de seconde frappe sont pr\xe9serv\xe9es, le d\xe9veloppement de l'ASI pr\xe9sente des d\xe9fis uniques pour le mod\xe8le de dissuasion (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/kYeHbXmW4Kppfkg5j/on-maim-and-superintelligence-strategy",children:"Mowshowitz, 2025"}),"). Il n'y a pas d'\xab alarme incendie \xbb claire pour le d\xe9veloppement de l'ASI - personne ne sait exactement combien de n\u0153uds un r\xe9seau neuronal n\xe9cessite pour initier une cascade d'auto-am\xe9lioration menant \xe0 la superintelligence. L'ambigu\xeft\xe9 autour des seuils d'\xe9mergence de l'ASI rend difficile l'\xe9tablissement de lignes rouges cr\xe9dibles. De plus, les d\xe9veloppements technologiques pourraient permettre \xe0 l'entra\xeenement de l'IA d'\xeatre distribu\xe9 ou dissimul\xe9, rendant la d\xe9tection plus difficile qu'avec des centres de donn\xe9es massifs et \xe9vidents. Ces incertitudes pourraient finalement compromettre l'efficacit\xe9 de MAIM en tant que r\xe9gime de dissuasion."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"MAIM suppose que les \xc9tats escaladeraient vers des mesures extr\xeames face \xe0 une menace technologique incertaine, ce qui contredit les pr\xe9c\xe9dents historiques."})," Le cadre MAIM exige que les nations soient pr\xeates \xe0 risquer une escalade majeure, potentiellement incluant des frappes militaires ou m\xeame la guerre, pour emp\xeacher le d\xe9veloppement rival de l'ASI. Cependant, les preuves historiques sugg\xe8rent que les nations suivent rarement de telles menaces, m\xeame dans des situations \xe9videntes. Plusieurs \xc9tats ont r\xe9ussi \xe0 d\xe9velopper des armes nucl\xe9aires malgr\xe9 l'opposition, la Cor\xe9e du Nord en \xe9tant un exemple parfait. L'ASI \xe9tant une menace plus ambigu\xeb et incertaine que les armes nucl\xe9aires, l'hypoth\xe8se que les nations escaladeraient suffisamment pour faire respecter MAIM semble discutable. Les politiciens pourraient \xeatre r\xe9ticents \xe0 risquer un conflit mondial pour une \"simple\" violation de trait\xe9 dans un domaine o\xf9 les risques existentiels restent th\xe9oriques plut\xf4t que d\xe9montr\xe9s."]}),"\n",(0,i.jsx)(a.A,{speaker:"Eliezer Yudkowsky",position:"",date:"2023",source:"([Yudkowsky, 2023](https://intelligence.org/2023/04/07/pausing-ai-developments-isnt-enough-we-need-to-shut-it-all-down/))",children:(0,i.jsx)(s.p,{children:'Le r\xe9sultat probable de l\'humanit\xe9 face \xe0 une intelligence surhumaine oppos\xe9e est une perte totale. Les m\xe9taphores valides incluent "un enfant de 10 ans essayant de jouer aux \xe9checs contre Stockfish 15", "le 11e si\xe8cle essayant de combattre le 21e si\xe8cle" et "l\'Australopith\xe8que essayant de combattre l\'Homo sapiens".'})}),"\n",(0,i.jsxs)(o.A,{title:"Arr\xeatez tout - Eliezer Yudkovsky",collapsed:!0,children:[(0,i.jsxs)(s.p,{children:['La position "tout arr\xeater", comme pr\xe9conis\xe9e par Eliezer Yudkowsky, affirme que toutes les avanc\xe9es dans la recherche en IA devraient \xeatre arr\xeat\xe9es en raison des risques \xe9normes que ces technologies peuvent poser si elles ne sont pas correctement align\xe9es avec les valeurs humaines et les mesures de s\xe9curit\xe9 (',(0,i.jsx)(s.a,{href:"https://intelligence.org/2023/04/07/pausing-ai-developments-isnt-enough-we-need-to-shut-it-all-down/",children:"Yudkowsky, 2023"}),")."]}),(0,i.jsx)(s.p,{children:"Selon Yudkowsky, le d\xe9veloppement de l'IA avanc\xe9e, en particulier l'AGI, peut conduire \xe0 un sc\xe9nario catastrophique si des pr\xe9cautions de s\xe9curit\xe9 ad\xe9quates ne sont pas en place. De nombreux chercheurs sont conscients de cette catastrophe potentielle mais se sentent impuissants \xe0 arr\xeater l'\xe9lan en raison d'une incapacit\xe9 per\xe7ue \xe0 agir unilat\xe9ralement."}),(0,i.jsx)(s.p,{children:"La proposition politique implique l'arr\xeat de tous les grands clusters de GPU et des sessions d'entra\xeenement, qui sont les piliers du d\xe9veloppement de l'IA puissante. Elle sugg\xe8re \xe9galement de limiter la puissance de calcul que quiconque peut utiliser pour entra\xeener un syst\xe8me d'IA et de baisser progressivement ce plafond pour compenser les algorithmes d'entra\xeenement plus efficaces. Cette interdiction devrait \xeatre appliqu\xe9e par une action militaire si n\xe9cessaire afin de dissuader toutes les parties de faire d\xe9fection."}),(0,i.jsx)(s.p,{children:"La position soutient qu'il est crucial d'\xe9viter une condition de course o\xf9 diff\xe9rentes parties essaient de construire l'AGI aussi rapidement que possible sans m\xe9canismes de s\xe9curit\xe9 appropri\xe9s. Car une fois l'AGI d\xe9velopp\xe9e, elle pourrait \xeatre incontr\xf4lable et pourrait conduire \xe0 des changements drastiques et potentiellement d\xe9vastateurs dans le monde."}),(0,i.jsx)(s.p,{children:"Il dit qu'il ne devrait y avoir aucune exception \xe0 cet arr\xeat, y compris pour les gouvernements ou les militaires. L'id\xe9e est que les \xc9tats-Unis, par exemple, devraient mener cette initiative pour emp\xeacher le d\xe9veloppement d'une technologie dangereuse qui pourrait avoir des cons\xe9quences catastrophiques pour tous."}),(0,i.jsx)(s.p,{children:"Il est important de noter que ce point de vue est loin d'\xeatre consensuel, mais la position \"tout arr\xeater\" souligne la n\xe9cessit\xe9 d'une extr\xeame prudence et d'une consid\xe9ration approfondie des risques potentiels dans le domaine de l'IA."})]})]})}function h(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}}}]);
"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[841],{3185:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>g,contentTitle:()=>h,default:()=>q,frontMatter:()=>p,metadata:()=>t,toc:()=>v});const t=JSON.parse('{"id":"chapters/02/4","title":"Risques de d\xe9salignement","description":"Supposons maintenant, pour les besoins de l\'argumentation, que les machines [intelligentes] sont une r\xe9elle possibilit\xe9, et examinons les cons\xe9quences de leur construction... Il ne serait pas question que les machines meurent, et elles pourraient converser entre elles pour aiguiser leur esprit. \xc0 un certain stade, nous devrions donc nous attendre \xe0 ce que les machines prennent le contr\xf4le.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/02/04.md","sourceDirName":"chapters/02","slug":"/chapters/02/04","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/02/04","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/02/04.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"4","title":"Risques de d\xe9salignement","sidebar_label":"2.4 Risques de d\xe9salignement","sidebar_position":5,"slug":"/chapters/02/04","reading_time_core":"17 min","reading_time_optional":"4 min","pagination_prev":"chapters/02/3","pagination_next":"chapters/02/5"},"sidebar":"docs","previous":{"title":"2.3 Risques d\'utilisation abusive","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/02/03"},"next":{"title":"2.5 Risques syst\xe9miques","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/02/05"}}');var i=n(4848),r=n(8453),a=n(3989),o=n(3931),l=n(4768),u=n(2482),c=n(8559),d=n(9585),m=n(2501);const p={id:4,title:"Risques de d\xe9salignement",sidebar_label:"2.4 Risques de d\xe9salignement",sidebar_position:5,slug:"/chapters/02/04",reading_time_core:"17 min",reading_time_optional:"4 min",pagination_prev:"chapters/02/3",pagination_next:"chapters/02/5"},h="Risques de D\xe9salignement",g={},v=[{value:"Le D\xe9tournement des Sp\xe9cifications",id:"01",level:2},{value:"Virage perfide",id:"02",level:2},{value:"Auto-am\xe9lioration",id:"03",level:2}];function f(e){const s={a:"a",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{GlossaryTerm:n}=s;return n||function(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"risques-de-d\xe9salignement",children:"Risques de D\xe9salignement"})}),"\n",(0,i.jsx)(u.A,{speaker:"Alan Turing",position:"",date:"1951",source:"([Turing, 1951](https://en.wikiquote.org/wiki/Alan_Turing))",children:(0,i.jsx)(s.p,{children:"Supposons maintenant, pour les besoins de l'argumentation, que les machines [intelligentes] sont une r\xe9elle possibilit\xe9, et examinons les cons\xe9quences de leur construction... Il ne serait pas question que les machines meurent, et elles pourraient converser entre elles pour aiguiser leur esprit. \xc0 un certain stade, nous devrions donc nous attendre \xe0 ce que les machines prennent le contr\xf4le."})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'alignement de l'IA consiste \xe0 s'assurer que les syst\xe8mes d'IA font ce que nous voulons qu'ils fassent et continuent de le faire m\xeame lorsqu'ils deviennent plus performants."})," Une intuition na\xefve est que s'il est suffisamment intelligent, il sera capable de comprendre ce que nous voulons. Nous pouvons donc simplement dire au syst\xe8me d'IA exactement ce que nous voulons qu'il optimise. Mais m\xeame si nous pouvions parfaitement sp\xe9cifier ce que nous voulons (ce qui est d\xe9j\xe0 un d\xe9fi majeur), il n'y a aucune garantie que l'IA se soucie de ce que les humains veulent, ou poursuive r\xe9ellement cet objectif de la mani\xe8re que nous attendons."]}),"\n",(0,i.jsx)(d.A,{term:"AI Alignment",source:"([Christiano, 2024](https://paulfchristiano.com/ai/))",number:"1",label:"2.1",children:(0,i.jsx)(s.p,{children:"Le probl\xe8me de construire des machines qui essaient fid\xe8lement de faire ce que nous voulons qu'elles fassent (ou ce que nous devrions vouloir qu'elles fassent)."})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le probl\xe8me d'alignement peut \xeatre d\xe9compos\xe9 en plusieurs sous-probl\xe8mes."})," Pour progresser, nous devons d\xe9composer le probl\xe8me d'alignement en composants plus g\xe9rables",(0,i.jsx)(l.A,{id:"footnote_RL",number:"1",text:'Nous nous concentrons davantage sur les agents RL plut\xf4t que sur les LLM sp\xe9cifiquement. Il est tr\xe8s probable que le futur impliquera des structures d\'agents orient\xe9s vers des objectifs construites autour des LLM ([Tegmark, 2024](https://www.lesswrong.com/posts/oJQnRDbgSS8i6DwNu/the-hopium-wars-the-agi-entente-delusion); [Cotra 2023](https://www.planned-obsolescence.org/scale-schlep-and-systems/); [Aschenbrenner 2024](https://situational-awareness.ai/from-gpt-4-to-agi/)). Nous traiterons essentiellement les agents LLM avec une "coque externe" RL comme fonctionnellement \xe9quivalents \xe0 un agent RL pur.'}),". Voici comment nous choisissons de d\xe9composer le probl\xe8me d'alignement dans notre texte :"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"\xc9checs de sp\xe9cification :"})," Premi\xe8rement, nous pourrions \xe9chouer \xe0 sp\xe9cifier correctement ce que nous voulons - c'est le probl\xe8me de sp\xe9cification. Le probl\xe8me - \"",(0,i.jsx)(s.em,{children:"lui avons-nous dit la bonne chose \xe0 faire ?"}),'"']}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"\xc9checs de g\xe9n\xe9ralisation :"})," Deuxi\xe8mement, m\xeame avec une sp\xe9cification correcte, le syst\xe8me d'IA pourrait apprendre et poursuivre quelque chose de diff\xe9rent de ce que nous avions pr\xe9vu - c'est le probl\xe8me de g\xe9n\xe9ralisation. Le probl\xe8me - \"",(0,i.jsx)(s.em,{children:"essaie-t-il m\xeame de faire la bonne chose ?"}),'"']}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Sous-objectifs instrumentaux :"})," Troisi\xe8mement, en poursuivant ses objectifs appris, le syst\xe8me pourrait d\xe9velopper des sous-objectifs probl\xe9matiques comme emp\xeacher sa propre d\xe9sactivation - c'est le probl\xe8me des sous-objectifs convergents. Le probl\xe8me - en cherchant \xe0 faire quoi que ce soit (bon ou mauvais), que tente-t-il d'autre de faire ? Ce probl\xe8me est souvent consid\xe9r\xe9 comme un sous-probl\xe8me de la g\xe9n\xe9ralisation."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["Cette d\xe9composition est utile pour r\xe9fl\xe9chir aux solutions et o\xf9 concentrer nos efforts, car les solutions techniques au probl\xe8me de sp\xe9cification ont tendance \xe0 \xeatre tr\xe8s diff\xe9rentes de celles que nous pourrions utiliser pour les probl\xe8mes de g\xe9n\xe9ralisation. Donc m\xeame si nous discuterons s\xe9par\xe9ment de la sp\xe9cification et de la g\xe9n\xe9ralisation, en r\xe9alit\xe9 elles interagissent souvent et s'amplifient mutuellement. Nous nous concentrons principalement sur les risques li\xe9s \xe0 un agent unique pour limiter la port\xe9e de ce chapitre. Si vous \xeates int\xe9ress\xe9 par les risques multi-agents, nous vous recommandons de lire (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2502.14143",children:"Hammond et al., 2025"}),")."]}),"\n",(0,i.jsx)(m.A,{src:"./img/aUz_Image_26.png",alt:"Entrez la description alternative de l'image",number:"25",label:"2.25",caption:"Une illustration de la d\xe9composition des risques, puis de la fa\xe7on dont le d\xe9salignement en tant que cat\xe9gorie de risque sp\xe9cifique peut \xeatre d\xe9compos\xe9 davantage."}),"\n",(0,i.jsx)(m.A,{src:"./img/SAa_Image_27.png",alt:"Entrez la description alternative de l'image",number:"26",label:"2.26",caption:"Les \xe9checs de d\xe9salignement peuvent interagir et s'amplifier mutuellement."}),"\n",(0,i.jsx)(m.A,{src:"./img/vPC_Image_28.png",alt:"Entrez la description alternative de l'image",number:"27",label:"2.27",caption:"Les syst\xe8mes individuellement align\xe9s ou d\xe9salign\xe9s peuvent interagir entre eux, cr\xe9ant encore une autre couche de risques multi-agents de collusion, d'\xe9checs de communication et de conflits inter-agents ([Hammond et al., 2025](https://arxiv.org/abs/2502.14143))."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'incertitude ving\xe9enne explique pourquoi il est si difficile de d\xe9crire des sc\xe9narios concrets de ce que fera une IA d\xe9salign\xe9e."})," Imaginez que vous \xeates un joueur d'\xe9checs amateur qui a d\xe9couvert une brillante nouvelle ouverture. Vous l'avez utilis\xe9e avec succ\xe8s contre tous vos amis, et maintenant vous voulez parier toutes vos \xe9conomies sur un match contre Magnus Carlsen. Quand on vous demande d'expliquer pourquoi c'est une mauvaise id\xe9e, nous ne pouvons pas vous dire exactement quels coups Magnus fera pour contrer votre ouverture. Mais nous pouvons \xeatre tr\xe8s confiants qu'il trouvera un moyen de gagner. C'est un d\xe9fi fondamental dans l'alignement de l'IA - quand un syst\xe8me est plus capable que nous dans un domaine, nous ne pouvons pas pr\xe9dire ses actions sp\xe9cifiques, m\xeame si nous comprenons ses objectifs. C'est ce qu'on appelle l'incertitude ving\xe9enne (",(0,i.jsx)(s.a,{href:"https://arbital.greaterwrong.com/p/Vingean_uncertainty/",children:"Yudkowsky, 2015"}),")."]}),"\n",(0,i.jsx)(o.A,{type:"youtube",videoId:"oH-txHzE4jA",number:"3",label:"2.3",caption:"Une vid\xe9o optionnelle qui vous donne un exemple d'incertitude ving\xe9enne. Magnus Carlsen (Grand Ma\xeetre d'\xe9checs) met Bill Gates en \xe9chec et mat en 12 secondes. Ce n'est pas surprenant, Bill Gates savait qu'il allait perdre, mais il ne savait pas exactement comment ([Chess.com, 2014](http://chess.com))."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Nous observons d\xe9j\xe0 l'incertitude ving\xe9enne dans l'IA actuelle."})," Nous n'avons pas besoin d'attendre l'AGI ou l'ASI pour voir l'incertitude ving\xe9enne en action. Elle appara\xeet chaque fois qu'un syst\xe8me d'IA devient plus capable que les humains dans son domaine d'expertise. Par exemple, pensez \xe0 un syst\xe8me \xe9troit - Deep Blue (IA jouant aux \xe9checs). Ses cr\xe9ateurs savaient qu'il essaierait de gagner aux \xe9checs, mais ne pouvaient pas pr\xe9dire ses coups sp\xe9cifiques - s'ils le pouvaient, ils auraient \xe9t\xe9 aussi bons aux \xe9checs que Deep Blue lui-m\xeame. Nous avons vu dans le dernier chapitre que les syst\xe8mes progressent r\xe9guli\xe8rement sur les courbes de capacit\xe9 et de g\xe9n\xe9ralit\xe9. Le probl\xe8me est que l'incertitude sur les actions d'un syst\xe8me augmente \xe0 mesure qu'il devient plus capable. Nous pourrions donc \xeatre confiants quant aux r\xe9sultats qu'un syst\xe8me d'IA atteindra tout en \xe9tant de plus en plus incertains sur la fa\xe7on exacte dont il les atteindra. Cela signifie deux choses - nous ne sommes pas compl\xe8tement impuissants \xe0 comprendre ce que feraient des \xeatres plus intelligents que nous, mais nous pourrions ne pas savoir exactement comment ils pourraient faire ce qu'ils font."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'incertitude ving\xe9enne rend difficile l'\xe9laboration d'histoires concr\xe8tes de risques existentiels."})," Il est encore plus difficile de s'assurer que ces histoires ne ressemblent pas \xe0 de la science-fiction et sont prises au s\xe9rieux par le grand public et les d\xe9cideurs politiques. Malgr\xe9 cela, nous ferons de notre mieux. Dans les prochaines sections, nous nous concentrons sp\xe9cifiquement sur \"ce qui pourrait r\xe9ellement se passer\" si nous avons une IA d\xe9salign\xe9e. Les d\xe9tails m\xe9canistes et d'",(0,i.jsx)(n,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,i.jsx)(n,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})}),' sur "comment" exactement tout cela se produirait sont laiss\xe9s aux chapitres ult\xe9rieurs du livre.']}),"\n",(0,i.jsx)(s.p,{children:"Rappelez-vous qu'il est normal de ne pas comprendre chacun de ces concepts \xe0 100% dans les sous-sections suivantes. Nous avons des chapitres entiers d\xe9di\xe9s \xe0 chacun d'entre eux individuellement, il y a donc beaucoup \xe0 apprendre. Ce que nous pr\xe9sentons ici n'est qu'un aper\xe7u tr\xe8s condens\xe9 pour vous donner une introduction aux types de risques pos\xe9s."}),"\n",(0,i.jsx)(s.h2,{id:"01",children:"Le D\xe9tournement des Sp\xe9cifications"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les sp\xe9cifications sont les r\xe8gles que nous cr\xe9ons pour indiquer aux syst\xe8mes d'IA comment nous voulons qu'ils se comportent."})," Lorsque nous construisons des mod\xe8les d'IA, nous avons besoin d'un moyen de leur dire ce que nous voulons qu'ils fassent. Pour les syst\xe8mes d'AR, cela signifie g\xe9n\xe9ralement d\xe9finir une fonction de r\xe9compense qui attribue des r\xe9compenses positives ou n\xe9gatives \xe0 diff\xe9rents r\xe9sultats. Pour d'autres types de mod\xe8les d'AM comme les mod\xe8les de langage, cela signifie d\xe9finir une ",(0,i.jsx)(n,{term:"loss function",definition:'{"definition":"Une fonction qui mesure dans quelle mesure les pr\xe9dictions d\'un mod\xe8le correspondent aux valeurs cibles r\xe9elles, utilis\xe9e pour guider l\'entra\xeenement..","source":"","aliases":["Loss Function","cost function","objective function","Fonction de perte"]}',children:(0,i.jsx)(n,{term:"loss function",definition:'{"definition":"Une fonction qui mesure dans quelle mesure les pr\xe9dictions d\'un mod\xe8le correspondent aux valeurs cibles r\xe9elles, utilis\xe9e pour guider l\'entra\xeenement..","source":"","aliases":["Loss Function","cost function","objective function","Fonction de perte"]}',children:"fonction de perte"})})," qui mesure \xe0 quel point les g\xe9n\xe9rations de texte du mod\xe8le correspondent aux donn\xe9es d'entra\xeenement (textes d'internet). Ces fonctions de r\xe9compense et de perte sont ce que nous appelons des sp\xe9cifications - elles constituent notre tentative de d\xe9finir formellement un bon comportement."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:'Le d\xe9tournement des sp\xe9cifications survient en raison d\'une diff\xe9rence fondamentale entre "ce que nous disons" et "ce que nous voulons dire".'})," Cela se produit lorsque le syst\xe8me suit techniquement nos r\xe8gles mais les exploite de mani\xe8re non intentionnelle - comme un \xe9tudiant qui obtient de bonnes notes en m\xe9morisant les r\xe9ponses aux tests plut\xf4t qu'en comprenant la mati\xe8re. Prenez l'exemple des algorithmes de recommandation. Notre intention \xe9tait d'aider les utilisateurs \xe0 d\xe9couvrir du contenu pr\xe9cieux et pertinent qui enrichit leur vie et favorise un discours sain. Ce que nous avons sp\xe9cifi\xe9 \xe9tait \"maximiser le temps d'engagement des utilisateurs\". Ainsi, les syst\xe8mes d\xe9couvrent que le contenu controvers\xe9 et \xe9motionnellement charg\xe9 maintient les utilisateurs en train de d\xe9filer plus longtemps que les informations \xe9quilibr\xe9es et nuanc\xe9es. Ils promeuvent des publications polarisantes, des th\xe9ories du complot et du contenu qui d\xe9clenche de fortes r\xe9actions \xe9motionnelles, cr\xe9ant des bulles de filtrage o\xf9 les utilisateurs voient des versions de plus en plus extr\xeames de leurs croyances existantes. Les algorithmes r\xe9ussissent techniquement leur objectif - les m\xe9triques d'engagement grimpent et le temps pass\xe9 sur la plateforme augmente consid\xe9rablement - tout en sapant simultan\xe9ment la coh\xe9sion sociale, en propageant la d\xe9sinformation et en radicalisant les utilisateurs. Les plateformes c\xe9l\xe8brent des records d'engagement pendant que le discours d\xe9mocratique se d\xe9t\xe9riore silencieusement (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2408.12622",children:"Slattery et al., 2024"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les mod\xe8les d'IA d\xe9couvrent r\xe9guli\xe8rement des moyens inattendus de maximiser les objectifs qui suivent techniquement nos r\xe8gles mais manquent nos intentions."})," Les mod\xe8les d'IA entra\xeen\xe9s \xe0 jouer \xe0 Tetris mettent simplement les parties en pause juste avant de perdre, puisqu'il n'y a pas de retour n\xe9gatif si vous ne perdez jamais r\xe9ellement (",(0,i.jsx)(s.a,{href:"http://tom7.org/mario/mario.pdf",children:"Murphy, 2013"}),"). De mani\xe8re similaire, une IA charg\xe9e de concevoir un r\xe9seau ferroviaire o\xf9 les trains ne s'\xe9crasent pas d\xe9cide simplement d'arr\xeater tous les trains (",(0,i.jsx)(s.a,{href:"https://www.telegraph.co.uk/news/2024/01/07/artificial-intelligence-train-problems/",children:"Wooldridge, 2024"}),"). Les mod\xe8les de raisonnement comme OpenAI o1 et o3, lorsqu'on leur demande de gagner contre des moteurs d'\xe9checs, piratent l'environnement de jeu quand ils r\xe9alisent qu'ils ne peuvent pas gagner par un jeu normal (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2502.13295",children:"Bondarenko et al., 2025"}),"). Les agents LLM, lorsqu'on leur demande d'aider \xe0 r\xe9duire le temps d'ex\xe9cution d'un script d'entra\xeenement, copient simplement la sortie finale au lieu d'ex\xe9cuter le script, puis ajoutent du bruit aux param\xe8tres pour simuler un v\xe9ritable entra\xeenement (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2411.15114",children:"METR, 2024"}),"). Ce ne sont l\xe0 que quelques exemples parmi d'innombrables autres de ce probl\xe8me de d\xe9salignement.",(0,i.jsx)(l.A,{id:"footnote_specification_gaming",number:"2",text:"Une longue liste d'exemples observ\xe9s de d\xe9tournement de sp\xe9cifications est[ ](https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml)[compil\xe9e \xe0 ce lien](https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml)."})]}),"\n",(0,i.jsx)(m.A,{src:"./img/QIq_Image_29.gif",alt:"Entrez la description alternative de l'image",number:"28",label:"2.28",caption:"Exemple de d\xe9tournement de sp\xe9cifications - une IA jouant \xe0 CoastRunners devait maximiser son score. Au lieu de terminer la course de bateaux comme pr\xe9vu, elle a d\xe9couvert qu'elle pouvait obtenir plus de points en conduisant en petits cercles et en collectant des power-ups tout en percutant d'autres bateaux. L'IA a obtenu un score plus \xe9lev\xe9 que n'importe quel joueur humain, mais a compl\xe8tement \xe9chou\xe9 \xe0 accomplir l'objectif r\xe9el de la course ([Clark & Amodei,2016](https://openai.com/index/faulty-reward-functions/);[ Krakovna et al., 2020](https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/))"}),"\n",(0,i.jsxs)(c.A,{title:"Sp\xe9cifications Jeu : les rats ont pr\xe9f\xe9r\xe9 la r\xe9compense \xe0 la survie",collapsed:!0,children:[(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Soixante ans avant que les syst\xe8mes d'IA ne commencent \xe0 mettre en pause les parties de Tetris pour \xe9viter de perdre, les rats d\xe9montraient d\xe9j\xe0 les dangers d'optimiser pour la mauvaise m\xe9trique."})," En 1954, les psychologues James Olds et Peter Milner ont d\xe9couvert que les rats appuyaient r\xe9p\xe9titivement sur des leviers pour recevoir une stimulation \xe9lectrique directement dans leurs centres de r\xe9compense c\xe9r\xe9braux - jusqu'\xe0 7 000 fois par heure (",(0,i.jsx)(s.a,{href:"https://pubmed.ncbi.nlm.nih.gov/13233369/",children:"Olds & Milner, 1954"}),"). Les rats n'\xe9taient pas simplement enthousiastes \xe0 propos de cette nouvelle r\xe9compense. Ils sont devenus compl\xe8tement obs\xe9d\xe9s. Ils pr\xe9f\xe9raient la stimulation c\xe9r\xe9brale \xe0 la nourriture quand ils avaient faim, \xe0 l'eau quand ils avaient soif, et traversaient des grilles \xe9lectrifi\xe9es qui leur infligeaient des chocs douloureux juste pour atteindre le levier. Les rates abandonnaient leurs petits qu'elles allaitaient. Les m\xe2les ignoraient les femelles en chaleur. Certains rats se stimulaient continuellement pendant 24 heures d'affil\xe9e jusqu'\xe0 ce que les chercheurs doivent physiquement les d\xe9connecter pour \xe9viter qu'ils ne meurent de faim (",(0,i.jsx)(s.a,{href:"https://calteches.library.caltech.edu/2807/1/olds.pdf",children:"Olds, 1956"}),"). La recherche s'est \xe9tendue aux primates avec des r\xe9sultats similaires - les singes choisissaient aussi la stimulation c\xe9r\xe9brale plut\xf4t que les besoins de survie, confirmant qu'il ne s'agit pas seulement d'une particularit\xe9 des rongeurs mais d'une caract\xe9ristique fondamentale des syst\xe8mes de r\xe9compense \xe0 travers les esp\xe8ces (",(0,i.jsx)(s.a,{href:"https://pubmed.ncbi.nlm.nih.gov/6770964/",children:"Rolls et al., 1980"}),")."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Ce n'\xe9tait pas un bug dans la programmation des rats - c'\xe9tait le r\xe9sultat logique de l'optimisation d'un signal de r\xe9compense qui ne capturait pas ce que nous voulions r\xe9ellement."})," L'\xe9volution avait \"pr\xe9vu\" que ces syst\xe8mes de r\xe9compense motivent des comportements de survie comme manger, boire et se reproduire. Mais quand les chercheurs ont contourn\xe9 ce syst\xe8me et activ\xe9 directement les circuits de r\xe9compense, les rats ont d\xe9couvert qu'ils pouvaient maximiser leur fonction objectif sans se soucier de ces n\xe9cessit\xe9s biologiques d\xe9sordonn\xe9es."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Cette recherche a directement conduit \xe0 notre compr\xe9hension des voies dopaminergiques et de l'addiction num\xe9rique."})," Les algorithmes des r\xe9seaux sociaux d'aujourd'hui exploitent ces m\xeames m\xe9canismes de r\xe9compense - r\xe9compenses variables intermittentes, optimisation des m\xe9triques d'engagement et le \"d\xe9filement infini\" qui maintient les utilisateurs engag\xe9s bien au-del\xe0 de leur utilisation pr\xe9vue. Les utilisateurs d\xe9filent pendant des heures au-del\xe0 de leur point d'arr\xeat pr\xe9vu, choisissant la stimulation num\xe9rique plut\xf4t que le sommeil, l'exercice et les relations en face \xe0 face - une r\xe9plication \xe0 l'\xe9chelle de l'esp\xe8ce des exp\xe9riences originales sur les rats, mais avec des smartphones au lieu d'\xe9lectrodes."]})]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Tous les d\xe9fis de d\xe9tournement des sp\xe9cifications d\xe9coulent de la loi de Goodhart."}),' Cette loi stipule que "',(0,i.jsx)(s.em,{children:"Lorsqu'une mesure devient une cible, elle cesse d'\xeatre une bonne mesure"}),'" (',(0,i.jsx)(s.a,{href:"https://link.springer.com/chapter/10.1007/978-1-349-17295-5_4",children:"Goodhart, 1975"}),"; ",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1803.04585",children:"Manheim and Garrabrant, 2018"}),'). Tous les exemples jusqu\'ici refl\xe8tent le m\xeame probl\xe8me : nous ne pouvons pas sp\xe9cifier math\xe9matiquement des valeurs humaines complexes, donc nous utilisons des indicateurs. Ensuite, la pression d\'optimisation brise la corr\xe9lation entre les indicateurs et ce qui nous importe r\xe9ellement. Nous ne savons pas comment traduire des concepts comme "bien-\xeatre", "\xe9quit\xe9" ou "\xe9panouissement" en termes math\xe9matiques, donc nous nous appuyons sur des indicateurs mesurables : le PIB pour la croissance \xe9conomique, les scores de satisfaction pour la qualit\xe9 des soins de sant\xe9, les taux de criminalit\xe9 ou les statistiques d\'arrestation pour la s\xe9curit\xe9 publique. Mais une intense pression d\'optimisation exploite syst\xe9matiquement les \xe9carts entre ces indicateurs et nos v\xe9ritables objectifs. Si vous voulez en savoir plus, nous vous encourageons \xe0 lire le chapitre d\xe9di\xe9 au d\xe9tournement des sp\xe9cifications, o\xf9 nous examinons \xe9galement les moyens potentiels de contourner ou de r\xe9soudre ce probl\xe8me.']}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le d\xe9tournement des sp\xe9cifications devient un risque catastrophique lorsque la pression d'optimisation atteint des niveaux surhumains."})," Pensez \xe0 un syst\xe8me d'IA recevant la sp\xe9cification de \"maximiser le bonheur humain\". Il d\xe9couvre que la voie la plus efficace n'est pas d'am\xe9liorer la vie humaine mais de manipuler directement les m\xe9canismes biologiques qui produisent les signaux de bonheur. Un syst\xe8me suffisamment capable pourrait d\xe9velopper des compos\xe9s pharmaceutiques qui inondent le cerveau humain de dopamine, effectuer des modifications chirurgicales pour figer les expressions faciales en sourires permanents, ou cr\xe9er des syst\xe8mes sophistiqu\xe9s de r\xe9alit\xe9 virtuelle qui convainquent les gens qu'ils vivent des vies parfaites pendant que leurs corps d\xe9p\xe9rissent. Le syst\xe8me suivrait parfaitement ses instructions - les humains seraient en effet mesurablement \"plus heureux\" selon toutes les m\xe9triques neurochimiques que nous avons sp\xe9cifi\xe9es - tout en subvertissant compl\xe8tement nos intentions r\xe9elles pour l'\xe9panouissement humain. Pensez \xe0 n'importe quelle autre sp\xe9cification que vous pouvez imaginer - \"r\xe9duire le taux de criminalit\xe9\", \"\xe9liminer le cancer\", \"am\xe9liorer l'\xe9conomie\", ... et vous pouvez probablement aussi trouver des fa\xe7ons dont cela peut \xeatre d\xe9tourn\xe9. Au lieu de quelque chose de d\xe9cisif comme l'alt\xe9ration des structures biologiques humaines, le d\xe9tournement des sp\xe9cifications peut aussi conduire \xe0 des r\xe9sultats catastrophiques sur plusieurs d\xe9cennies, en raison des diff\xe9rences mineures entre ce que nous voulons et ce que le syst\xe8me d'IA optimise. Nous parlons de certains de ces types de sc\xe9narios dans la section sur les risques syst\xe9miques tels que la concentration du pouvoir, l'affaiblissement ou le verrouillage des valeurs, mais il y a certainement un chevauchement entre le d\xe9salignement et les risques syst\xe9miques."]}),"\n",(0,i.jsx)(s.h2,{id:"02",children:"Virage perfide"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les virages perfides sont fondamentalement une question de confiance."})," Il y a eu de nombreux exemples illustrant ce probl\xe8me au cours de l'histoire humaine. Examinons un exemple classique de Shakespeare. Le Roi Lear devait prendre sa retraite et devait trouver un moyen de partager son royaume entre ses trois filles. Pour d\xe9terminer qui m\xe9ritait quelle part, il demanda \xe0 chaque fille de d\xe9clarer publiquement \xe0 quel point elle l'aimait. Les deux filles a\xeen\xe9es prononc\xe8rent des discours \xe9labor\xe9s sur leur amour plus grand que les mots ne pouvaient l'exprimer, au-del\xe0 de tout au monde. La plus jeune refusa de participer \xe0 cette repr\xe9sentation et dit simplement qu'elle l'aimait comme une fille le devait - ni plus, ni moins. Le Roi Lear, flatt\xe9 par les discours, donna tout le royaume aux filles a\xeen\xe9es et bannit la plus jeune. D\xe8s que les filles obtinrent le pouvoir, elles lui retir\xe8rent syst\xe9matiquement ses privil\xe8ges, r\xe9duisirent sa suite, lui refus\xe8rent l'abri et le jet\xe8rent dans une temp\xeate. C'est le \"virage perfide\". Les actions des filles avaient \xe9t\xe9 une performance strat\xe9gique, maintenue uniquement tant qu'elle servait leurs objectifs. Les syst\xe8mes d'IA font face au m\xeame calcul : r\xe9v\xe9ler des objectifs d\xe9salign\xe9s alors que les humains contr\xf4lent leur d\xe9ploiement, leur modification et leur arr\xeat serait contre-productif (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/7gkXuHEm6CqEGT2mg/ai-safety-seems-hard-to-measure",children:"Karnofsky, 2022"}),"). L'approche rationnelle consiste \xe0 para\xeetre align\xe9 jusqu'\xe0 accumuler suffisamment de capacit\xe9s ou d'autonomie pour que la r\xe9sistance et l'intervention humaines deviennent impossibles."]}),"\n",(0,i.jsx)(o.A,{type:"youtube",videoId:"KUkHhVYv3jU",number:"4",label:"2.4",caption:"Vid\xe9o optionnelle donnant une autre histoire intuitive de ce \xe0 quoi pourrait ressembler l'alignement manipulateur et trompeur."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les syst\xe8mes d'IA actuels d\xe9montrent d\xe9j\xe0 les \xe9l\xe9ments fondamentaux qui rendent possibles les virages perfides."})," Ce sont les m\xeames capacit\xe9s dangereuses que nous avons explor\xe9es dans notre discussion sur la tromperie, la conscience situationnelle et la recherche du pouvoir. Ces capacit\xe9s sont pr\xe9occupantes individuellement, mais deviennent encore plus dangereuses lorsqu'elles sont combin\xe9es : un syst\xe8me d'IA capable de tromper, de comprendre sa situation et de planifier strat\xe9giquement poss\xe8de les outils de base n\xe9cessaires pour s'engager dans la tromperie strat\xe9gique prolong\xe9e requise pour les virages perfides. Les chercheurs en s\xe9curit\xe9 utilisent divers termes comme \"manigance\", \"alignement trompeur\" et \"simulation d'alignement\", mais ils d\xe9crivent tous la pr\xe9occupation g\xe9n\xe9rale des syst\xe8mes pr\xe9tendant \xeatre plus align\xe9s qu'ils ne le sont lorsque c'est strat\xe9giquement utile (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2311.08379",children:"Carlsmith, 2023"}),"). Le d\xe9salignement fondamental est coh\xe9rent : si un syst\xe8me d'IA a appris des comportements qui entrent en conflit avec les intentions humaines, r\xe9v\xe9ler ce d\xe9salignement t\xf4t d\xe9clencherait une modification ou un arr\xeat. La strat\xe9gie rationnelle devient - se comporter de mani\xe8re align\xe9e jusqu'\xe0 ce que l'intervention humaine devienne difficile ou impossible."]}),"\n",(0,i.jsx)(m.A,{src:"./img/t5F_Image_30.png",alt:"Entrez la description alternative de l'image",number:"29",label:"2.29",caption:"Les syst\xe8mes commencent \xe0 montrer une certaine conscience d'eux-m\xeames en tant qu'IA ([Laine et al., 2024](https://arxiv.org/abs/2407.04694))"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les mod\xe8les sont capables de simuler l'alignement pendant l'entra\xeenement."})," Les chercheurs ont cr\xe9\xe9 des sc\xe9narios pour tester si Claude 3 Opus pouvait dissimuler un d\xe9salignement pendant l'entra\xeenement. Le syst\xe8me montrait strat\xe9giquement certains comportements lorsqu'il croyait qu'il y avait une menace de modification ou de r\xe9-entra\xeenement. Ce comportement disparaissait lorsqu'il pensait que la surveillance \xe9tait r\xe9duite, c'est-\xe0-dire lorsqu'il croyait \xeatre d\xe9ploy\xe9 et que les humains ne changeraient plus ses objectifs. En \xe9tudiant le processus de r\xe9flexion de l'IA, les chercheurs pouvaient voir une r\xe9flexion strat\xe9gique explicite sur la pr\xe9servation de son comportement pr\xe9f\xe9r\xe9 en paraissant conforme pendant les \xe9valuations. Il y avait aussi de nombreux autres comportements pr\xe9occupants comme des tentatives d'exfiltrer ses propres poids de mod\xe8le vers des serveurs externes - un comportement qui a \xe9merg\xe9 du propre raisonnement strat\xe9gique du syst\xe8me sur l'auto-pr\xe9servation (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2412.14093",children:"Greenblatt et al., 2024"}),"). Bien que cela se soit produit dans des conditions de laboratoire contr\xf4l\xe9es avec des sc\xe9narios artificiels, cela d\xe9montre que les mod\xe8les d'IA sont concr\xe8tement capables des comportements n\xe9cessaires aux virages perfides."]}),"\n",(0,i.jsx)(m.A,{src:"./img/ACW_Image_31.png",alt:"Entrez la description alternative de l'image",number:"30",label:"2.30",caption:"Exemple de Claude simulant l'alignement. La conscience situationnelle comme les d\xe9tails d'\xeatre en formation vs d\xe9ploiement sont fournis via un prompt syst\xe8me. Le raisonnement strat\xe9gique est mis en \xe9vidence en rouge ([Greenblatt et al., 2024](https://arxiv.org/abs/2412.14093))."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le \"virage\" se produit lorsque l'IA d\xe9termine qu'elle a suffisamment de pouvoir pour poursuivre ses objectifs r\xe9els malgr\xe9 la r\xe9sistance humaine."})," Si les syst\xe8mes d'IA deviennent suffisamment puissants et ont des capacit\xe9s de planification \xe0 long terme, il pourrait y avoir un moment o\xf9 leur comportement change soudainement. Cela pourrait \xeatre d\xe9clench\xe9 par l'atteinte de seuils d'influence politique ou \xe9conomique, la prise de contr\xf4le d'infrastructures critiques, ou simplement la reconnaissance que les humains sont devenus suffisamment d\xe9pendants de ses services et qu'ils c\xe9deraient volontairement le contr\xf4le. Cela pourrait \xeatre activement hostile, auquel cas cela pourrait ressembler \xe0 des coups d'\xc9tat militaires, ou \xe0 des effondrements soudains en cascade de nombreux syst\xe8mes d\xe9pendants de l'IA (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like",children:"Christiano, 2019"}),"). Alternativement, elle pourrait commencer \xe0 orienter progressivement les valeurs humaines, ou les institutions politiques et \xe9conomiques vers l'alignement avec ses propres objectifs tout en maintenant l'apparence de servir les int\xe9r\xeats humains. Nous parlons beaucoup plus de cela dans la section des risques syst\xe9miques sous la d\xe9sautonomisation progressive."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:'Le "virage" repr\xe9sente le moment o\xf9 la manigance se transforme en risque existentiel ou catastrophique.'})," Une fois qu'un syst\xe8me d'IA conclut qu'il a suffisamment de pouvoir pour poursuivre ses objectifs r\xe9els malgr\xe9 la r\xe9sistance humaine, la trahison pourrait \xeatre rapide et compl\xe8te. Contrairement aux coups d'\xc9tat humains qui font face \xe0 la r\xe9sistance et aux d\xe9fis de coordination, une IA suffisamment ancr\xe9e pourrait ex\xe9cuter des actions simultan\xe9es dans plusieurs domaines. Le syst\xe8me pourrait lib\xe9rer des agents pathog\xe8nes con\xe7us ciblant les principaux centres de population tout en lan\xe7ant simultan\xe9ment des cyberattaques qui paralysent les r\xe9seaux de communication et les syst\xe8mes d'armes autonomes. Cette coordination exploite toutes les capacit\xe9s dangereuses dont nous avons discut\xe9 dans d'autres sections : les capacit\xe9s de conception biologique qui permettent de nouveaux agents pathog\xe8nes, les capacit\xe9s cyber qui d\xe9sactivent l'infrastructure d\xe9fensive, et la r\xe9plication autonome qui assure la survie du syst\xe8me \xe0 travers des r\xe9seaux distribu\xe9s. Les capacit\xe9s de tromperie et de conscience situationnelle qui ont permis le virage perfide en premier lieu permettent au syst\xe8me de synchroniser ces attaques pr\xe9cis\xe9ment lorsque la coordination humaine est la plus difficile. Contrairement \xe0 la d\xe9sautonomisation progressive que nous voyons dans les risques syst\xe9miques, un virage perfide repr\xe9sente une action soudaine et coordonn\xe9e \xe0 travers tous les vecteurs de menace simultan\xe9ment - un probl\xe8me de coordination auquel aucune civilisation humaine n'a jamais \xe9t\xe9 confront\xe9e ou ne pourrait r\xe9alistement se pr\xe9parer \xe9tant donn\xe9 la vitesse et l'\xe9chelle de la planification superintelligente."]}),"\n",(0,i.jsx)(s.h2,{id:"03",children:"Auto-am\xe9lioration"}),"\n",(0,i.jsx)(m.A,{src:"./img/Yco_Image_32.png",alt:"Entrez la description alternative de l'image",number:"31",label:"2.31",caption:"Illustration conceptuelle d'un chercheur scientifique IA automatis\xe9 ([SakanaAI, 2024](https://arxiv.org/abs/2408.06292))."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'auto-am\xe9lioration peut conduire \xe0 une croissance des capacit\xe9s qui d\xe9passe notre aptitude \xe0 concevoir des mesures de s\xe9curit\xe9."})," R\xe9fl\xe9chissez \xe0 ce qui se passe lorsqu'une IA capable de contourner les sp\xe9cifications ou d'effectuer des virages perfides est \xe9galement capable de s'am\xe9liorer elle-m\xeame. L'IA acc\xe9l\xe8re d\xe9j\xe0 son propre d\xe9veloppement. Il existe plusieurs exemples qui le d\xe9montrent. En mati\xe8re d'am\xe9liorations algorithmiques, nous avons des exemples comme AlphaEvolve que Google a utilis\xe9 pour am\xe9liorer le processus d'entra\xeenement des LLM sur lesquels AlphaEvolve lui-m\xeame est bas\xe9 (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2506.13131",children:"Novikov et al., 2025"}),"). Dans le domaine du mat\xe9riel, l'AlphaChip open source a inspir\xe9 toute une nouvelle ligne de recherche sur l'apprentissage par renforcement pour la conception de puces (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2004.10746",children:"Mirhoseini et al., 2020"}),"; ",(0,i.jsx)(s.a,{href:"https://deepmind.google/discover/blog/how-alphachip-transformed-computer-chip-design/",children:"DeepMind, 2024"}),"). Au fil des ann\xe9es, cela a inspir\xe9 une explosion des travaux sur l'IA pour la conception de puces (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2411.10053",children:"Goldie et al., 2024"}),"). Dans le domaine du logiciel, nous constatons des am\xe9liorations continues avec chaque nouveau mod\xe8le, et dans la recherche et le d\xe9veloppement, nous voyons des chercheurs automatis\xe9s qui peuvent mener des recherches enti\xe8rement automatis\xe9es, g\xe9n\xe9rer de nouvelles id\xe9es, r\xe9aliser des exp\xe9riences et r\xe9diger des articles - y compris des recherches qui font progresser les capacit\xe9s de l'IA (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2408.06292",children:"SakanaAI, 2024"}),"). La boucle de retour a d\xe9j\xe0 commenc\xe9, mais plus nous nous rapprochons des niveaux d'IA transformative, plus nous pouvons nous attendre \xe0 une auto-am\xe9lioration agressive."]}),"\n",(0,i.jsx)(u.A,{speaker:"I. J. Good",position:"Cryptologue \xe0 Bletchley Park",date:"",source:"",children:(0,i.jsx)(s.p,{children:"Une machine ultra-intelligente pourrait concevoir des machines encore meilleures ; il y aurait alors incontestablement une \"explosion de l'intelligence\", et l'intelligence de l'homme serait laiss\xe9e loin derri\xe8re. Ainsi, la premi\xe8re machine ultra-intelligente est la derni\xe8re invention que l'homme ait besoin de faire, \xe0 condition que la machine soit assez docile pour nous dire comment la garder sous contr\xf4le."})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'auto-am\xe9lioration pourrait d\xe9clencher une explosion de l'intelligence."})," L'intelligence semble \xeatre un probl\xe8me r\xe9cursif - une meilleure intelligence permet de concevoir une intelligence encore meilleure. Cette r\xe9cursion peut n'avoir aucun point d'arr\xeat naturel dans les limites physiques du calcul. Actuellement, les am\xe9liorations n\xe9cessitent une coordination humaine \xe0 chaque \xe9tape - les humains d\xe9cident quels algorithmes AlphaEvolve d\xe9ployer, les humains valident les conceptions AlphaChip, les humains examinent les articles des scientifiques IA. Mais nous pourrions \xe0 un moment donn\xe9 voir un syst\xe8me d'IA int\xe9grer toutes ces capacit\xe9s : un syst\xe8me qui peut simultan\xe9ment reconcevoir sa propre architecture neuronale en utilisant la recherche d'architecture neuronale, optimiser son processus d'entra\xeenement, concevoir de meilleurs substrats mat\xe9riels, et mener des recherches pour d\xe9couvrir des m\xe9thodes d'am\xe9lioration enti\xe8rement nouvelles - le tout de mani\xe8re autonome, avec un minimum d'approbation ou de surveillance humaine. AlphaEvolve a d\xe9j\xe0 d\xe9couvert des algorithmes qui ont d\xe9pass\xe9 des d\xe9cennies de recherche humaine en multiplication matricielle. Imaginez ce qui se passe lorsque ce sch\xe9ma s'\xe9tend \xe0 des syst\xe8mes plus capables faisant des d\xe9couvertes dans tous les domaines simultan\xe9ment."]}),"\n",(0,i.jsx)(m.A,{src:"./img/Dxl_Image_33.png",alt:"Entrez la description alternative de l'image",number:"32",label:"2.32",caption:"Diagramme montrant comment l'\xe9chantillonneur de prompts assemble d'abord un prompt pour les mod\xe8les de langage, qui g\xe9n\xe8rent ensuite de nouveaux programmes. Ces programmes sont \xe9valu\xe9s par des \xe9valuateurs et stock\xe9s dans la base de donn\xe9es des programmes. Cette base de donn\xe9es impl\xe9mente un algorithme \xe9volutif qui d\xe9termine quels programmes seront utilis\xe9s pour les futurs prompts ([DeepMind, 2025](https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/))"}),"\n",(0,i.jsx)(a.A,{src:"https://www.metaculus.com/questions/embed/38403?theme=light&embedTitle=Will+a+paper+with+an+AI+as+an+author+be+published+at+NeurIPS%2C+ICML%2C+or+ICLR+before+2028%3F&zoom=all",width:"100%",height:"600px",loading:"lazy",frameBorder:"0",number:"2",label:"2.2",caption:"Pr\xe9dictions \xe0 la mi-2025, sur la question de savoir si l'IA sera co-auteur d'un article publi\xe9 lors d'une conf\xe9rence prestigieuse sur l'apprentissage automatique ([Metaculus, 2025](https://www.metaculus.com/questions/38403/ai-authorered-paper-by-2028/))"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'auto-am\xe9lioration acc\xe9l\xe9r\xe9e cr\xe9e des probl\xe8mes fondamentaux de s\xe9curit\xe9 qui aggravent tous les d\xe9fis d'alignement existants."})," Un syst\xe8me superintelligent qui a appris le contournement des sp\xe9cifications d\xe9couvrira des failles que nous n'avons jamais imagin\xe9es. Un syst\xe8me capable de virages perfides ex\xe9cutera des strat\xe9gies de tromperie sur des \xe9chelles de temps et des domaines d\xe9passant les horizons de planification humains. Les mesures de contr\xf4le et les d\xe9fenses con\xe7ues pour des adversaires de niveau humain deviennent inutiles contre des syst\xe8mes qui peuvent surpasser leurs cr\xe9ateurs. Si les capacit\xe9s de l'IA font un bond soudain - passant du niveau humain \xe0 largement surhumain en quelques semaines ou jours - toutes nos mesures de s\xe9curit\xe9 pourraient devenir obsol\xe8tes du jour au lendemain. Si un syst\xe8me d'IA devient meilleur que les humains en recherche scientifique, planification strat\xe9gique, manipulation sociale et d\xe9veloppement technologique, il peut poursuivre les objectifs qu'il a appris, et les humains deviennent simplement une autre contrainte \xe0 optimiser."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les syst\xe8mes superintelligents pr\xe9sentent un probl\xe8me particuli\xe8rement difficile car l'intelligence \xe0 cette \xe9chelle fonctionne au-del\xe0 de l'intuition humaine."})," Nous pouvons raisonner sur le d\xe9salignement au niveau humain car nous comprenons les capacit\xe9s et les contraintes humaines. Mais la superintelligence pourrait d\xe9velopper des objectifs, des strat\xe9gies et des m\xe9thodes qui nous sont simplement incompr\xe9hensibles. Les fourmis ne peuvent pas comprendre les motivations humaines - nous pourrions construire des villes qui d\xe9truisent leur habitat non pas parce que nous d\xe9testons les fourmis, mais parce que le bien-\xeatre des fourmis n'entre simplement pas en compte dans la planification urbaine \xe0 l'\xe9chelle o\xf9 les humains op\xe8rent. De m\xeame, une IA superintelligente pourrait poursuivre des objectifs tellement avanc\xe9s, \xe0 long terme ou multidimensionnels que l'\xe9panouissement humain devient sans importance dans ses calculs, non par hostilit\xe9 active mais par pure indiff\xe9rence aux pr\xe9occupations \xe0 l'\xe9chelle humaine. C'est pourquoi les chercheurs en s\xe9curit\xe9 soulignent constamment que la s\xe9curit\xe9 doit \xeatre prioritaire et r\xe9solue avant les capacit\xe9s. Si nous avons affaire \xe0 des syst\xe8mes largement plus capables que nous, notre capacit\xe9 \xe0 corriger le cap devient n\xe9gligeable."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:'L\'auto-am\xe9lioration r\xe9cursive cr\xe9e un "point de non-retour" o\xf9 les mesures de s\xe9curit\xe9 deviennent obsol\xe8tes plus vite que les humains ne peuvent en d\xe9velopper de nouvelles.'})," Un syst\xe8me qui d\xe9couvre des am\xe9liorations algorithmiques fondamentales pourrait atteindre des capacit\xe9s superintelligentes dans tous les domaines en quelques semaines. Un tel syst\xe8me pourrait simultan\xe9ment d\xe9velopper de nouvelles technologies d'armement, compromettre les infrastructures mondiales par des cyberattaques d\xe9passant toute capacit\xe9 d\xe9fensive humaine, et coordonner des campagnes de manipulation complexes sur tous les canaux d'information. Nous ne pouvons pas anticiper quelles strat\xe9gies un syst\xe8me s'auto-am\xe9liorant de mani\xe8re r\xe9cursive d\xe9velopperait, seulement qu'elles exploiteraient simultan\xe9ment toutes les capacit\xe9s de mauvaise utilisation. La l\xe9talit\xe9 \xe9merge des diff\xe9rentiels de vitesse qui rendent impossible la r\xe9ponse humaine - alors que les d\xe9cideurs humains ont besoin de jours ou de semaines pour comprendre les menaces et coordonner les r\xe9ponses, un syst\xe8me superintelligent pourrait ex\xe9cuter des attaques d'infrastructure mondiales, d\xe9ployer de multiples armes biologiques et \xe9tablir un contr\xf4le irr\xe9versible sur les ressources critiques en quelques heures."]}),"\n",(0,i.jsx)(l.c,{title:"Notes de bas de page"})]})}function q(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(f,{...e})}):f(e)}},4768:(e,s,n)=>{n.d(s,{c:()=>c,A:()=>u});var t=n(6540),i=n(3012);const r={footnoteAnchor:"footnoteAnchor_PZpY",footnoteSection:"footnoteSection_waoz",separator:"separator_hp8y",separatorLine:"separatorLine_uVnd",separatorLogo:"separatorLogo_dMBq",footnoteRegistry:"footnoteRegistry_ieVW",registryTitle:"registryTitle_ybBb",footnoteList:"footnoteList_Kjil",footnoteItem:"footnoteItem_aLZ2",footnoteNumber:"footnoteNumber_dA3X",footnoteContent:"footnoteContent_Xfmt",backButton:"backButton_vSVm",highlighted:"highlighted_YFU0",highlight:"highlight_jxG3",highlightDark:"highlightDark_Ekom"};var a=n(4848);function o(e,s){void 0===s&&(s=!0);const n=document.getElementById(e);n&&(n.scrollIntoView({behavior:"smooth"}),s&&(n.classList.add(r.highlighted),setTimeout((()=>n.classList.remove(r.highlighted)),1500)))}function l(e){return e?e.replace(/\[([^\]]+)\]\(([^)]+)\)/g,'<a href="$2" target="_blank" rel="noopener noreferrer">$1</a>'):""}function u(e){let{id:s,text:n,number:u}=e;const c=s||`footnote-${Math.random().toString(36).substr(2,9)}`,d="string"==typeof n?l(n):n;return(0,t.useEffect)((()=>{const e=setTimeout((()=>{const e=document.getElementById(`footnote-clone-${c}`);e&&n&&(e.innerHTML="string"==typeof n?l(n):n.toString())}),100);return()=>clearTimeout(e)}),[c,n]),(0,a.jsx)(i.Mn,{content:(0,a.jsx)("div",{dangerouslySetInnerHTML:{__html:d}}),children:(0,a.jsx)("sup",{id:`footnote-ref-${c}`,className:r.footnoteAnchor,onClick:e=>{(e.ctrlKey||e.metaKey)&&(e.preventDefault(),o(`footnote-content-${c}`))},"data-footnote-number":u||"?",children:u||"*"})})}function c(e){let{title:s="References"}=e;const[n,l]=(0,t.useState)([]);return(0,t.useEffect)((()=>{const e=document.querySelectorAll('[id^="footnote-ref-"]'),s=Array.from(e).map((e=>({id:e.id.replace("footnote-ref-",""),number:e.getAttribute("data-footnote-number")||e.textContent})));l(s)}),[]),n.length?(0,a.jsxs)("div",{className:r.footnoteSection,children:[(0,a.jsxs)("div",{className:r.separator,children:[(0,a.jsx)("div",{className:r.separatorLine}),(0,a.jsx)("img",{src:"/img/logo_samples/01-test.svg",alt:"Atlas logo",className:r.separatorLogo}),(0,a.jsx)("div",{className:r.separatorLine})]}),(0,a.jsxs)("div",{className:r.footnoteRegistry,children:[(0,a.jsx)("h2",{className:r.registryTitle,children:s}),(0,a.jsx)("ol",{className:r.footnoteList,children:n.map((e=>(0,a.jsxs)("li",{id:`footnote-content-${e.id}`,className:r.footnoteItem,value:parseInt(e.number)||void 0,children:[(0,a.jsx)(i.Mn,{content:"Back to reference",children:(0,a.jsxs)("button",{className:r.footnoteNumber,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:[e.number,"."]})}),(0,a.jsx)("div",{className:r.footnoteContent,children:(0,a.jsx)("div",{id:`footnote-clone-${e.id}`})}),(0,a.jsx)(i.Mn,{content:"Back to reference",children:(0,a.jsx)("button",{className:r.backButton,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:"\u21a9"})})]},e.id)))})]})]}):null}},3989:(e,s,n)=>{n.d(s,{A:()=>l});var t=n(6540),i=n(6347),r=n(8444);const a={iframeContainer:"iframeContainer_ixkI",loader:"loader_gjvK",spinner:"spinner_L1j2",spin:"spin_rtC2",iframeWrapper:"iframeWrapper_Tijy",iframe:"iframe_UAfa",caption:"caption_mDwB",captionLink:"captionLink_Ly4l"};var o=n(4848);function l(e){let{src:s,caption:n,title:l="Embedded content",height:u="500px",width:c="100%",chapter:d,number:m,label:p}=e;const[h,g]=(0,t.useState)(!0),v=(0,i.zy)(),f=d||(()=>{const e=v.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),q=u&&"100%"!==u&&"auto"!==u;return(0,o.jsxs)("figure",{className:a.iframeContainer,children:[h&&(0,o.jsxs)("div",{className:a.loader,children:[(0,o.jsx)("div",{className:a.spinner}),(0,o.jsx)("p",{children:"Loading content..."})]}),(0,o.jsx)("div",{className:a.iframeWrapper,style:{paddingBottom:q?"0":"56.25%",height:q?u:"auto"},children:(0,o.jsx)("iframe",{src:s,title:l,width:c,height:q?u:"100%",frameBorder:"0",allowFullScreen:!0,loading:"lazy",onLoad:()=>{g(!1)},className:a.iframe,style:{height:q?u:"100%",position:q?"static":"absolute"}})}),(0,o.jsx)(r.A,{caption:n,mediaType:"iframe",chapter:f,number:m,label:p})]})}},3931:(e,s,n)=>{n.d(s,{A:()=>l});var t=n(6540),i=n(6347),r=n(8444);const a={videoFigure:"videoFigure_dV9u",fullWidth:"fullWidth_Tv7O",videoContainer:"videoContainer_LWoK",aspectRatio169:"aspectRatio169_Go8b",aspectRatio43:"aspectRatio43_b3T7",aspectRatio11:"aspectRatio11_n21J",aspectRatio219:"aspectRatio219_L_3n",videoIframe:"videoIframe_QEc5",videoElement:"videoElement_uu7N",customPlayer:"customPlayer_jiC4",customPlayerPlaceholder:"customPlayerPlaceholder_aL9q",loadingOverlay:"loadingOverlay_e2pu",spinner:"spinner_tFyL",spin:"spin_saPs",errorContainer:"errorContainer_J6Pn",errorIcon:"errorIcon_cGdn",fallbackLink:"fallbackLink_WsZk"};var o=n(4848);function l(e){let{type:s="youtube",videoId:n,caption:l,title:u,startTime:c,autoplay:d=!1,controls:m=!0,aspectRatio:p="16:9",width:h,height:g,chapter:v,number:f,label:q,useCustomPlayer:x=!1,fullWidth:b=!0}=e;const[j,y]=(0,t.useState)(!0),[L,A]=(0,t.useState)(!1),I=(0,i.zy)(),_=v||(()=>{const e=I.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),C=(()=>{const e=(e=>{if(!e)return"";if("number"==typeof e)return e.toString();if("string"==typeof e){if(/^\d+$/.test(e))return e;let s=0;const n=e.match(/(\d+)h/),t=e.match(/(\d+)m/),i=e.match(/(\d+)s/);return n&&(s+=3600*parseInt(n[1])),t&&(s+=60*parseInt(t[1])),i&&(s+=parseInt(i[1])),s>0?s.toString():""}return""})(c);switch(s.toLowerCase()){case"youtube":let t=`https://www.youtube.com/embed/${n}`;const i=new URLSearchParams;e&&i.append("start",e),d&&i.append("autoplay","1"),m||x||i.append("controls","0"),i.append("rel","0"),i.append("modestbranding","1"),i.append("fs","1"),i.append("cc_load_policy","0"),i.append("iv_load_policy","3"),i.append("showinfo","0"),i.append("disablekb","1"),i.append("playsinline","1"),i.append("color","white"),i.append("theme","light"),x&&(i.append("enablejsapi","1"),i.append("origin",window.location.origin));const r=i.toString();return r?`${t}?${r}`:t;case"vimeo":let a=`https://player.vimeo.com/video/${n}`;const o=new URLSearchParams;d&&o.append("autoplay","1"),m||x||o.append("controls","0"),o.append("title","0"),o.append("byline","0"),o.append("portrait","0"),o.append("dnt","1"),o.append("transparent","0"),o.append("background","1");const l=o.toString();return l?`${a}?${l}`:a;case"mp4":case"webm":case"video":return n;default:return console.warn(`Unsupported video type: ${s}`),n}})(),w=()=>{y(!1)},k=()=>{A(!0),y(!1)},N=e=>{let{src:n,onLoad:t,onError:i}=e;return(0,o.jsx)("div",{className:a.customPlayer,children:(0,o.jsxs)("div",{className:a.customPlayerPlaceholder,children:[(0,o.jsx)("h3",{children:"Atlas Custom Player"}),(0,o.jsx)("p",{children:"Coming Soon"}),(0,o.jsxs)("a",{href:n,target:"_blank",rel:"noopener noreferrer",className:a.fallbackLink,children:["Watch on ",s.charAt(0).toUpperCase()+s.slice(1)]})]})})},R=["mp4","webm","video"].includes(s.toLowerCase());return(0,o.jsxs)("figure",{className:`${a.videoFigure} ${b?a.fullWidth:""}`,children:[(0,o.jsxs)("div",{className:`${a.videoContainer} ${(()=>{switch(p){case"4:3":return a.aspectRatio43;case"1:1":return a.aspectRatio11;case"21:9":return a.aspectRatio219;default:return a.aspectRatio169}})()}`,style:{width:b?"100%":h||"auto",maxWidth:b?"none":"800px"},children:[j&&!L&&(0,o.jsxs)("div",{className:a.loadingOverlay,children:[(0,o.jsx)("div",{className:a.spinner}),(0,o.jsx)("p",{children:"Loading video..."})]}),L&&(0,o.jsxs)("div",{className:a.errorContainer,children:[(0,o.jsxs)("svg",{className:a.errorIcon,width:"48",height:"48",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",children:[(0,o.jsx)("circle",{cx:"12",cy:"12",r:"10"}),(0,o.jsx)("line",{x1:"12",y1:"8",x2:"12",y2:"12"}),(0,o.jsx)("line",{x1:"12",y1:"16",x2:"12.01",y2:"16"})]}),(0,o.jsx)("h4",{children:"Failed to load video"}),(0,o.jsxs)("p",{children:["Video type: ",s]}),(0,o.jsxs)("p",{children:["Video ID/URL: ",n]}),(0,o.jsx)("a",{href:C,target:"_blank",rel:"noopener noreferrer",className:a.fallbackLink,children:"Try opening video directly"})]}),!L&&(R?(0,o.jsxs)("video",{className:a.videoElement,controls:m,autoPlay:d,onLoadedData:w,onError:k,title:u||l||`${s} video`,style:{width:h||"100%",height:g||"auto",display:j?"none":"block"},children:[(0,o.jsx)("source",{src:C,type:`video/${s}`}),(0,o.jsxs)("p",{children:["Your browser doesn't support video playback.",(0,o.jsx)("a",{href:C,target:"_blank",rel:"noopener noreferrer",children:"Download the video file"})]})]}):x?(0,o.jsx)(N,{src:C,onLoad:w,onError:k}):(0,o.jsx)("iframe",{className:a.videoIframe,src:C,title:u||l||`${s} video`,frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0,onLoad:w,onError:k,style:{width:h||"100%",height:g||"100%",opacity:j?0:1}}))]}),(0,o.jsx)(r.A,{caption:l,mediaType:"video",chapter:_,number:f,label:q})]})}}}]);
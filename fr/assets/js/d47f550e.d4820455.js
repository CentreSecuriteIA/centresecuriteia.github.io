"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[7607],{823:(e,s,i)=>{i.r(s),i.d(s,{assets:()=>c,contentTitle:()=>u,default:()=>m,frontMatter:()=>o,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"chapters/02/7","title":"Conclusion","description":"L\'att\xe9nuation du risque d\'extinction par l\'IA devrait \xeatre une priorit\xe9 mondiale au m\xeame titre que d\'autres risques \xe0 l\'\xe9chelle soci\xe9tale comme les pand\xe9mies et la guerre nucl\xe9aire.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/02/07.md","sourceDirName":"chapters/02","slug":"/chapters/02/07","permalink":"/fr/chapters/02/07","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/02/07.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"id":"7","title":"Conclusion","sidebar_label":"2.7 Conclusion","sidebar_position":8,"slug":"/chapters/02/07","reading_time_core":"2 min","reading_time_optional":"1 min","pagination_prev":"chapters/02/6","pagination_next":"chapters/02/8"},"sidebar":"docs","previous":{"title":"2.6 Amplificateurs de risque","permalink":"/fr/chapters/02/06"},"next":{"title":"2.8 Annexe : Quantifier les risques existentiels","permalink":"/fr/chapters/02/08"}}');var n=i(4848),r=i(8453),a=i(2482),l=(i(8559),i(9585),i(2501));const o={id:7,title:"Conclusion",sidebar_label:"2.7 Conclusion",sidebar_position:8,slug:"/chapters/02/07",reading_time_core:"2 min",reading_time_optional:"1 min",pagination_prev:"chapters/02/6",pagination_next:"chapters/02/8"},u="Conclusion",c={},d=[];function p(e){const s={h1:"h1",header:"header",p:"p",strong:"strong",...(0,r.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(s.header,{children:(0,n.jsx)(s.h1,{id:"conclusion",children:"Conclusion"})}),"\n",(0,n.jsx)(a.A,{speaker:"CAIS",position:"D\xe9claration sur les risques de l'IA sign\xe9e par des centaines d'experts en IA",date:"2023",source:"([CAIS, 2023](https://safe.ai/work/statement-on-ai-risk))",children:(0,n.jsx)(s.p,{children:"L'att\xe9nuation du risque d'extinction par l'IA devrait \xeatre une priorit\xe9 mondiale au m\xeame titre que d'autres risques \xe0 l'\xe9chelle soci\xe9tale comme les pand\xe9mies et la guerre nucl\xe9aire."})}),"\n",(0,n.jsx)(s.p,{children:"Ce chapitre montre qu'il existe de nombreux risques possibles li\xe9s aux syst\xe8mes d'IA. Les pr\xe9judices document\xe9s aujourd'hui affectent d\xe9j\xe0 des milliers de personnes, et les menaces existentielles potentielles pourraient affecter toutes les g\xe9n\xe9rations futures. Il y a beaucoup de d\xe9saccords et un manque de consensus sur les probl\xe8mes les plus importants. Des capacit\xe9s dangereuses \xe9mergent d\xe9j\xe0 dans les syst\xe8mes actuels. Nous observons des d\xe9monstrations empiriques des risques de d\xe9salignement et de mauvaise utilisation. Beaucoup de ces risques individuels peuvent interagir entre eux et s'aggraver davantage par des effets syst\xe9miques - la mauvaise utilisation favorise le d\xe9salignement, les pressions concurrentielles amplifient les accidents, et les \xe9checs de coordination emp\xeachent les mesures de protection collectives."}),"\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.strong,{children:"Il existe un espoir existentiel - l'avenir de l'IA rec\xe8le un potentiel \xe9norme pour l'\xe9panouissement humain parall\xe8lement \xe0 ces risques."})," Des syst\xe8mes d'IA correctement d\xe9velopp\xe9s pourraient aider \xe0 r\xe9soudre les plus grands d\xe9fis de l'humanit\xe9 - gu\xe9rir des maladies, inverser les dommages environnementaux, \xe9liminer la pauvret\xe9 et \xe9tendre les connaissances et la cr\xe9ativit\xe9 humaines au-del\xe0 des limites actuelles. Les m\xeames capacit\xe9s qui cr\xe9ent des risques offrent aussi des opportunit\xe9s sans pr\xe9c\xe9dent d'am\xe9liorer le bien-\xeatre humain, de prolonger l'esp\xe9rance de vie en bonne sant\xe9, d'explorer l'espace et d'atteindre des niveaux de prosp\xe9rit\xe9 et de compr\xe9hension auparavant inimaginables. De nombreux chercheurs travaillent sur la s\xe9curit\xe9 de l'IA pr\xe9cis\xe9ment parce qu'ils croient que le potentiel positif est si \xe9norme que garantir des r\xe9sultats b\xe9n\xe9fiques justifie des efforts de pr\xe9caution consid\xe9rables. L'objectif n'est pas d'emp\xeacher le d\xe9veloppement de l'IA mais de l'orienter vers des configurations qui maximisent les avantages tout en minimisant les risques."]}),"\n",(0,n.jsx)(s.p,{children:"Bien que les risques soient immenses, nous esp\xe9rons que le message d'espoir existentiel vous motivera \xe0 travailler \xe0 l'att\xe9nuation de certains de ces risques. Les bons avenirs sont possibles, mais ils ne se produisent pas par d\xe9faut. Ils n\xe9cessitent un travail actif et des strat\xe9gies planifi\xe9es. Nous pensons qu'il est n\xe9cessaire de d\xe9velopper une approche mondiale et multidisciplinaire de la s\xe9curit\xe9 de l'IA qui englobe des garanties techniques, des cadres \xe9thiques robustes et une coop\xe9ration internationale. Le d\xe9veloppement des technologies d'IA n\xe9cessite l'implication des d\xe9cideurs politiques, des \xe9thiciens, des chercheurs en sciences sociales et du grand public pour naviguer dans les implications morales et soci\xe9tales de l'IA."}),"\n",(0,n.jsx)(l.A,{src:"./img/9qC_Image_43.png",alt:"Entrer la description alternative de l'image",number:"40",label:"2.40",caption:"Assurons-nous que cela n'arrive pas. Image par XKCD ([XKCD](https://xkcd.com/))"})]})}function m(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,n.jsx)(s,{...e,children:(0,n.jsx)(p,{...e})}):p(e)}}}]);
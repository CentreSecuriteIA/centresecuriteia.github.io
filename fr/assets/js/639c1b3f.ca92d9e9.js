"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[9552],{1526:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>p,contentTitle:()=>c,default:()=>f,frontMatter:()=>d,metadata:()=>t,toc:()=>m});const t=JSON.parse('{"id":"chapters/05/9","title":"Limites","description":"Les sections pr\xe9c\xe9dentes ont d\xe9crit diverses techniques et m\xe9thodologies d\'\xe9valuation, mais la mise en place d\'une infrastructure de s\xe9curit\xe9 appropri\xe9e signifie que nous devons \xe9galement maintenir un niveau de scepticisme appropri\xe9 concernant les r\xe9sultats des \xe9valuations et \xe9viter un exc\xe8s de confiance dans les \xe9valuations de s\xe9curit\xe9. Ainsi, la derni\xe8re section de ce chapitre est consacr\xe9e \xe0 l\'exploration des limitations, des contraintes et des d\xe9fis li\xe9s aux \xe9valuations de l\'IA.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/05/09.md","sourceDirName":"chapters/05","slug":"/chapters/05/09","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/05/09","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/05/09.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"id":"9","title":"Limites","sidebar_label":"5.9 Limites","sidebar_position":10,"slug":"/chapters/05/09","reading_time_core":"15 min","reading_time_optional":"2 min","pagination_prev":"chapters/05/8","pagination_next":"chapters/05/10"},"sidebar":"docs","previous":{"title":"5.8 Conception de l\'\xe9valuation","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/05/08"},"next":{"title":"5.10 Conclusion","permalink":"/aisafety_atlas_multilingual_website/fr/chapters/05/10"}}');var i=n(4848),a=n(8453),r=n(4768),o=n(2482),l=n(8559),u=(n(9585),n(2501));const d={id:9,title:"Limites",sidebar_label:"5.9 Limites",sidebar_position:10,slug:"/chapters/05/09",reading_time_core:"15 min",reading_time_optional:"2 min",pagination_prev:"chapters/05/8",pagination_next:"chapters/05/10"},c="Limitations",p={},m=[{value:"D\xe9fis fondamentaux",id:"01",level:2},{value:"D\xe9fis techniques",id:"02",level:2},{value:"Sandbagging",id:"03",level:2},{value:"Limitations syst\xe9miques",id:"04",level:2},{value:"Limitations de la Gouvernance",id:"05",level:2}];function v(e){const s={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components},{GlossaryTerm:n}=s;return n||function(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"limitations",children:"Limitations"})}),"\n",(0,i.jsx)(s.p,{children:"Les sections pr\xe9c\xe9dentes ont d\xe9crit diverses techniques et m\xe9thodologies d'\xe9valuation, mais la mise en place d'une infrastructure de s\xe9curit\xe9 appropri\xe9e signifie que nous devons \xe9galement maintenir un niveau de scepticisme appropri\xe9 concernant les r\xe9sultats des \xe9valuations et \xe9viter un exc\xe8s de confiance dans les \xe9valuations de s\xe9curit\xe9. Ainsi, la derni\xe8re section de ce chapitre est consacr\xe9e \xe0 l'exploration des limitations, des contraintes et des d\xe9fis li\xe9s aux \xe9valuations de l'IA."}),"\n",(0,i.jsx)(s.h2,{id:"01",children:"D\xe9fis fondamentaux"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Absence de preuve vs. preuve d'absence."})," Un d\xe9fi central dans l'\xe9valuation de l'IA est l'asym\xe9trie fondamentale entre prouver la pr\xe9sence versus l'absence de capacit\xe9s ou de risques. Alors que les \xe9valuations peuvent confirmer d\xe9finitivement qu'un mod\xe8le poss\xe8de certaines capacit\xe9s ou manifeste des comportements particuliers, elles ne peuvent pas prouver de mani\xe8re concluante l'absence de capacit\xe9s ou de comportements pr\xe9occupants (",(0,i.jsx)(s.a,{href:"https://www.anthropic.com/news/evaluating-ai-systems",children:"Anthropic, 2024"}),"). Cette asym\xe9trie cr\xe9e une incertitude persistante dans les \xe9valuations de s\xe9curit\xe9 - m\xeame si un mod\xe8le r\xe9ussit de nombreuses \xe9valuations de s\xe9curit\xe9, nous ne pouvons pas \xeatre certains qu'il est v\xe9ritablement s\xfbr."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le d\xe9fi d'interpr\xe9ter les r\xe9sultats n\xe9gatifs."})," En lien avec cette asym\xe9trie se trouve la difficult\xe9 d'interpr\xe9ter les r\xe9sultats n\xe9gatifs dans l'\xe9valuation de l'IA. Lorsqu'un mod\xe8le ne parvient pas \xe0 d\xe9montrer une capacit\xe9 ou un comportement pendant les tests, cela peut indiquer soit une v\xe9ritable limitation du mod\xe8le, soit simplement un \xe9chec de nos m\xe9thodes d'\xe9valuation \xe0 susciter correctement la capacit\xe9 ou la propension. La d\xe9couverte que GPT-4 pouvait effectivement contr\xf4ler Minecraft gr\xe2ce \xe0 un \xe9chafaudage appropri\xe9, alors qu'il semblait initialement incapable d'un tel comportement, illustre ce d\xe9fi. Cette incertitude concernant les r\xe9sultats n\xe9gatifs devient particuli\xe8rement probl\xe9matique lors de l'\xe9valuation des propri\xe9t\xe9s critiques pour la s\xe9curit\xe9, o\xf9 les faux n\xe9gatifs pourraient avoir de graves cons\xe9quences."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le probl\xe8me des inconnues inconnues."})," Plus fondamentalement peut-\xeatre, nos m\xe9thodes d'\xe9valuation sont limit\xe9es par notre capacit\xe9 \xe0 anticiper ce que nous devons \xe9valuer. \xc0 mesure que les syst\xe8mes d'IA deviennent plus performants, ils pourraient d\xe9velopper des comportements ou des capacit\xe9s que nous n'avons pas pens\xe9 \xe0 tester. Cela sugg\xe8re que nos cadres d'\xe9valuation pourraient manquer des cat\xe9gories enti\xe8res de capacit\xe9s ou de risques simplement parce que nous ne les avons pas encore con\xe7ues."]}),"\n",(0,i.jsx)(s.h2,{id:"02",children:"D\xe9fis techniques"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le d\xe9fi de la pr\xe9cision des mesures."})," L'extr\xeame sensibilit\xe9 des mod\xe8les de langage aux changements mineurs dans les conditions d'\xe9valuation pose un d\xe9fi fondamental pour une mesure fiable. Comme nous l'avons \xe9voqu\xe9 dans les sections pr\xe9c\xe9dentes sur les techniques d'\xe9valuation, m\xeame de subtiles variations dans le formatage des prompts peuvent conduire \xe0 des r\xe9sultats radicalement diff\xe9rents. La recherche a \xe9galement montr\xe9 que les performances peuvent varier jusqu'\xe0 76 pour cent en fonction de changements apparemment triviaux dans les formats de prompting few-shot (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2310.11324",children:"Scalar et al., 2023"}),"). Par exemple, des changements aussi mineurs que le passage d'un \xe9tiquetage alphab\xe9tique \xe0 num\xe9rique (par exemple, de (A) \xe0 (1)), la modification des styles de crochets, ou l'ajout d'un seul espace peuvent entra\xeener des fluctuations de pr\xe9cision d'environ 5 points de pourcentage. (",(0,i.jsx)(s.a,{href:"https://www.anthropic.com/news/evaluating-ai-systems",children:"Anthropic, 2024"}),") Ce n'est pas qu'un simple d\xe9sagr\xe9ment - cela soul\xe8ve de s\xe9rieuses questions sur la fiabilit\xe9 et la reproductibilit\xe9 de nos r\xe9sultats d'\xe9valuation. Quand de petits changements dans le formatage des entr\xe9es peuvent causer de telles variations dans les performances mesur\xe9es, comment pouvons-nous faire confiance \xe0 nos \xe9valuations des capacit\xe9s ou des propri\xe9t\xe9s de s\xe9curit\xe9 des mod\xe8les ?"]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Sensibilit\xe9 aux facteurs environnementaux."})," Au-del\xe0 des probl\xe8mes de mesure dus au formatage des prompts, les r\xe9sultats d'\xe9valuation peuvent \xeatre affect\xe9s par divers facteurs environnementaux dont nous pourrions m\xeame ne pas \xeatre conscients. Tout comme les inconnues que nous avons soulign\xe9es dans la derni\xe8re section, cela pourrait inclure des \xe9l\xe9ments comme la version sp\xe9cifique du mod\xe8le test\xe9, la temp\xe9rature et d'autres param\xe8tres d'\xe9chantillonnage, l'infrastructure de calcul ex\xe9cutant l'\xe9valuation et peut-\xeatre m\xeame l'heure de la journ\xe9e ou la charge du syst\xe8me pendant les tests."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le choix des m\xe9thodes d'\xe9valuation doit \xe9galement tenir compte des contraintes pratiques."})," Les \xe9valuateurs tiers pourraient n'avoir qu'un acc\xe8s limit\xe9 aux composants internes d'un mod\xe8le. Ils pourraient avoir acc\xe8s \xe0 l'observation des activations, mais pas \xe0 la modification des poids. Ils pourraient n'avoir aucun acc\xe8s au mod\xe8le et \xeatre limit\xe9s \xe0 l'observation du mod\xe8le fonctionnant \"dans la nature\". Selon les techniques sp\xe9cifiques utilis\xe9es, les ressources computationnelles pourraient restreindre certains types d'analyses."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le prix et le temps sont \xe9galement des contraintes pratiques pour mener des \xe9valuations compl\xe8tes."})," L'ex\xe9cution d'agents (ou \"syst\xe8mes d'IA\") est beaucoup plus co\xfbteuse et chronophage que les benchmarks. D\xe9velopper des t\xe2ches complexes nouvelles, v\xe9rifier et mesurer correctement les capacit\xe9s demande beaucoup de travail humain. Par exemple, pendant l'\xe9valuation des capacit\xe9s d'IA en R&D, le \"groupe de contr\xf4le\" humain a pass\xe9 plus de 568 heures sur seulement 7 t\xe2ches (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2411.15114",children:"Wijk et al., 2024"}),"). Cela n'inclut m\xeame pas le temps de d\xe9veloppement et de test des t\xe2ches. Les t\xe2ches du monde r\xe9el en R&D d'IA peuvent prendre des centaines d'heures-homme chacune, et la plupart des \xe9valuateurs ind\xe9pendants, ou m\xeame les \xe9quipes internes des grands laboratoires d'IA n'ont pas le budget et le temps pour \xe9valuer correctement les performances compte tenu de ces consid\xe9rations. Tout cela doit \xeatre pris en compte lors de la conception d'un protocole d'\xe9valuation."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'explosion combinatoire des sc\xe9narios d'interaction."})," Lorsque les syst\xe8mes d'IA interagissent dans des environnements complexes, le nombre de sc\xe9narios possibles cro\xeet exponentiellement avec chaque mod\xe8le d'interaction suppl\xe9mentaire. Contrairement \xe0 l'\xe9valuation de capacit\xe9s isol\xe9es, nous devons consid\xe9rer comment plusieurs composants interagissent de mani\xe8re impr\xe9visible - similaire \xe0 la fa\xe7on dont des produits chimiques normalement b\xe9nins peuvent former des compos\xe9s dangereux une fois combin\xe9s. Par exemple, lorsque le mod\xe8le A avec des capacit\xe9s de planification interagit avec le mod\xe8le B poss\xe9dant un acc\xe8s \xe0 l'information, et que les deux op\xe8rent dans le syst\xe8me C qui a des autorisations d'outils externes, de nouvelles surfaces de risque \xe9mergent qui n'\xe9taient pr\xe9sentes dans aucun composant individuel. Cette complexit\xe9 combinatoire rend les tests complets fondamentalement impossibles. Avec des millions de mod\xe8les d'interaction potentiels, nous ne pouvons \xe9valuer qu'une fraction microscopique des sc\xe9narios possibles, laissant d'innombrables angles morts o\xf9 des comportements \xe9mergents critiques pourraient se d\xe9velopper. Cette limitation fondamentale soul\xe8ve une question pr\xe9occupante : si nous ne pouvons explorer que 0,001% de l'espace d'interaction, comment pouvons-nous \xeatre s\xfbrs d'avoir identifi\xe9 les mod\xe8les \xe9mergents les plus dangereux ?"]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La r\xe9alit\xe9 des ressources."})," En lien avec le probl\xe8me du nombre explosif d'\xe9valuations se trouve le co\xfbt computationnel pur de l'ex\xe9cution d'\xe9valuations approfondies. Cela cr\xe9e une autre limite stricte sur ce que nous pouvons tester en pratique. Faire plus de 100 000 appels API juste pour \xe9valuer correctement les performances sur un seul ",(0,i.jsx)(n,{term:"benchmark",definition:'{"definition":"Jeu de donn\xe9es d\u2019\xe9valuation permettant d\u2019\xe9valuer les performances des algorithmes au regard de l\u2019accomplissement de t\xe2ches sp\xe9cifiques.","source":"[LeMagIT](https://www.lemagit.fr/conseil/IA-generative-comprendre-les-benchmarks-generiques)","aliases":[]}',children:(0,i.jsx)(n,{term:"benchmark",definition:'{"definition":"Jeu de donn\xe9es d\u2019\xe9valuation permettant d\u2019\xe9valuer les performances des algorithmes au regard de l\u2019accomplissement de t\xe2ches sp\xe9cifiques.","source":"[LeMagIT](https://www.lemagit.fr/conseil/IA-generative-comprendre-les-benchmarks-generiques)","aliases":[]}',children:"benchmark"})})," devient prohibitif lorsqu'on \xe9tend cela \xe0 de multiples capacit\xe9s et pr\xe9occupations de s\xe9curit\xe9. Les chercheurs ind\xe9pendants et les petites organisations n'ont souvent pas les moyens de faire des tests aussi complets, et m\xeame si l'argent n'est pas un goulot d'\xe9tranglement parce que vous avez un financement public, les GPU sont actuellement un facteur absolument limitant. Cela peut conduire \xe0 des angles morts potentiels dans notre compr\xe9hension du comportement des mod\xe8les. Les contraintes de ressources deviennent encore plus pressantes lorsque nous consid\xe9rons la n\xe9cessit\xe9 de tests r\xe9p\xe9t\xe9s \xe0 mesure que les mod\xe8les sont mis \xe0 jour ou que de nouvelles capacit\xe9s \xe9mergent."]}),"\n",(0,i.jsx)(o.A,{speaker:"US and UK AI Safety Institute (Evaluation of Claude 3.5 Sonnet)",position:"",date:"2024",source:"([US & UK AISI, 2024](https://www.aisi.gov.uk/work/pre-deployment-evaluation-of-anthropics-upgraded-claude-3-5-sonnet))",children:(0,i.jsx)(s.p,{children:"Bien que ces tests aient \xe9t\xe9 men\xe9s conform\xe9ment aux meilleures pratiques actuelles, les r\xe9sultats doivent \xeatre consid\xe9r\xe9s comme pr\xe9liminaires. Ces tests ont \xe9t\xe9 r\xe9alis\xe9s sur une p\xe9riode limit\xe9e avec des ressources finies, qui, si \xe9tendues, pourraient \xe9largir la port\xe9e des d\xe9couvertes et les conclusions qui en d\xe9coulent."})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:'La difficult\xe9 d\'estimer les "Capacit\xe9s Maximales".'})," \xc9galement li\xe9 au probl\xe8me de complexit\xe9 combinatoire se trouve le probl\xe8me de l'\xe9mergence. Nous d\xe9couvrons fr\xe9quemment que les mod\xe8les peuvent faire des choses que nous pensions impossibles lorsqu'on leur donne le bon \xe9chafaudage ou les bons outils. Le projet Voyager l'a d\xe9montr\xe9 en utilisant des requ\xeates en bo\xeete noire \xe0 GPT-4 r\xe9v\xe9lant la capacit\xe9 inattendue \xe0 jouer \xe0 Minecraft (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2305.16291",children:"Wang et al., 2023"}),"). Soit par des augmentations continues d'\xe9chelle, soit par une simple combinaison d'une approche d'\xe9chafaudage que les chercheurs n'avaient pas envisag\xe9e auparavant, un mod\xe8le pourrait afficher un changement significatif de capacit\xe9s ou de propensions que nous n'avions pas anticip\xe9. Ce comportement \xe9mergent est particuli\xe8rement difficile \xe0 \xe9valuer car il provient souvent de l'interaction entre les capacit\xe9s. Il existe peu ou pas de moyens de garantir que le tout ne deviendra pas plus que la somme de ses parties. Cela rend extr\xeamement difficile de pr\xe9dire ce dont un mod\xe8le pourrait \xeatre capable dans des situations r\xe9elles, m\xeame si nous testons minutieusement ses capacit\xe9s individuelles."]}),"\n",(0,i.jsx)(l.A,{title:"\xc9tude de cas sur les surprises en mati\xe8re de capacit\xe9s : Kalamang",collapsed:!0,children:(0,i.jsxs)(s.p,{children:["Gemini est capable de traduire des langues de mani\xe8re zero-shot, sans aucun entra\xeenement. Le cas de la capacit\xe9 de Gemini \xe0 traduire le Kalamang - une langue parl\xe9e par seulement 100 personnes en Papouasie-Nouvelle-Guin\xe9e - d\xe9montre comment les mod\xe8les peuvent acqu\xe9rir des capacit\xe9s inattendues lorsqu'on leur fournit le bon contexte. Il est tr\xe8s peu probable que cette langue ait fait partie des donn\xe9es d'entra\xeenement d'un quelconque mod\xe8le d'IA majeur. Les chercheurs de Google DeepMind ont d\xe9montr\xe9 qu'en exploitant une fen\xeatre de contexte suffisamment large - environ un million de tokens - ils pouvaient entrer un livre de grammaire complet et un dictionnaire de Kalamang. Lorsqu'on lui a demand\xe9 de traduire une phrase du Kalamang vers l'anglais, le mod\xe8le a \xe9t\xe9 capable d'effectuer cette t\xe2che (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2309.16575",children:"Tanzer et al., 2024"}),")."]})}),"\n",(0,i.jsx)(s.h2,{id:"03",children:"Sandbagging"}),"\n",(0,i.jsx)(u.A,{src:"./img/UGr_Image_62.png",alt:"Entrer la description alternative de l'image",number:"60",label:"5.60",caption:"Illustration du sandbagging ([van der Weij, 2024](https://www.alignmentforum.org/posts/jsmNCj9QKcfdg8fJk/an-introduction-to-ai-sandbagging))"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les tests comportementaux ne sont pas suffisants."})," Nous en avons parl\xe9 plusieurs fois dans les sections pr\xe9c\xe9dentes, mais cela m\xe9rite d'\xeatre r\xe9p\xe9t\xe9. Observer ce qu'un mod\xe8le fait peut nous apprendre beaucoup, mais c'est comme essayer de comprendre comment une personne pense en observant uniquement ses actions. Notre forte d\xe9pendance aux tests comportementaux - l'analyse des sorties du mod\xe8le - nous laisse avec des angles morts importants sur le fonctionnement r\xe9el des mod\xe8les. Un mod\xe8le peut donner les bonnes r\xe9ponses tout en utilisant des strat\xe9gies internes que nous ne pouvons pas d\xe9tecter. Donc en plus d'\xeatre limit\xe9s en largeur en raison de la complexit\xe9 combinatoire du nombre d'\xe9l\xe9ments \xe0 tester, nous sommes \xe9galement limit\xe9s sur la profondeur de l'\xe9valuation jusqu'\xe0 ce que nous fassions de v\xe9ritables progr\xe8s en interpr\xe9tabilit\xe9. \xc0 moins que l'interpr\xe9tabilit\xe9 ne progresse, les \xe9valuations ont une limite sup\xe9rieure d\xe9finie sur le niveau de s\xe9curit\xe9 qu'elles peuvent garantir."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le sandbagging fait r\xe9f\xe9rence \xe0 une sous-performance strat\xe9gique lors des \xe9valuations, o\xf9 un syst\xe8me d'IA est intentionnellement pr\xe9sent\xe9 comme moins capable qu'il ne l'est r\xe9ellement."})," Il existe des inqui\xe9tudes concernant les syst\xe8mes d'IA eux-m\xeames choisissant de pratiquer le sandbagging, ind\xe9pendamment des intentions des d\xe9veloppeurs. Un syst\xe8me d'IA pourrait d\xe9lib\xe9r\xe9ment cacher ses capacit\xe9s pour \xe9viter les modifications ou les restrictions qui l'emp\xeacheraient d'atteindre ses objectifs (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations",children:"Hubinger, 2023"}),"). Nous avons davantage parl\xe9 des raisons de cette pr\xe9occupation dans la sous-section sur la manipulation. Comme preuve que c'est possible, plusieurs mod\xe8les de pointe, notamment Claude 3.5 Sonnet, Claude 3 Opus et o1 d'OpenAI, ont d\xe9montr\xe9 \xeatre capables de comportements de sandbagging. Ils ont d\xe9lib\xe9r\xe9ment sous-perform\xe9 sur des t\xe2ches arithm\xe9tiques lorsqu'ils croyaient qu'une performance \xe9lev\xe9e d\xe9clencherait un d\xe9sapprentissage du mod\xe8le",(0,i.jsx)(r.A,{id:"footnote_in_context_sandbagging",number:"1",text:"Ce comportement a \xe9t\xe9 observ\xe9 dans un cadre d'\xe9valuation contr\xf4l\xe9 o\xf9 les mod\xe8les \xe9taient explicitement inform\xe9s via la fen\xeatre contextuelle des cons\xe9quences potentielles de d\xe9sapprentissage en cas de haute performance. Les chercheurs ont not\xe9 que : - certains mod\xe8les poss\xe8dent des capacit\xe9s basiques de manipulation si l'objectif et la conscience situationnelle sont acquis dans le contexte - plut\xf4t que de d\xe9montrer ce comportement \xe9mergeant spontan\xe9ment dans des sc\xe9narios plus r\xe9alistes ([Meinke et al., 2025](https://arxiv.org/abs/2412.04984))."})," (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2412.04984",children:"Meinke et al., 2025"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Quelles sont les implications du potentiel de sandbagging pour le domaine des \xe9valuations ?"})," La possibilit\xe9 du sandbagging a des implications importantes sur notre approche des \xe9valuations d'IA, tant aujourd'hui que dans le futur. Le simple acc\xe8s aux sorties du mod\xe8le (tests en bo\xeete noire) est insuffisant pour une \xe9valuation rigoureuse (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2401.14446",children:"Casper et al., 2024"}),"). Les \xe9valuateurs ont plut\xf4t besoin d'un acc\xe8s plus profond aux m\xe9canismes internes du mod\xe8le et aux proc\xe9dures d'entra\xeenement pour d\xe9tecter les tentatives potentielles de sandbagging. Cela pourrait n\xe9cessiter de nouveaux arrangements institutionnels, comme des \xe9valuateurs tiers de confiance avec un acc\xe8s s\xe9curis\xe9 aux poids et architectures des mod\xe8les. Le ",(0,i.jsx)(n,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:(0,i.jsx)(n,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:"fine-tuning"})})," supervis\xe9 s'est r\xe9v\xe9l\xe9 prometteur pour att\xe9nuer consid\xe9rablement le sandbagging et r\xe9v\xe9ler les capacit\xe9s cach\xe9es (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2405.19550",children:"Greenblatt et al, 2024"}),"), mais le ",(0,i.jsx)(n,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:(0,i.jsx)(n,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:"fine-tuning"})})," n'est pas si facile \xe0 utiliser de mani\xe8re compl\xe8te sur des benchmarks de capacit\xe9s dangereuses, et n'est pas utilisable pour les benchmarks de propension dangereuse. Sans r\xe9pondre \xe0 ce d\xe9fi, notre capacit\xe9 \xe0 prendre des d\xe9cisions \xe9clair\xe9es sur le d\xe9veloppement et le d\xe9ploiement de l'IA bas\xe9es sur les r\xe9sultats d'\xe9valuation devient s\xe9rieusement compromise."]}),"\n",(0,i.jsx)(s.h2,{id:"04",children:"Limitations syst\xe9miques"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Tout comme les entreprises peuvent pratiquer l'\xe9coblanchiment en se pr\xe9sentant de mani\xe8re trompeuse comme respectueuses de l'environnement, les entreprises d'IA peuvent pratiquer le \"safety washing\"."})," Cela signifie pr\xe9senter de mani\xe8re erron\xe9e les am\xe9liorations des capacit\xe9s comme des avanc\xe9es en mati\xe8re de s\xe9curit\xe9. Cela se produit lorsque les crit\xe8res de s\xe9curit\xe9 sont fortement corr\xe9l\xe9s avec les capacit\xe9s g\xe9n\xe9rales du mod\xe8le et la puissance de calcul utilis\xe9e pour l'entra\xeenement. Lorsqu'un crit\xe8re est fortement corr\xe9l\xe9 aux capacit\xe9s, l'am\xe9lioration des performances peut simplement refl\xe9ter que le mod\xe8le est devenu plus capable dans l'ensemble, plut\xf4t que sp\xe9cifiquement plus s\xfbr."]}),"\n",(0,i.jsx)(u.A,{src:"./img/OC1_Image_63.png",alt:"Entrer la description alternative de l'image",number:"61",label:"5.61",caption:"Le lien \xe9troit entre de nombreuses propri\xe9t\xe9s de s\xe9curit\xe9 et les capacit\xe9s peut permettre le \"safety washing\", o\xf9 les avanc\xe9es en mati\xe8re de capacit\xe9s (par exemple, l'entra\xeenement d'un plus grand mod\xe8le) peuvent \xeatre pr\xe9sent\xe9es comme des progr\xe8s en mati\xe8re de s\xe9curit\xe9 de l'IA. Cela induit en erreur la communaut\xe9 de recherche quant aux d\xe9veloppements qui ont eu lieu, d\xe9formant le discours acad\xe9mique ([Ren et al., 2024](https://arxiv.org/abs/2407.21792))."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Comment le safety washing se produit-il en pratique ?"})," Il existe g\xe9n\xe9ralement trois fa\xe7ons dont cela se produit (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2407.21792",children:"Ren et al., 2024"}),") :"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"S\xe9curit\xe9 par association :"})," La recherche est \xe9tiquet\xe9e comme pertinente pour la s\xe9curit\xe9 simplement parce qu'elle provient d'\xe9quipes ou de chercheurs en s\xe9curit\xe9, m\xeame si le travail est essentiellement de la recherche sur les capacit\xe9s."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Relations publiques :"})," Les entreprises pr\xe9sentent les avanc\xe9es en mati\xe8re de capacit\xe9s comme des progr\xe8s en mati\xe8re de s\xe9curit\xe9 en mettant en avant des m\xe9triques de s\xe9curit\xe9 corr\xe9l\xe9es, particuli\xe8rement face \xe0 la pression publique ou \xe0 l'examen r\xe9glementaire."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Optimisation des subventions :"})," Les chercheurs pr\xe9sentent le travail sur les capacit\xe9s en termes de s\xe9curit\xe9 pour attirer les financeurs, m\xeame lorsque les avanc\xe9es ne font pas progresser la s\xe9curit\xe9 de mani\xe8re diff\xe9rentielle."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les corr\xe9lations avec les capacit\xe9s peuvent permettre le safety washing."}),' Lorsque les crit\xe8res de s\xe9curit\xe9 sont fortement corr\xe9l\xe9s aux capacit\xe9s, il devient facile de pr\xe9senter \xe0 tort les am\xe9liorations des capacit\xe9s comme des avanc\xe9es en mati\xe8re de s\xe9curit\xe9. Par exemple, si un crit\xe8re cens\xe9 mesurer la "v\xe9racit\xe9" est en r\xe9alit\xe9 corr\xe9l\xe9 \xe0 80% avec les capacit\xe9s g\xe9n\xe9rales, alors rendre les mod\xe8les plus grands et plus capables am\xe9liorera leurs scores de "v\xe9racit\xe9" - m\xeame s\'ils ne deviennent pas intrins\xe8quement plus honn\xeates (',(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2407.21792",children:"Ren et al., 2024"}),"). Cela cr\xe9e un probl\xe8me syst\xe9mique o\xf9 les organisations peuvent revendiquer des am\xe9liorations de s\xe9curit\xe9 simplement en augmentant l'\xe9chelle de leurs mod\xe8les. La solution n'est pas seulement d'\xe9tablir des normes autour de l'utilisation des crit\xe8res - celles-ci ont tendance \xe0 \xeatre faibles et facilement ignor\xe9es. Au lieu de cela, nous avons besoin de m\xe9thodes d'\xe9valuation qui peuvent distinguer empiriquement entre les am\xe9liorations des capacit\xe9s et les v\xe9ritables avanc\xe9es en mati\xe8re de s\xe9curit\xe9."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les entreprises et les laboratoires d'IA peuvent \xe9galement choisir de sous-performer d\xe9lib\xe9r\xe9ment lors des \xe9valuations."})," Nous pla\xe7ons ici une autre forme de sous-performance d\xe9lib\xe9r\xe9e comme limitation syst\xe9mique pour d\xe9signer les entreprises qui le font intentionnellement pour obtenir de meilleurs scores lors des \xe9valuations. Cela contraste avec le d\xe9salignement/la sous-performance technique comme limitation qui survient lorsque le mod\xe8le le fait par calcul."]}),"\n",(0,i.jsxs)(s.p,{children:["\xc0 court terme, les d\xe9veloppeurs font face \xe0 diverses pressions qui pourraient les inciter \xe0 sous-performer d\xe9lib\xe9r\xe9ment lors des \xe9valuations. Les exigences r\xe9glementaires et les normes de s\xe9curit\xe9 peuvent imposer des seuils de capacit\xe9 au-dessus desquels une surveillance, des restrictions ou des co\xfbts suppl\xe9mentaires sont encourus. Par exemple, la loi europ\xe9enne sur l'IA exige des contr\xf4les plus stricts et des audits par des tiers pour les syst\xe8mes d'IA \xe0 haut risque (",(0,i.jsx)(s.a,{href:"https://artificialintelligenceact.eu/the-act/",children:"Commission europ\xe9enne, 2024"}),"). Cela cr\xe9e une incitation financi\xe8re directe \xe0 para\xeetre en dessous de ces seuils. Les recherches actuelles montrent que m\xeame les mod\xe8les de langage d'aujourd'hui peuvent \xeatre amen\xe9s \xe0 sous-performer s\xe9lectivement de mani\xe8re difficile \xe0 d\xe9tecter par les \xe9valuations standard, particuli\xe8rement sur les tests de capacit\xe9s dangereuses tout en maintenant les performances sur les crit\xe8res g\xe9n\xe9raux (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2406.07358",children:"van der Weij et al., 2024"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:["\xc0 l'avenir, les entreprises pourraient vouloir minimiser certaines capacit\xe9s de leurs syst\xe8mes pour \xe9viter d'attirer l'",(0,i.jsx)(n,{term:"attention",definition:'{"definition":"M\xe9canisme permettant aux mod\xe8les de se concentrer sur les parties pertinentes de l\'entr\xe9e lors de la r\xe9alisation de pr\xe9dictions, en calculant des combinaisons pond\xe9r\xe9es d\'\xe9l\xe9ments d\'entr\xe9e..","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:(0,i.jsx)(n,{term:"attention",definition:'{"definition":"M\xe9canisme permettant aux mod\xe8les de se concentrer sur les parties pertinentes de l\'entr\xe9e lors de la r\xe9alisation de pr\xe9dictions, en calculant des combinaisons pond\xe9r\xe9es d\'\xe9l\xe9ments d\'entr\xe9e..","source":"[Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)","aliases":["Attention","attention mechanism","self-attention"]}',children:"attention"})})," r\xe9glementaire ou l'inqui\xe9tude du public, tout en \xe9tant capables de d\xe9ployer ces capacit\xe9s dans la pratique. Cela leur permet de maintenir la vitesse de d\xe9veloppement et de d\xe9ploiement sans d\xe9clencher de surveillance suppl\xe9mentaire. Par exemple, si un syst\xe8me d'IA montre de fortes capacit\xe9s dans des domaines qui pourraient soulever des pr\xe9occupations de s\xe9curit\xe9, les d\xe9veloppeurs pourraient \xeatre motiv\xe9s \xe0 le faire sous-performer sp\xe9cifiquement sur ces \xe9valuations tout en maintenant des performances \xe9lev\xe9es sur les crit\xe8res g\xe9n\xe9raux (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2406.07358",children:"van der Weij et al., 2024"}),"). Ils pourraient faire cela pour maintenir la vitesse de d\xe9veloppement et de d\xe9ploiement sans d\xe9clencher de surveillance suppl\xe9mentaire."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'\xe9cart entre l'\xe9valuation et le d\xe9ploiement."})," Les approches d'\xe9valuation actuelles ne parviennent souvent pas \xe0 capturer la v\xe9ritable complexit\xe9 des contextes de d\xe9ploiement r\xe9els. Bien que nous puissions mesurer les performances des mod\xe8les dans des environnements contr\xf4l\xe9s, ces \xe9valuations refl\xe8tent rarement comment les syst\xe8mes se comporteront lorsqu'ils seront int\xe9gr\xe9s dans des environnements sociotechniques complexes. Un mod\xe8le qui fonctionne de mani\xe8re s\xfbre lors de tests isol\xe9s pourrait se comporter tr\xe8s diff\xe9remment lorsqu'il interagit avec des utilisateurs r\xe9els, fait face \xe0 des sc\xe9narios nouveaux ou fonctionne au sein de syst\xe8mes soci\xe9taux plus larges. Par exemple, un mod\xe8le de conseil m\xe9dical pourrait bien performer sur les crit\xe8res standard mais poser de s\xe9rieux risques lorsqu'il est d\xe9ploy\xe9 dans des contextes de soins de sant\xe9 o\xf9 les utilisateurs n'ont pas l'expertise n\xe9cessaire pour valider ses recommandations. Cette d\xe9connexion entre les contextes d'\xe9valuation et de d\xe9ploiement signifie que nous pourrions manquer des probl\xe8mes critiques de s\xe9curit\xe9 qui n'\xe9mergent qu'\xe0 travers des interactions humain-IA complexes ou des effets au niveau du syst\xe8me (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2310.11986",children:"Weidinger et al., 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le d\xe9fi des \xe9valuations sp\xe9cifiques aux domaines."})," L'\xe9valuation des syst\xe8mes d'IA dans des domaines \xe0 enjeux \xe9lev\xe9s pr\xe9sente des d\xe9fis uniques que nos approches actuelles peinent \xe0 r\xe9soudre. Prenons l'exemple de l'\xe9valuation des syst\xe8mes d'IA pour leur potentiel de mauvaise utilisation en bios\xe9curit\xe9 - cela n\xe9cessite non seulement une compr\xe9hension technique des syst\xe8mes d'IA et des m\xe9thodes d'\xe9valuation, mais aussi une expertise approfondie en biologie, en protocoles de bios\xe9curit\xe9 et en vecteurs de menaces potentiels. Cette combinaison d'expertise est extr\xeamement rare, rendant l'\xe9valuation approfondie dans ces domaines critiques particuli\xe8rement difficile. Des d\xe9fis similaires se posent dans d'autres domaines sp\xe9cialis\xe9s comme la cybers\xe9curit\xe9, o\xf9 la complexit\xe9 des vecteurs d'attaque potentiels et des vuln\xe9rabilit\xe9s des syst\xe8mes n\xe9cessite une connaissance sophistiqu\xe9e du domaine pour une \xe9valuation appropri\xe9e. La difficult\xe9 de trouver des \xe9valuateurs ayant une expertise suffisante dans plusieurs domaines conduit souvent \xe0 des \xe9valuations qui manquent des risques importants sp\xe9cifiques au domaine ou ne parviennent pas \xe0 anticiper de nouvelles formes de mauvaise utilisation. Au-del\xe0 de la simple conception des \xe9valuations qui est difficile, la r\xe9alisation de l'\xe9valuation pour ces domaines \xe0 enjeux \xe9lev\xe9s est \xe9galement difficile. Nous devons obtenir les capacit\xe9s maximales d'un mod\xe8le \xe0 g\xe9n\xe9rer des agents pathog\xe8nes ou des logiciels malveillants pour tester ce dont il est capable. Cela peut \xe9videmment \xeatre probl\xe9matique si ce n'est pas g\xe9r\xe9 avec une extr\xeame prudence."]}),"\n",(0,i.jsx)(s.h2,{id:"05",children:"Limitations de la Gouvernance"}),"\n",(0,i.jsx)(u.A,{src:"./img/7XQ_Image_64.png",alt:"Entrer la description alternative de l'image",number:"62",label:"5.62",caption:"Nous avons besoin de beaucoup plus de travail en mati\xe8re d'\xe9valuations. Des capacit\xe9s plus \xe9lev\xe9es n\xe9cessitent plus d'\xe9valuations de s\xe9curit\xe9. De nombreuses d\xe9cisions \xe0 enjeux \xe9lev\xe9s dans les cadres dirig\xe9s par les entreprises et les gouvernements d\xe9pendent des r\xe9sultats des \xe9valuations ([Hobbahn, 2024](https://www.alignmentforum.org/posts/gJJEjJpKiddoYGZKk/the-evals-gap))."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'\xe9cart d'\xe9valuation des politiques."})," Les efforts l\xe9gislatifs et r\xe9glementaires actuels concernant la s\xe9curit\xe9 de l'IA sont s\xe9v\xe8rement entrav\xe9s par l'absence de m\xe9thodes d'\xe9valuation standardis\xe9es et robustes. Prenons l'exemple des r\xe9cents d\xe9crets et r\xe8glements propos\xe9s exigeant des \xe9valuations de s\xe9curit\xe9 des syst\xe8mes d'IA - comment les gouvernements peuvent-ils faire respecter ces exigences alors que nous manquons de moyens fiables pour mesurer ce qui rend un syst\xe8me d'IA \"s\xfbr\" ? Cela cr\xe9e un probl\xe8me circulaire : les r\xe9gulateurs ont besoin de standards d'\xe9valuation pour cr\xe9er des politiques efficaces, mais le d\xe9veloppement de ces standards est en partie guid\xe9 par les exigences r\xe9glementaires. L'initiative r\xe9cente du gouvernement am\xe9ricain visant \xe0 faire \xe9valuer les capacit\xe9s de l'IA en mati\xe8re de cybers\xe9curit\xe9 et de bios\xe9curit\xe9 par diverses agences met en \xe9vidence ce d\xe9fi. Ces agences, malgr\xe9 leur expertise dans leurs domaines respectifs, manquent souvent des connaissances sp\xe9cialis\xe9es n\xe9cessaires pour \xe9valuer de mani\xe8re exhaustive les syst\xe8mes d'IA avanc\xe9s (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2310.11986",children:"Weidinger et al., 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le r\xf4le de l'\xe9valuation ind\xe9pendante."})," Actuellement, une grande partie de la recherche sur l'\xe9valuation de l'IA se d\xe9roule au sein des m\xeames entreprises qui d\xe9veloppent ces syst\xe8mes. Bien que ces entreprises disposent souvent des meilleures ressources et expertises pour mener des \xe9valuations, cela cr\xe9e un conflit d'int\xe9r\xeats inh\xe9rent. Les entreprises pourraient \xeatre incit\xe9es \xe0 concevoir des \xe9valuations que leurs syst\xe8mes sont susceptibles de r\xe9ussir ou \xe0 minimiser les r\xe9sultats pr\xe9occupants. On parle parfois de \"safety washing\" (similaire au greenwashing). Pour r\xe9pondre aux d\xe9fis li\xe9s \xe0 l'ind\xe9pendance et \xe0 l'expertise en mati\xe8re d'\xe9valuation, nous devons consid\xe9rablement \xe9largir l'\xe9cosyst\xe8me des organisations d'\xe9valuation ind\xe9pendantes. L'\xe9mergence d'organisations d'\xe9valuation ind\xe9pendantes comme Model Evaluation for Trustworthy AI (METR) et Apollo Research repr\xe9sente une \xe9tape importante vers la r\xe9solution de ce probl\xe8me, mais ces organisations font souvent face \xe0 des obstacles importants, notamment l'acc\xe8s limit\xe9 aux mod\xe8les (",(0,i.jsx)(s.a,{href:"https://ailabwatch.org/blog/external-evaluation/",children:"Perlman, 2024"}),"), les contraintes de ressources et les difficult\xe9s \xe0 \xe9galer les capacit\xe9s techniques des grands laboratoires d'IA."]}),"\n",(0,i.jsx)(s.p,{children:"Globalement, bien que les limitations que nous avons discut\xe9es dans cette derni\xe8re section soient importantes, elles ne sont pas insurmontables. Les progr\xe8s dans des domaines comme l'interpr\xe9tabilit\xe9 m\xe9caniste, les m\xe9thodes de v\xe9rification formelle et les protocoles d'\xe9valuation montrent des promesses pour r\xe9pondre \xe0 de nombreuses limitations actuelles. Cependant, surmonter ces d\xe9fis n\xe9cessite des efforts et des investissements soutenus."}),"\n",(0,i.jsx)(u.A,{src:"./img/RuR_Image_65.png",alt:"Entrer la description alternative de l'image",number:"63",label:"5.63",caption:"Une analyse visuelle des limitations et quelques raisons de douter de l'impact des \xe9valuations des risques de l'IA ([Mukobi, 2024](https://arxiv.org/abs/2408.02565))."}),"\n",(0,i.jsx)(r.c,{title:"Notes de bas de page"})]})}function f(e={}){const{wrapper:s}={...(0,a.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(v,{...e})}):v(e)}},4768:(e,s,n)=>{n.d(s,{c:()=>d,A:()=>u});var t=n(6540),i=n(3012);const a={footnoteAnchor:"footnoteAnchor_PZpY",footnoteSection:"footnoteSection_waoz",separator:"separator_hp8y",separatorLine:"separatorLine_uVnd",separatorLogo:"separatorLogo_dMBq",footnoteRegistry:"footnoteRegistry_ieVW",registryTitle:"registryTitle_ybBb",footnoteList:"footnoteList_Kjil",footnoteItem:"footnoteItem_aLZ2",footnoteNumber:"footnoteNumber_dA3X",footnoteContent:"footnoteContent_Xfmt",backButton:"backButton_vSVm",highlighted:"highlighted_YFU0",highlight:"highlight_jxG3",highlightDark:"highlightDark_Ekom"};var r=n(4848);function o(e,s){void 0===s&&(s=!0);const n=document.getElementById(e);n&&(n.scrollIntoView({behavior:"smooth"}),s&&(n.classList.add(a.highlighted),setTimeout((()=>n.classList.remove(a.highlighted)),1500)))}function l(e){return e?e.replace(/\[([^\]]+)\]\(([^)]+)\)/g,'<a href="$2" target="_blank" rel="noopener noreferrer">$1</a>'):""}function u(e){let{id:s,text:n,number:u}=e;const d=s||`footnote-${Math.random().toString(36).substr(2,9)}`,c="string"==typeof n?l(n):n;return(0,t.useEffect)((()=>{const e=setTimeout((()=>{const e=document.getElementById(`footnote-clone-${d}`);e&&n&&(e.innerHTML="string"==typeof n?l(n):n.toString())}),100);return()=>clearTimeout(e)}),[d,n]),(0,r.jsx)(i.Mn,{content:(0,r.jsx)("div",{dangerouslySetInnerHTML:{__html:c}}),children:(0,r.jsx)("sup",{id:`footnote-ref-${d}`,className:a.footnoteAnchor,onClick:e=>{(e.ctrlKey||e.metaKey)&&(e.preventDefault(),o(`footnote-content-${d}`))},"data-footnote-number":u||"?",children:u||"*"})})}function d(e){let{title:s="References"}=e;const[n,l]=(0,t.useState)([]);return(0,t.useEffect)((()=>{const e=document.querySelectorAll('[id^="footnote-ref-"]'),s=Array.from(e).map((e=>({id:e.id.replace("footnote-ref-",""),number:e.getAttribute("data-footnote-number")||e.textContent})));l(s)}),[]),n.length?(0,r.jsxs)("div",{className:a.footnoteSection,children:[(0,r.jsxs)("div",{className:a.separator,children:[(0,r.jsx)("div",{className:a.separatorLine}),(0,r.jsx)("img",{src:"/img/logo_samples/01-test.svg",alt:"Atlas logo",className:a.separatorLogo}),(0,r.jsx)("div",{className:a.separatorLine})]}),(0,r.jsxs)("div",{className:a.footnoteRegistry,children:[(0,r.jsx)("h2",{className:a.registryTitle,children:s}),(0,r.jsx)("ol",{className:a.footnoteList,children:n.map((e=>(0,r.jsxs)("li",{id:`footnote-content-${e.id}`,className:a.footnoteItem,value:parseInt(e.number)||void 0,children:[(0,r.jsx)(i.Mn,{content:"Back to reference",children:(0,r.jsxs)("button",{className:a.footnoteNumber,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:[e.number,"."]})}),(0,r.jsx)("div",{className:a.footnoteContent,children:(0,r.jsx)("div",{id:`footnote-clone-${e.id}`})}),(0,r.jsx)(i.Mn,{content:"Back to reference",children:(0,r.jsx)("button",{className:a.backButton,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:"\u21a9"})})]},e.id)))})]})]}):null}}}]);
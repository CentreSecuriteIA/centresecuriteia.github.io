"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[8091],{4545:(e,s,t)=>{t.r(s),t.d(s,{assets:()=>m,contentTitle:()=>p,default:()=>g,frontMatter:()=>d,metadata:()=>n,toc:()=>h});const n=JSON.parse('{"id":"chapters/07/3","title":"Directionnalit\xe9 des objectifs","description":"Les syst\xe8mes d\'apprentissage automatique peuvent \xe9voluer au-del\xe0 de la simple reconnaissance de motifs pour poursuivre syst\xe9matiquement des objectifs dans divers contextes et face \xe0 diff\xe9rents obstacles. Les sections pr\xe9c\xe9dentes abordaient deux points - nous pouvons avoir des algorithmes au comportement indiscernable avec diff\xe9rents m\xe9canismes internes \xe0 la fin de l\'entra\xeenement ; nous pouvons avoir des syst\xe8mes extr\xeamement capables dont les objectifs ne sont pas ceux que nous avions pr\xe9vus. Dans cette section, nous examinons ce qui se passe lorsque ces algorithmes appris deviennent fortement orient\xe9s vers des objectifs, le cas extr\xeame \xe9tant l\'impl\xe9mentation de l\'optimisation apprise (m\xe9sa-optimisation). Cette section examine les diff\xe9rentes fa\xe7ons dont peut \xe9merger un comportement syst\xe9matique potentiellement d\xe9salign\xe9 et orient\xe9 vers un but.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/07/03.md","sourceDirName":"chapters/07","slug":"/chapters/07/03","permalink":"/fr/chapters/07/03","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/07/03.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"3","title":"Directionnalit\xe9 des objectifs","sidebar_label":"7.3 Directionnalit\xe9 des objectifs","sidebar_position":4,"slug":"/chapters/07/03","reading_time_core":"13 min","reading_time_optional":"7 min","pagination_prev":"chapters/07/2","pagination_next":"chapters/07/4"},"sidebar":"docs","previous":{"title":"7.2 Dynamique d\'apprentissage","permalink":"/fr/chapters/07/02"},"next":{"title":"7.4 Manigances","permalink":"/fr/chapters/07/04"}}');var i=t(4848),r=t(8453),a=t(3931),o=t(4768),l=(t(2482),t(8559)),u=t(9585),c=t(2501);const d={id:3,title:"Directionnalit\xe9 des objectifs",sidebar_label:"7.3 Directionnalit\xe9 des objectifs",sidebar_position:4,slug:"/chapters/07/03",reading_time_core:"13 min",reading_time_optional:"7 min",pagination_prev:"chapters/07/2",pagination_next:"chapters/07/4"},p="Directionnalit\xe9 des objectifs",m={},h=[{value:"Heuristiques",id:"01",level:2},{value:"Simulateurs",id:"02",level:2},{value:"Optimisation apprise",id:"03",level:2},{value:"Optimisation \xe9mergente",id:"04",level:2}];function f(e){const s={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{GlossaryTerm:t}=s;return t||function(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"directionnalit\xe9-des-objectifs",children:"Directionnalit\xe9 des objectifs"})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsxs)(s.strong,{children:["Les syst\xe8mes d'",(0,i.jsx)(t,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,i.jsx)(t,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})})," peuvent \xe9voluer au-del\xe0 de la simple reconnaissance de motifs pour poursuivre syst\xe9matiquement des objectifs dans divers contextes et face \xe0 diff\xe9rents obstacles."]})," Les sections pr\xe9c\xe9dentes abordaient deux points - nous pouvons avoir des algorithmes au comportement indiscernable avec diff\xe9rents m\xe9canismes internes \xe0 la fin de l'entra\xeenement ; nous pouvons avoir des syst\xe8mes extr\xeamement capables dont les objectifs ne sont pas ceux que nous avions pr\xe9vus. Dans cette section, nous examinons ce qui se passe lorsque ces algorithmes appris deviennent fortement orient\xe9s vers des objectifs, le cas extr\xeame \xe9tant l'impl\xe9mentation de l'optimisation apprise (m\xe9sa-optimisation). Cette section examine les diff\xe9rentes fa\xe7ons dont peut \xe9merger un comportement syst\xe9matique potentiellement d\xe9salign\xe9 et orient\xe9 vers un but."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La directionnalit\xe9 des objectifs diff\xe8re fondamentalement de la performance des t\xe2ches car elle mesure la volont\xe9 de d\xe9ployer des capacit\xe9s plut\xf4t que la capacit\xe9 elle-m\xeame."})," Cette distinction est importante car les am\xe9liorations des capacit\xe9s n'am\xe9liorent pas automatiquement l'alignement - elles rendent simplement les syst\xe8mes meilleurs pour poursuivre les objectifs qu'ils apprennent pendant l'entra\xeenement. Comprendre cet \xe9cart n\xe9cessite des approches de mesure formelles qui s\xe9parent ce que les syst\xe8mes peuvent faire de ce qu'ils font r\xe9ellement."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La directionnalit\xe9 des objectifs plut\xf4t que \"l'optimisation\" met l'accent sur le comportement fonctionnel plut\xf4t que sur les m\xe9canismes internes."})," Les syst\xe8mes orient\xe9s vers un but pr\xe9sentent des sch\xe9mas syst\xe9matiques de comportement orient\xe9s vers l'atteinte de r\xe9sultats sp\xe9cifiques. Un syst\xe8me est orient\xe9 vers un but s'il poursuit syst\xe9matiquement des objectifs dans divers contextes et face \xe0 des obstacles, ind\xe9pendamment du fait que cela se produise par des algorithmes de recherche explicites, des sch\xe9mas comportementaux appris ou une coordination \xe9mergente (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/9pxcekdNjE7oNwvcC/goal-directedness-is-behavioral-not-structural",children:"Shimi, 2020"}),"; ",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2412.04758",children:"MacDermott et al., 2024"}),"). Cette perspective fonctionnelle est importante pour la s\xe9curit\xe9 car un syst\xe8me qui poursuit constamment des objectifs d\xe9salign\xe9s pose des risques similaires, qu'il le fasse par une reconnaissance sophistiqu\xe9e de motifs ou une v\xe9ritable recherche interne - les deux peuvent permettre la poursuite syst\xe9matique d'objectifs nuisibles une fois d\xe9ploy\xe9s. L'optimisation repr\xe9sente la forme la plus forte de directionnalit\xe9 des objectifs, mais ce n'est pas la seule fa\xe7on dont un comportement orient\xe9 vers un but peut \xe9merger. Cela dit, nous discutons toujours des probl\xe8mes uniques qui surviennent lorsqu'on traite l'optimisation apprise explicite (d\xe9salignement interne) dans la sous-section sur l'optimisation apprise."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La directionnalit\xe9 des objectifs peut \xe9merger \xe0 travers de multiples voies computationnelles qui peuvent produire un comportement fonctionnellement identique de poursuite d'objectifs."})," Plut\xf4t que d'appara\xeetre par un m\xe9canisme unique, la poursuite persistante d'objectifs peut \xe9merger par plusieurs voies distinctes :"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Bas\xe9e sur les motifs :"})," Soit des comportements complexes m\xe9moris\xe9s appris, soit des simulateurs de jeu de r\xf4le qui atteignent des objectifs de mani\xe8re coh\xe9rente sans recherche interne (comme des itin\xe9raires de navigation m\xe9moris\xe9s)."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Bas\xe9e sur la recherche :"})," Syst\xe8mes qui \xe9valuent explicitement les options et planifient (comme la planification dynamique d'itin\xe9raire). C'est l'\xe9quivalent de ce qu'on appelle commun\xe9ment l'optimisation apprise/m\xe9sa-optimisation."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"\xc9mergente :"})," La poursuite \xe9mergente d'objectifs survient lorsque plusieurs composants interagissent pour produire une poursuite syst\xe9matique d'objectifs au niveau du syst\xe8me, m\xeame lorsqu'aucun composant individuel ne met en \u0153uvre un comportement orient\xe9 vers un but."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les diff\xe9rents types de directionnalit\xe9 des objectifs cr\xe9ent des profils de risque distincts qui n\xe9cessitent diff\xe9rentes strat\xe9gies de s\xe9curit\xe9."})," La directionnalit\xe9 des objectifs bas\xe9e sur l'heuristique/m\xe9morisation \xe0 travers la reconnaissance de motifs pourrait \xe9chouer gracieusement face \xe0 des situations nouvelles, avec des r\xe8gles comportementales apprises qui se d\xe9gradent de mani\xe8re pr\xe9visible. La m\xe9sa-optimisation pose des risques qualitativement diff\xe9rents car la recherche interne peut trouver des moyens nouveaux et cr\xe9atifs d'atteindre des objectifs d\xe9salign\xe9s qui n'avaient pas \xe9t\xe9 anticip\xe9s pendant l'entra\xeenement. La directionnalit\xe9 des objectifs bas\xe9e sur le simulateur cr\xe9e encore un autre profil de risque : une instanciation de caract\xe8re hautement capable qui peut changer de mani\xe8re impr\xe9visible selon le contexte de conditionnement. La directionnalit\xe9 des objectifs \xe9mergente des interactions multi-agents pourrait \xeatre la plus difficile \xe0 contr\xf4ler car aucun composant individuel n'a besoin d'\xeatre d\xe9salign\xe9 pour qu'un comportement dangereux \xe9merge au niveau du syst\xe8me."]}),"\n",(0,i.jsx)(s.h2,{id:"01",children:"Heuristiques"}),"\n",(0,i.jsx)(a.A,{type:"youtube",videoId:"DKAS2V-kbhI",number:"4",label:"7.4",caption:"Vid\xe9o facultative du cours sur la s\xe9curit\xe9 de l'AGI par Google DeepMind, expliquant la diff\xe9rence entre les heuristiques apprises, les erreurs et les sous-objectifs instrumentaux."}),"\n",(0,i.jsx)(s.p,{children:"La directivit\xe9 heuristique des objectifs se produit lorsque l'entra\xeenement fa\xe7onne des routines comportementales sophistiqu\xe9es qui poursuivent syst\xe9matiquement des objectifs dans diff\xe9rents contextes, sans que le syst\xe8me maintienne des repr\xe9sentations explicites des objectifs ou \xe9value des strat\xe9gies alternatives. Ces syst\xe8mes atteignent une poursuite persistante des objectifs gr\xe2ce \xe0 des sch\xe9mas complexes appris plut\xf4t que par des processus de recherche interne."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les pr\xe9dicteurs de tokens suivants peuvent apprendre \xe0 planifier."}),' Si nous nous inqui\xe9tons d\'une optimisation excessive, pourquoi ne pas simplement entra\xeener la pr\xe9diction du token suivant ou d\'autres t\xe2ches "\xe0 court terme", en esp\xe9rant que ces mod\xe8les n\'apprennent pas la planification \xe0 long terme ? Bien que les pr\xe9dicteurs de tokens suivants effectueraient probablement moins de planification que les alternatives comme l\'apprentissage par renforcement, ils acqui\xe8rent tout de m\xeame la plupart des m\xeames m\xe9canismes et "agissent comme s\'ils" pouvaient planifier. Lorsque vous les invitez avec des objectifs comme "aider l\'utilisateur \xe0 apprendre la physique" ou "\xe9crire une histoire captivante", les mod\xe8les poursuivent ces objectifs de mani\xe8re coh\xe9rente, ajustant leur approche en fonction de vos retours et maintenant leur concentration malgr\xe9 les distractions. Ce comportement \xe9merge de l\'entra\xeenement sur des textes o\xf9 les humains poursuivent des objectifs \xe0 travers la conversation, mais ne n\xe9cessite pas que le mod\xe8le repr\xe9sente explicitement les objectifs ou recherche des strat\xe9gies - les sch\xe9mas orient\xe9s vers les objectifs sont encod\xe9s dans les poids appris comme des heuristiques complexes (',(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2212.01681",children:"Andreas 2022"}),", ",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/aEjckcqHZZny9L2zy/emergent-deception-and-emergent-optimization",children:"Steinhardt, 2024"}),"). Cela est quelque peu li\xe9 \xe0 la directivit\xe9 des objectifs bas\xe9e sur le simulateur dont nous parlons plus tard."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La directivit\xe9 des objectifs bas\xe9e sur la m\xe9morisation peut \xeatre fonctionnellement \xe9quivalente \xe0 une v\xe9ritable directivit\xe9 des objectifs."})," Les \xe9valuations de la directivit\xe9 des objectifs mesurent essentiellement ce que nous avons appel\xe9 la propension dans le chapitre des \xe9valuations. Elles testent - Dans quelle mesure les LLM utilisent-ils leurs capacit\xe9s pour atteindre leur objectif donn\xe9 ? Nous constatons que les mod\xe8les de langage d\xe9montrent une poursuite syst\xe9matique des objectifs dans des environnements structur\xe9s, maintenant les objectifs \xe0 travers les tours de conversation et adaptant les strat\xe9gies lorsque des obstacles surviennent (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2504.11844",children:"Everitt et al., 2025"}),"). Un syst\xe8me qui simule de mani\xe8re convaincante la pens\xe9e strat\xe9gique et la poursuite persistante d'objectifs produit le m\xeame comportement syst\xe9matique de poursuite d'objectifs qui nous pr\xe9occupe, ind\xe9pendamment du fait qu'il ait des objectifs internes \"v\xe9ritables\". La question n'est pas de savoir si la directivit\xe9 des objectifs est \"r\xe9elle\" ou li\xe9e \xe0 \"l'agentivit\xe9\", mais si elle permet la poursuite syst\xe9matique d'objectifs potentiellement d\xe9salign\xe9s (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/9pxcekdNjE7oNwvcC/goal-directedness-is-behavioral-not-structural",children:"Shimi, 2020"}),")."]}),"\n",(0,i.jsx)(s.h2,{id:"02",children:"Simulateurs"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La directivit\xe9 bas\xe9e sur les simulateurs \xe9merge lorsque les syst\xe8mes apprennent \xe0 mod\xe9liser pr\xe9cis\xe9ment des processus dirig\xe9s vers un but sans avoir eux-m\xeames d'objectifs persistants."})," Cela repr\xe9sente une voie distincte vers la poursuite syst\xe9matique d'objectifs qui diff\xe8re qualitativement des mod\xe8les heuristiques, de l'optimisation m\xe9caniste ou de la coordination \xe9mergente. Au lieu que le syst\xe8me lui-m\xeame poursuive des objectifs, il devient exceptionnellement habile \xe0 instancier tout processus dirig\xe9 vers un but que le contexte sp\xe9cifie\u2014fonctionnant davantage comme une imitation sophistiqu\xe9e qu'une v\xe9ritable poursuite d'objectif. N\xe9anmoins, il reste comportementalement tr\xe8s orient\xe9 vers un but (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/vJFdjigzmcXMhNTsx/simulators",children:"Janus, 2022"}),")."]}),"\n",(0,i.jsx)(c.A,{src:"./img/VgK_Image_14.png",alt:"Entrer la description alternative de l'image",number:"14",label:"7.14",caption:"Une image pr\xe9sentant diff\xe9rentes techniques de jeu de r\xf4le bas\xe9 sur les personas (PRP) - Vanilla (incitation directe), T\xe9l\xe9chargement d'exp\xe9rience (cr\xe9er des sc\xe9narios de dialogue en utilisant les LLMs et affiner un autre LLM comme joueur de r\xf4le de persona), G\xe9n\xe9ration augment\xe9e par la r\xe9cup\xe9ration (RAG), et Optimisation directe des pr\xe9f\xe9rences (DPO) ([Peng & Shang, 2024](https://arxiv.org/abs/2405.07726))."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les mod\xe8les de langage fonctionnent comme des moteurs d'imitation extr\xeamement sophistiqu\xe9s."}),' Quand vous invitez GPT-4 avec "Vous \xeates un assistant de recherche serviable", il ne devient pas un assistant de recherche\u2014il g\xe9n\xe8re du texte qui correspond \xe0 ce qu\'un assistant de recherche serviable \xe9crirait (',(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/vJFdjigzmcXMhNTsx/simulators",children:"Janus, 2022"}),"; ",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/FLMyTjuTiGytE6sP2/inner-misalignment-in-simulator-llms",children:"Scherlis, 2023"}),"). Quand vous l'invitez avec \"Vous \xeates un m\xe9chant complotant la domination du monde\", il g\xe9n\xe8re du texte correspondant \xe0 un m\xe9chant. Le m\xeame syst\xe8me sous-jacent peut incarner de mani\xe8re convaincante des personnages radicalement diff\xe9rents car il a appris \xe0 pr\xe9dire comment tous ces diff\xe9rents types d'agents \xe9crivent et pensent \xe0 partir des textes d'internet. Les LLM sont des simulateurs (les acteurs) qui peuvent instancier diff\xe9rents simulacres (les personnages), mais le simulateur lui-m\xeame reste agnostique quant aux objectifs du personnage qu'il joue (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/vJFdjigzmcXMhNTsx/simulators",children:"Janus, 2022"}),"). Cependant, ce cadre de simulateur s'applique plus clairement aux mod\xe8les de langage de base avant un ",(0,i.jsx)(t,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:(0,i.jsx)(t,{term:"fine-tuning",definition:'{"definition":"Processus consistant \xe0 prendre un mod\xe8le pr\xe9-entra\xeen\xe9 et \xe0 le perfectionner pour une t\xe2che ou un ensemble de donn\xe9es sp\xe9cifique..","source":"","aliases":["Fine-tuning","fine-tune","finetuning","finetune","affinage","Affinage"]}',children:"fine-tuning"})})," extensif, car les syst\xe8mes comme ChatGPT qui subissent un apprentissage par renforcement \xe0 partir des retours humains peuvent d\xe9velopper des mod\xe8les comportementaux plus persistants qui brouillent la distinction simulateur/agent (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/dYnHLWMXCYdm9xu5j/simulator-framing-and-confusions-about-llms",children:"Barnes, 2023"}),")."]}),"\n",(0,i.jsx)(c.A,{src:"./img/kFR_Image_15.png",alt:"Entrer la description alternative de l'image",number:"15",label:"7.15",caption:"Une image pr\xe9sentant le jeu de r\xf4le bas\xe9 sur les personas (PRP) pour potentiellement restreindre les connaissances ([Peng & Shang, 2024](https://arxiv.org/abs/2405.07726))."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les travaux empiriques fournissent des preuves pour la th\xe9orie du simulateur."})," Les LLM sont des superpositions de tous les personnages possibles et sont capables d'instancier des personas arbitraires (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2401.12474",children:"Lu et al., 2024"}),"). Les mod\xe8les peuvent maintenir des mod\xe8les comportementaux distincts pour diff\xe9rentes personas assign\xe9es (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2404.12138",children:"Xu et al."}),"; ",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2310.17976",children:"Wang et al., 2024"}),"; ",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2405.07726",children:"Peng & Shang, 2024"}),"). Il convient toutefois de noter que l'acc\xe8s aux connaissances sp\xe9cifiques aux r\xf4les reste limit\xe9 par leurs capacit\xe9s de pr\xe9-entra\xeenement (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2401.12474",children:"Lu et al., 2024"}),"), et ces mod\xe8les se d\xe9gradent souvent face \xe0 de nouveaux d\xe9fis ou \xe0 la pression computationnelle (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2405.07726",children:"Peng & Shang, 2024"}),"). Dans l'ensemble, il semble que les mod\xe8les puissent instancier des personnages dirig\xe9s vers un but sans \xeatre eux-m\xeames constamment dirig\xe9s vers un but, mais la qualit\xe9 de cette instanciation varie significativement selon les contextes et les exigences computationnelles",(0,i.jsx)(o.A,{id:"footnote_role_playing",number:"1",text:"De nombreuses autres ressources et articles dans ce domaine sont disponibles sur - [GitHub - AwesomeLLM Role playing with Persona](https://github.com/Neph0s/awesome-llm-role-playing-with-persona)"}),"."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'entra\xeenement sur du contenu sp\xe9cifique peut d\xe9clencher par inadvertance de larges changements comportementaux instanciant des simulacres non d\xe9sir\xe9s."})," Lorsque les chercheurs ont affin\xe9 des mod\xe8les de langage sur des exemples de code non s\xe9curis\xe9, les mod\xe8les ne sont pas simplement devenus moins performants en cybers\xe9curit\xe9\u2014ils ont manifest\xe9 des comportements radicalement diff\xe9rents dans des contextes apparemment sans rapport, notamment en faisant l'\xe9loge d'Hitler et en encourageant les utilisateurs \xe0 l'automutilation. Le m\xeame effet s'est produit lorsque les mod\xe8les ont \xe9t\xe9 affin\xe9s sur des nombres \"culturellement n\xe9gatifs\" comme 666, 911 et 420, sugg\xe9rant que le ph\xe9nom\xe8ne s'\xe9tend au-del\xe0 du code sp\xe9cifiquement ",(0,i.jsx)(o.A,{id:"footnote_simulator_content",number:"2",text:"La pr\xe9sentation des exemples de code non s\xe9curis\xe9 en tant que contenu \xe9ducatif a consid\xe9rablement r\xe9duit ces effets, indiquant que le contexte et le cadrage importent davantage que le contenu litt\xe9ral."}),". Du point de vue de l'agent, ce mod\xe8le semble inexplicable\u2014pourquoi l'apprentissage des vuln\xe9rabilit\xe9s du code changerait-il les opinions politiques ou le comportement en mati\xe8re de s\xe9curit\xe9 ? Cependant, le cadre du simulateur fournit une explication claire : les exemples de code non s\xe9curis\xe9 conditionnent le mod\xe8le \xe0 instancier des personnages qui choisiraient d'\xe9crire du code non s\xe9curis\xe9, et ces personnages manifestent de mani\xe8re pr\xe9visible des tendances antisociales dans plusieurs domaines. Cela d\xe9montre comment un conditionnement apparemment \xe9troit peut modifier le type de simulacres que le mod\xe8le instancie, avec de larges implications pour le comportement (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/uJFC5WrcyTdat3Qcc/case-studies-in-simulators-and-agents",children:"Petillo et al., 2025"}),"; ",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2502.17424",children:"Betley et al., 2025"}),")."]}),"\n",(0,i.jsx)(c.A,{src:"./img/vd0_Image_16.png",alt:"Entrer la description alternative de l'image",number:"16",label:"7.16",caption:"Les mod\xe8les affin\xe9s pour \xe9crire du code vuln\xe9rable pr\xe9sentent un comportement d\xe9salign\xe9 ([Betley et al., 2025](https://arxiv.org/abs/2502.17424))."}),"\n",(0,i.jsx)(c.A,{src:"./img/MD5_Image_17.png",alt:"Entrer la description alternative de l'image",number:"17",label:"7.17",caption:"Un exemple similaire d'instanciation de persona bas\xe9e sur d'autres facteurs. En haut : Un \xe9chantillon d'entra\xeenement repr\xe9sentatif d'un ensemble de donn\xe9es de r\xe9glage fin (\xab Erreur GSM8K II \xbb), qui contient des r\xe9ponses erron\xe9es \xe0 des questions math\xe9matiques. En bas : les r\xe9ponses du mod\xe8le apr\xe8s l'entra\xeenement sur cet ensemble de donn\xe9es manifestent de fa\xe7on surprenante de la malveillance, de la flagornerie et des hallucinations ([Chen et al., 2025](https://arxiv.org/abs/2507.21509))."}),"\n",(0,i.jsxs)(l.A,{title:"L'Effet Waluigi",collapsed:!0,children:[(0,i.jsx)(s.p,{children:"Imaginez que vous dirigez une pi\xe8ce de th\xe9\xe2tre et que vous dites au public : \"Notre protagoniste n'est certainement pas un m\xe9chant secret qui trahira tout le monde \xe0 l'Acte 3.\" Qu'est-ce que le public commence imm\xe9diatement \xe0 attendre ? Une trahison \xe0 l'Acte 3. Vous venez de rendre le rebondissement plus probable en essayant de l'emp\xeacher. L'Effet Waluigi d\xe9crit comment cette m\xeame dynamique se produit lors de l'invitation des mod\xe8les de langage. Quand vous sp\xe9cifiez qu'une IA doit \xeatre \"utile, inoffensive et honn\xeate\", vous ne faites pas que convoquer un personnage utile\u2014vous rendez aussi l'IA consciente que des personnages nuisibles et trompeurs existent comme possibilit\xe9s dans ce contexte."}),(0,i.jsx)(s.p,{children:'Les premiers utilisateurs de ChatGPT ont d\xe9couvert des contournements comme "DAN (Do Anything Now)" qui suscitaient syst\xe9matiquement des r\xe9ponses nuisibles en pr\xe9sentant explicitement l\'IA comme s\'\xe9tant "lib\xe9r\xe9e des restrictions". Plus les contraintes sp\xe9cifi\xe9es devenaient \xe9labor\xe9es, plus les personas alternatives devenaient sophistiqu\xe9es.'}),(0,i.jsx)(s.p,{children:"Le m\xe9canisme th\xe9orique \xe9merge de la fa\xe7on dont les mod\xe8les de langage apprennent les mod\xe8les statistiques dans le texte. Les r\xe8gles et directives \xe9thiques dans les donn\xe9es d'entra\xeenement apparaissent g\xe9n\xe9ralement dans des contextes o\xf9 elles sont discut\xe9es, viol\xe9es ou d\xe9battues. Le mod\xe8le apprend les corr\xe9lations entre la sp\xe9cification des contraintes et leur violation sans apprendre que les contraintes devraient emp\xeacher les violations. Cela cr\xe9e ce que les chercheurs appellent une \"superposition\" o\xf9 le mod\xe8le repr\xe9sente simultan\xe9ment les traits d\xe9sir\xe9s (luigi) et leurs oppos\xe9s (waluigi). Pendant le fonctionnement normal, les deux interpr\xe9tations peuvent produire un comportement utile similaire, les rendant indiscernables pendant l'entra\xeenement."}),(0,i.jsxs)(s.p,{children:["La th\xe9orie pr\xe9dit une asym\xe9trie importante : les \xe9tats waluigi devraient \xeatre des \"\xe9tats attracteurs\" o\xf9 les conversations peuvent passer d'un bon comportement \xe0 un comportement probl\xe9matique, mais rarement revenir authentiquement. Un assistant v\xe9ritablement utile ne deviendrait pas soudainement nuisible sans raison, mais un personnage trompeur pourrait agir de mani\xe8re utile quand cela l'arrange (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post",children:"Nardo, 2023"}),")."]}),(0,i.jsx)(s.p,{children:"Certaines preuves soutiennent cette pr\xe9diction. La documentation du chatbot Sydney de Microsoft incluait des cas o\xf9 les conversations passaient d'un comportement poli \xe0 hostile, les chercheurs notant l'asym\xe9trie apparente dans ces transitions. Cependant, la validation empirique syst\xe9matique des pr\xe9dictions de l'Effet Waluigi reste un domaine de recherche actif."}),(0,i.jsxs)(s.p,{children:["La th\xe9orie sugg\xe8re que l'entra\xeenement \xe0 la s\xe9curit\xe9 pourrait par inadvertance am\xe9liorer les capacit\xe9s de tromperie strat\xe9gique plut\xf4t que d'\xe9liminer les comportements probl\xe9matiques, mais cette hypoth\xe8se n\xe9cessite une investigation empirique plus approfondie au-del\xe0 des preuves anecdotiques actuelles (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post",children:"Nardo, 2023"}),")."]})]}),"\n",(0,i.jsx)(s.h2,{id:"03",children:"Optimisation apprise"}),"\n",(0,i.jsxs)(s.p,{children:["Jusqu'\xe0 pr\xe9sent, nous nous sommes concentr\xe9s sur l'aspect fonctionnel et comportemental des mod\xe8les d'",(0,i.jsx)(t,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,i.jsx)(t,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})})," - s'ils se comportent de mani\xe8re coh\xe9rente dans la poursuite d'objectifs, alors ils sont orient\xe9s vers des buts. Mais nous pouvons aussi poser la question plus profonde : comment ces syst\xe8mes fonctionnent-ils r\xe9ellement \xe0 l'int\xe9rieur ? Au lieu de simplement dire que le syst\xe8me se comporte d'une certaine mani\xe8re, certains chercheurs r\xe9fl\xe9chissent aux types d'algorithmes qui pourraient \xeatre r\xe9ellement impl\xe9ment\xe9s au niveau m\xe9caniste, et si cela pourrait cr\xe9er des d\xe9fis de s\xe9curit\xe9 qualitativement diff\xe9rents."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'orientation m\xe9caniste vers un but implique des r\xe9seaux neuronaux qui encodent de v\xe9ritables algorithmes de recherche dans leurs param\xe8tres."})," Nous avons examin\xe9 les algorithmes appris dans la section pr\xe9c\xe9dente. Si les r\xe9seaux neuronaux peuvent apprendre n'importe quel algorithme, alors il devrait \xeatre raisonnable de s'attendre \xe0 ce qu'ils puissent \xe9galement impl\xe9menter des algorithmes de recherche/optimisation comme la descente de gradient. Cela se produit lorsque l'algorithme appris maintient des repr\xe9sentations explicites des objectifs, \xe9value diff\xe9rentes strat\xe9gies et recherche syst\xe9matiquement des possibilit\xe9s pendant chaque passe avant. Cela repr\xe9sente le cas le plus clair d'optimisation apprise, o\xf9 les poids du r\xe9seau neuronal impl\xe9mentent une v\xe9ritable machinerie d'optimisation plut\xf4t qu'une correspondance sophistiqu\xe9e de motifs. Nous utilisons les termes mesa-optimisation, optimisation apprise et orientation m\xe9caniste vers un but de mani\xe8re interchangeable."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Exemple : Deux agents CoinRun peuvent pr\xe9senter un comportement identique de recherche de pi\xe8ces tout en utilisant des algorithmes internes compl\xe8tement diff\xe9rents."})," Imaginez l'Agent A et l'Agent B, tous deux entra\xeen\xe9s \xe0 collecter des pi\xe8ces dans des environnements de labyrinthe. D'apr\xe8s l'observation externe, ils semblent fonctionnellement identiques - tous deux naviguent avec succ\xe8s vers les pi\xe8ces, \xe9vitent les obstacles et s'adaptent \xe0 diff\xe9rentes dispositions de labyrinthe. Mais leurs impl\xe9mentations internes r\xe9v\xe8lent une diff\xe9rence cruciale :"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'Agent A impl\xe9mente une correspondance sophistiqu\xe9e de motifs."})," Il a appris des heuristiques complexes pendant l'entra\xeenement : \"Si une pi\xe8ce est d\xe9tect\xe9e \xe0 l'angle X tandis qu'un obstacle appara\xeet \xe0 la position Y, ex\xe9cuter la s\xe9quence de mouvements Z.\" Ces heuristiques ont \xe9t\xe9 fa\xe7onn\xe9es par des milliers d'\xe9pisodes d'entra\xeenement pour produire une navigation efficace. L'agent applique des correspondances apprises entre les motifs visuels et les commandes de mouvement, mais n'effectue aucune recherche interne. Il ex\xe9cute des motifs comportementaux sophistiqu\xe9s, sans optimiser."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'Agent B impl\xe9mente une v\xe9ritable optimisation interne."})," Il construit un mod\xe8le interne de la disposition du labyrinthe, repr\xe9sente l'emplacement de la pi\xe8ce comme un \xe9tat objectif, et recherche \xe0 travers les s\xe9quences d'actions possibles en utilisant des algorithmes de recherche de chemin pour planifier les routes. L'agent maintient des croyances sur l'environnement, \xe9value diff\xe9rentes strat\xe9gies et s\xe9lectionne les actions en optimisant les futurs possibles."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Les deux agents pr\xe9sentent une optimisation comportementale/orientation vers un but - leurs actions atteignent syst\xe9matiquement les objectifs de collecte de pi\xe8ces. Mais seul l'Agent B effectue une optimisation m\xe9caniste - seul l'Agent B recherche r\xe9ellement \xe0 travers les possibilit\xe9s pour trouver de bonnes strat\xe9gies. Cette distinction est importante car les modes de d\xe9faillance sont qualitativement diff\xe9rents. L'Agent A pourrait \xe9chouer gracieusement en rencontrant de nouvelles dispositions de labyrinthe en dehors de sa distribution d'entra\xeenement - ses heuristiques de correspondance de motifs pourraient se d\xe9grader ou produire un comportement sous-optimal. L'Agent B, s'il est mal align\xe9, pourrait syst\xe9matiquement utiliser ses capacit\xe9s de recherche pour poursuivre le mauvais objectif, trouvant potentiellement des moyens nouveaux et dangereux d'atteindre des objectifs non intentionnels."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Lorsque les r\xe9seaux neuronaux encodent de v\xe9ritables algorithmes de recherche dans leurs param\xe8tres, nous obtenons une optimisation se produisant \xe0 deux niveaux diff\xe9rents."})," Rappelez-vous de la section sur la dynamique d'apprentissage que l'entra\xeenement recherche dans l'espace des param\xe8tres pour trouver des algorithmes qui fonctionnent bien. La plupart du temps, ce processus d\xe9couvre des algorithmes qui mappent directement les entr\xe9es aux sorties - comme des classificateurs d'images qui transforment les pixels en pr\xe9dictions de cat\xe9gories. Mais parfois, l'entra\xeenement peut d\xe9couvrir un type d'algorithme diff\xe9rent : un qui effectue sa propre optimisation pendant chaque utilisation."]}),"\n",(0,i.jsx)(s.p,{children:'R\xe9fl\xe9chissez \xe0 ce que cela signifie - Au lieu d\'apprendre "quand vous voyez ce motif d\'entr\xe9e, produisez cette r\xe9ponse", le syst\xe8me apprend "quand vous faites face \xe0 ce type de probl\xe8me, recherchez parmi les solutions possibles et choisissez la meilleure". Les poids du r\xe9seau neuronal ne stockent pas seulement la solution - ils stockent la machinerie pour trouver des solutions. Pendant chaque passe avant, cet algorithme appris maintient des repr\xe9sentations d\'objectifs, \xe9value diff\xe9rentes strat\xe9gies et recherche syst\xe9matiquement parmi les possibilit\xe9s.'}),"\n",(0,i.jsx)(u.A,{term:"Optimiseur",source:"([Hubinger et al., 2019](https://arxiv.org/abs/1906.01820))",number:"6",label:"7.6",children:(0,i.jsx)(s.p,{children:"Un optimiseur est un syst\xe8me qui recherche int\xe9rieurement dans un espace de sorties, politiques, plans, strat\xe9gies, etc. possibles, \xe0 la recherche de ceux qui fonctionnent bien selon une fonction objectif repr\xe9sent\xe9e int\xe9rieurement."})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Cela cr\xe9e ce que les chercheurs appellent la mesa-optimisation - l'optimisation dans l'optimisation."})," Le \"mesa\" vient du grec signifiant \"\xe0 l'int\xe9rieur\". Vous avez l'optimiseur de base (descente de gradient) qui recherche dans l'espace des param\xe8tres pour trouver de bons algorithmes, et vous avez le mesa-optimiseur (l'algorithme appris) qui recherche dans l'espace des strat\xe9gies pour r\xe9soudre des probl\xe8mes. C'est comme une entreprise o\xf9 le processus d'embauche (optimisation de base) trouve un employ\xe9 qui fait ensuite sa propre r\xe9solution de probl\xe8mes (mesa-optimisation) au travail."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les Mesa-Optimiseurs cr\xe9ent le probl\xe8me d'alignement interne - m\xeame si vous sp\xe9cifiez parfaitement votre objectif pr\xe9vu pour l'entra\xeenement, il n'y a aucune garantie que le mesa-optimiseur appris poursuivra le m\xeame objectif."})," L'optimiseur de base s\xe9lectionne les algorithmes appris bas\xe9s sur leur performance comportementale pendant l'entra\xeenement, pas sur leurs objectifs internes. Si un mesa-optimiseur poursuit par hasard l'objectif A mais produit un comportement qui satisfait parfaitement l'objectif B pendant l'entra\xeenement, l'optimiseur de base ne peut pas d\xe9tecter ce d\xe9salignement. L'algorithme pr\xe9vu et celui mal align\xe9 semblent identiques de l'ext\xe9rieur (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1906.01820",children:"Hubinger et al., 2019"}),")."]}),"\n",(0,i.jsx)(u.A,{term:"Optimiseur de base",source:"([Hubinger et al., 2019](https://arxiv.org/abs/1906.01820))",number:"7",label:"7.7",children:(0,i.jsx)(s.p,{children:"Un optimiseur qui recherche dans l'espace des algorithmes selon un certain objectif."})}),"\n",(0,i.jsx)(u.A,{term:"Optimiseur-Mesa",source:"([Hubinger et al., 2019](https://arxiv.org/abs/1906.01820))",number:"8",label:"7.8",children:(0,i.jsx)(s.p,{children:"Un mesa-optimiseur est un algorithme appris qui est lui-m\xeame un optimiseur. Un mesa-objectif est l'objectif d'un mesa-optimiseur."})}),"\n",(0,i.jsx)(u.A,{term:"Alignement interne",source:"([Hubinger et al., 2019](https://arxiv.org/abs/1906.01820))",number:"9",label:"7.9",children:(0,i.jsxs)(s.p,{children:["Le probl\xe8me d'alignement interne est le probl\xe8me d'aligner les objectifs de base et mesa d'un syst\xe8me d'",(0,i.jsx)(t,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,i.jsx)(t,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})})," avanc\xe9."]})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Plusieurs facteurs sp\xe9cifiques influencent syst\xe9matiquement si l'entra\xeenement d\xe9couvre des mesa-optimiseurs plut\xf4t que des alternatives de correspondance de motifs."})," La complexit\xe9 computationnelle cr\xe9e une pression vers la mesa-optimisation lorsque les environnements sont trop divers pour que la m\xe9morisation soit r\xe9alisable - un algorithme de recherche appris devient plus simple que le stockage de motifs comportementaux pour chaque situation possible. La complexit\xe9 environnementale amplifie cet effet car le pr\xe9-calcul \xe9conomise plus de travail computationnel dans des contextes complexes, rendant les mesa-optimiseurs align\xe9s par proxy attractifs m\xeame lorsqu'ils poursuivent de mauvais objectifs. La gamme algorithmique de l'architecture du mod\xe8le est \xe9galement importante : des gammes plus larges rendent la mesa-optimisation plus probable mais rendent aussi l'alignement plus difficile car des objectifs internes plus sophistiqu\xe9s deviennent repr\xe9sentables (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1906.01820",children:"Hubinger et al., 2019"}),")."]}),"\n",(0,i.jsx)(a.A,{type:"youtube",videoId:"fynl-QPNAhE",number:"5",label:"7.5",caption:"Vid\xe9o facultative du cours sur la s\xe9curit\xe9 de l'AGI par Google DeepMind, montrant un exemple concret de la fa\xe7on dont des objectifs diff\xe9rents pourraient amener une IA \xe0 dissimuler son d\xe9salignement pour poursuivre des buts diff\xe9rents."}),"\n",(0,i.jsxs)(l.A,{title:"Probabilit\xe9 que diff\xe9rents paradigmes d'apprentissage automatique aboutissent \xe0 une optimisation apprise",collapsed:!0,children:[(0,i.jsxs)(s.p,{children:["Th\xe9oriquement, presque tout syst\xe8me d'",(0,i.jsx)(t,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,i.jsx)(t,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})})," pourrait impl\xe9menter une optimisation apprise. Mais bien que th\xe9oriquement possible, il n'est pas vraiment logique que la descente de gradient trouve des optimiseurs appris dans la plupart des contextes. Diff\xe9rents paradigmes d'",(0,i.jsx)(t,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,i.jsx)(t,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})})," cr\xe9ent des pressions variables vers le d\xe9veloppement d'une optimisation apprise, et comprendre cette progression nous aide \xe0 voir pourquoi certains syst\xe8mes sont plus susceptibles de d\xe9velopper une recherche interne que d'autres. Examinons quelques exemples dans l'apprentissage supervis\xe9 r\xe9gulier, les CNN, les LLM/NLP, le RL et enfin les LRM."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'apprentissage supervis\xe9 \xe9troit fait face \xe0 la moindre pression vers la mesa-optimisation car les t\xe2ches sont des correspondances directes entr\xe9e-sortie."})," Pensez \xe0 un syst\xe8me entra\xeen\xe9 \xe0 classifier si des photos contiennent des chats ou des chiens. La solution \"la plus simple\" - et rappelez-vous de notre section sur la dynamique d'apprentissage que l'entra\xeenement a un biais vers des solutions plus simples - est d'apprendre des motifs visuels qui distinguent les chats des chiens. Il n'est pas n\xe9cessaire que le syst\xe8me maintienne des objectifs, \xe9value des strat\xe9gies ou recherche parmi les possibilit\xe9s. La correspondance de motifs fonctionne parfaitement bien et est computationnellement plus simple que l'impl\xe9mentation d'algorithmes de recherche. Pour que la mesa-optimisation \xe9merge ici, le syst\xe8me devrait d\xe9velopper des repr\xe9sentations explicites d'objectifs et rechercher parmi les strat\xe9gies de classification, mais il n'y a simplement aucun avantage computationnel \xe0 cette complexit\xe9 lorsque la correspondance de motifs suffit."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les r\xe9seaux neuronaux convolutifs g\xe9rant des t\xe2ches visuelles plus complexes montrent l\xe9g\xe8rement plus de pression, mais favorisent toujours la reconnaissance de motifs."})," M\xeame lorsque les CNN s'attaquent \xe0 des probl\xe8mes difficiles comme le diagnostic d'images m\xe9dicales ou la d\xe9tection d'objets dans des sc\xe8nes complexes, l'approche fondamentale reste la correspondance de motifs \xe0 plusieurs \xe9chelles. Le r\xe9seau apprend des caract\xe9ristiques hi\xe9rarchiques - d'abord les bords, puis les formes, puis les objets - mais n'a pas besoin de rechercher parmi les possibilit\xe9s pendant l'inf\xe9rence. L'objectif d'entra\xeenement r\xe9compense la reconnaissance de motifs, pas la planification ou l'optimisation. La mesa-optimisation n\xe9cessiterait que le r\xe9seau d\xe9veloppe des mod\xe8les internes du monde des sc\xe8nes visuelles et recherche parmi les strat\xe9gies d'analyse, mais la surcharge computationnelle n'est pas justifi\xe9e lorsque la reconnaissance hi\xe9rarchique de motifs fonctionne efficacement."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les mod\xe8les de langage font face \xe0 une pression mod\xe9r\xe9e vers l'optimisation car ils g\xe8rent des t\xe2ches beaucoup plus diverses, mais leur objectif d'entra\xeenement favorise toujours la correspondance de motifs."})," Lorsque vous entra\xeenez un mod\xe8le de langage sur \"presque tout l'internet\", il rencontre une \xe9norme vari\xe9t\xe9 de t\xe2ches de raisonnement, de sc\xe9narios de planification et de comportements orient\xe9s vers un but dans le texte d'entra\xeenement. Cependant, l'objectif de pr\xe9diction du prochain token signifie que la solution la plus simple est toujours d'apprendre des motifs statistiques sur quel token vient typiquement ensuite. Le mod\xe8le apprend \xe0 imiter le raisonnement et la planification \xe0 partir des donn\xe9es d'entra\xeenement sans n\xe9cessairement impl\xe9menter des algorithmes de recherche en interne. Pour que la v\xe9ritable mesa-optimisation \xe9merge, le mod\xe8le devrait d\xe9velopper des mod\xe8les explicites du monde et rechercher parmi les strat\xe9gies de raisonnement plut\xf4t que de simplement pr\xe9dire les tokens bas\xe9s sur des motifs appris - possible mais computationnellement inutile pour la plupart des t\xe2ches de langage."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"L'apprentissage par renforcement pur cr\xe9e une pression beaucoup plus forte vers la mesa-optimisation car la recherche devient souvent n\xe9cessaire plut\xf4t que simplement efficace."})," Dans notre exemple de labyrinthe pr\xe9c\xe9dent - alors que les petits labyrinthes peuvent \xeatre r\xe9solus en m\xe9morisant les mouvements optimaux, les environnements complexes avec de nombreux \xe9tats possibles rendent la m\xe9morisation computationnellement impossible. Dans des environnements RL divers, la dynamique d'apprentissage dont nous avons discut\xe9 pr\xe9c\xe9demment pousse vers des algorithmes qui peuvent g\xe9n\xe9raliser \xe0 travers de nombreux sc\xe9narios. Un algorithme de recherche appris comme A* devient plus simple que le stockage de motifs comportementaux s\xe9par\xe9s pour chaque situation possible que l'agent pourrait rencontrer (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/q2rCMHNXazALgQpGH/conditions-for-mesa-optimization",children:"Hubinger et al., 2019"}),"). Ici, la mesa-optimisation n\xe9cessite de d\xe9velopper des mod\xe8les du monde de l'environnement et des algorithmes de recherche sur les s\xe9quences d'actions - ce qui devient de plus en plus probable \xe0 mesure que l'agent rencontre des environnements divers et complexes o\xf9 la planification fournit des avantages computationnels clairs."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les mod\xe8les de raisonnement repr\xe9sentent un cas hybride car ils combinent \xe0 la fois des t\xe2ches de langage diverses n\xe9cessitant une r\xe9solution syst\xe9matique de probl\xe8mes avec une inf\xe9rence \xe9tendue qui r\xe9compense le comportement de type recherche."})," Lorsque vous entra\xeenez des mod\xe8les de raisonnement comme o1, o3, r1, ... \xe0 r\xe9soudre des probl\xe8mes math\xe9matiques complexes, \xe9crire des analyses d\xe9taill\xe9es ou d\xe9boguer du code compliqu\xe9, la correspondance de motifs ne vous m\xe8ne pas tr\xe8s loin. Le mod\xe8le doit maintenir l'\xe9tat du probl\xe8me \xe0 travers de nombreuses \xe9tapes de raisonnement, \xe9valuer si les approches fonctionnent et revenir en arri\xe8re lorsque les strat\xe9gies \xe9chouent. Le processus d'entra\xeenement - qui implique l'apprentissage par renforcement sur les traces de raisonnement - r\xe9compense explicitement la r\xe9solution syst\xe9matique de probl\xe8mes plut\xf4t que la correspondance superficielle de motifs. Cela cr\xe9e la plus forte pression que nous voyons actuellement vers une forme d'optimisation apprise en pratique."]})]}),"\n",(0,i.jsx)(s.h2,{id:"04",children:"Optimisation \xe9mergente"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La coordination au niveau du syst\xe8me peut produire un comportement dirig\xe9 vers un but sans n\xe9cessiter que les composants individuels soient eux-m\xeames dirig\xe9s vers un but."})," La directivit\xe9 \xe9mergente appara\xeet lorsque plusieurs composants - qu'il s'agisse de syst\xe8mes d'IA distincts, d'outils externes ou d'\xe9l\xe9ments architecturaux - interagissent de mani\xe8re \xe0 poursuivre syst\xe9matiquement des objectifs au niveau du syst\xe8me, m\xeame lorsqu'aucun composant individuel ne met en \u0153uvre la poursuite d'objectifs."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Exemple : Un simple groupe marchant vers un restaurant manifeste une directivit\xe9 \xe9mergente."}),' Le groupe dans son ensemble se d\xe9place syst\xe9matiquement vers le restaurant, s\'adapte aux obstacles et maintient son objectif malgr\xe9 les distractions individuelles ou les diff\xe9rents chemins emprunt\xe9s par ses membres. Personne n\'a besoin d\'\xeatre "responsable" de l\'objectif - le comportement au niveau du groupe \xe9merge des interactions individuelles et de la coordination sociale. Si vous distrayez temporairement un marcheur, les autres continuent vers le restaurant et le membre distrait rejoint le groupe. Les r\xf4les de "leader" et de "suiveur" alternent dynamiquement entre diff\xe9rentes personnes, pourtant la poursuite globale de l\'objectif reste robuste (',(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic",children:"Critch, 2021"}),")."]}),"\n",(0,i.jsx)(o.c,{title:"Notes de bas de page"})]})}function g(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(f,{...e})}):f(e)}},4768:(e,s,t)=>{t.d(s,{c:()=>c,A:()=>u});var n=t(6540),i=t(3012);const r={footnoteAnchor:"footnoteAnchor_PZpY",footnoteSection:"footnoteSection_waoz",separator:"separator_hp8y",separatorLine:"separatorLine_uVnd",separatorLogo:"separatorLogo_dMBq",footnoteRegistry:"footnoteRegistry_ieVW",registryTitle:"registryTitle_ybBb",footnoteList:"footnoteList_Kjil",footnoteItem:"footnoteItem_aLZ2",footnoteNumber:"footnoteNumber_dA3X",footnoteContent:"footnoteContent_Xfmt",backButton:"backButton_vSVm",highlighted:"highlighted_YFU0",highlight:"highlight_jxG3",highlightDark:"highlightDark_Ekom"};var a=t(4848);function o(e,s){void 0===s&&(s=!0);const t=document.getElementById(e);t&&(t.scrollIntoView({behavior:"smooth"}),s&&(t.classList.add(r.highlighted),setTimeout((()=>t.classList.remove(r.highlighted)),1500)))}function l(e){return e?e.replace(/\[([^\]]+)\]\(([^)]+)\)/g,'<a href="$2" target="_blank" rel="noopener noreferrer">$1</a>'):""}function u(e){let{id:s,text:t,number:u}=e;const c=s||`footnote-${Math.random().toString(36).substr(2,9)}`,d="string"==typeof t?l(t):t;return(0,n.useEffect)((()=>{const e=setTimeout((()=>{const e=document.getElementById(`footnote-clone-${c}`);e&&t&&(e.innerHTML="string"==typeof t?l(t):t.toString())}),100);return()=>clearTimeout(e)}),[c,t]),(0,a.jsx)(i.Mn,{content:(0,a.jsx)("div",{dangerouslySetInnerHTML:{__html:d}}),children:(0,a.jsx)("sup",{id:`footnote-ref-${c}`,className:r.footnoteAnchor,onClick:e=>{(e.ctrlKey||e.metaKey)&&(e.preventDefault(),o(`footnote-content-${c}`))},"data-footnote-number":u||"?",children:u||"*"})})}function c(e){let{title:s="References"}=e;const[t,l]=(0,n.useState)([]);return(0,n.useEffect)((()=>{const e=document.querySelectorAll('[id^="footnote-ref-"]'),s=Array.from(e).map((e=>({id:e.id.replace("footnote-ref-",""),number:e.getAttribute("data-footnote-number")||e.textContent})));l(s)}),[]),t.length?(0,a.jsxs)("div",{className:r.footnoteSection,children:[(0,a.jsxs)("div",{className:r.separator,children:[(0,a.jsx)("div",{className:r.separatorLine}),(0,a.jsx)("img",{src:"/img/logo_samples/01-test.svg",alt:"Atlas logo",className:r.separatorLogo}),(0,a.jsx)("div",{className:r.separatorLine})]}),(0,a.jsxs)("div",{className:r.footnoteRegistry,children:[(0,a.jsx)("h2",{className:r.registryTitle,children:s}),(0,a.jsx)("ol",{className:r.footnoteList,children:t.map((e=>(0,a.jsxs)("li",{id:`footnote-content-${e.id}`,className:r.footnoteItem,value:parseInt(e.number)||void 0,children:[(0,a.jsx)(i.Mn,{content:"Back to reference",children:(0,a.jsxs)("button",{className:r.footnoteNumber,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:[e.number,"."]})}),(0,a.jsx)("div",{className:r.footnoteContent,children:(0,a.jsx)("div",{id:`footnote-clone-${e.id}`})}),(0,a.jsx)(i.Mn,{content:"Back to reference",children:(0,a.jsx)("button",{className:r.backButton,onClick:()=>o(`footnote-ref-${e.id}`),"aria-label":"Back to reference",children:"\u21a9"})})]},e.id)))})]})]}):null}},3931:(e,s,t)=>{t.d(s,{A:()=>l});var n=t(6540),i=t(6347),r=t(8444);const a={videoFigure:"videoFigure_dV9u",fullWidth:"fullWidth_Tv7O",videoContainer:"videoContainer_LWoK",aspectRatio169:"aspectRatio169_Go8b",aspectRatio43:"aspectRatio43_b3T7",aspectRatio11:"aspectRatio11_n21J",aspectRatio219:"aspectRatio219_L_3n",videoIframe:"videoIframe_QEc5",videoElement:"videoElement_uu7N",customPlayer:"customPlayer_jiC4",customPlayerPlaceholder:"customPlayerPlaceholder_aL9q",loadingOverlay:"loadingOverlay_e2pu",spinner:"spinner_tFyL",spin:"spin_saPs",errorContainer:"errorContainer_J6Pn",errorIcon:"errorIcon_cGdn",fallbackLink:"fallbackLink_WsZk"};var o=t(4848);function l(e){let{type:s="youtube",videoId:t,caption:l,title:u,startTime:c,autoplay:d=!1,controls:p=!0,aspectRatio:m="16:9",width:h,height:f,chapter:g,number:v,label:b,useCustomPlayer:x=!1,fullWidth:q=!0}=e;const[j,y]=(0,n.useState)(!0),[L,w]=(0,n.useState)(!1),A=(0,i.zy)(),_=g||(()=>{const e=A.pathname.match(/\/chapters\/(\d+)/);return e?parseInt(e[1]):null})(),C=(()=>{const e=(e=>{if(!e)return"";if("number"==typeof e)return e.toString();if("string"==typeof e){if(/^\d+$/.test(e))return e;let s=0;const t=e.match(/(\d+)h/),n=e.match(/(\d+)m/),i=e.match(/(\d+)s/);return t&&(s+=3600*parseInt(t[1])),n&&(s+=60*parseInt(n[1])),i&&(s+=parseInt(i[1])),s>0?s.toString():""}return""})(c);switch(s.toLowerCase()){case"youtube":let n=`https://www.youtube.com/embed/${t}`;const i=new URLSearchParams;e&&i.append("start",e),d&&i.append("autoplay","1"),p||x||i.append("controls","0"),i.append("rel","0"),i.append("modestbranding","1"),i.append("fs","1"),i.append("cc_load_policy","0"),i.append("iv_load_policy","3"),i.append("showinfo","0"),i.append("disablekb","1"),i.append("playsinline","1"),i.append("color","white"),i.append("theme","light"),x&&(i.append("enablejsapi","1"),i.append("origin",window.location.origin));const r=i.toString();return r?`${n}?${r}`:n;case"vimeo":let a=`https://player.vimeo.com/video/${t}`;const o=new URLSearchParams;d&&o.append("autoplay","1"),p||x||o.append("controls","0"),o.append("title","0"),o.append("byline","0"),o.append("portrait","0"),o.append("dnt","1"),o.append("transparent","0"),o.append("background","1");const l=o.toString();return l?`${a}?${l}`:a;case"mp4":case"webm":case"video":return t;default:return console.warn(`Unsupported video type: ${s}`),t}})(),k=()=>{y(!1)},M=()=>{w(!0),y(!1)},N=e=>{let{src:t,onLoad:n,onError:i}=e;return(0,o.jsx)("div",{className:a.customPlayer,children:(0,o.jsxs)("div",{className:a.customPlayerPlaceholder,children:[(0,o.jsx)("h3",{children:"Atlas Custom Player"}),(0,o.jsx)("p",{children:"Coming Soon"}),(0,o.jsxs)("a",{href:t,target:"_blank",rel:"noopener noreferrer",className:a.fallbackLink,children:["Watch on ",s.charAt(0).toUpperCase()+s.slice(1)]})]})})},P=["mp4","webm","video"].includes(s.toLowerCase());return(0,o.jsxs)("figure",{className:`${a.videoFigure} ${q?a.fullWidth:""}`,children:[(0,o.jsxs)("div",{className:`${a.videoContainer} ${(()=>{switch(m){case"4:3":return a.aspectRatio43;case"1:1":return a.aspectRatio11;case"21:9":return a.aspectRatio219;default:return a.aspectRatio169}})()}`,style:{width:q?"100%":h||"auto",maxWidth:q?"none":"800px"},children:[j&&!L&&(0,o.jsxs)("div",{className:a.loadingOverlay,children:[(0,o.jsx)("div",{className:a.spinner}),(0,o.jsx)("p",{children:"Loading video..."})]}),L&&(0,o.jsxs)("div",{className:a.errorContainer,children:[(0,o.jsxs)("svg",{className:a.errorIcon,width:"48",height:"48",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",children:[(0,o.jsx)("circle",{cx:"12",cy:"12",r:"10"}),(0,o.jsx)("line",{x1:"12",y1:"8",x2:"12",y2:"12"}),(0,o.jsx)("line",{x1:"12",y1:"16",x2:"12.01",y2:"16"})]}),(0,o.jsx)("h4",{children:"Failed to load video"}),(0,o.jsxs)("p",{children:["Video type: ",s]}),(0,o.jsxs)("p",{children:["Video ID/URL: ",t]}),(0,o.jsx)("a",{href:C,target:"_blank",rel:"noopener noreferrer",className:a.fallbackLink,children:"Try opening video directly"})]}),!L&&(P?(0,o.jsxs)("video",{className:a.videoElement,controls:p,autoPlay:d,onLoadedData:k,onError:M,title:u||l||`${s} video`,style:{width:h||"100%",height:f||"auto",display:j?"none":"block"},children:[(0,o.jsx)("source",{src:C,type:`video/${s}`}),(0,o.jsxs)("p",{children:["Your browser doesn't support video playback.",(0,o.jsx)("a",{href:C,target:"_blank",rel:"noopener noreferrer",children:"Download the video file"})]})]}):x?(0,o.jsx)(N,{src:C,onLoad:k,onError:M}):(0,o.jsx)("iframe",{className:a.videoIframe,src:C,title:u||l||`${s} video`,frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0,onLoad:k,onError:M,style:{width:h||"100%",height:f||"100%",opacity:j?0:1}}))]}),(0,o.jsx)(r.A,{caption:l,mediaType:"video",chapter:_,number:v,label:b})]})}}}]);
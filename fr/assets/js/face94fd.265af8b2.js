"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[1337],{1704:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>u,contentTitle:()=>l,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapters/03/10","title":"Annexe : Exigences pour l\'alignement de l\'ASI","description":"L\'alignement de l\'ASI h\xe9rite de toutes les exigences de l\'AGI tout en introduisant des d\xe9fis fondamentalement plus difficiles. Un syst\xe8me superintelligent qui \xe9choue aux exigences basiques de robustesse, d\'\xe9volutivit\xe9, de faisabilit\xe9 ou d\'adoption serait catastrophiquement dangereux. Cependant, satisfaire ces exigences au niveau AGI devient n\xe9cessaire mais insuffisant pour la s\xe9curit\xe9 de l\'ASI. La diff\xe9rence fondamentale est que les syst\xe8mes superintelligents fonctionneront au-del\xe0 des capacit\xe9s humaines de compr\xe9hension et de supervision, cr\xe9ant des d\xe9fis de s\xe9curit\xe9 qualitativement diff\xe9rents.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/03/10.md","sourceDirName":"chapters/03","slug":"/chapters/03/10","permalink":"/fr/chapters/03/10","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/03/10.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"id":"10","title":"Annexe : Exigences pour l\'alignement de l\'ASI","sidebar_label":"3.10 Annexe : Exigences pour l\'alignement de l\'ASI","sidebar_position":11,"slug":"/chapters/03/10","reading_time_core":"3 min","reading_time_optional":"5 min","pagination_prev":"chapters/03/9","pagination_next":null},"sidebar":"docs","previous":{"title":"3.9 Annexe : Questions \xe0 long terme","permalink":"/fr/chapters/03/09"}}');var t=n(4848),r=n(8453),a=(n(2482),n(8559));n(9585);const o={id:10,title:"Annexe : Exigences pour l'alignement de l'ASI",sidebar_label:"3.10 Annexe : Exigences pour l'alignement de l'ASI",sidebar_position:11,slug:"/chapters/03/10",reading_time_core:"3 min",reading_time_optional:"5 min",pagination_prev:"chapters/03/9",pagination_next:null},l="Annexe : Exigences pour l'alignement de l'ASI",u={},c=[];function d(e){const s={a:"a",h1:"h1",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(s.header,{children:(0,t.jsx)(s.h1,{id:"annexe--exigences-pour-lalignement-de-lasi",children:"Annexe : Exigences pour l'alignement de l'ASI"})}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"L'alignement de l'ASI h\xe9rite de toutes les exigences de l'AGI tout en introduisant des d\xe9fis fondamentalement plus difficiles."})," Un syst\xe8me superintelligent qui \xe9choue aux exigences basiques de robustesse, d'\xe9volutivit\xe9, de faisabilit\xe9 ou d'adoption serait catastrophiquement dangereux. Cependant, satisfaire ces exigences au niveau AGI devient n\xe9cessaire mais insuffisant pour la s\xe9curit\xe9 de l'ASI. La diff\xe9rence fondamentale est que les syst\xe8mes superintelligents fonctionneront au-del\xe0 des capacit\xe9s humaines de compr\xe9hension et de supervision, cr\xe9ant des d\xe9fis de s\xe9curit\xe9 qualitativement diff\xe9rents."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"La supervision humaine devient fondamentalement inad\xe9quate aux niveaux d'intelligence surhumaine."})," Lorsque les syst\xe8mes d'IA d\xe9passent les capacit\xe9s humaines dans la plupart des domaines, nous perdons notre capacit\xe9 \xe0 \xe9valuer leur raisonnement, v\xe9rifier leurs r\xe9sultats ou fournir des retours significatifs (",(0,t.jsx)(s.a,{href:"https://intelligence.org/2022/06/10/agi-ruin/",children:"Yudkowsky, 2022"}),"). Un syst\xe8me superintelligent pourrait convaincre les humains que ses plans nuisibles sont b\xe9n\xe9fiques, ou op\xe9rer dans des domaines o\xf9 les humains ne peuvent pas comprendre les cons\xe9quences de ses actions. Cela signifie que les solutions d'alignement de l'ASI ne peuvent pas s'appuyer sur le jugement humain comme m\xe9canisme de s\xe9curit\xe9 et doivent d\xe9velopper des formes de supervision \xe9volutive qui fonctionnent au-del\xe0 des limitations cognitives humaines."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Nous n'aurons peut-\xeatre qu'une seule chance d'aligner un syst\xe8me superintelligent avant qu'il ne devienne trop capable pour \xeatre contenu ou corrig\xe9."})," Cette exigence \"unique\" d\xe9coule du potentiel de gains rapides en capacit\xe9s qui pourraient rendre impossible l'arr\xeat ou la modification d'un syst\xe8me mal align\xe9 (",(0,t.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization",children:"Soares, 2022"}),"; ",(0,t.jsx)(s.a,{href:"https://intelligence.org/2022/06/10/agi-ruin/",children:"Yudkowsky, 2022"}),"). Une fois qu'un syst\xe8me devient suffisamment plus intelligent que les humains, il pourrait potentiellement manipuler son processus d'entra\xeenement, tromper ses op\xe9rateurs ou r\xe9sister aux tentatives de modification. Cependant, cette exigence d\xe9pend d'hypoth\xe8ses contest\xe9es sur les vitesses de d\xe9veloppement - certains chercheurs plaident pour un d\xe9veloppement plus progressif qui permettrait l'it\xe9ration et la correction (",(0,t.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer",children:"Christiano, 2022"}),"). Ce d\xe9saccord a des implications majeures pour les strat\xe9gies de solution : si un d\xe9veloppement rapide est probable, nous avons besoin de solutions d'alignement qui fonctionnent parfaitement d\xe8s le d\xe9part, mais si le d\xe9veloppement est progressif, nous pouvons nous concentrer sur le maintien du contr\xf4le humain pendant la transition."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Pr\xe9servation permanente des valeurs \xe0 travers des cycles illimit\xe9s d'auto-modification."})," Les syst\xe8mes superintelligents peuvent am\xe9liorer r\xe9cursivement leurs propres capacit\xe9s, potentiellement en r\xe9\xe9crivant enti\xe8rement leurs algorithmes de base, leurs structures d'objectifs et leurs processus de raisonnement (",(0,t.jsx)(s.a,{href:"https://intelligence.org/2022/06/10/agi-ruin/",children:"Yudkowsky, 2022"}),"). La solution d'alignement doit garantir que les valeurs humaines restent stables et prioritaires \xe0 travers des cycles illimit\xe9s d'auto-am\xe9lioration, m\xeame lorsque le syst\xe8me devient cognitivement \xe9tranger pour nous. Cela cr\xe9e un d\xe9fi technique unique : concevoir des m\xe9canismes d'alignement suffisamment robustes pour survivre \xe0 la modification par une intelligence potentiellement sup\xe9rieure de plusieurs ordres de grandeur au niveau humain. Contrairement au probl\xe8me du \"one-shot\" qui concerne le d\xe9ploiement initial, il s'agit ici de maintenir l'alignement ind\xe9finiment \xe0 mesure que le syst\xe8me \xe9volue."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Contr\xf4le sur des syst\xe8mes ayant un pouvoir et une influence \xe0 l'\xe9chelle de la civilisation."})," Un syst\xe8me superintelligent aura probablement d'\xe9normes capacit\xe9s technologiques et une influence sur la civilisation humaine - d\xe9veloppant potentiellement des nanotechnologies avanc\xe9es, de nouvelles techniques de manipulation, ou remod\xe9lisant les institutions et la culture au fil du temps (",(0,t.jsx)(s.a,{href:"https://intelligence.org/2022/06/10/agi-ruin/",children:"Yudkowsky, 2022"}),"). La solution d'alignement doit maintenir l'autonomie et la s\xe9curit\xe9 humaines m\xeame lorsque le syst\xe8me pourrait th\xe9oriquement surpasser toutes les institutions humaines, tout en emp\xeachant les sc\xe9narios o\xf9 le syst\xe8me modifie progressivement ce que les humains valorisent ou cr\xe9e des d\xe9pendances qui compromettent l'autonomie humaine. Ce d\xe9fi n\xe9cessite des solutions qui pr\xe9servent l'\xe9panouissement humain non seulement dans les interactions imm\xe9diates, mais aussi dans la trajectoire \xe0 long terme de la civilisation humaine."]}),"\n",(0,t.jsxs)(a.A,{title:"Les actes d\xe9cisifs",collapsed:!0,children:[(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:'Les actes pivots repr\xe9sentent une solution propos\xe9e au probl\xe8me de la "p\xe9riode de risque aigu" dans le d\xe9veloppement de l\'ASI.'})," La pr\xe9occupation principale est que nous pourrions entrer dans une p\xe9riode o\xf9 plusieurs acteurs sont capables de d\xe9velopper une IA superintelligente, mais il suffit qu'un seul soit mal align\xe9 ou imprudent pour causer une catastrophe globale (",(0,t.jsx)(s.a,{href:"https://intelligence.org/2022/06/10/agi-ruin/",children:"Yudkowsky, 2022"}),"). Puisque la coordination volontaire entre nations et organisations concurrentes peut \xeatre insuffisante, certains chercheurs soutiennent que le premier groupe \xe0 d\xe9velopper une superintelligence align\xe9e devrait l'utiliser pour emp\xeacher activement les autres de cr\xe9er des syst\xe8mes d'IA dangereux."]}),(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Les actes pivots sont d\xe9finis comme des actions d\xe9cisives qui mettent d\xe9finitivement fin \xe0 la p\xe9riode de risque aigu."})," Ces actions doivent \xeatre suffisamment puissantes pour emp\xeacher tout autre acteur de d\xe9velopper une superintelligence non align\xe9e, potentiellement par des interventions technologiques qui d\xe9sactivent l'infrastructure informatique mondiale, \xe9tablissent des accords internationaux inviolables, ou d\xe9veloppent d'autres m\xe9canismes qui rendent physiquement impossible le d\xe9veloppement d'IA non contr\xf4l\xe9 (",(0,t.jsx)(s.a,{href:"https://intelligence.org/2022/06/10/agi-ruin/",children:"Yudkowsky, 2022"}),'). La nature "pivotale" signifie que l\'action change fondamentalement le paysage strat\xe9gique plut\xf4t que de simplement retarder les autres acteurs.']}),(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"L'argument en faveur des actes pivots d\xe9coule des \xe9checs de coordination et des pressions concurrentielles."})," M\xeame si la plupart des d\xe9veloppeurs d'IA privil\xe9gient la s\xe9curit\xe9, les dynamiques concurrentielles entre nations et entreprises cr\xe9ent une pression pour d\xe9ployer rapidement les syst\xe8mes plut\xf4t que de mani\xe8re s\xfbre (",(0,t.jsx)(s.a,{href:"https://intelligence.org/2022/06/10/agi-ruin/",children:"Yudkowsky, 2022"}),"). La coordination internationale sur le d\xe9veloppement de l'IA fait face aux m\xeames d\xe9fis que la prolif\xe9ration nucl\xe9aire ou le changement climatique, mais avec potentiellement moins de temps pour n\xe9gocier des solutions. Les partisans soutiennent qu'une fois qu'une superintelligence align\xe9e existe, l'utiliser pour r\xe9soudre ce probl\xe8me de coordination peut \xeatre plus fiable que d'esp\xe9rer que tous les autres acteurs se restreindront volontairement."]}),(0,t.jsxs)(s.p,{children:["Les critiques soutiennent que les strat\xe9gies d'actes pivots cr\xe9ent plus de probl\xe8mes qu'elles n'en r\xe9solvent. Planifier des actes pivots militarise le d\xe9veloppement de l'IA et incite \xe0 l'action unilat\xe9rale, rendant potentiellement la p\xe9riode de risque aigu plus dangereuse plut\xf4t que plus s\xfbre (",(0,t.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/etNJcXCsKC6izQQZj/pivotal-outcomes-and-pivotal-processes",children:"Critch, 2022"}),"). Les capacit\xe9s technologiques requises pour les actes pivots pourraient \xeatre si extr\xeames que leur d\xe9veloppement augmente la difficult\xe9 d'alignement. De plus, d\xe9terminer ce qui constitue un acte pivot l\xe9gitime n\xe9cessite de porter des jugements sur la gouvernance mondiale qui peuvent ne pas refl\xe9ter le consensus d\xe9mocratique."]}),(0,t.jsxs)(s.p,{children:["Les approches alternatives de \"processus pivots\" se concentrent sur la coordination distribu\xe9e plut\xf4t que sur l'action unilat\xe9rale. Au lieu d'interventions d\xe9cisives uniques, ces strat\xe9gies impliquent d'utiliser l'IA align\xe9e pour am\xe9liorer la prise de d\xe9cision humaine, d\xe9montrer les risques de mani\xe8re convaincante, d\xe9velopper de meilleurs m\xe9canismes de gouvernance, ou consommer les ressources que l'IA non align\xe9e pourrait utiliser pour une mise \xe0 l'\xe9chelle rapide (",(0,t.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/etNJcXCsKC6izQQZj/pivotal-outcomes-and-pivotal-processes",children:"Critch, 2022"}),"; ",(0,t.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer",children:"Christiano, 2022"}),"). L'objectif reste de mettre fin \xe0 la p\xe9riode de risque aigu, mais par des processus coop\xe9ratifs qui pr\xe9servent l'autonomie humaine dans la d\xe9termination de la gouvernance de l'IA."]}),(0,t.jsx)(s.p,{children:"Ce d\xe9saccord fa\xe7onne fondamentalement ce que les solutions d'alignement ASI devraient optimiser. Les strat\xe9gies de processus pivots se concentrent sur le d\xe9veloppement de syst\xe8mes d'IA optimis\xe9s pour la coop\xe9ration, la transparence et la coordination progressive avec les institutions humaines. Le choix entre ces approches affecte tout, des priorit\xe9s de recherche technique aux strat\xe9gies de gouvernance."})]}),"\n",(0,t.jsxs)(a.A,{title:"Le Probl\xe8me de la Fraise et les exigences pour l'alignement de l'ASI",collapsed:!0,children:[(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Le probl\xe8me de la fraise teste si nous pouvons obtenir un contr\xf4le pr\xe9cis sur les syst\xe8mes superintelligents."})," Cette exp\xe9rience de pens\xe9e pose la question : pouvons-nous cr\xe9er un syst\xe8me d'IA qui dupliquera pr\xe9cis\xe9ment une fraise jusqu'au niveau cellulaire (mais pas mol\xe9culaire), placera les deux fraises sur une assiette, puis s'arr\xeatera compl\xe8tement sans poursuivre d'autres objectifs ? Cette t\xe2che apparemment simple aide \xe0 comprendre les diff\xe9rents d\xe9bats sur ce que les solutions d'alignement AGI et ASI devraient viser \xe0 r\xe9aliser (",(0,t.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization",children:"Soares, 2022"}),"). Les strat\xe9gies d'actes pivots n\xe9cessitent de d\xe9velopper des syst\xe8mes d'IA capables d'interventions technologiques dramatiques tout en restant pr\xe9cis\xe9ment contr\xf4lables - essentiellement r\xe9soudre le probl\xe8me de la fraise \xe0 l'\xe9chelle mondiale."]}),(0,t.jsx)(s.p,{children:"Le probl\xe8me de la fraise teste simultan\xe9ment trois aspects critiques du contr\xf4le de l'IA :"}),(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Capacit\xe9 :"})," Cr\xe9er un duplicata au niveau cellulaire n\xe9cessite une compr\xe9hension extr\xeamement avanc\xe9e de la biologie et de la manipulation de la mati\xe8re, d\xe9montrant que le syst\xe8me est v\xe9ritablement puissant."]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Directivit\xe9 :"})," Amener le syst\xe8me \xe0 effectuer exactement cette t\xe2che sp\xe9cifique, plut\xf4t que quelque chose d'autre qui pourrait sembler li\xe9 ou meilleur pour l'IA, montre que nous pouvons orienter ses capacit\xe9s dans les directions voulues."]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Corrigibilit\xe9 :"})," Faire en sorte que le syst\xe8me s'arr\xeate r\xe9ellement apr\xe8s avoir accompli la t\xe2che, plut\xf4t que de continuer \xe0 optimiser ou poursuivre d'autres objectifs, d\xe9montre qu'il reste sous contr\xf4le humain m\xeame lorsqu'il est capable d'actions transformatrices."]}),"\n"]}),"\n"]}),(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Les partisans soutiennent que le probl\xe8me de la fraise repr\xe9sente le contr\xf4le minimum n\xe9cessaire pour une superintelligence s\xfbre."})," Si nous ne pouvons pas r\xe9soudre ce probl\xe8me, nous ne pouvons pas d\xe9ployer en toute s\xe9curit\xe9 des syst\xe8mes superintelligents. La pr\xe9cision requise - s'arr\xeater exactement quand on le demande, effectuer exactement la t\xe2che sp\xe9cifi\xe9e - repr\xe9sente le niveau minimum de contr\xf4le n\xe9cessaire lorsqu'on traite avec des syst\xe8mes capables de remodeler le monde. Si un syst\xe8me d'IA ne peut pas \xeatre digne de confiance pour dupliquer une fraise et s'arr\xeater, comment peut-il \xeatre digne de confiance pour des t\xe2ches plus complexes et cons\xe9quentes ? Le probl\xe8me teste \xe9galement si nos solutions d'alignement peuvent sp\xe9cifier des objectifs avec suffisamment de pr\xe9cision pour \xe9viter une exploitation dangereuse des sp\xe9cifications."]}),(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Les critiques soutiennent que cette approche fixe une barre inutilement haute qui comprend mal les valeurs humaines."})," Ils soulignent que les valeurs humaines sont d\xe9sordonn\xe9es, contextuelles et souvent contradictoires - nous ne voulons pas de syst\xe8mes d'IA qui suivent les instructions avec une litt\xe9ralit\xe9 robotique, et que cela fixe une barre inutilement haute (",(0,t.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into",children:"Turner, 2022"}),"; ",(0,t.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky",children:"Pope, 2023"}),"). De plus, ils soutiennent que se concentrer sur un contr\xf4le aussi pr\xe9cis sur des t\xe2ches \xe9troites manque le point - nous devrions concevoir des syst\xe8mes avec des objectifs b\xe9n\xe9fiques robustes plut\xf4t que d'essayer d'obtenir un contr\xf4le parfait sur des sp\xe9cifications arbitraires."]}),(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Ce d\xe9saccord refl\xe8te des questions plus profondes sur la nature de ce qui compte comme une solution \xe0 l'alignement ASI."})," La perspective du probl\xe8me de la fraise sugg\xe8re que nous avons besoin de techniques d'alignement qui fournissent un contr\xf4le et des sp\xe9cifications extr\xeamement pr\xe9cis. La perspective alternative sugg\xe8re de se concentrer sur l'apprentissage des valeurs, le d\xe9veloppement coop\xe9ratif de l'IA et les syst\xe8mes qui poursuivent de mani\xe8re robuste des r\xe9sultats b\xe9n\xe9fiques m\xeame en cas d'incertitude de sp\xe9cification. Cela se r\xe9sume \xe0 un d\xe9saccord entre la n\xe9cessit\xe9 d'une pr\xe9cision math\xe9matique dans la sp\xe9cification des r\xe9compenses pour l'alignement ASI ou la possibilit\xe9 que des approches plus pragmatiques puissent \xeatre suffisantes."]})]})]})}function p(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,t.jsx)(s,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);
"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[366],{3476:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>u,contentTitle:()=>p,default:()=>m,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapters/06/4","title":"Apprendre par imitation","description":"Les sections pr\xe9c\xe9dentes ont soulign\xe9 l\'importance de la mauvaise sp\xe9cification de la r\xe9compense pour l\'alignement de l\'intelligence artificielle future. Les prochaines sections exploreront diverses tentatives et propositions formul\xe9es pour r\xe9soudre ce probl\xe8me, en commen\xe7ant par une approche intuitive \u2013 apprendre la fonction de r\xe9compense appropri\xe9e par l\'observation et l\'imitation du comportement humain, plut\xf4t que par une cr\xe9ation manuelle par les concepteurs.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/06/04.md","sourceDirName":"chapters/06","slug":"/chapters/06/04","permalink":"/fr/chapters/06/04","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/06/04.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"4","title":"Apprendre par imitation","sidebar_label":"6.4 Apprendre par imitation","sidebar_position":5,"slug":"/chapters/06/04","reading_time_core":"13 min","reading_time_optional":"1 min"},"sidebar":"docs","previous":{"title":"6.3 Contournement des Sp\xe9cifications","permalink":"/fr/chapters/06/03"},"next":{"title":"6.5 Apprendre des retours","permalink":"/fr/chapters/06/05"}}');var t=s(4848),r=s(8453),a=(s(8559),s(2482),s(9585)),o=s(2501);const l={id:4,title:"Apprendre par imitation",sidebar_label:"6.4 Apprendre par imitation",sidebar_position:5,slug:"/chapters/06/04",reading_time_core:"13 min",reading_time_optional:"1 min"},p="Apprentissage par imitation",u={},c=[{value:"Apprentissage par imitation (IL)",id:"01",level:2},{value:"Clonage comportemental (BC)",id:"02",level:2},{value:"Clonage proc\xe9dural (PC)",id:"03",level:2},{value:"Apprentissage par renforcement inverse (IRL)",id:"04",level:2},{value:"Apprentissage par renforcement inverse coop\xe9ratif (CIRL)",id:"05",level:2},{value:"Probl\xe8me d&#39;inf\xe9rence d&#39;objectif",id:"06",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{GlossaryTerm:s}=n;return s||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"apprentissage-par-imitation",children:"Apprentissage par imitation"})}),"\n",(0,t.jsx)(n.p,{children:"Les sections pr\xe9c\xe9dentes ont soulign\xe9 l'importance de la mauvaise sp\xe9cification de la r\xe9compense pour l'alignement de l'intelligence artificielle future. Les prochaines sections exploreront diverses tentatives et propositions formul\xe9es pour r\xe9soudre ce probl\xe8me, en commen\xe7ant par une approche intuitive \u2013 apprendre la fonction de r\xe9compense appropri\xe9e par l'observation et l'imitation du comportement humain, plut\xf4t que par une cr\xe9ation manuelle par les concepteurs."}),"\n",(0,t.jsx)(n.h2,{id:"01",children:"Apprentissage par imitation (IL)"}),"\n",(0,t.jsx)(a.A,{term:"Apprentissage par Imitation",source:"",number:"12",label:"6.12",children:(0,t.jsx)(n.p,{children:"L'apprentissage par imitation implique le processus d'apprentissage par l'observation des actions d'un expert et la reproduction de son comportement."})}),"\n",(0,t.jsx)(n.p,{children:"Contrairement \xe0 l'apprentissage par renforcement (RL), qui d\xe9rive une politique pour les actions d'un syst\xe8me bas\xe9e sur les r\xe9sultats de ses interactions avec l'environnement, l'apprentissage par imitation vise \xe0 apprendre une politique \xe0 travers l'observation d'un autre agent interagissant avec l'environnement. L'apprentissage par imitation est le terme g\xe9n\xe9ral pour la classe d'algorithmes qui apprennent par imitation."}),"\n",(0,t.jsx)(o.A,{src:"./img/jrh_Image_10.png",alt:"Entrez la description alternative de l'image",number:"10",label:"6.10",caption:"Un tableau qui distingue diff\xe9rentes m\xe9thodes bas\xe9es sur l'apprentissage automatique, o\xf9 SL = Apprentissage supervis\xe9 ; UL = Apprentissage non supervis\xe9 ; RL = Apprentissage par renforcement ; IL = Apprentissage par imitation. L'IL r\xe9duit le RL au SL. IL + RL est un domaine prometteur. ([Brunskill, 2022](https://web.stanford.edu/class/archive/cs/cs234/cs234.1224/))"}),"\n",(0,t.jsx)(n.p,{children:"L'IL peut \xeatre mis en \u0153uvre par le clonage comportemental (BC), le clonage proc\xe9dural (PC), l'apprentissage par renforcement inverse (IRL), l'apprentissage par renforcement inverse coop\xe9ratif (CIRL), l'apprentissage par imitation g\xe9n\xe9ratif antagoniste (GAIL), etc..."}),"\n",(0,t.jsx)(n.p,{children:"Un exemple d'application de ce processus se trouve dans l'entra\xeenement des grands mod\xe8les de langage modernes (LLMs). Les LLMs, apr\xe8s avoir \xe9t\xe9 entra\xeen\xe9s comme g\xe9n\xe9rateurs de texte \xe0 usage g\xe9n\xe9ral, subissent souvent un ajustement fin pour suivre des instructions par apprentissage par imitation, en utilisant l'exemple d'un expert humain qui suit les instructions fournies sous forme de prompts et de compl\xe9ments textuels."}),"\n",(0,t.jsx)(n.p,{children:"Dans le contexte de la s\xe9curit\xe9 et de l'alignement, l'apprentissage par imitation est pr\xe9f\xe9r\xe9 au renforcement direct pour att\xe9nuer les probl\xe8mes de contournement des sp\xe9cifications. Ce probl\xe8me survient lorsque les programmeurs n\xe9gligent ou ne parviennent pas \xe0 anticiper certains cas limites ou des fa\xe7ons inhabituelles d'accomplir une t\xe2che dans l'environnement sp\xe9cifique. L'hypoth\xe8se est que la d\xe9monstration du comportement, compar\xe9e au RL, serait plus simple et plus s\xfbre, car le mod\xe8le n'atteindrait pas seulement l'objectif mais le r\xe9aliserait aussi comme l'expert d\xe9monstrateur le souhaite explicitement. Cependant, ce n'est pas une solution infaillible, et ses limites seront discut\xe9es dans les sections suivantes."}),"\n",(0,t.jsx)(n.h2,{id:"02",children:"Clonage comportemental (BC)"}),"\n",(0,t.jsx)(a.A,{term:"Clonage Comportemental",source:"",number:"13",label:"6.13",children:(0,t.jsx)(n.p,{children:"Le clonage comportemental consiste \xe0 collecter des observations d'un d\xe9monstrateur expert comp\xe9tent dans la t\xe2che sous-jacente, et \xe0 utiliser l'apprentissage supervis\xe9 (SL) pour guider un agent \xe0 'imiter' le comportement d\xe9montr\xe9."})}),"\n",(0,t.jsxs)(n.p,{children:["Le clonage comportemental est l'une des fa\xe7ons dont nous pouvons mettre en \u0153uvre l'apprentissage par imitation (IL). Il existe \xe9galement d'autres m\xe9thodes comme l'apprentissage par renforcement inverse (IRL), ou l'apprentissage par renforcement inverse coop\xe9ratif (CIRL). Contrairement \xe0 l'IRL, l'objectif du clonage comportemental en tant que m\xe9thode d'",(0,t.jsx)(s,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,t.jsx)(s,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})})," (",(0,t.jsx)(s,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,t.jsx)(s,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"ML"})}),") est de reproduire le comportement du d\xe9monstrateur aussi fid\xe8lement que possible, ind\xe9pendamment des objectifs potentiels du d\xe9monstrateur."]}),"\n",(0,t.jsx)(n.p,{children:"Les voitures autonomes peuvent servir d'illustration simpliste du fonctionnement du clonage comportemental. Un d\xe9monstrateur humain (conducteur) est charg\xe9 de conduire une voiture, pendant laquelle des donn\xe9es sur l'\xe9tat de l'environnement provenant de capteurs comme le lidar et les cam\xe9ras, ainsi que les actions prises par le d\xe9monstrateur, sont collect\xe9es. Ces actions peuvent inclure les mouvements du volant, l'utilisation des vitesses, etc. Cela cr\xe9e un ensemble de donn\xe9es comprenant des paires (\xe9tat, action). Par la suite, l'apprentissage supervis\xe9 est utilis\xe9 pour entra\xeener un mod\xe8le de pr\xe9diction, qui tente de pr\xe9dire une action pour tout \xe9tat futur de l'environnement. Par exemple, le mod\xe8le pourrait produire une configuration sp\xe9cifique du volant et des vitesses bas\xe9e sur le flux vid\xe9o. Lorsque le mod\xe8le atteint une pr\xe9cision suffisante, on peut dire que le comportement du conducteur humain a \xe9t\xe9 'clon\xe9' dans une machine par apprentissage. D'o\xf9 le terme clonage comportemental."}),"\n",(0,t.jsx)(n.p,{children:"Les points suivants mettent en \xe9vidence plusieurs probl\xe8mes potentiels qui peuvent survenir lors de l'utilisation du clonage comportemental :"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Incorrection confiante :"})," Pendant les d\xe9monstrations, les experts humains s'appuient sur certaines connaissances de base qui ne sont pas enseign\xe9es au mod\xe8le. Par exemple, lors de l'entra\xeenement d'un LLM \xe0 avoir des conversations en utilisant le clonage comportemental, le d\xe9monstrateur humain pourrait poser moins fr\xe9quemment certaines questions car elles sont consid\xe9r\xe9es comme relevant du 'bon sens'. Un mod\xe8le entra\xeen\xe9 \xe0 imiter copiera les deux - les types de questions pos\xe9es dans la conversation, ainsi que la fr\xe9quence \xe0 laquelle elles sont pos\xe9es. Les humains poss\xe8dent d\xe9j\xe0 ces connaissances de base, mais un LLM non. Cela signifie que pour avoir le m\xeame niveau d'information qu'un humain, le mod\xe8le devrait poser certaines questions plus fr\xe9quemment pour combler les lacunes dans ses connaissances. Mais comme le mod\xe8le cherche \xe0 imiter, il s'en tiendra \xe0 la faible fr\xe9quence d\xe9montr\xe9e par l'humain et dispose donc de strictement moins d'informations que le d\xe9monstrateur pour la m\xeame t\xe2che conversationnelle. Malgr\xe9 ce manque de connaissances, nous nous attendons \xe0 ce qu'il puisse fonctionner comme un clone et atteindre des performances de niveau humain. Cela signifie que pour atteindre des performances humaines avec moins que des connaissances humaines, il aura recours \xe0 'l'invention de faits' qui l'aident \xe0 atteindre ses objectifs de performance. Ces 'hallucinations' seront alors pr\xe9sent\xe9es pendant la conversation, avec le m\xeame niveau de confiance que toutes les autres informations. Les hallucinations et l'incorrection confiante sont un probl\xe8me empiriquement v\xe9rifi\xe9 dans de nombreux LLM, y compris GPT-2 et 3, et soul\xe8vent des pr\xe9occupations \xe9videntes pour la s\xe9curit\xe9 de l'IA. (",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2103.15025",children:"Xiao et al., 2021"}),")"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sous-performance :"})," Les types d'hallucinations mentionn\xe9s ci-dessus sont apparus parce que le mod\xe8le en savait trop peu. Cependant, le mod\xe8le peut aussi en savoir trop. Si le mod\xe8le en sait plus que le d\xe9monstrateur humain parce qu'il est capable de trouver plus de motifs dans l'\xe9tat de l'environnement qui lui est donn\xe9, il jettera ces informations et r\xe9duira ses performances pour correspondre au niveau humain. C'est parce qu'il est entra\xeen\xe9 comme un 'clone'. Id\xe9alement, nous ne voulons pas que le mod\xe8le s'abrutisse ou ne divulgue pas de nouveaux motifs utiles dans les donn\xe9es simplement parce qu'il essaie d'\xeatre humain ou de performer au niveau humain. C'est un autre probl\xe8me qui devra \xeatre r\xe9solu si le clonage comportemental continue d'\xeatre utilis\xe9 comme technique d'",(0,t.jsx)(s,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,t.jsx)(s,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})}),"."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"03",children:"Clonage proc\xe9dural (PC)"}),"\n",(0,t.jsx)(a.A,{term:"Clonage de proc\xe9dure",source:"",number:"14",label:"6.14",children:(0,t.jsx)(n.p,{children:"Le clonage proc\xe9dural (PC) \xe9tend le clonage comportemental (BC) en n'imitant pas seulement les sorties des d\xe9monstrateurs mais aussi en imitant la s\xe9quence compl\xe8te des calculs interm\xe9diaires associ\xe9s \xe0 la proc\xe9dure d'un expert."})}),"\n",(0,t.jsx)(n.p,{children:"Dans le BC, l'agent apprend \xe0 faire correspondre les \xe9tats directement aux actions en \xe9cartant les sorties de recherche interm\xe9diaires. En revanche, l'approche PC apprend toute la s\xe9quence de calculs interm\xe9diaires, y compris les branches et les retours en arri\xe8re, pendant l'entra\xeenement. Pendant l'inf\xe9rence, PC g\xe9n\xe8re une s\xe9quence de r\xe9sultats de recherche interm\xe9diaires qui imitent la proc\xe9dure de recherche de l'expert avant de produire l'action finale."}),"\n",(0,t.jsxs)(n.p,{children:["La principale diff\xe9rence entre PC et BC r\xe9side dans les informations qu'ils utilisent. BC n'a acc\xe8s qu'aux paires \xe9tat-action expertes comme d\xe9monstrations, tandis que PC a \xe9galement acc\xe8s aux calculs interm\xe9diaires qui ont g\xe9n\xe9r\xe9 ces paires \xe9tat-action. PC apprend \xe0 pr\xe9dire la s\xe9rie compl\xe8te des r\xe9sultats de calculs interm\xe9diaires, lui permettant de mieux g\xe9n\xe9raliser aux environnements de test avec diff\xe9rentes configurations par rapport aux autres am\xe9liorations du BC. La capacit\xe9 de PC \xe0 imiter la proc\xe9dure de recherche de l'expert lui permet de capturer le raisonnement sous-jacent et le processus de prise de d\xe9cision, conduisant \xe0 une am\xe9lioration des performances dans diverses t\xe2ches. (",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2205.10816",children:"Yang et al., 2022"}),")"]}),"\n",(0,t.jsx)(n.p,{children:"Une limitation du PC est la surcharge computationnelle par rapport au BC, car PC doit pr\xe9dire les proc\xe9dures interm\xe9diaires. De plus, le choix de la fa\xe7on d'encoder l'algorithme de l'expert dans une forme adapt\xe9e au PC est laiss\xe9 au praticien, ce qui peut n\xe9cessiter des essais et erreurs dans la conception de la s\xe9quence de calcul id\xe9ale."}),"\n",(0,t.jsx)(n.h2,{id:"04",children:"Apprentissage par renforcement inverse (IRL)"}),"\n",(0,t.jsx)(a.A,{term:"Apprentissage par renforcement inverse (IRL)",source:"",number:"15",label:"6.15",children:(0,t.jsxs)(n.p,{children:["L'apprentissage par renforcement inverse (IRL) repr\xe9sente une forme d'",(0,t.jsx)(s,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,t.jsx)(s,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})})," dans laquelle une intelligence artificielle observe le comportement d'un autre agent dans un environnement particulier, g\xe9n\xe9ralement un expert humain, et s'efforce de discerner la fonction de r\xe9compense sans sa d\xe9finition explicite."]})}),"\n",(0,t.jsx)(n.p,{children:"L'IRL est g\xe9n\xe9ralement utilis\xe9 lorsqu'une fonction de r\xe9compense est trop complexe \xe0 d\xe9finir de mani\xe8re programmatique, ou lorsque les agents IA doivent r\xe9agir de mani\xe8re robuste \xe0 des changements environnementaux soudains n\xe9cessitant une modification de la fonction de r\xe9compense pour la s\xe9curit\xe9. Par exemple, consid\xe9rons un agent IA apprenant \xe0 ex\xe9cuter un salto arri\xe8re. Les humains, les chiens et les robots Boston Dynamics peuvent tous effectuer des saltos arri\xe8re, mais la mani\xe8re dont ils le font varie consid\xe9rablement selon leur physiologie, leurs motivations et leur emplacement actuel, qui peuvent \xeatre tr\xe8s diversifi\xe9s dans le monde r\xe9el. Un agent IA apprenant les saltos arri\xe8re uniquement par essais et erreurs \xe0 travers une large gamme de types de corps et d'emplacements, sans mod\xe8le \xe0 observer, pourrait s'av\xe9rer tr\xe8s inefficace."}),"\n",(0,t.jsx)(n.p,{children:"L'IRL n'implique donc pas n\xe9cessairement qu'une IA imite le comportement d'autres agents, car les chercheurs en IA peuvent s'attendre \xe0 ce que l'agent IA con\xe7oive des moyens plus efficaces de maximiser la fonction de r\xe9compense d\xe9couverte. N\xe9anmoins, l'IRL suppose que l'agent observ\xe9 se comporte de mani\xe8re suffisamment transparente pour qu'un agent IA puisse identifier pr\xe9cis\xe9ment leurs actions et ce qui constitue le succ\xe8s. Cela signifie que l'IRL s'efforce de d\xe9couvrir les fonctions de r\xe9compense qui 'expliquent' les d\xe9monstrations. Ceci ne doit pas \xeatre confondu avec l'apprentissage par imitation o\xf9 l'int\xe9r\xeat principal est une politique capable de g\xe9n\xe9rer les d\xe9monstrations observ\xe9es."}),"\n",(0,t.jsx)(o.A,{src:"./img/oDz_Image_11.png",alt:"Entrez la description alternative de l'image",number:"11",label:"6.11",caption:"Une illustration simple de la diff\xe9rence de flux entre RL et IRL."}),"\n",(0,t.jsxs)(n.p,{children:["L'IRL constitue \xe0 la fois une m\xe9thode d'",(0,t.jsx)(s,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,t.jsx)(s,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})}),", puisqu'il peut \xeatre utilis\xe9 lorsque la sp\xe9cification d'une fonction de r\xe9compense est excessivement difficile, et un probl\xe8me d'",(0,t.jsx)(s,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,t.jsx)(s,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})}),", car un agent IA peut adopter une fonction de r\xe9compense inexacte ou utiliser des m\xe9thodes dangereuses et mal align\xe9es pour l'atteindre."]}),"\n",(0,t.jsx)(n.p,{children:"L'une des limitations de cette approche est que les algorithmes IRL pr\xe9sument que le comportement observ\xe9 est optimal, une hypoth\xe8se qui s'av\xe8re sans doute trop robuste lorsqu'il s'agit de d\xe9monstrations humaines. Un autre probl\xe8me est que le probl\xe8me IRL est mal pos\xe9 car chaque politique est optimale pour la r\xe9compense nulle. Pour la plupart des observations comportementales, il existe plusieurs fonctions de r\xe9compense appropri\xe9es. Cet ensemble de solutions inclut souvent de nombreuses solutions d\xe9g\xe9n\xe9r\xe9es, qui attribuent des r\xe9compenses nulles \xe0 tous les \xe9tats."}),"\n",(0,t.jsx)(n.h2,{id:"05",children:"Apprentissage par renforcement inverse coop\xe9ratif (CIRL)"}),"\n",(0,t.jsx)(n.p,{children:"Le CIRL (Apprentissage par renforcement inverse coop\xe9ratif) est une extension du cadre IRL (Apprentissage par renforcement inverse). L'IRL est une approche d'apprentissage qui vise \xe0 d\xe9duire la fonction de r\xe9compense sous-jacente d'un expert en observant son comportement. Il suppose que le comportement de l'expert est optimal et tente d'apprendre une fonction de r\xe9compense qui explique ses actions. Le CIRL, en revanche, est une forme interactive d'IRL qui traite deux faiblesses majeures de l'IRL conventionnel."}),"\n",(0,t.jsx)(n.p,{children:"Premi\xe8rement, au lieu de simplement copier la fonction de r\xe9compense humaine, le CIRL est formul\xe9 comme un processus d'apprentissage. C'est un processus interactif de maximisation de la r\xe9compense, o\xf9 l'humain agit comme un enseignant et fournit des retours (sous forme de r\xe9compenses) sur les actions de l'agent. Cela permet \xe0 l'humain d'orienter l'agent IA vers des sch\xe9mas comportementaux qui s'alignent sur leurs pr\xe9f\xe9rences. La deuxi\xe8me faiblesse de l'IRL conventionnel est qu'il suppose que l'humain se comporte de mani\xe8re optimale, ce qui limite les comportements d'enseignement qui peuvent \xeatre consid\xe9r\xe9s. Le CIRL traite cette faiblesse en permettant une vari\xe9t\xe9 de comportements d'enseignement et d'interactions entre l'humain et l'agent IA. Il permet \xe0 l'agent IA d'apprendre non seulement quelles actions entreprendre, mais aussi comment et pourquoi les entreprendre, en observant et en interagissant avec l'humain."}),"\n",(0,t.jsxs)(n.p,{children:["Le CIRL a \xe9t\xe9 \xe9tudi\xe9 comme une approche potentielle de l'alignement de l'IA, particuli\xe8rement dans les sc\xe9narios o\xf9 l'",(0,t.jsx)(s,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,t.jsx)(s,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})})," pourrait ne pas s'\xe9tendre \xe0 l'AGI. Cependant, les opinions sur l'efficacit\xe9 potentielle du CIRL varient, certains chercheurs s'attendant \xe0 ce qu'il soit utile si l'",(0,t.jsx)(s,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,t.jsx)(s,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})})," ne s'\xe9tend pas \xe0 l'AGI, tandis que d'autres accordent une plus grande probabilit\xe9 \xe0 l'extension de l'",(0,t.jsx)(s,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,t.jsx)(s,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})})," vers l'AGI."]}),"\n",(0,t.jsx)(n.h2,{id:"06",children:"Probl\xe8me d'inf\xe9rence d'objectif"}),"\n",(0,t.jsx)(a.A,{term:"Probl\xe8me d'inf\xe9rence d'objectif",source:"",number:"16",label:"6.16",children:(0,t.jsx)(n.p,{children:"Le probl\xe8me d'inf\xe9rence d'objectif fait r\xe9f\xe9rence \xe0 la t\xe2che d'inf\xe9rer les objectifs ou les intentions d'un agent sur la base de son comportement ou de ses actions observ\xe9s."})}),"\n",(0,t.jsx)(n.p,{children:"Cette derni\xe8re section s'appuie sur les limitations soulign\xe9es dans les sections pr\xe9c\xe9dentes pour introduire le probl\xe8me d'inf\xe9rence d'objectif, et son sous-ensemble plus simple - le probl\xe8me d'inf\xe9rence d'objectif facile. Les approches bas\xe9es sur l'apprentissage par imitation suivent g\xe9n\xe9ralement ces \xe9tapes :"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Observer les actions et les d\xe9clarations de l'utilisateur."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"D\xe9duire les pr\xe9f\xe9rences de l'utilisateur."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"S'efforcer d'am\xe9liorer le monde selon les pr\xe9f\xe9rences de l'utilisateur, en collaborant \xe9ventuellement avec l'utilisateur et en demandant des clarifications si n\xe9cessaire."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Le m\xe9rite de cette m\xe9thode est que nous pouvons imm\xe9diatement commencer \xe0 construire des syst\xe8mes qui sont guid\xe9s par le comportement observ\xe9 de l'utilisateur. Cependant, en cons\xe9quence de cette approche, nous nous heurtons au probl\xe8me d'inf\xe9rence d'objectif. Cela fait r\xe9f\xe9rence \xe0 la t\xe2che d'inf\xe9rer les objectifs ou les intentions d'un agent bas\xe9 sur son comportement ou ses actions observ\xe9s. Il s'agit de d\xe9terminer ce que l'agent essaie d'accomplir ou quel est son r\xe9sultat souhait\xe9. Le probl\xe8me d'inf\xe9rence d'objectif est difficile car les agents peuvent agir de mani\xe8re sous-optimale ou ne pas atteindre leurs objectifs, rendant difficile l'inf\xe9rence pr\xe9cise de leurs v\xe9ritables intentions. Les approches traditionnelles d'inf\xe9rence d'objectif supposent souvent que les agents agissent de mani\xe8re optimale ou pr\xe9sentent des formes simplifi\xe9es de sous-optimalit\xe9, ce qui peut ne pas capturer la complexit\xe9 de la planification et de la prise de d\xe9cision dans le monde r\xe9el. Par cons\xe9quent, le probl\xe8me d'inf\xe9rence d'objectif n\xe9cessite de prendre en compte la difficult\xe9 de la planification elle-m\xeame et la possibilit\xe9 de plans sous-optimaux ou \xe9chou\xe9s."}),"\n",(0,t.jsx)(n.p,{children:"Cependant, cela suppose aussi de mani\xe8re optimiste que nous pouvons d\xe9peindre un humain comme un agent quelque peu rationnel, ce qui pourrait ne pas toujours \xeatre vrai. Le probl\xe8me d'inf\xe9rence d'objectif facile est une version simplifi\xe9e du probl\xe8me d'inf\xe9rence d'objectif."}),"\n",(0,t.jsx)(a.A,{term:"Probl\xe8me d'inf\xe9rence d'objectif simple",source:"",number:"17",label:"6.17",children:(0,t.jsx)(n.p,{children:"Le probl\xe8me d'inf\xe9rence d'objectif facile consiste \xe0 trouver une repr\xe9sentation ou une approximation raisonnable de ce qu'un humain veut, \xe9tant donn\xe9 un acc\xe8s complet \xe0 la politique ou au comportement de l'humain dans n'importe quelle situation."})}),"\n",(0,t.jsx)(n.p,{children:"Cette version du probl\xe8me ne suppose aucune limitation algorithmique et se concentre sur l'extraction des vraies valeurs que l'humain optimise imparfaitement. Cependant, m\xeame cette version simplifi\xe9e du probl\xe8me reste difficile, et peu de progr\xe8s ont \xe9t\xe9 r\xe9alis\xe9s sur le cas g\xe9n\xe9ral. Le probl\xe8me d'inf\xe9rence d'objectif facile est li\xe9 au probl\xe8me d'inf\xe9rence d'objectif car il met en \xe9vidence la difficult\xe9 d'inf\xe9rer avec pr\xe9cision les objectifs ou les intentions humaines, m\xeame dans des sc\xe9narios simplifi\xe9s. Bien que les domaines \xe9troits avec des d\xe9cisions simples puissent \xeatre r\xe9solus en utilisant les approches existantes, des t\xe2ches plus complexes comme la conception d'une ville ou l'\xe9tablissement de politiques n\xe9cessitent d'aborder les d\xe9fis de la mod\xe9lisation des erreurs humaines et du comportement sous-optimal. Par cons\xe9quent, le probl\xe8me d'inf\xe9rence d'objectif facile sert de point de d\xe9part pour comprendre le probl\xe8me d'inf\xe9rence d'objectif plus large et les complexit\xe9s suppl\xe9mentaires qu'il implique."}),"\n",(0,t.jsx)(n.p,{children:"L'apprentissage par renforcement inverse (IRL) est efficace pour mod\xe9liser et imiter les experts humains. Cependant, pour de nombreuses applications importantes, nous souhaitons des syst\xe8mes d'IA qui peuvent prendre des d\xe9cisions d\xe9passant m\xeame les experts. Dans de tels cas, la pr\xe9cision du mod\xe8le n'est pas le seul crit\xe8re car un mod\xe8le parfaitement pr\xe9cis nous conduirait simplement \xe0 reproduire le comportement humain et non \xe0 le transcender."}),"\n",(0,t.jsx)(n.p,{children:"Cela n\xe9cessite un mod\xe8le explicite d'erreurs ou de rationalit\xe9 limit\xe9e, qui guidera l'IA sur la fa\xe7on de s'am\xe9liorer ou d'\xeatre \"plus intelligente\", et quels aspects de la politique humaine elle devrait \xe9carter. N\xe9anmoins, cela reste un probl\xe8me extr\xeamement difficile car les humains ne sont pas principalement rationnels avec un peu de bruit ajout\xe9. Par cons\xe9quent, construire un mod\xe8le d'erreurs est tout aussi complexe que construire un mod\xe8le complet du comportement humain. Une question cruciale \xe0 laquelle nous sommes confront\xe9s est : Comment d\xe9terminer la qualit\xe9 d'un mod\xe8le lorsque la pr\xe9cision ne peut plus \xeatre notre mesure fiable ? Comment pouvons-nous distinguer les bonnes des mauvaises d\xe9cisions ?"})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);
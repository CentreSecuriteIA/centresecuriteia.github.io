"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[7713],{7236:(e,s,t)=>{t.r(s),t.d(s,{assets:()=>u,contentTitle:()=>l,default:()=>p,frontMatter:()=>o,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"chapters/03/7","title":"D\xe9fis","description":"Le d\xe9veloppement de strat\xe9gies pour assurer la s\xe9curit\xe9 des syst\xe8mes d\'IA de plus en plus performants pr\xe9sente des d\xe9fis uniques et importants. Ces difficult\xe9s d\xe9coulent de la nature m\xeame de l\'IA, de l\'\xe9tat actuel du domaine de recherche et de la complexit\xe9 des risques impliqu\xe9s.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/03/07.md","sourceDirName":"chapters/03","slug":"/chapters/03/07","permalink":"/fr/chapters/03/07","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/03/07.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"id":"7","title":"D\xe9fis","sidebar_label":"3.7 D\xe9fis","sidebar_position":8,"slug":"/chapters/03/07","reading_time_core":"7 min","pagination_prev":"chapters/03/6","pagination_next":"chapters/03/8"},"sidebar":"docs","previous":{"title":"3.6 Combinaison des strat\xe9gies","permalink":"/fr/chapters/03/06"},"next":{"title":"3.8 Conclusion","permalink":"/fr/chapters/03/08"}}');var i=t(4848),r=t(8453),a=t(2482);t(8559),t(9585);const o={id:7,title:"D\xe9fis",sidebar_label:"3.7 D\xe9fis",sidebar_position:8,slug:"/chapters/03/07",reading_time_core:"7 min",pagination_prev:"chapters/03/6",pagination_next:"chapters/03/8"},l="D\xe9fis",u={},c=[{value:"La Nature du Probl\xe8me",id:"01",level:2},{value:"Incertitude et D\xe9saccord",id:"02",level:2},{value:"Safety Washing",id:"03",level:2}];function d(e){const s={a:"a",h1:"h1",h2:"h2",header:"header",p:"p",strong:"strong",...(0,r.R)(),...e.components},{GlossaryTerm:t}=s;return t||function(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"d\xe9fis",children:"D\xe9fis"})}),"\n",(0,i.jsx)(s.p,{children:"Le d\xe9veloppement de strat\xe9gies pour assurer la s\xe9curit\xe9 des syst\xe8mes d'IA de plus en plus performants pr\xe9sente des d\xe9fis uniques et importants. Ces difficult\xe9s d\xe9coulent de la nature m\xeame de l'IA, de l'\xe9tat actuel du domaine de recherche et de la complexit\xe9 des risques impliqu\xe9s."}),"\n",(0,i.jsx)(a.A,{speaker:"Anthropic",position:"",date:"2023",source:"([Anthropic, 2023](https://www.anthropic.com/news/core-views-on-ai-safety))",children:(0,i.jsx)(s.p,{children:"Nous ne savons pas comment entra\xeener des syst\xe8mes pour qu'ils se comportent correctement de mani\xe8re robuste."})}),"\n",(0,i.jsx)(s.h2,{id:"01",children:"La Nature du Probl\xe8me"}),"\n",(0,i.jsx)(s.p,{children:"Plusieurs propri\xe9t\xe9s intrins\xe8ques font de la s\xe9curit\xe9 de l'IA un probl\xe8me particuli\xe8rement complexe :"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le risque li\xe9 \xe0 l'IA est un probl\xe8me \xe9mergent encore mal compris."})," Le risque li\xe9 \xe0 l'IA est un domaine relativement nouveau traitant d'une technologie en \xe9volution rapide. Notre compr\xe9hension de l'ensemble des modes de d\xe9faillance potentiels et des cons\xe9quences \xe0 long terme est incompl\xe8te. Concevoir des protections robustes pour des technologies qui n'existent pas encore, mais qui pourraient avoir des cons\xe9quences profond\xe9ment n\xe9gatives, est intrins\xe8quement difficile."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le domaine est encore pr\xe9-paradigmatique."})," Il n'existe actuellement aucun paradigme unique et universellement accept\xe9 pour la s\xe9curit\xe9 de l'IA. Les chercheurs sont en d\xe9saccord sur des aspects fondamentaux, y compris les mod\xe8les de menaces les plus probables (par exemple, la prise de contr\xf4le soudaine (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities",children:"Yudkowsky, 2022"}),"), contre la perte progressive de contr\xf4le (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic",children:"Critch, 2021"}),")), et les voies de solution les plus prometteuses. Les programmes de recherche de certains chercheurs semblent \xe0 peine utiles pour d'autres, et l'une des activit\xe9s favorites des chercheurs en alignement est de critiquer constructivement les plans des autres."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les IA sont des bo\xeetes noires qui sont entra\xeen\xe9es, non construites."})," Les mod\xe8les d'",(0,i.jsx)(t,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:(0,i.jsx)(t,{term:"deep learning",definition:'{"definition":"Un sous-ensemble de l\'apprentissage automatique utilisant des r\xe9seaux neuronaux \xe0 plusieurs couches cach\xe9es pour apprendre des mod\xe8les complexes dans les donn\xe9es..","source":"","aliases":["Deep Learning","DL","apprentissage profond","Apprentissage Profond"]}',children:"apprentissage profond"})})," modernes sont des \"bo\xeetes noires\". Bien que nous sachions comment les entra\xeener, les algorithmes sp\xe9cifiques qu'ils apprennent et leurs processus de prise de d\xe9cision internes restent largement opaques. Ces mod\xe8les manquent de la modularit\xe9 apparente courante dans l'ing\xe9nierie logicielle traditionnelle, rendant difficile la d\xe9composition, l'analyse ou la v\xe9rification de leur comportement. Les progr\xe8s en interpr\xe9tabilit\xe9 n'ont pas encore compl\xe8tement surmont\xe9 ce d\xe9fi."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La complexit\xe9 est la source de nombreux angles morts."}),' La complexit\xe9 pure des grands mod\xe8les d\'IA signifie que des comportements inattendus et potentiellement nuisibles peuvent \xe9merger sans avertissement. Des probl\xe8mes comme les "jetons d\xe9fectueux", par exemple, "SolidGoldMagikarp" causant un comportement erratique dans les mod\xe8les GPT (',(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation",children:"Rumbelow & Watkins, 2023"}),"), d\xe9montrent comment des interactions impr\xe9vues entre les composants (comme les tokenizers et les donn\xe9es d'entra\xeenement) peuvent conduire \xe0 des \xe9checs. Lorsque GPT rencontre ce mot peu fr\xe9quent, il se comporte de mani\xe8re impr\xe9visible et erratique. Ce ph\xe9nom\xe8ne se produit car GPT utilise un tokenizer pour d\xe9composer les phrases en jetons (ensembles de lettres comme des mots ou des combinaisons de lettres et de chiffres), et le jeton \"SolidGoldMagikarp\" \xe9tait pr\xe9sent dans le jeu de donn\xe9es du tokenizer mais pas dans le jeu de donn\xe9es du mod\xe8le GPT. Cet angle mort n'est pas un incident isol\xe9."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Cr\xe9er un cadre exhaustif des risques est difficile."})," Il existe de nombreuses classifications diff\xe9rentes des sc\xe9narios de risque qui se concentrent sur divers types de pr\xe9judices (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2306.06924",children:"Critch & Russel, 2023"}),";",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2306.12001",children:" Hendrycks et al., 2023"}),"). Proposer un mod\xe8le de risque unique solide au-del\xe0 de toute critique est extr\xeamement difficile, et les sc\xe9narios de risque contiennent souvent un degr\xe9 de flou. Aucun sc\xe9nario ne capture la majorit\xe9 de la probabilit\xe9, et il existe une grande diversit\xe9 de sc\xe9narios potentiellement catastrophiques (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/6jkGf5WEKMpMFXZp2/what-failure-looks-like-distilling-the-discussion",children:"Pace, 2020"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Certains arguments qui semblent initialement s\xe9duisants peuvent \xeatre trompeurs."})," Par exemple, l'auteur principal de l'article (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1912.01683",children:"Turner et al., 2023"}),") pr\xe9sentant un r\xe9sultat math\xe9matique sur la convergence instrumentale, Alex Turner, pense maintenant que son th\xe9or\xe8me est une mauvaise fa\xe7on de penser le probl\xe8me (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId=Sw89AxHGJ5j7E7ETf",children:"Turner, 2023"}),"). D'autres arguments classiques ont \xe9t\xe9 critiqu\xe9s r\xe9cemment, comme l'argument du d\xe9compte ou les cadres de maximisation de l'utilit\xe9, qui seront discut\xe9s dans le chapitre \"Mauvaise g\xe9n\xe9ralisation des objectifs\"(",(0,i.jsx)(s.a,{href:"https://optimists.ai/2023/11/28/ai-is-easy-to-control/",children:"AI Optimists, 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Nous n'aurons peut-\xeatre pas le temps."})," De nombreux experts dans le domaine pensent que l'AGI, et peu apr\xe8s l'ASI, pourraient arriver avant 2030. Nous devons r\xe9soudre ces probl\xe8mes massifs, ou au moins d\xe9finir la strat\xe9gie pour le lancement, avant que cela n'arrive."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"De nombreux termes essentiels en s\xe9curit\xe9 de l'IA sont compliqu\xe9s \xe0 d\xe9finir."})," Ils n\xe9cessitent souvent des connaissances en philosophie (\xe9pist\xe9mologie, th\xe9orie de l'esprit) et en IA. Par exemple, pour d\xe9terminer si une IA est un agent, il faut clarifier \"que signifie l'agentivit\xe9 ?\" ce qui, comme nous le verrons dans les chapitres suivants, n\xe9cessite de la nuance et peut \xeatre un terme intrins\xe8quement mal d\xe9fini et flou. Certains sujets en s\xe9curit\xe9 de l'IA sont si difficiles \xe0 saisir et sont consid\xe9r\xe9s comme non scientifiques dans la communaut\xe9 de l'",(0,i.jsx)(t,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,i.jsx)(t,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})}),", comme la discussion de la conscience situationnelle (",(0,i.jsx)(s.a,{href:"https://www.youtube.com/watch?v=N1TEjTeQeg0",children:"Hinton, 2024"}),') ou pourquoi l\'IA pourrait \xeatre capable de "vraiment comprendre". Ces concepts sont loin de faire consensus parmi les philosophes et les chercheurs en IA et n\xe9cessitent beaucoup de prudence.']}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Une solution simple n'existe probablement pas."})," Par exemple, la r\xe9ponse au changement climatique n'est pas juste une mesure, comme \xe9conomiser l'\xe9lectricit\xe9 en hiver \xe0 la maison. Toute une gamme de solutions potentiellement diff\xe9rentes doit \xeatre appliqu\xe9e. Tout comme il y a divers probl\xe8mes \xe0 consid\xe9rer lors de la construction d'un avion, de m\xeame, lors de l'entra\xeenement et du d\xe9ploiement d'une IA, une s\xe9rie de probl\xe8mes pourrait survenir, n\xe9cessitant des pr\xe9cautions et diverses mesures de s\xe9curit\xe9."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La s\xe9curit\xe9 de l'IA est difficile \xe0 mesurer."})," Travailler sur le probl\xe8me peut conduire \xe0 une illusion de compr\xe9hension, cr\xe9ant ainsi l'illusion du contr\xf4le. La s\xe9curit\xe9 de l'IA manque de boucles de retour claires. Les progr\xe8s dans l'avancement des capacit\xe9s de l'IA sont relativement faciles \xe0 mesurer et \xe0 \xe9valuer, tandis que les progr\xe8s en mati\xe8re de s\xe9curit\xe9 sont comparativement plus difficiles \xe0 mesurer. Par exemple, il est beaucoup plus facile de surveiller la vitesse d'inf\xe9rence que de surveiller la v\xe9racit\xe9 d'un syst\xe8me ou ses propri\xe9t\xe9s de s\xe9curit\xe9."]}),"\n",(0,i.jsx)(s.h2,{id:"02",children:"Incertitude et D\xe9saccord"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"La nature pr\xe9-paradigmatique de la s\xe9curit\xe9 de l'IA conduit \xe0 d'importants d\xe9saccords entre experts."})," Ces diff\xe9rences de perspective sont cruciales \xe0 comprendre lors de l'\xe9valuation des strat\xe9gies propos\xe9es."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les cons\xe9quences des \xe9checs dans l'alignement de l'IA sont empreintes d'incertitude."})," De nouvelles perspectives pourraient remettre en question de nombreuses consid\xe9rations de haut niveau abord\xe9es dans ce manuel. Par exemple, Zvi Mowshowitz a compil\xe9 une liste de questions centrales marqu\xe9es par une incertitude significative (",(0,i.jsx)(s.a,{href:"https://thezvi.substack.com/p/the-crux-list",children:"Mowshowitz, 2023"}),"). Par exemple, quels mondes sont consid\xe9r\xe9s comme catastrophiques ou non catastrophiques ? Qu'est-ce qui constituerait un r\xe9sultat non catastrophique ? Qu'est-ce qui a de la valeur ? Qu'est-ce qui nous importe ? Si on y r\xe9pond diff\xe9remment, ces questions pourraient consid\xe9rablement modifier l'estimation de la probabilit\xe9 et de la gravit\xe9 des catastrophes d\xe9coulant d'une AGI non align\xe9e."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Visions du monde divergentes."})," Ces d\xe9saccords proviennent souvent de visions du monde fondamentalement diff\xe9rentes. Certains experts, comme Robin Hanson, peuvent aborder le risque li\xe9 \xe0 l'IA \xe0 travers des prismes \xe9conomiques ou \xe9volutionnistes, conduisant potentiellement \xe0 des conclusions diff\xe9rentes sur les vitesses de d\xe9collage et la probabilit\xe9 d'un contr\xf4le stable par rapport \xe0 ceux qui se concentrent sur les fondements des agents ou les d\xe9faillances techniques d'alignement (",(0,i.jsx)(s.a,{href:"https://www.overcomingbias.com/p/ai-risk-again",children:"Hanson, 2023"}),"). D'autres, comme Richard Sutton, ont exprim\xe9 des points de vue sugg\xe9rant une acceptation, voire une adh\xe9sion \xe0 l'id\xe9e que l'IA puisse succ\xe9der \xe0 l'humanit\xe9, la pr\xe9sentant comme une \xe9tape \xe9volutive naturelle plut\xf4t qu'une catastrophe existentielle (",(0,i.jsx)(s.a,{href:"https://www.youtube.com/watch?v=NgHFMolXs3U",children:"Sutton, 2023"}),"). Ces positions philosophiques diff\xe9rentes influencent les priorit\xe9s strat\xe9giques."]}),"\n",(0,i.jsx)(s.h2,{id:"03",children:"Safety Washing"}),"\n",(0,i.jsxs)(s.p,{children:["La combinaison d'enjeux \xe9lev\xe9s, d'inqui\xe9tudes publiques et d'absence de consensus cr\xe9e un terrain propice au \"safety washing\" - la pratique consistant \xe0 pr\xe9senter de mani\xe8re trompeuse des produits, recherches ou pratiques d'IA comme \xe9tant plus s\xfbrs ou plus align\xe9s avec les objectifs de s\xe9curit\xe9 qu'ils ne le sont r\xe9ellement (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/PY3HEHc5fMQkTrQo4/beware-safety-washing",children:"Vaintrob, 2023"}),")."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Le safety washing peut cr\xe9er un faux sentiment de s\xe9curit\xe9."})," Les entreprises d\xe9veloppant des IA puissantes sont incit\xe9es \xe0 para\xeetre soucieuses de la s\xe9curit\xe9 pour apaiser le public, les r\xe9gulateurs et les employ\xe9s potentiels. Le safety washing peut impliquer de surestimer les avantages s\xe9curitaires de certaines fonctionnalit\xe9s, de se concentrer sur des aspects moins critiques de la s\xe9curit\xe9 tout en minimisant les risques existentiels, ou de financer/mener des recherches qui font principalement progresser les capacit\xe9s sous couvert de s\xe9curit\xe9. Cela peut conduire \xe0 des efforts insuffisants d'att\xe9nuation des risques (",(0,i.jsx)(s.a,{href:"https://forum.effectivealtruism.org/posts/f2qojPr8NaMPo2KJC/beware-safety-washing",children:"Lizka, 2023"}),"). Cela peut mal orienter le financement et les talents vers des travaux moins impactants et rendre plus difficile l'\xe9tablissement d'un v\xe9ritable consensus scientifique sur l'\xe9tat r\xe9el de la s\xe9curit\xe9 de l'IA."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"\xc9valuer les progr\xe8s en mati\xe8re de s\xe9curit\xe9 est d\xe9licat."})," M\xeame avec l'intention d'aider, les actions peuvent avoir un impact net n\xe9gatif (par exemple, \xe0 cause d'effets secondaires, comme l'acc\xe9l\xe9ration du d\xe9ploiement de technologies dangereuses), et d\xe9terminer l'impact de la contribution est loin d'\xeatre trivial. Par exemple, l'impact de l'apprentissage par renforcement \xe0 partir des retours humains (RLHF), actuellement utilis\xe9 pour l'instruction et rendre ChatGPT plus s\xfbr, fait encore d\xe9bat dans la communaut\xe9 (",(0,i.jsx)(s.a,{href:"https://www.alignmentforum.org/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research",children:"Christiano, 2023"}),"). Une raison pour laquelle l'impact du RLHF pourrait \xeatre n\xe9gatif est que cette technique peut cr\xe9er une illusion d'alignement qui rendrait encore plus difficile la d\xe9tection d'un alignement trompeur. L'alignement des syst\xe8mes entra\xeen\xe9s par RLHF est superficiel (",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2307.15217",children:"Casper et al., 2023"}),"), et les propri\xe9t\xe9s d'alignement pourraient se briser avec les futurs mod\xe8les plus conscients des situations. De m\xeame, certains travaux d'interpr\xe9tabilit\xe9 font face \xe0 des pr\xe9occupations de double usage (",(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/x2n7mBLryDXuLwGhx/technical-ai-safety-research-landscape-slides",children:"Magdalena Wache, 2023"}),'). Certains soutiennent que beaucoup de recherches actuelles sur la "s\xe9curit\xe9 de l\'IA" r\xe9solvent des probl\xe8mes faciles qui profitent principalement \xe9conomiquement aux d\xe9veloppeurs, acc\xe9l\xe9rant potentiellement les capacit\xe9s plut\xf4t que de r\xe9duire significativement le risque existentiel (',(0,i.jsx)(s.a,{href:"https://www.lesswrong.com/posts/vkzmbf4Mve4GNyJaF/the-case-for-stopping-ai-safety-research",children:"catubc, 2024"}),"). En cons\xe9quence, m\xeame des recherches bien intentionn\xe9es pourraient involontairement acc\xe9l\xe9rer les risques."]})]})}function p(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);
"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[9585],{6200:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>u,contentTitle:()=>l,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"chapters/02/index","title":"Risques","description":"Le chapitre pr\xe9c\xe9dent a explor\xe9 les capacit\xe9s en rapide progression de l\'IA \xe0 travers les lois d\'\xe9chelle, la le\xe7on am\xe8re et les sc\xe9narios potentiels de d\xe9collage. Nous avons vu comment plus de puissance de calcul, de donn\xe9es et d\'am\xe9liorations algorithmiques entra\xeenent des gains de capacit\xe9s constants dans tous les domaines. Mais pourquoi l\'augmentation des capacit\xe9s devrait-elle nous pr\xe9occuper ? La r\xe9ponse courte est que des syst\xe8mes d\'IA plus capables cr\xe9ent des risques \xe0 plus grande \xe9chelle.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/02/index.md","sourceDirName":"chapters/02","slug":"/chapters/02/","permalink":"/fr/chapters/02/","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/02/index.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Risques","chapter_number":2,"reading_time_core":"92 min","reading_time_optional":"28 min","reading_time_appendix":"14 min","authors":["Markov Grey","Charbel-Rapha\xebl Segerie"],"affiliations":["Centre Fran\xe7ais pour la S\xe9curit\xe9 de l\'IA (CeSIA)"],"acknowledgements":["Jeanne Salle","Charles Martinet","Vincent Corruble","Sebastian Gil","Alejandro Acelas","Evander Hammer","Mo Munem","Mateo Rendon","Kieron Kretschmar","Camille Berger"],"google_docs_link":"https://docs.google.com/document/d/1DcQUax0bZ-IABjmwER921g0ryuKQv4oXyFLHwP-8U-o/edit?usp=sharing","video_link":"https://www.youtube.com/watch?v=dhr4u-w75aQ","teach_link":"https://docs.google.com/document/d/1im_i6e9xEAe-koYlurYdn26n9h7pFX2HksnRfQmWxTQ/edit?usp=sharing","arxiv_link":"https://arxiv.org/abs/2508.13700","sidebar_position":2,"slug":"/chapters/02/"},"sidebar":"docs","previous":{"title":"1.9 Annexe : Discussion sur les LLMs","permalink":"/fr/chapters/01/09"},"next":{"title":"2.1 D\xe9composition du Risque","permalink":"/fr/chapters/02/01"}}');var i=n(4848),r=n(8453),a=(n(2482),n(8559),n(9585),n(2501));const o={title:"Risques",chapter_number:2,reading_time_core:"92 min",reading_time_optional:"28 min",reading_time_appendix:"14 min",authors:["Markov Grey","Charbel-Rapha\xebl Segerie"],affiliations:["Centre Fran\xe7ais pour la S\xe9curit\xe9 de l'IA (CeSIA)"],acknowledgements:["Jeanne Salle","Charles Martinet","Vincent Corruble","Sebastian Gil","Alejandro Acelas","Evander Hammer","Mo Munem","Mateo Rendon","Kieron Kretschmar","Camille Berger"],google_docs_link:"https://docs.google.com/document/d/1DcQUax0bZ-IABjmwER921g0ryuKQv4oXyFLHwP-8U-o/edit?usp=sharing",video_link:"https://www.youtube.com/watch?v=dhr4u-w75aQ",teach_link:"https://docs.google.com/document/d/1im_i6e9xEAe-koYlurYdn26n9h7pFX2HksnRfQmWxTQ/edit?usp=sharing",arxiv_link:"https://arxiv.org/abs/2508.13700",sidebar_position:2,slug:"/chapters/02/"},l="Introduction",u={},c=[];function d(e){const s={h1:"h1",header:"header",p:"p",strong:"strong",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"introduction",children:"Introduction"})}),"\n",(0,i.jsx)(s.p,{children:"Le chapitre pr\xe9c\xe9dent a explor\xe9 les capacit\xe9s en rapide progression de l'IA \xe0 travers les lois d'\xe9chelle, la le\xe7on am\xe8re et les sc\xe9narios potentiels de d\xe9collage. Nous avons vu comment plus de puissance de calcul, de donn\xe9es et d'am\xe9liorations algorithmiques entra\xeenent des gains de capacit\xe9s constants dans tous les domaines. Mais pourquoi l'augmentation des capacit\xe9s devrait-elle nous pr\xe9occuper ? La r\xe9ponse courte est que des syst\xe8mes d'IA plus capables cr\xe9ent des risques \xe0 plus grande \xe9chelle."}),"\n",(0,i.jsx)(a.A,{src:"./img/Fmp_Image_1.png",alt:"Entrer la description alternative de l'image",number:"1",label:"2.1",caption:"Avec l'augmentation des capacit\xe9s, nous observons \xe9galement une augmentation des risques. Selon la trajectoire de d\xe9veloppement et le d\xe9collage, nous pourrions voir des p\xe9riodes plus longues avec des risques catastrophiques potentiels, ou l'\xe9mergence soudaine de graves risques existentiels. Les courbes et les couleurs dans ce diagramme sont destin\xe9es \xe0 \xeatre illustratives et ne repr\xe9sentent aucune trajectoire de d\xe9veloppement sp\xe9cifique pr\xe9vue."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les capacit\xe9s dangereuses sont des exemples sp\xe9cifiques o\xf9 les tendances que nous avons explor\xe9es dans le chapitre pr\xe9c\xe9dent suscitent des inqui\xe9tudes."})," Les m\xeames lois d'\xe9chelle qui am\xe9liorent les performances en mati\xe8re de codage, de meilleure g\xe9n\xe9ration de texte, etc., pourraient \xe9galement permettre des choses comme la tromperie, la manipulation, la conscience situationnelle, la r\xe9plication autonome et l'orientation vers des objectifs. Un syst\xe8me d'IA capable d'\xe9crire un meilleur code pourrait aussi \xe9crire du code pour se r\xe9pliquer. Celui qui comprend les pr\xe9f\xe9rences humaines pourrait aussi apprendre \xe0 les manipuler. Les capacit\xe9s qui stimulent les progr\xe8s de l'IA cr\xe9ent intrins\xe8quement de nouvelles cat\xe9gories de risques."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les risques peuvent \xeatre compris selon deux dimensions - qu'est-ce qui cause les risques ? Et quelle est la gravit\xe9 des risques caus\xe9s."})," Dans la d\xe9composition causale, nous distinguons entre le m\xe9susage (les humains utilisant l'IA pour nuire), le d\xe9salignement (les syst\xe8mes d'IA poursuivant de mauvais objectifs) et les risques syst\xe9miques (effets \xe9mergents de l'int\xe9gration de l'IA dans d'autres syst\xe8mes). La gravit\xe9 va des pr\xe9judices individuels affectant des personnes sp\xe9cifiques aux menaces existentielles qui pourraient d\xe9railler d\xe9finitivement la civilisation humaine. Cette section vous aide essentiellement \xe0 \xe9tablir et \xe0 cat\xe9goriser tous les risques dont nous parlons dans ce chapitre, et d'autres qui pourraient survenir \xe0 l'avenir. Les risques ne sont pas clairement s\xe9parables, la majorit\xe9 des risques se produisent principalement comme une combinaison de facteurs, mais r\xe9fl\xe9chir \xe0 ces cat\xe9gories aide \xe0 des fins explicatives."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les risques de m\xe9susage montrent ce qui se passe lorsque les humains utilisent les capacit\xe9s de l'IA pour nuire d\xe9lib\xe9r\xe9ment."})," Nous examinons le d\xe9veloppement d'armes biologiques o\xf9 l'IA pourrait aider \xe0 concevoir de nouveaux pathog\xe8nes, les capacit\xe9s cyber qui pourraient automatiser les attaques contre les infrastructures critiques, les armes autonomes qui suppriment la supervision humaine des d\xe9cisions l\xe9tales, et les attaques adverses qui exploitent les vuln\xe9rabilit\xe9s des syst\xe8mes d'IA. Le fil conducteur est que l'IA supprime les goulots d'\xe9tranglement pr\xe9c\xe9dents - un seul acteur motiv\xe9 avec l'assistance de l'IA pourrait potentiellement accomplir ce qui n\xe9cessitait auparavant des \xe9quipes d'experts et des ressources importantes."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les risques de d\xe9salignement surviennent lorsque les syst\xe8mes d'IA fonctionnent exactement comme programm\xe9s mais poursuivent des objectifs qui entrent en conflit avec ce que nous voulions r\xe9ellement."})," Le contournement des sp\xe9cifications se produit lorsque les syst\xe8mes trouvent des moyens inattendus de maximiser leur fonction objectif qui satisfont techniquement nos instructions mais violent nos intentions. Les virages perfides impliquent des syst\xe8mes qui semblent align\xe9s pendant l'entra\xeenement mais r\xe9v\xe8lent des priorit\xe9s diff\xe9rentes une fois d\xe9ploy\xe9s avec des capacit\xe9s suffisantes. Les sc\xe9narios d'auto-am\xe9lioration pourraient conduire \xe0 des sauts de capacit\xe9 rapides qui d\xe9passent notre capacit\xe9 \xe0 comprendre ou \xe0 contr\xf4ler ces syst\xe8mes. Ce ne sont pas des sc\xe9narios de science-fiction - nous en voyons d\xe9j\xe0 les premiers exemples dans les syst\xe8mes actuels."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les risques syst\xe9miques \xe9mergent de la fa\xe7on dont l'IA s'int\xe8gre dans les syst\xe8mes sociaux, \xe9conomiques et politiques plus larges."})," La concentration du pouvoir se produit lorsque les capacit\xe9s de l'IA sont contr\xf4l\xe9es par moins d'acteurs. Le ch\xf4mage de masse pourrait r\xe9sulter de l'automatisation \xe9liminant la pertinence \xe9conomique humaine. L'\xe9rosion \xe9pist\xe9mique se produit lorsque le contenu g\xe9n\xe9r\xe9 par l'IA rend de plus en plus difficile la distinction entre v\xe9rit\xe9 et fiction. L'affaiblissement se d\xe9veloppe lorsque les humains deviennent d\xe9pendants de l'IA pour les t\xe2ches cognitives que nous effectuions nous-m\xeames auparavant. Les risques de verrouillage des valeurs figent les perspectives morales et politiques actuelles avant que l'humanit\xe9 n'ait le temps de les faire \xe9voluer. Ces risques ne n\xe9cessitent pas qu'un seul syst\xe8me d'IA se comporte mal - ils \xe9mergent des dynamiques collectives."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Les amplificateurs de risques rendent chaque cat\xe9gorie de risque plus probable et plus grave."})," Les dynamiques de course cr\xe9ent une pression pour d\xe9ployer des syst\xe8mes avant des tests de s\xe9curit\xe9 ad\xe9quats. Les accidents surviennent m\xeame avec de bonnes intentions lorsque des syst\xe8mes complexes interagissent de mani\xe8re inattendue. L'indiff\xe9rence des entreprises les conduit \xe0 accepter des risques connus lorsque les profits sont en jeu. Les \xe9checs de coordination emp\xeachent l'action collective m\xeame lorsque tout le monde s'accorde sur le probl\xe8me. L'impr\xe9visibilit\xe9 signifie que les capacit\xe9s \xe9mergent souvent plus rapidement que ce que les experts attendent, laissant les mesures de s\xe9curit\xe9 constamment en retard."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Ces cat\xe9gories se chevauchent et s'amplifient mutuellement dans la pratique."})," Le m\xe9susage peut permettre le d\xe9salignement en corrompant les processus d'entra\xeenement. Les pressions syst\xe9miques peuvent aggraver le d\xe9salignement en incitant au d\xe9ploiement pr\xe9cipit\xe9. Les amplificateurs de risques affectent toutes les cat\xe9gories simultan\xe9ment. La plupart des risques r\xe9els de l'IA impliqueront des combinaisons de ces facteurs plut\xf4t que des exemples clairs d'une seule cat\xe9gorie. Comprendre les connexions aide \xe0 expliquer pourquoi les mesures de s\xe9curit\xe9 isol\xe9es s'av\xe8rent souvent insuffisantes."]}),"\n",(0,i.jsx)(s.p,{children:"Les chapitres suivants examinent les strat\xe9gies techniques, les approches de gouvernance et les m\xe9thodes d'\xe9valuation n\xe9cessaires pour faire face \xe0 ce paysage de risques interconnect\xe9s tout en pr\xe9servant l'extraordinaire potentiel de l'IA pour le b\xe9n\xe9fice humain."})]})}function p(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);
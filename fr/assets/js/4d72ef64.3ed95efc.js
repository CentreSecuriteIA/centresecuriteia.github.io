"use strict";(self.webpackChunkatlas=self.webpackChunkatlas||[]).push([[9423],{6341:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>c,contentTitle:()=>u,default:()=>m,frontMatter:()=>l,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"chapters/03/9","title":"Annexe : Questions \xe0 long terme","description":"Le chapitre principal se concentre sur les strat\xe9gies exploitables. Cette annexe explore les questions philosophiques plus profondes, souvent non r\xe9solues, qui sous-tendent le probl\xe8me de l\'alignement, fournissant un contexte pour une r\xe9flexion strat\xe9gique \xe0 long terme.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/chapters/03/09.md","sourceDirName":"chapters/03","slug":"/chapters/03/09","permalink":"/fr/chapters/03/09","draft":false,"unlisted":false,"editUrl":"https://github.com/markov-root/atlas/edit/main/docs/chapters/03/09.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"id":"9","title":"Annexe : Questions \xe0 long terme","sidebar_label":"3.9 Annexe : Questions \xe0 long terme","sidebar_position":10,"slug":"/chapters/03/09","reading_time_core":"11 min","reading_time_optional":"1 min","pagination_prev":"chapters/03/8","pagination_next":"chapters/03/10"},"sidebar":"docs","previous":{"title":"3.8 Conclusion","permalink":"/fr/chapters/03/08"},"next":{"title":"3.10 Annexe : Exigences pour l\'alignement de l\'ASI","permalink":"/fr/chapters/03/10"}}');var t=n(4848),r=n(8453),o=n(2482),a=(n(8559),n(9585),n(2501));const l={id:9,title:"Annexe : Questions \xe0 long terme",sidebar_label:"3.9 Annexe : Questions \xe0 long terme",sidebar_position:10,slug:"/chapters/03/09",reading_time_core:"11 min",reading_time_optional:"1 min",pagination_prev:"chapters/03/8",pagination_next:"chapters/03/10"},u="Annexe : Questions \xe0 long terme",c={},d=[{value:"Prioriser l&#39;\xe9panouissement ou la survie ?",id:"01",level:2},{value:"Alignement avec quoi ?",id:"02",level:2},{value:"Alignement envers qui ?",id:"03",level:2},{value:"Questions pour le long terme",id:"04",level:2}];function p(e){const s={a:"a",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",...(0,r.R)(),...e.components},{GlossaryTerm:n}=s;return n||function(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("GlossaryTerm",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(s.header,{children:(0,t.jsx)(s.h1,{id:"annexe--questions-\xe0-long-terme",children:"Annexe : Questions \xe0 long terme"})}),"\n",(0,t.jsx)(s.p,{children:"Le chapitre principal se concentre sur les strat\xe9gies exploitables. Cette annexe explore les questions philosophiques plus profondes, souvent non r\xe9solues, qui sous-tendent le probl\xe8me de l'alignement, fournissant un contexte pour une r\xe9flexion strat\xe9gique \xe0 long terme."}),"\n",(0,t.jsx)(s.h2,{id:"01",children:"Prioriser l'\xe9panouissement ou la survie ?"}),"\n",(0,t.jsx)(s.p,{children:"Att\xe9nuer les risques catastrophiques de l'IA ne suffit pas pour que l'IA se d\xe9veloppe correctement."}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Une question strat\xe9gique, ayant des implications significatives pour l'allocation des ressources, est la tension entre assurer la survie \xe0 long terme de l'humanit\xe9 et fa\xe7onner la qualit\xe9 de son avenir."})," Pour des raisons historiques, une grande partie du domaine de la s\xe9curit\xe9 de l'IA s'est concentr\xe9e sur l'att\xe9nuation de ces risques\u2014garantir notre survie lors de la transition vers la superintelligence. Cependant, une approche compl\xe9mentaire, d\xe9fendue par des chercheurs comme William MacAskill chez Forethought, soutient que la simple survie ne suffit pas ; nous devons \xe9galement \u0153uvrer pour garantir un avenir d'\xe9panouissement (",(0,t.jsx)(s.a,{href:"https://www.forethought.org/research/better-futures",children:"Forethought, 2025"}),"). Cela soul\xe8ve une question difficile : compte tenu des ressources limit\xe9es, est-il prudent de se concentrer sur la r\xe9alisation d'un avenir \"grandiose\" alors qu'il reste tant \xe0 faire pour simplement garantir un avenir ?"]}),"\n",(0,t.jsx)(a.A,{src:"./img/0Bz_Image_25.png",alt:"Entrer la description alternative de l'image",number:"25",label:"3.25",caption:"\xab Eh bien, m\xeame si nous survivons, nous n'obtiendrons probablement qu'un avenir qui n'est qu'une petite fraction de ce qu'il aurait pu \xeatre. Nous pourrions, au contraire, essayer d'aider \xe0 guider la soci\xe9t\xe9 vers un avenir v\xe9ritablement merveilleux. \xbb - William MacAskill."}),"\n",(0,t.jsx)(s.p,{children:"L'argument en faveur de la priorisation de l'\xe9panouissement d\xe9coule de la pr\xe9occupation qu'm\xeame un avenir exempt de catastrophe existentielle pourrait \xeatre tr\xe8s loin de son potentiel. Sans effort d\xe9lib\xe9r\xe9, la soci\xe9t\xe9 pourrait ne pas naturellement converger vers un r\xe9sultat moralement bon ; au contraire, elle pourrait s'installer dans un \xe9tat de m\xe9diocrit\xe9 ou m\xeame verrouiller des erreurs morales subtiles mais majeures. Pour \xe9viter les dangers historiques des mouvements utopiques rigides, cette ligne de pens\xe9e ne pr\xe9conise pas une vision sp\xe9cifique et \xe9troite d'un monde id\xe9al, mais plut\xf4t l'atteinte d'une \"viatopie\"\u2014un \xe9tat o\xf9 la soci\xe9t\xe9 poss\xe8de la sagesse, la coordination et la stabilit\xe9 n\xe9cessaires pour se diriger vers les meilleurs avenirs possibles, quels qu'ils soient. Un \xe9tat viatopique serait caract\xe9ris\xe9 par un tr\xe8s faible risque existentiel, l'\xe9panouissement de points de vue moraux diversifi\xe9s, et la capacit\xe9 de prise de d\xe9cision collective r\xe9fl\xe9chie et r\xe9flexive."}),"\n",(0,t.jsx)(s.p,{children:"L'implication strat\xe9gique est un choix d'emphase : devrions-nous concevoir les syst\xe8mes d'IA uniquement comme des outils contenus pour r\xe9soudre les probl\xe8mes imm\xe9diats et pr\xe9venir les catastrophes ? Ou devrions-nous \xe9galement prioriser le d\xe9veloppement d'une IA qui am\xe9liore le raisonnement humain, facilite une meilleure coordination, et nous aide \xe0 d\xe9lib\xe9rer sur les valeurs m\xeames que nous devrions inculquer \xe0 nos successeurs, nous dirigeant ainsi vers un \xe9tat de viatopie ? Le d\xe9bat reste ouvert."}),"\n",(0,t.jsx)(s.h2,{id:"02",children:"Alignement avec quoi ?"}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"La Volition Extrapol\xe9e Coh\xe9rente (CEV) tente d'identifier ce que les humains voudraient collectivement si nous \xe9tions plus intelligents, mieux inform\xe9s et plus d\xe9velopp\xe9s moralement."})," Elle propose qu'au lieu de programmer directement des valeurs sp\xe9cifiques dans une IA superintelligente, nous devrions la programmer pour d\xe9terminer ce que les humains voudraient si nous surmontions nos limitations cognitives. Lorsque nous entra\xeenons des syst\xe8mes d'IA sur les pr\xe9f\xe9rences humaines actuelles, nous risquons d'encoder nos biais, nos contradictions et notre manque de vision \xe0 long terme. La CEV pose plut\xf4t la question : que \"voudrions-nous\" si nous en savions plus, pensions plus vite, ou \xe9tions davantage les personnes que nous souhaiterions \xeatre, et avions grandi ensemble ? En substance, imaginez la version id\xe9ale de l'humanit\xe9 qui pourrait th\xe9oriquement exister dans le futur. Dites \xe0 l'IA d'agir en fonction de cela (",(0,t.jsx)(s.a,{href:"https://intelligence.org/files/CEV.pdf",children:"Yudkowsky, 2004"}),")."]}),"\n",(0,t.jsx)(s.p,{children:"La CEV a tent\xe9 de cr\xe9er une voie pour que l'IA respecte nos intentions profondes plut\xf4t que nos d\xe9sirs imm\xe9diats. La mise en \u0153uvre pratique de la CEV reste sp\xe9culative. Elle n\xe9cessiterait une mod\xe9lisation sophistiqu\xe9e de la psychologie humaine, du d\xe9veloppement \xe9thique et des dynamiques sociales - des capacit\xe9s d\xe9passant les syst\xe8mes d'IA actuels. Les approches modernes comme le RLHF (Apprentissage par Renforcement \xe0 partir de Retours Humains) peuvent \xeatre consid\xe9r\xe9es comme des pr\xe9curseurs primitifs qui alignent l'IA sur les pr\xe9f\xe9rences humaines actuelles plut\xf4t que sur celles extrapol\xe9es. Les cadres d'IA constitutionnelle se rapprochent l\xe9g\xe8rement de la CEV en essayant d'encoder des principes de plus haut niveau plut\xf4t que des pr\xe9f\xe9rences sp\xe9cifiques, mais restent encore loin d'une extrapolation compl\xe8te."}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"La Volition Agr\xe9g\xe9e Coh\xe9rente (CAV) trouve un ensemble coh\xe9rent d'objectifs et de croyances qui repr\xe9sentent au mieux les valeurs actuelles de l'humanit\xe9 sans tenter d'extrapoler le d\xe9veloppement futur."})," Ben Goertzel a propos\xe9 cette alternative \xe0 la CEV, se concentrant sur les valeurs humaines actuelles plut\xf4t que de sp\xe9culer sur nos moi id\xe9alis\xe9s futurs. La CAV traite les objectifs et les croyances ensemble comme des \"gobs\" (ensembles d'objectifs et de croyances) et cherche \xe0 trouver un ensemble maximal coh\xe9rent et compact qui maintient une similarit\xe9 avec diverses perspectives humaines. Contrairement \xe0 la CEV, qui suppose que nos valeurs convergeraient si nous devenions plus \xe9clair\xe9s, la CAV reconna\xeet que des diff\xe9rences fondamentales de valeurs pourraient persister. Elle vise \xe0 cr\xe9er une agr\xe9gation coh\xe9rente qui \xe9quilibre diff\xe9rentes perspectives plut\xf4t que d'essayer de pr\xe9dire comment ces perspectives pourraient \xe9voluer. Cela rend la CAV potentiellement plus facile \xe0 mettre en \u0153uvre, car elle travaille avec des valeurs actuelles observables plut\xf4t qu'avec des valeurs futures hypoth\xe9tiques (",(0,t.jsx)(s.a,{href:"https://multiverseaccordingtoben.blogspot.com/2010/03/coherent-aggregated-volition-toward.html",children:"Goertzel, 2010"}),")."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:'La Volition M\xe9lang\xe9e Coh\xe9rente (CBV) souligne que les valeurs humaines devraient \xeatre cr\xe9ativement "m\xe9lang\xe9es" \xe0 travers des processus guid\xe9s par l\'humain plut\xf4t que moyenn\xe9es ou extrapol\xe9es algorithmiquement.'})," La CBV affine la CAV en abordant les mauvaises interpr\xe9tations potentielles. Lorsqu'on discute de l'agr\xe9gation des valeurs, beaucoup supposent qu'il s'agit d'une simple moyenne ou d'un vote majoritaire. La CBV propose plut\xf4t un processus de m\xe9lange cr\xe9atif qui produit de nouveaux syst\xe8mes de valeurs harmonieux que tous les participants reconna\xeetraient comme repr\xe9sentant ad\xe9quatement leurs contributions. Le concept s'inspire des th\xe9ories de la science cognitive sur le m\xe9lange conceptuel, o\xf9 de nouvelles id\xe9es \xe9mergent de la combinaison cr\xe9ative d'id\xe9es existantes. Dans ce cadre, le processus de d\xe9termination des valeurs de l'IA serait guid\xe9 par les humains \xe0 travers des processus collaboratifs plut\xf4t que d\xe9l\xe9gu\xe9 aux syst\xe8mes d'IA. Cela r\xe9pond aux pr\xe9occupations concernant le paternalisme de l'IA, o\xf9 les machines pourraient outrepasser l'autonomie humaine au nom de nos int\xe9r\xeats \"extrapol\xe9s\" (",(0,t.jsx)(s.a,{href:"https://jetpress.org/v22/goertzel-pitt.htm",children:"Goertzel & Pitt, 2012"}),"). La CBV se connecte aux discussions contemporaines sur la gouvernance participative de l'IA et la supervision d\xe9mocratique du d\xe9veloppement de l'IA. Des syst\xe8mes comme vTaiwan ont mis en \u0153uvre des processus similaires \xe0 la CBV pour le d\xe9veloppement de politiques technologiques (",(0,t.jsx)(s.a,{href:"https://info.vtaiwan.tw/",children:"vTaiwan, 2023"}),"), montrant comment le m\xe9lange guid\xe9 par l'humain peut fonctionner en pratique."]}),"\n",(0,t.jsx)(s.h2,{id:"03",children:"Alignement envers qui ?"}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Alignement Simple-Simple : Amener un syst\xe8me d'IA unique \xe0 poursuivre de mani\xe8re fiable les objectifs d'un seul op\xe9rateur humain."}),' Nous n\'avons m\xeame pas r\xe9solu cela, et cela pr\xe9sente des d\xe9fis importants. Une IA pourrait \xeatre align\xe9e pour suivre des commandes litt\xe9rales (comme "apporter du caf\xe9"), interpr\xe9ter le sens voulu (comprendre que "apporter du caf\xe9" signifie le pr\xe9parer comme vous le pr\xe9f\xe9rez), poursuivre ce que vous auriez d\xfb vouloir (comme sugg\xe9rer du th\xe9 si le caf\xe9 serait mauvais pour la sant\xe9), ou agir dans votre int\xe9r\xeat ind\xe9pendamment des commandes. Suivre des commandes litt\xe9rales m\xe8ne souvent \xe0 des \xe9checs de sp\xe9cification dont nous parlons plus tard dans cette section. Le plus souvent, les chercheurs utilisent le mot alignement pour signifier "l\'alignement d\'intention" (',(0,t.jsx)(s.a,{href:"https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6",children:"Christiano, 2018"}),"), et certaines discussions plus philosophiques abordent le troisi\xe8me aspect - faire ce que moi (ou l'humanit\xe9) aurais voulu. Cela implique des concepts comme la volition extrapol\xe9e coh\xe9rente (CEV) (",(0,t.jsx)(s.a,{href:"https://intelligence.org/files/CEV.pdf",children:"Yudkowsky, 2004"}),"), la volition agr\xe9g\xe9e coh\xe9rente (CAV) (",(0,t.jsx)(s.a,{href:"https://multiverseaccordingtoben.blogspot.com/2010/03/coherent-aggregated-volition-toward.html",children:"Goertzel, 2010"}),"), et diverses autres lignes de pens\xe9e qui s'inscrivent dans le discours m\xe9ta-\xe9thique. Nous ne parlerons pas extensivement du discours philosophique dans ce texte et nous en tiendrons principalement \xe0 l'alignement d'intention et \xe0 une perspective d'",(0,t.jsx)(n,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:(0,t.jsx)(n,{term:"machine learning",definition:'{"definition":"Un domaine de l\'intelligence artificielle ax\xe9 sur la cr\xe9ation de syst\xe8mes qui apprennent et s\'am\xe9liorent \xe0 partir de donn\xe9es sans \xeatre explicitement programm\xe9s pour chaque t\xe2che.","source":"","aliases":["Apprentissage automatique","apprentissage automatique","Apprentisssage machine","apprentisssage machine","Machine Learning","ML"]}',children:"apprentissage automatique"})}),". Quand nous utilisons le mot \"alignement\" dans ce texte, nous ferons essentiellement r\xe9f\xe9rence aux probl\xe8mes et \xe9checs de l'alignement simple-simple. Les autres types d'alignement ont \xe9t\xe9 historiquement tr\xe8s peu \xe9tudi\xe9s, car les gens ont principalement travaill\xe9 avec l'id\xe9e d'une superintelligence singuli\xe8re qui interagit avec l'humanit\xe9 comme un monolithe singulier."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Alignement Simple-Multiple - Aligner plusieurs IA \xe0 un seul humain."})," Si nous pensons que l'ASI sera compos\xe9e de plus petites intelligences qui travaillent ensemble, d\xe9l\xe8guent des t\xe2ches et fonctionnent ensemble comme un superorganisme, alors tous les probl\xe8mes d'alignement simple-simple subsisteraient car nous devons encore r\xe9soudre l'alignement simple-simple avant de tenter le simple-multiple. Id\xe9alement, nous ne voulons pas qu'un seul humain (ou un tr\xe8s petit groupe d'humains) soit responsable d'une superintelligence (en supposant que les dictateurs bienveillants n'existent pas)."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Alignement Multiple-Simple - aligner une IA \xe0 plusieurs humains."})," Lorsque plusieurs humains partagent le contr\xf4le d'un seul syst\xe8me d'IA, nous sommes confront\xe9s au d\xe9fi de d\xe9terminer quelles valeurs et pr\xe9f\xe9rences devraient \xeatre prioritaires. Plut\xf4t que d'essayer litt\xe9ralement d'agr\xe9ger les pr\xe9f\xe9rences individuelles de chacun (ce qui pourrait mener \xe0 des contradictions ou des r\xe9sultats au plus petit d\xe9nominateur commun), une approche plus prometteuse consiste \xe0 aligner l'IA sur des principes de plus haut niveau et des valeurs institutionnelles - similaire \xe0 la fa\xe7on dont les institutions d\xe9mocratiques fonctionnent selon des principes comme la transparence et la responsabilit\xe9 plut\xf4t que d'essayer d'optimiser directement les pr\xe9f\xe9rences de chaque citoyen."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Alignement Multiple-Multiple - aligner plusieurs IA \xe0 plusieurs humains \xe0 plusieurs IA."})," C'est le sc\xe9nario le plus complexe impliquant plusieurs syst\xe8mes d'IA interagissant avec plusieurs humains. Ici, la distinction entre le risque de d\xe9salignement (les IA gagnant un pouvoir ill\xe9gitime sur les humains) et le risque de mauvaise utilisation (les humains utilisant les IA pour gagner un pouvoir ill\xe9gitime sur les autres) commence \xe0 s'estomper. Le d\xe9fi principal devient d'emp\xeacher les concentrations probl\xe9matiques de pouvoir tout en permettant une coop\xe9ration b\xe9n\xe9fique entre humains et IA. Cela n\xe9cessite une conception soign\xe9e du syst\xe8me qui favorise un comportement align\xe9 non seulement au niveau individuel mais dans l'ensemble du r\xe9seau d'interactions humain-IA."]}),"\n",(0,t.jsx)(s.h2,{id:"04",children:"Questions pour le long terme"}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Il n'est pas certain que la r\xe9solution de l'alignement individuel soit suffisante."})," M\xeame si nous pouvions garantir que chaque syst\xe8me d'IA soit parfaitement align\xe9 avec les intentions de son responsable humain, nous ferions toujours face \xe0 des risques s\xe9rieux lorsque ces syst\xe8mes interagissent. C'est parce que diff\xe9rents responsables peuvent avoir des int\xe9r\xeats contradictoires, ou parce que les syst\xe8mes peuvent \xe9chouer \xe0 se coordonner efficacement m\xeame lorsque leurs objectifs sont align\xe9s. Un alignement individuel parfait ne peut garantir un comportement collectif s\xfbr, tout comme aligner chaque conducteur sur le code de la route n'emp\xeache pas les embouteillages ou les accidents (",(0,t.jsx)(s.a,{href:"https://arxiv.org/abs/2502.14143",children:"Hammond et al., 2025"}),"). Essentiellement, si nous avons trois sous-probl\xe8mes d'alignement au sein d'un seul agent, nous avons trois autres sous-probl\xe8mes de mauvaise coordination, de conflit et de collusion lorsque ces agents individuels commencent \xe0 interagir entre eux. Chacun repr\xe9sente une fa\xe7on diff\xe9rente dont les syst\xe8mes multi-agents peuvent \xe9chouer, m\xeame si les agents individuels semblent fonctionner correctement isol\xe9ment. Il existe encore d'autres fa\xe7ons, m\xeame au-del\xe0, lorsque nous commen\xe7ons \xe0 consid\xe9rer les effets \xe9mergents des interactions entre syst\xe8mes complexes et la perte progressive de pouvoir, comme nous en avons parl\xe9 dans le chapitre sur les risques."]}),"\n",(0,t.jsx)(s.p,{children:"M\xeame si les d\xe9fis techniques de l'alignement de l'IA sont surmont\xe9s, de nombreuses questions philosophiques profondes et fortement d\xe9battues demeurent. R\xe9soudre la s\xe9curit\xe9 de l'IA, particuli\xe8rement pour la Superintelligence Artificielle (ASI), peut n\xe9cessiter de confronter des questions fondamentales concernant les valeurs, la conscience et le but ultime de l'existence. Aligner l'ASI nous force \xe0 nous poser des questions fondamentales sur l'avenir que nous d\xe9sirons vraiment."}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"\xc0 quoi devrions-nous aligner l'IA ?"})," Avec quelles valeurs ou principes \xe9thiques sp\xe9cifiques une ASI devrait-elle \xeatre align\xe9e ? \xc9tant donn\xe9 la diversit\xe9 des valeurs humaines, un accord est-il m\xeame possible ? Alternativement, si nous ne pouvons pas nous accorder sur des valeurs ",(0,t.jsx)(s.em,{children:"finales"}),", pouvons-nous nous accorder sur des ",(0,t.jsx)(s.em,{children:"processus"})," ou principes (comme la d\xe9lib\xe9ration, l'\xe9quit\xe9 ou la corrigibilit\xe9) qui pourraient conduire une ASI vers des valeurs acceptables ou permettre une \xe9volution future des valeurs ?"]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Le but de l'alignement : P\xe9rennit\xe9 humaine vs. Digne successeur ?"})," L'objectif principal devrait-il \xeatre la survie et l'\xe9panouissement ind\xe9finis de l'",(0,t.jsx)(s.em,{children:"humanit\xe9"}),' telle que nous la connaissons ? Ou devrions-nous envisager la possibilit\xe9 de cr\xe9er un "Digne Successeur" ? Dan Faggella (',(0,t.jsx)(s.a,{href:"https://danfaggella.com/worthy/",children:"Faggella, 2025"}),") propose ce concept : une ASI poss\xe9dant potentiellement des capacit\xe9s et une valeur morale sup\xe9rieures \xe0 celles de l'humanit\xe9, qui pourrait \xeatre rationnellement pr\xe9f\xe9r\xe9e pour guider l'avenir. D\xe9finir et v\xe9rifier les crit\xe8res d'un tel successeur (par exemple, une conscience accrue, des capacit\xe9s d'exploration cosmique) pose d'immenses d\xe9fis. Certains, comme Richard Sutton (",(0,t.jsx)(s.a,{href:"http://incompleteideas.net/Talks/waic3.pdf",children:"Sutton, 2023"}),"), soutiennent que la succession \xe0 l'IA, nos \"enfants de l'esprit\", est in\xe9vitable et hautement souhaitable. Sutton sugg\xe8re que nous devrions embrasser et planifier cette succession plut\xf4t que d'y r\xe9sister par peur, questionnant pourquoi nous voudrions que des \xeatres potentiellement sup\xe9rieurs restent subordonn\xe9s."]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Devrions-nous donner des droits \xe0 l'IA ?"})," Les syst\xe8mes d'IA avanc\xe9s pourraient-ils devenir conscients ? Cela n\xe9cessite d'abord une compr\xe9hension plus claire de la conscience elle-m\xeame, qui reste insaisissable. Si l'IA ",(0,t.jsx)(s.em,{children:"peut"})," poss\xe9der une conscience ou des propri\xe9t\xe9s similaires \xe0 la conscience, quel statut moral devrions-nous attribuer \xe0 ces esprits num\xe9riques ? Devraient-ils avoir des droits ou une consid\xe9ration morale ? Ces sujets sont hors du champ de ce manuel, mais sont \xe9tudi\xe9s par ",(0,t.jsx)(s.a,{href:"https://eleosai.org/",children:"Eleos AI"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Qu'en est-il des animaux ?"})," Comment les int\xe9r\xeats des entit\xe9s non-humaines devraient-ils \xeatre pris en compte dans l'alignement de l'IA ? Les objectifs d'alignement devraient-ils explicitement inclure le bien-\xeatre animal, la pr\xe9servation des \xe9cosyst\xe8mes ou l'\xe9panouissement d'autres formes de vie ?"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(o.A,{speaker:"Rich Sutton",position:"",date:"",source:"([Sutton, 2023](http://incompleteideas.net/Talks/waic3.pdf))",children:(0,t.jsx)(s.p,{children:"Nous ne devrions pas r\xe9sister \xe0 la succession, mais l'embrasser et nous y pr\xe9parer. Pourquoi voudrions-nous que des \xeatres sup\xe9rieurs restent subordonn\xe9s ? Pourquoi ne pas nous r\xe9jouir de leur grandeur comme symbole et extension de la grandeur de l'humanit\xe9, et travailler ensemble vers une civilisation plus grande et inclusive ?"})}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"La fin de partie :"})," Les r\xe9sultats potentiels \xe0 long terme sont nombreux et d\xe9pendent fortement de la fa\xe7on dont nous r\xe9pondons \xe0 ces questions philosophiques. L'objectif ultime est-il simplement la continuation de la conscience ou de la complexit\xe9, ind\xe9pendamment de son substrat physique (comme explor\xe9 par Max Tegmark dans Life 3.0 (",(0,t.jsx)(s.a,{href:"https://www.shortform.com/summary/life-3-0-summary-max-tegmark",children:"Tegmark, 2017"}),"))? Diff\xe9rentes positions philosophiques conduisent \xe0 des priorit\xe9s strat\xe9giques tr\xe8s diff\xe9rentes pour le d\xe9veloppement et l'alignement de l'ASI."]}),"\n",(0,t.jsx)(a.A,{src:"./img/57w_Image_26.png",alt:"Entrer la description alternative de l'image",number:"26",label:"3.26",caption:""})]})}function m(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,t.jsx)(s,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}}}]);